{
    "name": "databricks",
    "displayName": "Databricks",
    "description": "A Pulumi package for creating and managing databricks cloud resources.",
    "keywords": [
        "pulumi",
        "databricks",
        "category/infrastructure"
    ],
    "homepage": "https://www.pulumi.com",
    "license": "Apache-2.0",
    "attribution": "This Pulumi package is based on the [`databricks` Terraform Provider](https://github.com/databricks/terraform-provider-databricks).",
    "repository": "https://github.com/pulumi/pulumi-databricks",
    "publisher": "Pulumi",
    "meta": {
        "moduleFormat": "(.*)(?:/[^/]*)"
    },
    "language": {
        "csharp": {
            "compatibility": "tfbridge20",
            "namespaces": null,
            "packageReferences": {
                "Pulumi": "3.*"
            }
        },
        "go": {
            "generateExtraInputTypes": true,
            "generateResourceContainerTypes": true,
            "importBasePath": "github.com/pulumi/pulumi-databricks/sdk/go/databricks"
        },
        "nodejs": {
            "compatibility": "tfbridge20",
            "dependencies": {
                "@pulumi/pulumi": "^3.0.0"
            },
            "devDependencies": {
                "@types/mime": "^2.0.0",
                "@types/node": "^10.0.0"
            },
            "disableUnionOutputTypes": true,
            "packageDescription": "A Pulumi package for creating and managing databricks cloud resources.",
            "packageName": "",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "typescriptVersion": ""
        },
        "python": {
            "compatibility": "tfbridge20",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "requires": {
                "pulumi": "\u003e=3.0.0,\u003c4.0.0"
            }
        }
    },
    "config": {
        "variables": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "tokenEndpoint": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        }
    },
    "types": {
        "databricks:index/ClusterAutoscale:ClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAwsAttributes:ClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributes:ClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConf:ClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/ClusterDockerImage:ClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/ClusterGcpAttributes:ClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScript:ClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptFile:ClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptS3:ClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptFile:ClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptS3:ClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterLibrary:ClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterLibraryCran:ClusterLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterLibraryMaven:ClusterLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterLibraryPypi:ClusterLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterLibraryCran:ClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterLibraryMaven:ClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterLibraryPypi:ClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterWorkloadType:ClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/GrantsGrant:GrantsGrant": {
            "properties": {
                "principal": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "(String) Availability type used for all instances in the pool. Only `ON_DEMAND` and `SPOT` are supported.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "(Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "(String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `\"us-west-2a\"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `\"us-west-2a\"` is not a valid zone ID if the Databricks deployment resides in the `\"us-east-1\"` region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max price for Azure spot instances.  Use `-1` to specify the lowest price.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer",
                    "description": "(Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskSize": {
                    "type": "integer",
                    "description": "(Integer) The size of each disk (in GiB) to attach.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instanceType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobDbtTask:JobDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified in `git_source` where dbt should look in for the `dbt_project.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--project-dir` to a dbt command.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobEmailNotifications:JobEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run fails.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run starts.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run completes successfully.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobGitSource:JobGitSource": {
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `tag` and `commit`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commit": {
                    "type": "string",
                    "description": "hash of Git commit to use. Conflicts with `branch` and `tag`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "provider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tag": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `branch` and `commit`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobCluster:JobJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibrary:JobLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobLibraryCran:JobLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobLibraryMaven:JobLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobLibraryPypi:JobLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibraryCran:JobLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobLibraryMaven:JobLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobLibraryPypi:JobLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewCluster:JobNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterInitScript:JobNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScript:JobNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNotebookTask:JobNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in git_source. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobPipelineTask:JobPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobPythonWheelTask:JobPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobSchedule:JobSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this schedule is paused or not. Either “PAUSED” or “UNPAUSED”. When the pause_status field is omitted and a schedule is provided, the server will default to using \"UNPAUSED\" as a value for pause_status.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timezoneId": {
                    "type": "string",
                    "description": "A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pauseStatus",
                        "quartzCronExpression",
                        "timezoneId"
                    ]
                }
            }
        },
        "databricks:index/JobSparkJarTask:JobSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobSparkPythonTask:JobSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`) and workspace paths are supported. For python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobSparkSubmitTask:JobSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTask:JobTask": {
            "properties": {
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskDbtTask:JobTaskDbtTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskDependsOn:JobTaskDependsOn"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "description": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "existingClusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskLibrary:JobTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskNewCluster:JobTaskNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskNotebookTask:JobTaskNotebookTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskPipelineTask:JobTaskPipelineTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTask:JobTaskSqlTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout"
                    ]
                }
            }
        },
        "databricks:index/JobTaskDbtTask:JobTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified in `git_source` where dbt should look in for the `dbt_project.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--project-dir` to a dbt command.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskDependsOn:JobTaskDependsOn": {
            "properties": {
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run fails.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run starts.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run completes successfully.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibrary:JobTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryCran:JobTaskLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibraryCran:JobTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewCluster:JobTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNotebookTask:JobTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in git_source. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskPipelineTask:JobTaskPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`) and workspace paths are supported. For python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTask:JobTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert",
                    "description": "block consisting of single string field: `alert_id` - identifier of the Databricks SQL Alert.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard",
                    "description": "block consisting of single string field: `dashboard_id` - identifier of the Databricks SQL Dashboard databricks_sql_dashboard.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery",
                    "description": "block consisting of single string field: `query_id` - identifier of the Databricks SQL Query (databricks_sql_query).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only serverless warehouses are supported right now.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard": {
            "properties": {
                "dashboardId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/JobWebhookNotifications:JobWebhookNotifications": {
            "properties": {
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the system notification that is notified when an event defined in `webhook_notifications` is triggered.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the system notification that is notified when an event defined in `webhook_notifications` is triggered.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the system notification that is notified when an event defined in `webhook_notifications` is triggered.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/LibraryCran:LibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/LibraryMaven:LibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/LibraryPypi:LibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole": {
            "properties": {
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "privateKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "privateKeyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/MlflowModelTag:MlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec": {
            "properties": {
                "authorization": {
                    "type": "string",
                    "description": "Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `\u003cauth type\u003e \u003ccredentials\u003e`, e.g. `Bearer \u003caccess_token\u003e`. If set to an empty string, no authorization header will be included in the request.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "enableSslVerification": {
                    "type": "boolean",
                    "description": "Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "secret": {
                    "type": "string",
                    "description": "Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as `X-Databricks-Signature: encoded_payload`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec": {
            "properties": {
                "accessToken": {
                    "type": "string",
                    "description": "The personal access token used to authorize webhook's job runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "jobId": {
                    "type": "string",
                    "description": "ID of the Databricks job that the webhook runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "accessToken",
                "jobId"
            ]
        },
        "databricks:index/MountAbfs:MountAbfs": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initializeFileSystem": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "initializeFileSystem"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "containerName",
                        "initializeFileSystem",
                        "storageAccountName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountAdl:MountAdl": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "sparkConfPrefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageResourceName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "storageResourceName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountGs:MountGs": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "serviceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountS3:MountS3": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instanceProfile": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountWasb:MountWasb": {
            "properties": {
                "authType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tokenSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tokenSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "authType",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "authType",
                        "containerName",
                        "storageAccountName",
                        "tokenSecretKey",
                        "tokenSecretScope"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo": {
            "properties": {
                "keyAlias": {
                    "type": "string",
                    "description": "The AWS KMS key alias.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "keyArn": {
                    "type": "string",
                    "description": "The AWS KMS key's Amazon Resource Name (ARN).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "keyRegion": {
                    "type": "string",
                    "description": "(Computed) The AWS region in which KMS key is deployed to. This is not required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "keyAlias",
                "keyArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "keyAlias",
                        "keyArn",
                        "keyRegion"
                    ]
                }
            }
        },
        "databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage": {
            "properties": {
                "errorMessage": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "errorType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo": {
            "properties": {
                "networkProjectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID of the VPC network.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "podIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "serviceIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "subnetId": {
                    "type": "string",
                    "description": "The ID of the subnet associated with this network.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "subnetRegion": {
                    "type": "string",
                    "description": "The Google Cloud region of the workspace data plane. For example, `us-east4`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "networkProjectId",
                "podIpRangeName",
                "serviceIpRangeName",
                "subnetId",
                "subnetRegion",
                "vpcId"
            ]
        },
        "databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints": {
            "properties": {
                "dataplaneRelays": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "restApis": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dataplaneRelays",
                "restApis"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer": {
            "properties": {
                "gcp": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp",
                    "description": "A block that consists of the following field:\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "gcp"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp": {
            "properties": {
                "projectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "projectId"
            ]
        },
        "databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo": {
            "properties": {
                "authoritativeUserEmail": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "authoritativeUserFullName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "authoritativeUserEmail",
                "authoritativeUserFullName",
                "customerName"
            ]
        },
        "databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig": {
            "properties": {
                "gkeClusterPodIpRange": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "gkeClusterServiceIpRange": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "subnetCidr": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "gkeClusterPodIpRange",
                "gkeClusterServiceIpRange",
                "subnetCidr"
            ]
        },
        "databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig": {
            "properties": {
                "connectivityType": {
                    "type": "string",
                    "description": "Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: `PRIVATE_NODE_PUBLIC_MASTER`, `PUBLIC_NODE_PUBLIC_MASTER`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "masterIpRange": {
                    "type": "string",
                    "description": "The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as `/28`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "connectivityType",
                "masterIpRange"
            ]
        },
        "databricks:index/MwsWorkspacesToken:MwsWorkspacesToken": {
            "properties": {
                "comment": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tokenId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tokenValue": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "tokenId",
                        "tokenValue"
                    ]
                }
            }
        },
        "databricks:index/PermissionsAccessControl:PermissionsAccessControl": {
            "properties": {
                "groupName": {
                    "type": "string",
                    "description": "name of the group. We recommend setting permissions on groups.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "permissionLevel": {
                    "type": "string",
                    "description": "permission level according to specific resource. See examples above for the reference.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "Application ID of the service_principal.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "userName": {
                    "type": "string",
                    "description": "name of the user.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "permissionLevel"
            ]
        },
        "databricks:index/PipelineCluster:PipelineCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineClusterInitScript:PipelineClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "label": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverNodeTypeId",
                        "enableLocalDiskEncryption",
                        "nodeTypeId"
                    ]
                }
            }
        },
        "databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScript:PipelineClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineFilters:PipelineFilters": {
            "properties": {
                "excludes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "includes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibrary:PipelineLibrary": {
            "properties": {
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/PipelineLibraryMaven:PipelineLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebook": {
                    "$ref": "#/types/databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibraryMaven:PipelineLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook": {
            "properties": {
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/RecipientIpAccessList:RecipientIpAccessList": {
            "properties": {
                "allowedIpAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Allowed IP Addresses in CIDR notation. Limit of 100.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allowedIpAddresses"
            ]
        },
        "databricks:index/RecipientToken:RecipientToken": {
            "properties": {
                "activationUrl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "createdAt": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "createdBy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "expirationTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "id": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "updatedAt": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "updatedBy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "activationUrl",
                        "createdAt",
                        "createdBy",
                        "expirationTime",
                        "id",
                        "updatedAt",
                        "updatedBy"
                    ]
                }
            }
        },
        "databricks:index/RepoSparseCheckout:RepoSparseCheckout": {
            "properties": {
                "patterns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "patterns"
            ]
        },
        "databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata": {
            "properties": {
                "dnsName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "dnsName",
                "resourceId"
            ]
        },
        "databricks:index/ShareObject:ShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "addedBy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the object, currently only `TABLE` is allowed.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the object, e.g. `catalog.schema.name` for a table.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sharedAs": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dataObjectType",
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "addedAt",
                        "addedBy",
                        "dataObjectType",
                        "name",
                        "sharedAs"
                    ]
                }
            }
        },
        "databricks:index/SqlEndpointChannel:SqlEndpointChannel": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams": {
            "properties": {
                "host": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "hostname": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "port": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "protocol": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path",
                "port",
                "protocol"
            ]
        },
        "databricks:index/SqlEndpointTags:SqlEndpointTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "customTags"
            ]
        },
        "databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment": {
            "properties": {
                "principal": {
                    "type": "string",
                    "description": "`display_name` for a databricks.Group or databricks_user, `application_id` for a databricks_service_principal.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "set of available privilege names in upper case.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/SqlQueryParameter:SqlQueryParameter": {
            "properties": {
                "date": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDate:SqlQueryParameterDate",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dateRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetime": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimeRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimesec": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimesecRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enum": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "number": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "text": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterText:SqlQueryParameterText",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "title": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/SqlQueryParameterDate:SqlQueryParameterDate": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange": {
            "properties": {
                "end": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "start": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange": {
            "properties": {
                "end": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "start": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange": {
            "properties": {
                "end": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "start": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "options": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "options"
            ]
        },
        "databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "separator": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "suffix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "prefix",
                "separator",
                "suffix"
            ]
        },
        "databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber": {
            "properties": {
                "value": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "queryId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "separator": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "suffix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "prefix",
                "separator",
                "suffix"
            ]
        },
        "databricks:index/SqlQueryParameterText:SqlQueryParameterText": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQuerySchedule:SqlQuerySchedule": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "daily": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "weekly": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous": {
            "properties": {
                "intervalSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "intervalSeconds"
            ]
        },
        "databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily": {
            "properties": {
                "intervalDays": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeOfDay": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "intervalDays",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly": {
            "properties": {
                "dayOfWeek": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "intervalWeeks": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeOfDay": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dayOfWeek",
                "intervalWeeks",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlWidgetParameter:SqlWidgetParameter": {
            "properties": {
                "mapTo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "title": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "type": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "type"
            ]
        },
        "databricks:index/SqlWidgetPosition:SqlWidgetPosition": {
            "properties": {
                "autoHeight": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "posX": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "posY": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sizeX": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sizeY": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sizeX",
                "sizeY"
            ]
        },
        "databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole": {
            "properties": {
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privateKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "privateKeyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/TableColumn:TableColumn": {
            "properties": {
                "comment": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nullable": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "partitionIndex": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "position": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeIntervalType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeJson": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typePrecision": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeScale": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeText": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/getClusterClusterInfo:getClusterClusterInfo": {
            "properties": {
                "autoscale": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterCores": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "description": "The id of the cluster\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogStatus": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMemoryMb": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "The exact name of the cluster to search\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterSource": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "creatorUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driver": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "Use autoscaling local storage.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Enable local disk encryption.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "executors": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "The pool of idle instances the cluster is attached to.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jdbcPort": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastActivityTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastStateLossTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime of the cluster\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkContextId": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "state": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "stateMessage": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "terminateTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "terminationReason": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "defaultTags",
                "driverInstancePoolId",
                "sparkVersion",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "defaultTags",
                        "sparkVersion",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus": {
            "properties": {
                "lastAttempted": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastException": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "publicDns": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTimestamp": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "publicDns": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTimestamp": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason": {
            "properties": {
                "code": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "type": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList": {
            "properties": {
                "fileSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "description": "Path on DBFS for the file to perform listing\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo": {
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolFleetAttributes": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxCapacity": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minIdleInstances": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "state": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "stats": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "defaultTags",
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "idleInstanceAutoterminationMinutes",
                        "instancePoolName"
                    ]
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "zoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats": {
            "properties": {
                "idleCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pendingIdleCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pendingUsedCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usedCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettings:getJobJobSettings": {
            "properties": {
                "createdTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "creatorUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobId": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "settings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings": {
            "properties": {
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "existingClusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "format": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxRetries": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "description": "the job name of databricks.Job if the resource was matched by id.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "format"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "profilesDirectory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "projectDirectory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schema": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource": {
            "properties": {
                "branch": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commit": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "provider": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tag": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "source": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "quartzCronExpression": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timezoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pauseStatus",
                "quartzCronExpression",
                "timezoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "quartzCronExpression",
                        "timezoneId"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask": {
            "properties": {
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "description": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "existingClusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobClusterKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxRetries": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "retryOnTimeout"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "profilesDirectory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "projectDirectory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schema": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn": {
            "properties": {
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "runtimeEngine": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "serverAddress": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "secret": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "source": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard": {
            "properties": {
                "dashboardId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications": {
            "properties": {
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList": {
            "properties": {
                "language": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "description": "Path to workspace directory\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getShareObject:getShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "addedBy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the object.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "description": "The name of the share\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sharedAs": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "addedAt",
                "addedBy",
                "dataObjectType",
                "name",
                "sharedAs"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "dataObjectType",
                        "name"
                    ]
                }
            }
        },
        "databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams": {
            "properties": {
                "host": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "hostname": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "port": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "protocol": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path",
                "port",
                "protocol"
            ]
        },
        "databricks:index/getSqlWarehouseTags:getSqlWarehouseTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "customTags"
            ]
        },
        "databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        }
    },
    "provider": {
        "description": "The provider type for the databricks package. By default, resources use package-wide configuration\nsettings, however an explicit `Provider` instance may be created and passed during resource\nconstruction to achieve fine-grained programmatic control over provider settings. See the\n[documentation](https://www.pulumi.com/docs/reference/programming-model/#providers) for more information.\n",
        "properties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "tokenEndpoint": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        },
        "inputProperties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "tokenEndpoint": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        }
    },
    "resources": {
        "databricks:index/awsS3Mount:AwsS3Mount": {
            "properties": {
                "clusterId": {
                    "type": "string"
                },
                "instanceProfile": {
                    "type": "string"
                },
                "mountName": {
                    "type": "string"
                },
                "s3BucketName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "required": [
                "clusterId",
                "mountName",
                "s3BucketName",
                "source"
            ],
            "inputProperties": {
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instanceProfile": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "mountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "s3BucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "mountName",
                "s3BucketName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AwsS3Mount resources.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "instanceProfile": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "mountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "s3BucketName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/azureAdlsGen1Mount:AzureAdlsGen1Mount": {
            "properties": {
                "clientId": {
                    "type": "string"
                },
                "clientSecretKey": {
                    "type": "string"
                },
                "clientSecretScope": {
                    "type": "string"
                },
                "clusterId": {
                    "type": "string"
                },
                "directory": {
                    "type": "string"
                },
                "mountName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "sparkConfPrefix": {
                    "type": "string"
                },
                "storageResourceName": {
                    "type": "string"
                },
                "tenantId": {
                    "type": "string"
                }
            },
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "directory",
                "mountName",
                "source",
                "storageResourceName",
                "tenantId"
            ],
            "inputProperties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "mountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sparkConfPrefix": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageResourceName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "mountName",
                "storageResourceName",
                "tenantId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AzureAdlsGen1Mount resources.\n",
                "properties": {
                    "clientId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clientSecretKey": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clientSecretScope": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directory": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "mountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string"
                    },
                    "sparkConfPrefix": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageResourceName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "tenantId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/azureAdlsGen2Mount:AzureAdlsGen2Mount": {
            "properties": {
                "clientId": {
                    "type": "string"
                },
                "clientSecretKey": {
                    "type": "string"
                },
                "clientSecretScope": {
                    "type": "string"
                },
                "clusterId": {
                    "type": "string"
                },
                "containerName": {
                    "type": "string"
                },
                "directory": {
                    "type": "string"
                },
                "initializeFileSystem": {
                    "type": "boolean"
                },
                "mountName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "storageAccountName": {
                    "type": "string"
                },
                "tenantId": {
                    "type": "string"
                }
            },
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "containerName",
                "directory",
                "initializeFileSystem",
                "mountName",
                "source",
                "storageAccountName",
                "tenantId"
            ],
            "inputProperties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initializeFileSystem": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "mountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "containerName",
                "initializeFileSystem",
                "mountName",
                "storageAccountName",
                "tenantId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AzureAdlsGen2Mount resources.\n",
                "properties": {
                    "clientId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clientSecretKey": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clientSecretScope": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "containerName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directory": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "initializeFileSystem": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "mountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string"
                    },
                    "storageAccountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "tenantId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/azureBlobMount:AzureBlobMount": {
            "properties": {
                "authType": {
                    "type": "string"
                },
                "clusterId": {
                    "type": "string"
                },
                "containerName": {
                    "type": "string"
                },
                "directory": {
                    "type": "string"
                },
                "mountName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "storageAccountName": {
                    "type": "string"
                },
                "tokenSecretKey": {
                    "type": "string",
                    "secret": true
                },
                "tokenSecretScope": {
                    "type": "string"
                }
            },
            "required": [
                "authType",
                "containerName",
                "mountName",
                "source",
                "storageAccountName",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "inputProperties": {
                "authType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "mountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tokenSecretKey": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "tokenSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "authType",
                "containerName",
                "mountName",
                "storageAccountName",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AzureBlobMount resources.\n",
                "properties": {
                    "authType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "containerName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directory": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "mountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string"
                    },
                    "storageAccountName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "tokenSecretKey": {
                        "type": "string",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "tokenSecretScope": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/catalog:Catalog": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: databricks_metastore[\"this\"].id,\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=databricks_metastore[\"this\"][\"id\"],\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = databricks_metastore.This.Id,\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(databricks_metastore.This.Id),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .metastoreId(databricks_metastore.this().id())\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${databricks_metastore.this.id}\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/catalog:Catalog this \u003cname\u003e\n```\n\n ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n"
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Catalog resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete catalog regardless of its contents.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the catalog owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Catalog properties.\n"
                    },
                    "providerName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "shareName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/cluster:Cluster": {
            "description": "\n\n\n## Import\n\nThe resource cluster can be imported using cluster id. bash\n\n```sh\n $ pulumi import databricks:index/cluster:Cluster this \u003ccluster-id\u003e\n```\n\n ",
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an `x_` when it is propagated.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "state": {
                    "type": "string",
                    "description": "(string) State of the cluster.\n"
                },
                "url": {
                    "type": "string"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "required": [
                "clusterId",
                "defaultTags",
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "sparkVersion",
                "state",
                "url"
            ],
            "inputProperties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an `x_` when it is propagated.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "requiredInputs": [
                "sparkVersion"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Cluster resources.\n",
                "properties": {
                    "applyPolicyDefaultValues": {
                        "type": "boolean",
                        "description": "Whether to use policy default values for missing cluster attributes.\n"
                    },
                    "autoscale": {
                        "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                    },
                    "autoterminationMinutes": {
                        "type": "integer",
                        "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                    },
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterLogConf": {
                        "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                    },
                    "clusterMountInfos": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                        }
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`. If a custom cluster tag has the same name as a default cluster tag, the custom tag is prefixed with an `x_` when it is propagated.\n"
                    },
                    "dataSecurityMode": {
                        "type": "string",
                        "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                    },
                    "defaultTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                    },
                    "dockerImage": {
                        "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                    },
                    "driverInstancePoolId": {
                        "type": "string",
                        "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                    },
                    "driverNodeTypeId": {
                        "type": "string",
                        "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                    },
                    "enableLocalDiskEncryption": {
                        "type": "boolean",
                        "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                    },
                    "idempotencyToken": {
                        "type": "string",
                        "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "initScripts": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                        }
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                    },
                    "isPinned": {
                        "type": "boolean",
                        "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                        }
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                    },
                    "numWorkers": {
                        "type": "integer",
                        "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                    },
                    "runtimeEngine": {
                        "type": "string",
                        "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                    },
                    "singleUserName": {
                        "type": "string",
                        "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                    },
                    "sparkConf": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                    },
                    "sparkEnvVars": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                    },
                    "sshPublicKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "(string) State of the cluster.\n"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workloadType": {
                        "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/clusterPolicy:ClusterPolicy": {
            "description": "This resource creates a cluster policy, which limits the ability to create clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. cluster policies have ACLs that limit their use to specific users and groups. Only admin users can create, edit, and delete policies. Admin users also have access to all policies.\n\nCluster policies let you:\n\n* Limit users to create clusters with prescribed settings.\n* Simplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).\n* Control cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).\n\nCluster policy permissions limit which policies a user can select in the Policy drop-down when the user creates a cluster:\n\n* If no policies have been created in the workspace, the Policy drop-down does not display.\n* A user who has cluster create permission can select the `Free form` policy and create fully-configurable clusters.\n* A user who has both cluster create permission and access to cluster policies can select the Free form policy and policies they have access to.\n* A user that has access to only cluster policies, can select the policies they have access to.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* Dynamic Passthrough Clusters for a Group guide\n* End to end workspace management guide\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.getNodeType data to get the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n\n## Import\n\nThe resource cluster policy can be imported using the policy idbash\n\n```sh\n $ pulumi import databricks:index/clusterPolicy:ClusterPolicy this \u003ccluster-policy-id\u003e\n```\n\n ",
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the cluster policy.\n"
                }
            },
            "required": [
                "definition",
                "name",
                "policyId"
            ],
            "inputProperties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                }
            },
            "requiredInputs": [
                "definition"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ClusterPolicy resources.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                    },
                    "maxClustersPerUser": {
                        "type": "integer",
                        "description": "Maximum number of clusters allowed per user. When omitted, there is no limit.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the cluster policy.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/dbfsFile:DbfsFile": {
            "description": "\n\n\n## Import\n\nThe resource dbfs file can be imported using the path of the file bash\n\n```sh\n $ pulumi import databricks:index/dbfsFile:DbfsFile this \u003cpath\u003e\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "dbfsPath": {
                    "type": "string",
                    "description": "Path, but with `dbfs:` prefix\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "dbfsPath",
                "fileSize",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "md5": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DbfsFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "dbfsPath": {
                        "type": "string",
                        "description": "Path, but with `dbfs:` prefix\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save.\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/directory:Directory": {
            "description": "\n\n\n## Import\n\nThe resource directory can be imported using directory path bash\n\n```sh\n $ pulumi import databricks:index/directory:Directory this /path/to/directory\n```\n\n ",
            "properties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n"
                }
            },
            "required": [
                "objectId",
                "path"
            ],
            "inputProperties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Directory resources.\n",
                "properties": {
                    "deleteRecursive": {
                        "type": "boolean"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a DIRECTORY\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/entitlements:Entitlements": {
            "description": "This resource allows you to set entitlements to existing databricks_users, databricks.Group or databricks_service_principal.\n\n\u003e **Note** You must define entitlements of a principal using either `databricks.Entitlements` or directly within one of databricks_users, databricks.Group or databricks_service_principal. Having entitlements defined in both resources will result in non-deterministic behaviour.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nSetting entitlements for a regular user:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst meUser = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst meEntitlements = new databricks.Entitlements(\"meEntitlements\", {\n    userId: meUser.then(meUser =\u003e meUser.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme_user = databricks.get_user(user_name=\"me@example.com\")\nme_entitlements = databricks.Entitlements(\"meEntitlements\",\n    user_id=me_user.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var meUser = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var meEntitlements = new Databricks.Entitlements(\"meEntitlements\", new()\n    {\n        UserId = meUser.Apply(getUserResult =\u003e getUserResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmeUser, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"meEntitlements\", \u0026databricks.EntitlementsArgs{\n\t\t\tUserId:                  *pulumi.String(meUser.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var meUser = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var meEntitlements = new Entitlements(\"meEntitlements\", EntitlementsArgs.builder()        \n            .userId(meUser.applyValue(getUserResult -\u003e getUserResult.id()))\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  meEntitlements:\n    type: databricks:Entitlements\n    properties:\n      userId: ${meUser.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  meUser:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: me@example.com\n```\n\nSetting entitlements for a service principal:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisServicePrincipal = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst thisEntitlements = new databricks.Entitlements(\"thisEntitlements\", {\n    servicePrincipalId: thisServicePrincipal.then(thisServicePrincipal =\u003e thisServicePrincipal.spId),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_service_principal = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nthis_entitlements = databricks.Entitlements(\"thisEntitlements\",\n    service_principal_id=this_service_principal.sp_id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisServicePrincipal = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var thisEntitlements = new Databricks.Entitlements(\"thisEntitlements\", new()\n    {\n        ServicePrincipalId = thisServicePrincipal.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.SpId),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisServicePrincipal, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"thisEntitlements\", \u0026databricks.EntitlementsArgs{\n\t\t\tServicePrincipalId:      *pulumi.String(thisServicePrincipal.SpId),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var thisServicePrincipal = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var thisEntitlements = new Entitlements(\"thisEntitlements\", EntitlementsArgs.builder()        \n            .servicePrincipalId(thisServicePrincipal.applyValue(getServicePrincipalResult -\u003e getServicePrincipalResult.spId()))\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisEntitlements:\n    type: databricks:Entitlements\n    properties:\n      servicePrincipalId: ${thisServicePrincipal.spId}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  thisServicePrincipal:\n    fn::invoke:\n      Function: databricks:getServicePrincipal\n      Arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n\nSetting entitlements to all users in a workspace - referencing special `users` databricks.Group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst workspace_users = new databricks.Entitlements(\"workspace-users\", {\n    groupId: users.then(users =\u003e users.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusers = databricks.get_group(display_name=\"users\")\nworkspace_users = databricks.Entitlements(\"workspace-users\",\n    group_id=users.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var workspace_users = new Databricks.Entitlements(\"workspace-users\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"workspace-users\", \u0026databricks.EntitlementsArgs{\n\t\t\tGroupId:                 *pulumi.String(users.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var workspace_users = new Entitlements(\"workspace-users\", EntitlementsArgs.builder()        \n            .groupId(users.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  workspace-users:\n    type: databricks:Entitlements\n    properties:\n      groupId: ${users.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  users:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: users\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are* `user/user_id` - user `user_id`. * `group/group_id` - group `group_id`. * `spn/spn_id` - service principal `spn_id`. bash\n\n```sh\n $ pulumi import databricks:index/entitlements:Entitlements me user/\u003cuser-id\u003e\n```\n\n ",
            "properties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "inputProperties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Entitlements resources.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "groupId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the group.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the service principal.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the user.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/externalLocation:ExternalLocation": {
            "description": "To work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- databricks.StorageCredential represent authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- `databricks.ExternalLocation` are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/externalLocation:ExternalLocation this \u003cname\u003e\n```\n\n ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external Location owner.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                }
            },
            "required": [
                "credentialName",
                "metastoreId",
                "name",
                "owner",
                "url"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external Location owner.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                }
            },
            "requiredInputs": [
                "credentialName",
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ExternalLocation resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "credentialName": {
                        "type": "string",
                        "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the external Location owner.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the external location\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/gitCredential:GitCredential": {
            "description": "\n\n\n## Import\n\nThe resource cluster can be imported using ID of Git credential that could be obtained via REST APIbash\n\n```sh\n $ pulumi import databricks:index/gitCredential:GitCredential this \u003cgit-credential-id\u003e\n```\n\n ",
            "properties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "secret": true
                }
            },
            "required": [
                "gitProvider",
                "gitUsername",
                "personalAccessToken"
            ],
            "inputProperties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "secret": true
                }
            },
            "requiredInputs": [
                "gitProvider",
                "gitUsername",
                "personalAccessToken"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GitCredential resources.\n",
                "properties": {
                    "force": {
                        "type": "boolean",
                        "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                    },
                    "gitUsername": {
                        "type": "string",
                        "description": "user name at Git provider.\n"
                    },
                    "personalAccessToken": {
                        "type": "string",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/globalInitScript:GlobalInitScript": {
            "description": "\n\n\n## Import\n\nThe resource global init script can be imported using script IDbash\n\n```sh\n $ pulumi import databricks:index/globalInitScript:GlobalInitScript this script_id\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "required": [
                "name",
                "position"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GlobalInitScript resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "enabled": {
                        "type": "boolean",
                        "description": "specifies if the script is enabled for execution, or not\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "the name of the script.  It should be unique\n"
                    },
                    "position": {
                        "type": "integer",
                        "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grants:Grants": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "materializedView": {
                    "type": "string"
                },
                "metastore": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "share": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "view": {
                    "type": "string"
                }
            },
            "required": [
                "grants"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "materializedView": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "share": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "grants"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grants resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "grants": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                        },
                        "language": {
                            "csharp": {
                                "name": "GrantDetails"
                            }
                        }
                    },
                    "materializedView": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "share": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/group:Group": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nCreating some group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()        \n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n```\n\nAdding databricks.User as databricks.GroupMember of some group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisGroup = new databricks.Group(\"thisGroup\", {\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\nconst thisUser = new databricks.User(\"thisUser\", {userName: \"someone@example.com\"});\nconst vipMember = new databricks.GroupMember(\"vipMember\", {\n    groupId: thisGroup.id,\n    memberId: thisUser.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_group = databricks.Group(\"thisGroup\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\nthis_user = databricks.User(\"thisUser\", user_name=\"someone@example.com\")\nvip_member = databricks.GroupMember(\"vipMember\",\n    group_id=this_group.id,\n    member_id=this_user.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisGroup = new Databricks.Group(\"thisGroup\", new()\n    {\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n    var thisUser = new Databricks.User(\"thisUser\", new()\n    {\n        UserName = \"someone@example.com\",\n    });\n\n    var vipMember = new Databricks.GroupMember(\"vipMember\", new()\n    {\n        GroupId = thisGroup.Id,\n        MemberId = thisUser.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisGroup, err := databricks.NewGroup(ctx, \"thisGroup\", \u0026databricks.GroupArgs{\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisUser, err := databricks.NewUser(ctx, \"thisUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"someone@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"vipMember\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  thisGroup.ID(),\n\t\t\tMemberId: thisUser.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisGroup = new Group(\"thisGroup\", GroupArgs.builder()        \n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n        var thisUser = new User(\"thisUser\", UserArgs.builder()        \n            .userName(\"someone@example.com\")\n            .build());\n\n        var vipMember = new GroupMember(\"vipMember\", GroupMemberArgs.builder()        \n            .groupId(thisGroup.id())\n            .memberId(thisUser.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisGroup:\n    type: databricks:Group\n    properties:\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n  thisUser:\n    type: databricks:User\n    properties:\n      userName: someone@example.com\n  vipMember:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${thisGroup.id}\n      memberId: ${thisUser.id}\n```\n\nCreating group in AWS Databricks account:\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at account-level\nconst mws = new databricks.Provider(\"mws\", {\n    host: \"https://accounts.cloud.databricks.com\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    username: _var.databricks_account_username,\n    password: _var.databricks_account_password,\n});\nconst _this = new databricks.Group(\"this\", {}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at account-level\nmws = databricks.Provider(\"mws\",\n    host=\"https://accounts.cloud.databricks.com\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    username=var[\"databricks_account_username\"],\n    password=var[\"databricks_account_password\"])\nthis = databricks.Group(\"this\", opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at account-level\n    var mws = new Databricks.Provider(\"mws\", new()\n    {\n        Host = \"https://accounts.cloud.databricks.com\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        Username = @var.Databricks_account_username,\n        Password = @var.Databricks_account_password,\n    });\n\n    var @this = new Databricks.Group(\"this\", new()\n    {\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"mws\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.cloud.databricks.com\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tUsername:  pulumi.Any(_var.Databricks_account_username),\n\t\t\tPassword:  pulumi.Any(_var.Databricks_account_password),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroup(ctx, \"this\", nil, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mws = new Provider(\"mws\", ProviderArgs.builder()        \n            .host(\"https://accounts.cloud.databricks.com\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .username(var_.databricks_account_username())\n            .password(var_.databricks_account_password())\n            .build());\n\n        var this_ = new Group(\"this\", GroupArgs.Empty, CustomResourceOptions.builder()\n            .provider(databricks.mws())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at account-level\n  mws:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.cloud.databricks.com\n      accountId: 00000000-0000-0000-0000-000000000000\n      username: ${var.databricks_account_username}\n      password: ${var.databricks_account_password}\n  this:\n    type: databricks:Group\n    options:\n      provider: ${databricks.mws}\n```\n\nCreating group in Azure Databricks account:\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at Azure account-level\nconst azureAccount = new databricks.Provider(\"azureAccount\", {\n    host: \"https://accounts.azuredatabricks.net\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    authType: \"azure-cli\",\n});\nconst _this = new databricks.Group(\"this\", {}, {\n    provider: databricks.azure_account,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at Azure account-level\nazure_account = databricks.Provider(\"azureAccount\",\n    host=\"https://accounts.azuredatabricks.net\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    auth_type=\"azure-cli\")\nthis = databricks.Group(\"this\", opts=pulumi.ResourceOptions(provider=databricks[\"azure_account\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at Azure account-level\n    var azureAccount = new Databricks.Provider(\"azureAccount\", new()\n    {\n        Host = \"https://accounts.azuredatabricks.net\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        AuthType = \"azure-cli\",\n    });\n\n    var @this = new Databricks.Group(\"this\", new()\n    {\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Azure_account,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"azureAccount\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.azuredatabricks.net\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tAuthType:  pulumi.String(\"azure-cli\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroup(ctx, \"this\", nil, pulumi.Provider(databricks.Azure_account))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var azureAccount = new Provider(\"azureAccount\", ProviderArgs.builder()        \n            .host(\"https://accounts.azuredatabricks.net\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .authType(\"azure-cli\")\n            .build());\n\n        var this_ = new Group(\"this\", GroupArgs.Empty, CustomResourceOptions.builder()\n            .provider(databricks.azure_account())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at Azure account-level\n  azureAccount:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.azuredatabricks.net\n      accountId: 00000000-0000-0000-0000-000000000000\n      authType: azure-cli\n  this:\n    type: databricks:Group\n    options:\n      provider: ${databricks.azure_account}\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\nYou can import a `databricks_group` resource with the name `my_group` like the followingbash\n\n```sh\n $ pulumi import databricks:index/group:Group my_group \u003cgroup_id\u003e\n```\n\n ",
            "properties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "displayName",
                "url"
            ],
            "inputProperties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n",
                    "willReplaceOnChanges": true
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Group resources.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is the display name for the given group.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n",
                        "willReplaceOnChanges": true
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupInstanceProfile:GroupInstanceProfile": {
            "description": "\u003e **Deprecated** Please migrate to databricks_group_role.\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_group.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"myGroup\", {});\nconst myGroupInstanceProfile = new databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", {\n    groupId: myGroup.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"myGroup\")\nmy_group_instance_profile = databricks.GroupInstanceProfile(\"myGroupInstanceProfile\",\n    group_id=my_group.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"myGroup\");\n\n    var myGroupInstanceProfile = new Databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", new()\n    {\n        GroupId = myGroup.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"myGroup\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"myGroupInstanceProfile\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           myGroup.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\");\n\n        var myGroupInstanceProfile = new GroupInstanceProfile(\"myGroupInstanceProfile\", GroupInstanceProfileArgs.builder()        \n            .groupId(myGroup.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n  myGroupInstanceProfile:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${myGroup.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "instanceProfileId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "instanceProfileId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupInstanceProfile resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupMember:GroupMember": {
            "description": "This resource allows you to attach `users`, `service principals`, and `groups` as group members.\n\nTo attach members to groups in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.ServicePrincipal to grant access to a workspace to an automation tool or application.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the `group` resource.\n"
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the `group`, `service principal`, or `user`.\n"
                }
            },
            "required": [
                "groupId",
                "memberId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the `group` resource.\n",
                    "willReplaceOnChanges": true
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the `group`, `service principal`, or `user`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "memberId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupMember resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the `group` resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "memberId": {
                        "type": "string",
                        "description": "This is the id of the `group`, `service principal`, or `user`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupRole:GroupRole": {
            "description": "This resource allows you to attach a role to databricks_group. This role could be a pre-defined role such as account admin, or an instance profile ARN.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAttach an instance profile to a group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"myGroup\", {});\nconst myGroupInstanceProfile = new databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", {\n    groupId: myGroup.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"myGroup\")\nmy_group_instance_profile = databricks.GroupInstanceProfile(\"myGroupInstanceProfile\",\n    group_id=my_group.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"myGroup\");\n\n    var myGroupInstanceProfile = new Databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", new()\n    {\n        GroupId = myGroup.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"myGroup\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"myGroupInstanceProfile\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           myGroup.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\");\n\n        var myGroupInstanceProfile = new GroupInstanceProfile(\"myGroupInstanceProfile\", GroupInstanceProfileArgs.builder()        \n            .groupId(myGroup.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n  myGroupInstanceProfile:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${myGroup.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n\nAttach account admin role to an account-level group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myGroup = new databricks.Group(\"myGroup\", {});\nconst myUserAccountAdmin = new databricks.UserRole(\"myUserAccountAdmin\", {\n    userId: myGroup.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_group = databricks.Group(\"myGroup\")\nmy_user_account_admin = databricks.UserRole(\"myUserAccountAdmin\",\n    user_id=my_group.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myGroup = new Databricks.Group(\"myGroup\");\n\n    var myUserAccountAdmin = new Databricks.UserRole(\"myUserAccountAdmin\", new()\n    {\n        UserId = myGroup.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"myGroup\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"myUserAccountAdmin\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myGroup.ID(),\n\t\t\tRole:   pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myGroup = new Group(\"myGroup\");\n\n        var myUserAccountAdmin = new UserRole(\"myUserAccountAdmin\", UserRoleArgs.builder()        \n            .userId(myGroup.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myGroup:\n    type: databricks:Group\n  myUserAccountAdmin:\n    type: databricks:UserRole\n    properties:\n      userId: ${myGroup.id}\n      role: account_admin\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "role"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "role"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupRole resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instancePool:InstancePool": {
            "description": "\n\n\n## Import\n\nThe resource instance pool can be imported using it's idbash\n\n```sh\n $ pulumi import databricks:index/instancePool:InstancePool this \u003cinstance-pool-id\u003e\n```\n\n ",
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n"
                }
            },
            "required": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "inputProperties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                    "willReplaceOnChanges": true
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                    "willReplaceOnChanges": true
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n",
                    "willReplaceOnChanges": true
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                    "willReplaceOnChanges": true
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                    "willReplaceOnChanges": true
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                    "willReplaceOnChanges": true
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                    "willReplaceOnChanges": true
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    },
                    "willReplaceOnChanges": true
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstancePool resources.\n",
                "properties": {
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                        "willReplaceOnChanges": true
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                        "willReplaceOnChanges": true
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n",
                        "willReplaceOnChanges": true
                    },
                    "diskSpec": {
                        "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                        "willReplaceOnChanges": true
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                        "willReplaceOnChanges": true
                    },
                    "idleInstanceAutoterminationMinutes": {
                        "type": "integer",
                        "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                    },
                    "instancePoolFleetAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string"
                    },
                    "instancePoolName": {
                        "type": "string",
                        "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                    },
                    "maxCapacity": {
                        "type": "integer",
                        "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                    },
                    "minIdleInstances": {
                        "type": "integer",
                        "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                        "willReplaceOnChanges": true
                    },
                    "preloadedDockerImages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                        },
                        "willReplaceOnChanges": true
                    },
                    "preloadedSparkVersions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instanceProfile:InstanceProfile": {
            "description": "This resource allows you to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount. The following example demonstrates how to create an instance profile and create a cluster with it. When creating a new `databricks.InstanceProfile`, Databricks validates that it has sufficient permissions to launch instances with the instance profile. This validation uses AWS dry-run mode for the [AWS EC2 RunInstances API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html).\n\n\u003e **Note** Please switch to databricks.StorageCredential with Unity Catalog to manage storage credentials, which provides a better and faster way for managing credential security.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst crossaccountRoleName = config.require(\"crossaccountRoleName\");\nconst assumeRoleForEc2 = aws.iam.getPolicyDocument({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            identifiers: [\"ec2.amazonaws.com\"],\n            type: \"Service\",\n        }],\n    }],\n});\nconst roleForS3Access = new aws.iam.Role(\"roleForS3Access\", {\n    description: \"Role for shared access\",\n    assumeRolePolicy: assumeRoleForEc2.then(assumeRoleForEc2 =\u003e assumeRoleForEc2.json),\n});\nconst passRoleForS3AccessPolicyDocument = aws.iam.getPolicyDocumentOutput({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"iam:PassRole\"],\n        resources: [roleForS3Access.arn],\n    }],\n});\nconst passRoleForS3AccessPolicy = new aws.iam.Policy(\"passRoleForS3AccessPolicy\", {\n    path: \"/\",\n    policy: passRoleForS3AccessPolicyDocument.apply(passRoleForS3AccessPolicyDocument =\u003e passRoleForS3AccessPolicyDocument.json),\n});\nconst crossAccount = new aws.iam.RolePolicyAttachment(\"crossAccount\", {\n    policyArn: passRoleForS3AccessPolicy.arn,\n    role: crossaccountRoleName,\n});\nconst sharedInstanceProfile = new aws.iam.InstanceProfile(\"sharedInstanceProfile\", {role: roleForS3Access.name});\nconst sharedIndex_instanceProfileInstanceProfile = new databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", {instanceProfileArn: sharedInstanceProfile.arn});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Cluster(\"this\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    awsAttributes: {\n        instanceProfileArn: sharedIndex / instanceProfileInstanceProfile.id,\n        availability: \"SPOT\",\n        zoneId: \"us-east-1\",\n        firstOnDemand: 1,\n        spotBidPricePercent: 100,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ncrossaccount_role_name = config.require(\"crossaccountRoleName\")\nassume_role_for_ec2 = aws.iam.get_policy_document(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"sts:AssumeRole\"],\n    principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n        identifiers=[\"ec2.amazonaws.com\"],\n        type=\"Service\",\n    )],\n)])\nrole_for_s3_access = aws.iam.Role(\"roleForS3Access\",\n    description=\"Role for shared access\",\n    assume_role_policy=assume_role_for_ec2.json)\npass_role_for_s3_access_policy_document = aws.iam.get_policy_document_output(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"iam:PassRole\"],\n    resources=[role_for_s3_access.arn],\n)])\npass_role_for_s3_access_policy = aws.iam.Policy(\"passRoleForS3AccessPolicy\",\n    path=\"/\",\n    policy=pass_role_for_s3_access_policy_document.json)\ncross_account = aws.iam.RolePolicyAttachment(\"crossAccount\",\n    policy_arn=pass_role_for_s3_access_policy.arn,\n    role=crossaccount_role_name)\nshared_instance_profile = aws.iam.InstanceProfile(\"sharedInstanceProfile\", role=role_for_s3_access.name)\nshared_index_instance_profile_instance_profile = databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", instance_profile_arn=shared_instance_profile.arn)\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Cluster(\"this\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    aws_attributes=databricks.ClusterAwsAttributesArgs(\n        instance_profile_arn=shared_index / instance_profile_instance_profile[\"id\"],\n        availability=\"SPOT\",\n        zone_id=\"us-east-1\",\n        first_on_demand=1,\n        spot_bid_price_percent=100,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var crossaccountRoleName = config.Require(\"crossaccountRoleName\");\n    var assumeRoleForEc2 = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Identifiers = new[]\n                        {\n                            \"ec2.amazonaws.com\",\n                        },\n                        Type = \"Service\",\n                    },\n                },\n            },\n        },\n    });\n\n    var roleForS3Access = new Aws.Iam.Role(\"roleForS3Access\", new()\n    {\n        Description = \"Role for shared access\",\n        AssumeRolePolicy = assumeRoleForEc2.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var passRoleForS3AccessPolicyDocument = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"iam:PassRole\",\n                },\n                Resources = new[]\n                {\n                    roleForS3Access.Arn,\n                },\n            },\n        },\n    });\n\n    var passRoleForS3AccessPolicy = new Aws.Iam.Policy(\"passRoleForS3AccessPolicy\", new()\n    {\n        Path = \"/\",\n        PolicyDocument = passRoleForS3AccessPolicyDocument.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var crossAccount = new Aws.Iam.RolePolicyAttachment(\"crossAccount\", new()\n    {\n        PolicyArn = passRoleForS3AccessPolicy.Arn,\n        Role = crossaccountRoleName,\n    });\n\n    var sharedInstanceProfile = new Aws.Iam.InstanceProfile(\"sharedInstanceProfile\", new()\n    {\n        Role = roleForS3Access.Name,\n    });\n\n    var sharedIndex_instanceProfileInstanceProfile = new Databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", new()\n    {\n        InstanceProfileArn = sharedInstanceProfile.Arn,\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        AwsAttributes = new Databricks.Inputs.ClusterAwsAttributesArgs\n        {\n            InstanceProfileArn = sharedIndex / instanceProfileInstanceProfile.Id,\n            Availability = \"SPOT\",\n            ZoneId = \"us-east-1\",\n            FirstOnDemand = 1,\n            SpotBidPricePercent = 100,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tcrossaccountRoleName := cfg.Require(\"crossaccountRoleName\")\n\t\tassumeRoleForEc2, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tEffect: pulumi.StringRef(\"Allow\"),\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"ec2.amazonaws.com\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tType: \"Service\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\troleForS3Access, err := iam.NewRole(ctx, \"roleForS3Access\", \u0026iam.RoleArgs{\n\t\t\tDescription:      pulumi.String(\"Role for shared access\"),\n\t\t\tAssumeRolePolicy: *pulumi.String(assumeRoleForEc2.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpassRoleForS3AccessPolicyDocument := iam.GetPolicyDocumentOutput(ctx, iam.GetPolicyDocumentOutputArgs{\n\t\t\tStatements: iam.GetPolicyDocumentStatementArray{\n\t\t\t\t\u0026iam.GetPolicyDocumentStatementArgs{\n\t\t\t\t\tEffect: pulumi.String(\"Allow\"),\n\t\t\t\t\tActions: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"iam:PassRole\"),\n\t\t\t\t\t},\n\t\t\t\t\tResources: pulumi.StringArray{\n\t\t\t\t\t\troleForS3Access.Arn,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tpassRoleForS3AccessPolicy, err := iam.NewPolicy(ctx, \"passRoleForS3AccessPolicy\", \u0026iam.PolicyArgs{\n\t\t\tPath: pulumi.String(\"/\"),\n\t\t\tPolicy: passRoleForS3AccessPolicyDocument.ApplyT(func(passRoleForS3AccessPolicyDocument iam.GetPolicyDocumentResult) (*string, error) {\n\t\t\t\treturn \u0026passRoleForS3AccessPolicyDocument.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"crossAccount\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: passRoleForS3AccessPolicy.Arn,\n\t\t\tRole:      pulumi.String(crossaccountRoleName),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedInstanceProfile, err := iam.NewInstanceProfile(ctx, \"sharedInstanceProfile\", \u0026iam.InstanceProfileArgs{\n\t\t\tRole: roleForS3Access.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstanceProfile(ctx, \"sharedIndex/instanceProfileInstanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: sharedInstanceProfile.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           *pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             *pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tAwsAttributes: \u0026databricks.ClusterAwsAttributesArgs{\n\t\t\t\tInstanceProfileArn:  sharedIndex / instanceProfileInstanceProfile.Id,\n\t\t\t\tAvailability:        pulumi.String(\"SPOT\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1\"),\n\t\t\t\tFirstOnDemand:       pulumi.Int(1),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.aws.iam.InstanceProfile;\nimport com.pulumi.aws.iam.InstanceProfileArgs;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport com.pulumi.databricks.inputs.ClusterAwsAttributesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var crossaccountRoleName = config.get(\"crossaccountRoleName\");\n        final var assumeRoleForEc2 = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .identifiers(\"ec2.amazonaws.com\")\n                    .type(\"Service\")\n                    .build())\n                .build())\n            .build());\n\n        var roleForS3Access = new Role(\"roleForS3Access\", RoleArgs.builder()        \n            .description(\"Role for shared access\")\n            .assumeRolePolicy(assumeRoleForEc2.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        final var passRoleForS3AccessPolicyDocument = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"iam:PassRole\")\n                .resources(roleForS3Access.arn())\n                .build())\n            .build());\n\n        var passRoleForS3AccessPolicy = new Policy(\"passRoleForS3AccessPolicy\", PolicyArgs.builder()        \n            .path(\"/\")\n            .policy(passRoleForS3AccessPolicyDocument.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult).applyValue(passRoleForS3AccessPolicyDocument -\u003e passRoleForS3AccessPolicyDocument.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json())))\n            .build());\n\n        var crossAccount = new RolePolicyAttachment(\"crossAccount\", RolePolicyAttachmentArgs.builder()        \n            .policyArn(passRoleForS3AccessPolicy.arn())\n            .role(crossaccountRoleName)\n            .build());\n\n        var sharedInstanceProfile = new InstanceProfile(\"sharedInstanceProfile\", InstanceProfileArgs.builder()        \n            .role(roleForS3Access.name())\n            .build());\n\n        var sharedIndex_instanceProfileInstanceProfile = new InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(sharedInstanceProfile.arn())\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion();\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latest.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .awsAttributes(ClusterAwsAttributesArgs.builder()\n                .instanceProfileArn(sharedIndex / instanceProfileInstanceProfile.id())\n                .availability(\"SPOT\")\n                .zoneId(\"us-east-1\")\n                .firstOnDemand(1)\n                .spotBidPricePercent(100)\n                .build())\n            .build());\n\n    }\n}\n```\n\n## Usage with Cluster Policies\n\nIt is advised to keep all common configurations in Cluster Policies to maintain control of the environments launched, so `databricks.Cluster` above could be replaced with `databricks.ClusterPolicy`:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ClusterPolicy(\"this\", {definition: JSON.stringify({\n    \"aws_attributes.instance_profile_arn\": {\n        type: \"fixed\",\n        value: databricks_instance_profile.shared.arn,\n    },\n})});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nthis = databricks.ClusterPolicy(\"this\", definition=json.dumps({\n    \"aws_attributes.instance_profile_arn\": {\n        \"type\": \"fixed\",\n        \"value\": databricks_instance_profile[\"shared\"][\"arn\"],\n    },\n}))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ClusterPolicy(\"this\", new()\n    {\n        Definition = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"aws_attributes.instance_profile_arn\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"fixed\",\n                [\"value\"] = databricks_instance_profile.Shared.Arn,\n            },\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"aws_attributes.instance_profile_arn\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": databricks_instance_profile.Shared.Arn,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"this\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tDefinition: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ClusterPolicy(\"this\", ClusterPolicyArgs.builder()        \n            .definition(serializeJson(\n                jsonObject(\n                    jsonProperty(\"aws_attributes.instance_profile_arn\", jsonObject(\n                        jsonProperty(\"type\", \"fixed\"),\n                        jsonProperty(\"value\", databricks_instance_profile.shared().arn())\n                    ))\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ClusterPolicy\n    properties:\n      definition:\n        fn::toJSON:\n          aws_attributes.instance_profile_arn:\n            type: fixed\n            value: ${databricks_instance_profile.shared.arn}\n```\n\n## Granting access to all users\n\nYou can make instance profile available to all users by associating it with the special group called `users` through databricks.Group data source.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.InstanceProfile(\"this\", {instanceProfileArn: aws_iam_instance_profile.shared.arn});\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst all = new databricks.GroupInstanceProfile(\"all\", {\n    groupId: users.then(users =\u003e users.id),\n    instanceProfileId: _this.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.InstanceProfile(\"this\", instance_profile_arn=aws_iam_instance_profile[\"shared\"][\"arn\"])\nusers = databricks.get_group(display_name=\"users\")\nall = databricks.GroupInstanceProfile(\"all\",\n    group_id=users.id,\n    instance_profile_id=this.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = aws_iam_instance_profile.Shared.Arn,\n    });\n\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var all = new Databricks.GroupInstanceProfile(\"all\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        InstanceProfileId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.Any(aws_iam_instance_profile.Shared.Arn),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"all\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           *pulumi.String(users.Id),\n\t\t\tInstanceProfileId: this.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new InstanceProfile(\"this\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(aws_iam_instance_profile.shared().arn())\n            .build());\n\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var all = new GroupInstanceProfile(\"all\", GroupInstanceProfileArgs.builder()        \n            .groupId(users.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .instanceProfileId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: ${aws_iam_instance_profile.shared.arn}\n  all:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${users.id}\n      instanceProfileId: ${this.id}\nvariables:\n  users:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: users\n```\n## Usage with Databricks SQL serverless\n\nWhen the instance profile ARN and its associated IAM role ARN don't match and the instance profile is intended for use with Databricks SQL serverless, the `iam_role_arn` parameter can be specified\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlServerlessAssumeRole = aws.iam.getPolicyDocument({\n    statements: [{\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            type: \"AWS\",\n            identifiers: [\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n        }],\n        conditions: [{\n            test: \"StringEquals\",\n            variable: \"sts:ExternalID\",\n            values: [\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n            ],\n        }],\n    }],\n});\nconst thisRole = new aws.iam.Role(\"thisRole\", {assumeRolePolicy: sqlServerlessAssumeRole.then(sqlServerlessAssumeRole =\u003e sqlServerlessAssumeRole.json)});\nconst thisInstanceProfile = new aws.iam.InstanceProfile(\"thisInstanceProfile\", {role: thisRole.name});\nconst thisIndex_instanceProfileInstanceProfile = new databricks.InstanceProfile(\"thisIndex/instanceProfileInstanceProfile\", {\n    instanceProfileArn: thisInstanceProfile.arn,\n    iamRoleArn: thisRole.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nsql_serverless_assume_role = aws.iam.get_policy_document(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    actions=[\"sts:AssumeRole\"],\n    principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n        type=\"AWS\",\n        identifiers=[\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n    )],\n    conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n        test=\"StringEquals\",\n        variable=\"sts:ExternalID\",\n        values=[\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n        ],\n    )],\n)])\nthis_role = aws.iam.Role(\"thisRole\", assume_role_policy=sql_serverless_assume_role.json)\nthis_instance_profile = aws.iam.InstanceProfile(\"thisInstanceProfile\", role=this_role.name)\nthis_index_instance_profile_instance_profile = databricks.InstanceProfile(\"thisIndex/instanceProfileInstanceProfile\",\n    instance_profile_arn=this_instance_profile.arn,\n    iam_role_arn=this_role.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlServerlessAssumeRole = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n                        },\n                    },\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"StringEquals\",\n                        Variable = \"sts:ExternalID\",\n                        Values = new[]\n                        {\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var thisRole = new Aws.Iam.Role(\"thisRole\", new()\n    {\n        AssumeRolePolicy = sqlServerlessAssumeRole.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var thisInstanceProfile = new Aws.Iam.InstanceProfile(\"thisInstanceProfile\", new()\n    {\n        Role = thisRole.Name,\n    });\n\n    var thisIndex_instanceProfileInstanceProfile = new Databricks.InstanceProfile(\"thisIndex/instanceProfileInstanceProfile\", new()\n    {\n        InstanceProfileArn = thisInstanceProfile.Arn,\n        IamRoleArn = thisRole.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsqlServerlessAssumeRole, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tType: \"AWS\",\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tConditions: []iam.GetPolicyDocumentStatementCondition{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tTest:     \"StringEquals\",\n\t\t\t\t\t\t\tVariable: \"sts:ExternalID\",\n\t\t\t\t\t\t\tValues: []string{\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisRole, err := iam.NewRole(ctx, \"thisRole\", \u0026iam.RoleArgs{\n\t\t\tAssumeRolePolicy: *pulumi.String(sqlServerlessAssumeRole.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisInstanceProfile, err := iam.NewInstanceProfile(ctx, \"thisInstanceProfile\", \u0026iam.InstanceProfileArgs{\n\t\t\tRole: thisRole.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstanceProfile(ctx, \"thisIndex/instanceProfileInstanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: thisInstanceProfile.Arn,\n\t\t\tIamRoleArn:         thisRole.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.InstanceProfile;\nimport com.pulumi.aws.iam.InstanceProfileArgs;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sqlServerlessAssumeRole = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .type(\"AWS\")\n                    .identifiers(\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\")\n                    .build())\n                .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                    .test(\"StringEquals\")\n                    .variable(\"sts:ExternalID\")\n                    .values(                    \n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\")\n                    .build())\n                .build())\n            .build());\n\n        var thisRole = new Role(\"thisRole\", RoleArgs.builder()        \n            .assumeRolePolicy(sqlServerlessAssumeRole.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var thisInstanceProfile = new InstanceProfile(\"thisInstanceProfile\", InstanceProfileArgs.builder()        \n            .role(thisRole.name())\n            .build());\n\n        var thisIndex_instanceProfileInstanceProfile = new InstanceProfile(\"thisIndex/instanceProfileInstanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(thisInstanceProfile.arn())\n            .iamRoleArn(thisRole.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisRole:\n    type: aws:iam:Role\n    properties:\n      assumeRolePolicy: ${sqlServerlessAssumeRole.json}\n  thisInstanceProfile:\n    type: aws:iam:InstanceProfile\n    properties:\n      role: ${thisRole.name}\n  thisIndex/instanceProfileInstanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: ${thisInstanceProfile.arn}\n      iamRoleArn: ${thisRole.arn}\nvariables:\n  sqlServerlessAssumeRole:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        statements:\n          - actions:\n              - sts:AssumeRole\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::790110701330:role/serverless-customer-resource-role\n            conditions:\n              - test: StringEquals\n                variable: sts:ExternalID\n                values:\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\n```\n\n\n## Import\n\nThe resource instance profile can be imported using the ARN of it bash\n\n```sh\n $ pulumi import databricks:index/instanceProfile:InstanceProfile this \u003cinstance-profile-arn\u003e\n```\n\n ",
            "properties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "required": [
                "instanceProfileArn",
                "skipValidation"
            ],
            "inputProperties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "requiredInputs": [
                "instanceProfileArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstanceProfile resources.\n",
                "properties": {
                    "iamRoleArn": {
                        "type": "string",
                        "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                    },
                    "isMetaInstanceProfile": {
                        "type": "boolean",
                        "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/ipAccessList:IpAccessList": {
            "description": "Security-conscious enterprises that use cloud SaaS applications need to restrict access to their own employees. Authentication helps to prove user identity, but that does not enforce network location of the users. Accessing a cloud service from an unsecured network can pose security risks to an enterprise, especially when the user may have authorized access to sensitive or personal data. Enterprise network perimeters apply security policies and limit access to external services (for example, firewalls, proxies, DLP, and logging), so access beyond these controls are assumed to be untrusted. Please see [IP Access List](https://docs.databricks.com/security/network/ip-access-list.html) for full feature documentation.\n\n\u003e **Note** The total number of IP addresses and CIDR scopes provided across all ACL Lists in a workspace can not exceed 1000.  Refer to the docs above for specifics.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\nconst allowed_list = new databricks.IpAccessList(\"allowed-list\", {\n    label: \"allow_in\",\n    listType: \"ALLOW\",\n    ipAddresses: [\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n}, {\n    dependsOn: [_this],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\nallowed_list = databricks.IpAccessList(\"allowed-list\",\n    label=\"allow_in\",\n    list_type=\"ALLOW\",\n    ip_addresses=[\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n    opts=pulumi.ResourceOptions(depends_on=[this]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n    var allowed_list = new Databricks.IpAccessList(\"allowed-list\", new()\n    {\n        Label = \"allow_in\",\n        ListType = \"ALLOW\",\n        IpAddresses = new[]\n        {\n            \"1.2.3.0/24\",\n            \"1.2.5.0/24\",\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn = new[]\n        {\n            @this,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.AnyMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewIpAccessList(ctx, \"allowed-list\", \u0026databricks.IpAccessListArgs{\n\t\t\tLabel:    pulumi.String(\"allow_in\"),\n\t\t\tListType: pulumi.String(\"ALLOW\"),\n\t\t\tIpAddresses: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"1.2.3.0/24\"),\n\t\t\t\tpulumi.String(\"1.2.5.0/24\"),\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthis,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport com.pulumi.databricks.IpAccessList;\nimport com.pulumi.databricks.IpAccessListArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n        var allowed_list = new IpAccessList(\"allowed-list\", IpAccessListArgs.builder()        \n            .label(\"allow_in\")\n            .listType(\"ALLOW\")\n            .ipAddresses(            \n                \"1.2.3.0/24\",\n                \"1.2.5.0/24\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(this_)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n  allowed-list:\n    type: databricks:IpAccessList\n    properties:\n      label: allow_in\n      listType: ALLOW\n      ipAddresses:\n        - 1.2.3.0/24\n        - 1.2.5.0/24\n    options:\n      dependson:\n        - ${this}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsPrivateAccessSettings to create a [Private Access Setting](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-5-create-a-private-access-settings-configuration-using-the-databricks-account-api) that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nThe databricks_ip_access_list can be imported using idbash\n\n```sh\n $ pulumi import databricks:index/ipAccessList:IpAccessList this \u003clist-id\u003e\n```\n\n ",
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "This is a field to allow the group to have instance pool create privileges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                }
            },
            "required": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "inputProperties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "This is a field to allow the group to have instance pool create privileges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                }
            },
            "requiredInputs": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering IpAccessList resources.\n",
                "properties": {
                    "enabled": {
                        "type": "boolean",
                        "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                    },
                    "ipAddresses": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "This is a field to allow the group to have instance pool create privileges.\n"
                    },
                    "label": {
                        "type": "string",
                        "description": "This is the display name for the given IP ACL List.\n"
                    },
                    "listType": {
                        "type": "string",
                        "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/job:Job": {
            "description": "\n\n\n## Import\n\nThe resource job can be imported using the id of the job bash\n\n```sh\n $ pulumi import databricks:index/job:Job this \u003cjob-id\u003e\n```\n\n ",
            "properties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "required": [
                "format",
                "name",
                "url"
            ],
            "inputProperties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Job resources.\n",
                "properties": {
                    "alwaysRunning": {
                        "type": "boolean",
                        "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                    },
                    "dbtTask": {
                        "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask"
                    },
                    "emailNotifications": {
                        "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                        "description": "(List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                    },
                    "existingClusterId": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "gitSource": {
                        "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                    },
                    "jobClusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                        }
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                        },
                        "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                    },
                    "maxConcurrentRuns": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                    },
                    "maxRetries": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED or INTERNAL_ERROR lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: PENDING, RUNNING, TERMINATING, TERMINATED, SKIPPED or INTERNAL_ERROR\n"
                    },
                    "minRetryIntervalMillis": {
                        "type": "integer",
                        "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "An optional name for the job. The default value is Untitled.\n"
                    },
                    "newCluster": {
                        "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                        "description": "Same set of parameters as for databricks.Cluster resource.\n"
                    },
                    "notebookTask": {
                        "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                    },
                    "pipelineTask": {
                        "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                    },
                    "pythonWheelTask": {
                        "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                    },
                    "retryOnTimeout": {
                        "type": "boolean",
                        "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                        "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                    },
                    "sparkJarTask": {
                        "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                    },
                    "sparkPythonTask": {
                        "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                    },
                    "sparkSubmitTask": {
                        "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                    },
                    "tags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                    },
                    "tasks": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobTask:JobTask"
                        }
                    },
                    "timeoutSeconds": {
                        "type": "integer",
                        "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the Git repository to use.\n"
                    },
                    "webhookNotifications": {
                        "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                        "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/library:Library": {
            "description": "Installs a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster. Each different type of library has a slightly different syntax. It's possible to set only one type of library within one resource. Otherwise, the plan will fail with an error. \n\n\u003e **Note** `databricks.Library` resource would always start the associated cluster if it's not running, so make sure to have auto-termination configured. It's not possible to atomically change the version of the same library without cluster restart. Libraries are fully removed from the cluster only after restart.\n\n## Java/Scala JAR\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/app-0.0.1.jar`,\n    path: \"/FileStore/app-0.0.1.jar\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    jar: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/app-0.0.1.jar\",\n    path=\"/FileStore/app-0.0.1.jar\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    jar=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/app-0.0.1.jar\",\n        Path = \"/FileStore/app-0.0.1.jar\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Jar = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/app-0.0.1.jar\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/app-0.0.1.jar\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tJar:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/app-0.0.1.jar\", path.module()))\n            .path(\"/FileStore/app-0.0.1.jar\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .jar(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/app-0.0.1.jar\n      path: /FileStore/app-0.0.1.jar\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      jar: ${appDbfsFile.dbfsPath}\n```\n\n## Java/Scala Maven\n\nInstalling artifacts from Maven repository. You can also optionally specify a `repo` parameter for a custom Maven-style repository, that should be accessible without any authentication. Maven libraries are resolved in Databricks Control Plane, so repo should be accessible from it. It can even be properly configured [maven s3 wagon](https://github.com/seahen/maven-s3-wagon), [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) or [Azure Artifacts](https://azure.microsoft.com/en-us/services/devops/artifacts/).\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst deequ = new databricks.Library(\"deequ\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    maven: {\n        coordinates: \"com.amazon.deequ:deequ:1.0.4\",\n        exclusions: [\"org.apache.avro:avro\"],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndeequ = databricks.Library(\"deequ\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    maven=databricks.LibraryMavenArgs(\n        coordinates=\"com.amazon.deequ:deequ:1.0.4\",\n        exclusions=[\"org.apache.avro:avro\"],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var deequ = new Databricks.Library(\"deequ\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Maven = new Databricks.Inputs.LibraryMavenArgs\n        {\n            Coordinates = \"com.amazon.deequ:deequ:1.0.4\",\n            Exclusions = new[]\n            {\n                \"org.apache.avro:avro\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"deequ\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tMaven: \u0026databricks.LibraryMavenArgs{\n\t\t\t\tCoordinates: pulumi.String(\"com.amazon.deequ:deequ:1.0.4\"),\n\t\t\t\tExclusions: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"org.apache.avro:avro\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryMavenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var deequ = new Library(\"deequ\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .maven(LibraryMavenArgs.builder()\n                .coordinates(\"com.amazon.deequ:deequ:1.0.4\")\n                .exclusions(\"org.apache.avro:avro\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  deequ:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      maven:\n        coordinates: com.amazon.deequ:deequ:1.0.4\n        exclusions:\n          - org.apache.avro:avro\n```\n\n## Python Wheel\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/baz.whl`,\n    path: \"/FileStore/baz.whl\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    whl: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/baz.whl\",\n    path=\"/FileStore/baz.whl\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    whl=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/baz.whl\",\n        Path = \"/FileStore/baz.whl\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Whl = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/baz.whl\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/baz.whl\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tWhl:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/baz.whl\", path.module()))\n            .path(\"/FileStore/baz.whl\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .whl(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/baz.whl\n      path: /FileStore/baz.whl\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      whl: ${appDbfsFile.dbfsPath}\n```\n\n## Python PyPI\n\nInstalling Python PyPI artifacts. You can optionally also specify the `repo` parameter for a custom PyPI mirror, which should be accessible without any authentication for the network that cluster runs in.\n\n\u003e **Note** `repo` host should be accessible from the Internet by Databricks control plane. If connectivity to custom PyPI repositories is required, please modify cluster-node `/etc/pip.conf` through databricks_global_init_script.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fbprophet = new databricks.Library(\"fbprophet\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    pypi: {\n        \"package\": \"fbprophet==0.6\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfbprophet = databricks.Library(\"fbprophet\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    pypi=databricks.LibraryPypiArgs(\n        package=\"fbprophet==0.6\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fbprophet = new Databricks.Library(\"fbprophet\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Pypi = new Databricks.Inputs.LibraryPypiArgs\n        {\n            Package = \"fbprophet==0.6\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"fbprophet\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tPypi: \u0026databricks.LibraryPypiArgs{\n\t\t\t\tPackage: pulumi.String(\"fbprophet==0.6\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fbprophet = new Library(\"fbprophet\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .pypi(LibraryPypiArgs.builder()\n                .package_(\"fbprophet==0.6\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fbprophet:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      pypi:\n        package: fbprophet==0.6\n```\n\n## Python EGG\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/foo.egg`,\n    path: \"/FileStore/foo.egg\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    egg: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/foo.egg\",\n    path=\"/FileStore/foo.egg\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    egg=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/foo.egg\",\n        Path = \"/FileStore/foo.egg\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Egg = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/foo.egg\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/foo.egg\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tEgg:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/foo.egg\", path.module()))\n            .path(\"/FileStore/foo.egg\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .egg(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/foo.egg\n      path: /FileStore/foo.egg\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      egg: ${appDbfsFile.dbfsPath}\n```\n\n## R CRan\n\nInstalling artifacts from CRan. You can also optionally specify a `repo` parameter for a custom cran mirror.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst rkeops = new databricks.Library(\"rkeops\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    cran: {\n        \"package\": \"rkeops\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nrkeops = databricks.Library(\"rkeops\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    cran=databricks.LibraryCranArgs(\n        package=\"rkeops\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var rkeops = new Databricks.Library(\"rkeops\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Cran = new Databricks.Inputs.LibraryCranArgs\n        {\n            Package = \"rkeops\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"rkeops\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tCran: \u0026databricks.LibraryCranArgs{\n\t\t\t\tPackage: pulumi.String(\"rkeops\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryCranArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var rkeops = new Library(\"rkeops\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .cran(LibraryCranArgs.builder()\n                .package_(\"rkeops\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  rkeops:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      cran:\n        package: rkeops\n```\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "clusterId": {
                    "type": "string"
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                    "willReplaceOnChanges": true
                },
                "egg": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "jar": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                    "willReplaceOnChanges": true
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                    "willReplaceOnChanges": true
                },
                "whl": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "clusterId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Library resources.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "cran": {
                        "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                        "willReplaceOnChanges": true
                    },
                    "egg": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "jar": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "maven": {
                        "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                        "willReplaceOnChanges": true
                    },
                    "pypi": {
                        "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                        "willReplaceOnChanges": true
                    },
                    "whl": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastore:Metastore": {
            "description": "\u003e **Notes**\n  Unity Catalog APIs are accessible via **workspace-level APIs**. This design may change in the future.\n\nA metastore is the top-level container of objects in Unity Catalog. It stores data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create metastores and assign them to Databricks workspaces in order to control which workloads use each metastore.\n\nUnity Catalog offers a new metastore with built in security and auditing. This is distinct to the metastore used in previous versions of Databricks (based on the Hive Metastore).\n\n\n## Import\n\nThis resource can be imported by IDbash\n\n```sh\n $ pulumi import databricks:index/metastore:Metastore this \u003cid\u003e\n```\n\n ",
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "required": [
                "cloud",
                "createdAt",
                "createdBy",
                "globalMetastoreId",
                "name",
                "owner",
                "region",
                "storageRoot",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "storageRoot"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Metastore resources.\n",
                "properties": {
                    "cloud": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer"
                    },
                    "createdBy": {
                        "type": "string"
                    },
                    "defaultDataAccessConfigId": {
                        "type": "string"
                    },
                    "deltaSharingOrganizationName": {
                        "type": "string",
                        "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                    },
                    "deltaSharingRecipientTokenLifetimeInSeconds": {
                        "type": "integer",
                        "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                    },
                    "deltaSharingScope": {
                        "type": "string",
                        "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy metastore regardless of its contents.\n"
                    },
                    "globalMetastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of metastore.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the metastore owner.\n"
                    },
                    "region": {
                        "type": "string"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreAssignment:MetastoreAssignment": {
            "description": "A single databricks.Metastore can be shared across Databricks workspaces, and each linked workspace has a consistent view of the data and a single set of access policies. You can only create a single metastore for each region in which your organization operates.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisMetastore = new databricks.Metastore(\"thisMetastore\", {\n    storageRoot: `s3://${aws_s3_bucket.metastore.id}/metastore`,\n    owner: \"uc admins\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"thisMetastoreAssignment\", {\n    metastoreId: thisMetastore.id,\n    workspaceId: local.workspace_id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_metastore = databricks.Metastore(\"thisMetastore\",\n    storage_root=f\"s3://{aws_s3_bucket['metastore']['id']}/metastore\",\n    owner=\"uc admins\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"thisMetastoreAssignment\",\n    metastore_id=this_metastore.id,\n    workspace_id=local[\"workspace_id\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisMetastore = new Databricks.Metastore(\"thisMetastore\", new()\n    {\n        StorageRoot = $\"s3://{aws_s3_bucket.Metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"thisMetastoreAssignment\", new()\n    {\n        MetastoreId = thisMetastore.Id,\n        WorkspaceId = local.Workspace_id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisMetastore, err := databricks.NewMetastore(ctx, \"thisMetastore\", \u0026databricks.MetastoreArgs{\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", aws_s3_bucket.Metastore.Id)),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"thisMetastoreAssignment\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: thisMetastore.ID(),\n\t\t\tWorkspaceId: pulumi.Any(local.Workspace_id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisMetastore = new Metastore(\"thisMetastore\", MetastoreArgs.builder()        \n            .storageRoot(String.format(\"s3://%s/metastore\", aws_s3_bucket.metastore().id()))\n            .owner(\"uc admins\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()        \n            .metastoreId(thisMetastore.id())\n            .workspaceId(local.workspace_id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisMetastore:\n    type: databricks:Metastore\n    properties:\n      storageRoot: s3://${aws_s3_bucket.metastore.id}/metastore\n      owner: uc admins\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    properties:\n      metastoreId: ${thisMetastore.id}\n      workspaceId: ${local.workspace_id}\n```\n{{% /example %}}\n{{% /examples %}}",
            "properties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "id of the workspace for the assignment\n"
                }
            },
            "required": [
                "metastoreId",
                "workspaceId"
            ],
            "inputProperties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "id of the workspace for the assignment\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreAssignment resources.\n",
                "properties": {
                    "defaultCatalogName": {
                        "type": "string",
                        "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n"
                    },
                    "workspaceId": {
                        "type": "integer",
                        "description": "id of the workspace for the assignment\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreDataAccess:MetastoreDataAccess": {
            "description": "Each databricks.Metastore requires an IAM role that will be assumed by Unity Catalog to access data. `databricks.MetastoreDataAccess` defines this\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal"
                },
                "configurationType": {
                    "type": "string"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey"
                },
                "isDefault": {
                    "type": "boolean"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "configurationType",
                "metastoreId",
                "name"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                    "willReplaceOnChanges": true
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                    "willReplaceOnChanges": true
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                    "willReplaceOnChanges": true
                },
                "configurationType": {
                    "type": "string"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                    "willReplaceOnChanges": true
                },
                "isDefault": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreDataAccess resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                        "willReplaceOnChanges": true
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                        "willReplaceOnChanges": true
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                        "willReplaceOnChanges": true
                    },
                    "configurationType": {
                        "type": "string"
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                        "willReplaceOnChanges": true
                    },
                    "isDefault": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreProvider:MetastoreProvider": {
            "description": "Within a metastore, Unity Catalog provides the ability to create a provider which contains a list of shares that have been shared with you.\n\nA `databricks.MetastoreProvider` is contained within databricks.Metastore and can contain a list of shares that have been shared with you.\n\nNote that Databricks to Databricks sharing automatically creates the provider.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dbprovider = new databricks.MetastoreProvider(\"dbprovider\", {\n    comment: \"made by terraform 2\",\n    authenticationType: \"TOKEN\",\n    recipientProfileStr: JSON.stringify({\n        shareCredentialsVersion: 1,\n        bearerToken: \"token\",\n        endpoint: \"endpoint\",\n        expirationTime: \"expiration-time\",\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\ndbprovider = databricks.MetastoreProvider(\"dbprovider\",\n    comment=\"made by terraform 2\",\n    authentication_type=\"TOKEN\",\n    recipient_profile_str=json.dumps({\n        \"shareCredentialsVersion\": 1,\n        \"bearerToken\": \"token\",\n        \"endpoint\": \"endpoint\",\n        \"expirationTime\": \"expiration-time\",\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dbprovider = new Databricks.MetastoreProvider(\"dbprovider\", new()\n    {\n        Comment = \"made by terraform 2\",\n        AuthenticationType = \"TOKEN\",\n        RecipientProfileStr = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"shareCredentialsVersion\"] = 1,\n            [\"bearerToken\"] = \"token\",\n            [\"endpoint\"] = \"endpoint\",\n            [\"expirationTime\"] = \"expiration-time\",\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"shareCredentialsVersion\": 1,\n\t\t\t\"bearerToken\":             \"token\",\n\t\t\t\"endpoint\":                \"endpoint\",\n\t\t\t\"expirationTime\":          \"expiration-time\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewMetastoreProvider(ctx, \"dbprovider\", \u0026databricks.MetastoreProviderArgs{\n\t\t\tComment:             pulumi.String(\"made by terraform 2\"),\n\t\t\tAuthenticationType:  pulumi.String(\"TOKEN\"),\n\t\t\tRecipientProfileStr: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MetastoreProvider;\nimport com.pulumi.databricks.MetastoreProviderArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dbprovider = new MetastoreProvider(\"dbprovider\", MetastoreProviderArgs.builder()        \n            .comment(\"made by terraform 2\")\n            .authenticationType(\"TOKEN\")\n            .recipientProfileStr(serializeJson(\n                jsonObject(\n                    jsonProperty(\"shareCredentialsVersion\", 1),\n                    jsonProperty(\"bearerToken\", \"token\"),\n                    jsonProperty(\"endpoint\", \"endpoint\"),\n                    jsonProperty(\"expirationTime\", \"expiration-time\")\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dbprovider:\n    type: databricks:MetastoreProvider\n    properties:\n      comment: made by terraform 2\n      authenticationType: TOKEN\n      recipientProfileStr:\n        fn::toJSON:\n          shareCredentialsVersion: 1\n          bearerToken: token\n          endpoint: endpoint\n          expirationTime: expiration-time\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n",
            "properties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n"
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "required": [
                "authenticationType",
                "name",
                "recipientProfileStr"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "requiredInputs": [
                "authenticationType",
                "recipientProfileStr"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreProvider resources.\n",
                "properties": {
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the provider.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "recipientProfileStr": {
                        "type": "string",
                        "description": "This is the json file that is created from a recipient url.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowExperiment:MlflowExperiment": {
            "description": "This resource allows you to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.MlflowExperiment(\"this\", {\n    artifactLocation: \"dbfs:/tmp/my-experiment\",\n    description: \"My MLflow experiment description\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.MlflowExperiment(\"this\",\n    artifact_location=\"dbfs:/tmp/my-experiment\",\n    description=\"My MLflow experiment description\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.MlflowExperiment(\"this\", new()\n    {\n        ArtifactLocation = \"dbfs:/tmp/my-experiment\",\n        Description = \"My MLflow experiment description\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowExperiment(ctx, \"this\", \u0026databricks.MlflowExperimentArgs{\n\t\t\tArtifactLocation: pulumi.String(\"dbfs:/tmp/my-experiment\"),\n\t\t\tDescription:      pulumi.String(\"My MLflow experiment description\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.MlflowExperiment;\nimport com.pulumi.databricks.MlflowExperimentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new MlflowExperiment(\"this\", MlflowExperimentArgs.builder()        \n            .artifactLocation(\"dbfs:/tmp/my-experiment\")\n            .description(\"My MLflow experiment description\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowExperiment\n    properties:\n      artifactLocation: dbfs:/tmp/my-experiment\n      description: My MLflow experiment description\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, or *Manage* individual experiments.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\nThe experiment resource can be imported using the id of the experiment bash\n\n```sh\n $ pulumi import databricks:index/mlflowExperiment:MlflowExperiment this \u003cexperiment-id\u003e\n```\n\n ",
            "properties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "required": [
                "creationTime",
                "experimentId",
                "lastUpdateTime",
                "lifecycleStage",
                "name"
            ],
            "inputProperties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowExperiment resources.\n",
                "properties": {
                    "artifactLocation": {
                        "type": "string",
                        "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow experiment.\n"
                    },
                    "experimentId": {
                        "type": "string"
                    },
                    "lastUpdateTime": {
                        "type": "integer"
                    },
                    "lifecycleStage": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowModel:MlflowModel": {
            "description": "This resource allows you to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst test = new databricks.MlflowModel(\"test\", {\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest = databricks.MlflowModel(\"test\",\n    description=\"My MLflow model description\",\n    tags=[\n        databricks.MlflowModelTagArgs(\n            key=\"key1\",\n            value=\"value1\",\n        ),\n        databricks.MlflowModelTagArgs(\n            key=\"key2\",\n            value=\"value2\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var test = new Databricks.MlflowModel(\"test\", new()\n    {\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"test\", \u0026databricks.MlflowModelArgs{\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: databricks.MlflowModelTagArray{\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var test = new MlflowModel(\"test\", MlflowModelArgs.builder()        \n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  test:\n    type: databricks:MlflowModel\n    properties:\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, *Manage Staging Versions*, *Manage Production Versions*, and *Manage* individual models.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\nThe model resource can be imported using the name bash\n\n```sh\n $ pulumi import databricks:index/mlflowModel:MlflowModel this \u003cname\u003e\n```\n\n ",
            "properties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                },
                "userId": {
                    "type": "string"
                }
            },
            "required": [
                "creationTimestamp",
                "lastUpdatedTimestamp",
                "name",
                "registeredModelId",
                "userId"
            ],
            "inputProperties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                },
                "userId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowModel resources.\n",
                "properties": {
                    "creationTimestamp": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow model.\n"
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow model. Change of name triggers new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                        },
                        "description": "Tags for the MLflow model.\n"
                    },
                    "userId": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowWebhook:MlflowWebhook": {
            "description": "This resource allows you to create [MLflow Model Registry Webhooks](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) in Databricks.  Webhooks enable you to listen for Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. Webhooks allow trigger execution of a Databricks job or call a web service on specific event(s) that is generated in the MLflow Registry - stage transitioning, creation of registered model, creation of transition request, etc.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n### POSTing to URL\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst url = new databricks.MlflowWebhook(\"url\", {\n    description: \"URL webhook trigger\",\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    httpUrlSpec: {\n        url: \"https://my_cool_host/webhook\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nurl = databricks.MlflowWebhook(\"url\",\n    description=\"URL webhook trigger\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    http_url_spec=databricks.MlflowWebhookHttpUrlSpecArgs(\n        url=\"https://my_cool_host/webhook\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var url = new Databricks.MlflowWebhook(\"url\", new()\n    {\n        Description = \"URL webhook trigger\",\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        HttpUrlSpec = new Databricks.Inputs.MlflowWebhookHttpUrlSpecArgs\n        {\n            Url = \"https://my_cool_host/webhook\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowWebhook(ctx, \"url\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tDescription: pulumi.String(\"URL webhook trigger\"),\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tHttpUrlSpec: \u0026databricks.MlflowWebhookHttpUrlSpecArgs{\n\t\t\t\tUrl: pulumi.String(\"https://my_cool_host/webhook\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookHttpUrlSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var url = new MlflowWebhook(\"url\", MlflowWebhookArgs.builder()        \n            .description(\"URL webhook trigger\")\n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .httpUrlSpec(MlflowWebhookHttpUrlSpecArgs.builder()\n                .url(\"https://my_cool_host/webhook\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  url:\n    type: databricks:MlflowWebhook\n    properties:\n      description: URL webhook trigger\n      events:\n        - TRANSITION_REQUEST_CREATED\n      httpUrlSpec:\n        url: https://my_cool_host/webhook\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* MLflow webhooks could be configured only by workspace admins.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "required": [
                "events"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "requiredInputs": [
                "events"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowWebhook resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Optional description of the MLflow webhook.\n"
                    },
                    "events": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                    },
                    "httpUrlSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                    },
                    "jobSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                    },
                    "modelName": {
                        "type": "string",
                        "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mount:Mount": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs"
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl"
                },
                "clusterId": {
                    "type": "string"
                },
                "encryptionType": {
                    "type": "string"
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs"
                },
                "name": {
                    "type": "string"
                },
                "resourceId": {
                    "type": "string"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3"
                },
                "source": {
                    "type": "string",
                    "description": "(String) HDFS-compatible url\n"
                },
                "uri": {
                    "type": "string"
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb"
                }
            },
            "required": [
                "clusterId",
                "name",
                "source"
            ],
            "inputProperties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                    "willReplaceOnChanges": true
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "encryptionType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "willReplaceOnChanges": true
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3",
                    "willReplaceOnChanges": true
                },
                "uri": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Mount resources.\n",
                "properties": {
                    "abfs": {
                        "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                        "willReplaceOnChanges": true
                    },
                    "adl": {
                        "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "encryptionType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "extraConfigs": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "willReplaceOnChanges": true
                    },
                    "gs": {
                        "$ref": "#/types/databricks:index/MountGs:MountGs",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "s3": {
                        "$ref": "#/types/databricks:index/MountS3:MountS3",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "(String) HDFS-compatible url\n"
                    },
                    "uri": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "wasb": {
                        "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCredentials:MwsCredentials": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst thisAwsAssumeRolePolicy = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccountRole = new aws.iam.Role(\"crossAccountRole\", {\n    assumeRolePolicy: thisAwsAssumeRolePolicy.then(thisAwsAssumeRolePolicy =\u003e thisAwsAssumeRolePolicy.json),\n    tags: _var.tags,\n});\nconst thisAwsCrossAccountPolicy = databricks.getAwsCrossAccountPolicy({});\nconst thisRolePolicy = new aws.iam.RolePolicy(\"thisRolePolicy\", {\n    role: crossAccountRole.id,\n    policy: thisAwsCrossAccountPolicy.then(thisAwsCrossAccountPolicy =\u003e thisAwsCrossAccountPolicy.json),\n});\nconst thisMwsCredentials = new databricks.MwsCredentials(\"thisMwsCredentials\", {\n    accountId: databricksAccountId,\n    credentialsName: `${local.prefix}-creds`,\n    roleArn: crossAccountRole.arn,\n}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nthis_aws_assume_role_policy = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account_role = aws.iam.Role(\"crossAccountRole\",\n    assume_role_policy=this_aws_assume_role_policy.json,\n    tags=var[\"tags\"])\nthis_aws_cross_account_policy = databricks.get_aws_cross_account_policy()\nthis_role_policy = aws.iam.RolePolicy(\"thisRolePolicy\",\n    role=cross_account_role.id,\n    policy=this_aws_cross_account_policy.json)\nthis_mws_credentials = databricks.MwsCredentials(\"thisMwsCredentials\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{local['prefix']}-creds\",\n    role_arn=cross_account_role.arn,\n    opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var thisAwsAssumeRolePolicy = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccountRole = new Aws.Iam.Role(\"crossAccountRole\", new()\n    {\n        AssumeRolePolicy = thisAwsAssumeRolePolicy.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json),\n        Tags = @var.Tags,\n    });\n\n    var thisAwsCrossAccountPolicy = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var thisRolePolicy = new Aws.Iam.RolePolicy(\"thisRolePolicy\", new()\n    {\n        Role = crossAccountRole.Id,\n        Policy = thisAwsCrossAccountPolicy.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json),\n    });\n\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"thisMwsCredentials\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{local.Prefix}-creds\",\n        RoleArn = crossAccountRole.Arn,\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tthisAwsAssumeRolePolicy, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountRole, err := iam.NewRole(ctx, \"crossAccountRole\", \u0026iam.RoleArgs{\n\t\t\tAssumeRolePolicy: *pulumi.String(thisAwsAssumeRolePolicy.Json),\n\t\t\tTags:             pulumi.Any(_var.Tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisAwsCrossAccountPolicy, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicy(ctx, \"thisRolePolicy\", \u0026iam.RolePolicyArgs{\n\t\t\tRole:   crossAccountRole.ID(),\n\t\t\tPolicy: *pulumi.String(thisAwsCrossAccountPolicy.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsCredentials(ctx, \"thisMwsCredentials\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.String(fmt.Sprintf(\"%v-creds\", local.Prefix)),\n\t\t\tRoleArn:         crossAccountRole.Arn,\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.RolePolicy;\nimport com.pulumi.aws.iam.RolePolicyArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccountRole = new Role(\"crossAccountRole\", RoleArgs.builder()        \n            .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -\u003e getAwsAssumeRolePolicyResult.json()))\n            .tags(var_.tags())\n            .build());\n\n        final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n        var thisRolePolicy = new RolePolicy(\"thisRolePolicy\", RolePolicyArgs.builder()        \n            .role(crossAccountRole.id())\n            .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -\u003e getAwsCrossAccountPolicyResult.json()))\n            .build());\n\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", local.prefix()))\n            .roleArn(crossAccountRole.arn())\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  crossAccountRole:\n    type: aws:iam:Role\n    properties:\n      assumeRolePolicy: ${thisAwsAssumeRolePolicy.json}\n      tags: ${var.tags}\n  thisRolePolicy:\n    type: aws:iam:RolePolicy\n    properties:\n      role: ${crossAccountRole.id}\n      policy: ${thisAwsCrossAccountPolicy.json}\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${local.prefix}-creds\n      roleArn: ${crossAccountRole.arn}\n    options:\n      provider: ${databricks.mws}\nvariables:\n  thisAwsAssumeRolePolicy:\n    fn::invoke:\n      Function: databricks:getAwsAssumeRolePolicy\n      Arguments:\n        externalId: ${databricksAccountId}\n  thisAwsCrossAccountPolicy:\n    fn::invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n"
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "credentialsId",
                "credentialsName",
                "externalId",
                "roleArn"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n",
                    "willReplaceOnChanges": true
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsName",
                "roleArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCredentials resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time of credentials registration\n"
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "(String) identifier of credentials\n"
                    },
                    "credentialsName": {
                        "type": "string",
                        "description": "name of credentials to register\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "roleArn": {
                        "type": "string",
                        "description": "ARN of cross-account role\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCustomerManagedKeys:MwsCustomerManagedKeys": {
            "description": "{{% examples %}}\n## Example Usage\n\n\u003e **Note** If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.\n{{% example %}}\n### Customer-managed key for managed services\n\nYou must configure this during workspace creation\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst current = aws.getCallerIdentity({});\nconst databricksManagedServicesCmk = current.then(current =\u003e aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for control plane managed services\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources: [\"*\"],\n        },\n    ],\n}));\nconst managedServicesCustomerManagedKey = new aws.kms.Key(\"managedServicesCustomerManagedKey\", {policy: databricksManagedServicesCmk.then(databricksManagedServicesCmk =\u003e databricksManagedServicesCmk.json)});\nconst managedServicesCustomerManagedKeyAlias = new aws.kms.Alias(\"managedServicesCustomerManagedKeyAlias\", {targetKeyId: managedServicesCustomerManagedKey.keyId});\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managedServices\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: managedServicesCustomerManagedKey.arn,\n        keyAlias: managedServicesCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ncurrent = aws.get_caller_identity()\ndatabricks_managed_services_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[current.account_id],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for control plane managed services\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources=[\"*\"],\n        ),\n    ])\nmanaged_services_customer_managed_key = aws.kms.Key(\"managedServicesCustomerManagedKey\", policy=databricks_managed_services_cmk.json)\nmanaged_services_customer_managed_key_alias = aws.kms.Alias(\"managedServicesCustomerManagedKeyAlias\", target_key_id=managed_services_customer_managed_key.key_id)\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managedServices\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=managed_services_customer_managed_key.arn,\n        key_alias=managed_services_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var current = Aws.GetCallerIdentity.Invoke();\n\n    var databricksManagedServicesCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.Apply(getCallerIdentityResult =\u003e getCallerIdentityResult.AccountId),\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for control plane managed services\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n        },\n    });\n\n    var managedServicesCustomerManagedKey = new Aws.Kms.Key(\"managedServicesCustomerManagedKey\", new()\n    {\n        Policy = databricksManagedServicesCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var managedServicesCustomerManagedKeyAlias = new Aws.Kms.Alias(\"managedServicesCustomerManagedKeyAlias\", new()\n    {\n        TargetKeyId = managedServicesCustomerManagedKey.KeyId,\n    });\n\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managedServices\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = managedServicesCustomerManagedKey.Arn,\n            KeyAlias = managedServicesCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.AwsFunctions;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var current = AwsFunctions.getCallerIdentity();\n\n        final var databricksManagedServicesCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.applyValue(getCallerIdentityResult -\u003e getCallerIdentityResult.accountId()))\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for control plane managed services\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\")\n                    .resources(\"*\")\n                    .build())\n            .build());\n\n        var managedServicesCustomerManagedKey = new Key(\"managedServicesCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksManagedServicesCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var managedServicesCustomerManagedKeyAlias = new Alias(\"managedServicesCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .targetKeyId(managedServicesCustomerManagedKey.keyId())\n            .build());\n\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(managedServicesCustomerManagedKey.arn())\n                .keyAlias(managedServicesCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  managedServicesCustomerManagedKey:\n    type: aws:kms:Key\n    properties:\n      policy: ${databricksManagedServicesCmk.json}\n  managedServicesCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    properties:\n      targetKeyId: ${managedServicesCustomerManagedKey.keyId}\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${managedServicesCustomerManagedKey.arn}\n        keyAlias: ${managedServicesCustomerManagedKeyAlias.name}\n      useCases:\n        - MANAGED_SERVICES\nvariables:\n  current:\n    fn::invoke:\n      Function: aws:getCallerIdentity\n      Arguments: {}\n  databricksManagedServicesCmk:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for control plane managed services\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n            resources:\n              - '*'\n```\n{{% /example %}}\n{{% example %}}\n### Customer-managed key for workspace storage\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst databricksCrossAccountRole = config.requireObject(\"databricksCrossAccountRole\");\nconst databricksStorageCmk = aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [data.aws_caller_identity.current.account_id],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"Bool\",\n                variable: \"kms:GrantIsForAWSResource\",\n                values: [\"true\"],\n            }],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for EBS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [databricksCrossAccountRole],\n            }],\n            actions: [\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"ForAnyValue:StringLike\",\n                variable: \"kms:ViaService\",\n                values: [\"ec2.*.amazonaws.com\"],\n            }],\n        },\n    ],\n});\nconst storageCustomerManagedKey = new aws.kms.Key(\"storageCustomerManagedKey\", {policy: databricksStorageCmk.then(databricksStorageCmk =\u003e databricksStorageCmk.json)});\nconst storageCustomerManagedKeyAlias = new aws.kms.Alias(\"storageCustomerManagedKeyAlias\", {targetKeyId: storageCustomerManagedKey.keyId});\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: storageCustomerManagedKey.arn,\n        keyAlias: storageCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndatabricks_cross_account_role = config.require_object(\"databricksCrossAccountRole\")\ndatabricks_storage_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[data[\"aws_caller_identity\"][\"current\"][\"account_id\"]],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"Bool\",\n                variable=\"kms:GrantIsForAWSResource\",\n                values=[\"true\"],\n            )],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for EBS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[databricks_cross_account_role],\n            )],\n            actions=[\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"ForAnyValue:StringLike\",\n                variable=\"kms:ViaService\",\n                values=[\"ec2.*.amazonaws.com\"],\n            )],\n        ),\n    ])\nstorage_customer_managed_key = aws.kms.Key(\"storageCustomerManagedKey\", policy=databricks_storage_cmk.json)\nstorage_customer_managed_key_alias = aws.kms.Alias(\"storageCustomerManagedKeyAlias\", target_key_id=storage_customer_managed_key.key_id)\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=storage_customer_managed_key.arn,\n        key_alias=storage_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var databricksCrossAccountRole = config.RequireObject\u003cdynamic\u003e(\"databricksCrossAccountRole\");\n    var databricksStorageCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            data.Aws_caller_identity.Current.Account_id,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                    \"kms:ReEncrypt*\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS (Grants)\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:CreateGrant\",\n                    \"kms:ListGrants\",\n                    \"kms:RevokeGrant\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"Bool\",\n                        Variable = \"kms:GrantIsForAWSResource\",\n                        Values = new[]\n                        {\n                            \"true\",\n                        },\n                    },\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for EBS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            databricksCrossAccountRole,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Decrypt\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:CreateGrant\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"ForAnyValue:StringLike\",\n                        Variable = \"kms:ViaService\",\n                        Values = new[]\n                        {\n                            \"ec2.*.amazonaws.com\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var storageCustomerManagedKey = new Aws.Kms.Key(\"storageCustomerManagedKey\", new()\n    {\n        Policy = databricksStorageCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var storageCustomerManagedKeyAlias = new Aws.Kms.Alias(\"storageCustomerManagedKeyAlias\", new()\n    {\n        TargetKeyId = storageCustomerManagedKey.KeyId,\n    });\n\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = storageCustomerManagedKey.Arn,\n            KeyAlias = storageCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksCrossAccountRole = config.get(\"databricksCrossAccountRole\");\n        final var databricksStorageCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(data.aws_caller_identity().current().account_id())\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\",\n                        \"kms:ReEncrypt*\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS (Grants)\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:CreateGrant\",\n                        \"kms:ListGrants\",\n                        \"kms:RevokeGrant\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"Bool\")\n                        .variable(\"kms:GrantIsForAWSResource\")\n                        .values(\"true\")\n                        .build())\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for EBS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(databricksCrossAccountRole)\n                        .build())\n                    .actions(                    \n                        \"kms:Decrypt\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:CreateGrant\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"ForAnyValue:StringLike\")\n                        .variable(\"kms:ViaService\")\n                        .values(\"ec2.*.amazonaws.com\")\n                        .build())\n                    .build())\n            .build());\n\n        var storageCustomerManagedKey = new Key(\"storageCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksStorageCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var storageCustomerManagedKeyAlias = new Alias(\"storageCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .targetKeyId(storageCustomerManagedKey.keyId())\n            .build());\n\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(storageCustomerManagedKey.arn())\n                .keyAlias(storageCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksCrossAccountRole:\n    type: dynamic\nresources:\n  storageCustomerManagedKey:\n    type: aws:kms:Key\n    properties:\n      policy: ${databricksStorageCmk.json}\n  storageCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    properties:\n      targetKeyId: ${storageCustomerManagedKey.keyId}\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${storageCustomerManagedKey.arn}\n        keyAlias: ${storageCustomerManagedKeyAlias.name}\n      useCases:\n        - STORAGE\nvariables:\n  databricksStorageCmk:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${data.aws_caller_identity.current.account_id}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n              - kms:ReEncrypt*\n              - kms:GenerateDataKey*\n              - kms:DescribeKey\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS (Grants)\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:CreateGrant\n              - kms:ListGrants\n              - kms:RevokeGrant\n            resources:\n              - '*'\n            conditions:\n              - test: Bool\n                variable: kms:GrantIsForAWSResource\n                values:\n                  - 'true'\n          - sid: Allow Databricks to use KMS key for EBS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${databricksCrossAccountRole}\n            actions:\n              - kms:Decrypt\n              - kms:GenerateDataKey*\n              - kms:CreateGrant\n              - kms:DescribeKey\n            resources:\n              - '*'\n            conditions:\n              - test: ForAnyValue:StringLike\n                variable: kms:ViaService\n                values:\n                  - ec2.*.amazonaws.com\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below.\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n"
                }
            },
            "required": [
                "accountId",
                "awsKeyInfo",
                "creationTime",
                "customerManagedKeyId",
                "useCases"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "awsKeyInfo",
                "useCases"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCustomerManagedKeys resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                        "description": "This field is a block and is documented below.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "description": "(String) ID of the encryption key configuration object.\n"
                    },
                    "useCases": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsLogDelivery:MwsLogDelivery": {
            "description": "\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws` for all `databricks_mws_*` resources.\n\n\u003e **Note** This resource has an evolving API, which will change in the upcoming versions of the provider in order to simplify user experience.\n\nMake sure you have authenticated with username and password for Accounts Console. This resource configures the delivery of the two supported log types from Databricks workspaces: [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n\nYou cannot delete a log delivery configuration, but you can disable it when you no longer need it. This fact is important because there is a limit to the number of enabled log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.\n\n## Billable Usage\n\nCSV files are delivered to `\u003cdelivery_path_prefix\u003e/billable-usage/csv/` and are named `workspaceId=\u003cworkspace-id\u003e-usageMonth=\u003cmonth\u003e.csv`, which are delivered daily by overwriting the month's CSV file for each workspace. Format of CSV file, as well as some usage examples, can be found [here](https://docs.databricks.com/administration-guide/account-settings/usage.html#download-usage-as-a-csv-file).\n\nCommon processing scenario is to apply [cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html), that could be enforced by setting custom_tags on a cluster or through cluster policy. Report contains `clusterId` field, that could be joined with data from AWS [cost and usage reports](https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html), that can be joined with `user:ClusterId` tag from AWS usage report.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst usageLogs = new databricks.MwsLogDelivery(\"usageLogs\", {\n    accountId: _var.databricks_account_id,\n    credentialsId: databricks_mws_credentials.log_writer.credentials_id,\n    storageConfigurationId: databricks_mws_storage_configurations.log_bucket.storage_configuration_id,\n    deliveryPathPrefix: \"billable-usage\",\n    configName: \"Usage Logs\",\n    logType: \"BILLABLE_USAGE\",\n    outputFormat: \"CSV\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusage_logs = databricks.MwsLogDelivery(\"usageLogs\",\n    account_id=var[\"databricks_account_id\"],\n    credentials_id=databricks_mws_credentials[\"log_writer\"][\"credentials_id\"],\n    storage_configuration_id=databricks_mws_storage_configurations[\"log_bucket\"][\"storage_configuration_id\"],\n    delivery_path_prefix=\"billable-usage\",\n    config_name=\"Usage Logs\",\n    log_type=\"BILLABLE_USAGE\",\n    output_format=\"CSV\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var usageLogs = new Databricks.MwsLogDelivery(\"usageLogs\", new()\n    {\n        AccountId = @var.Databricks_account_id,\n        CredentialsId = databricks_mws_credentials.Log_writer.Credentials_id,\n        StorageConfigurationId = databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id,\n        DeliveryPathPrefix = \"billable-usage\",\n        ConfigName = \"Usage Logs\",\n        LogType = \"BILLABLE_USAGE\",\n        OutputFormat = \"CSV\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"usageLogs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(_var.Databricks_account_id),\n\t\t\tCredentialsId:          pulumi.Any(databricks_mws_credentials.Log_writer.Credentials_id),\n\t\t\tStorageConfigurationId: pulumi.Any(databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"billable-usage\"),\n\t\t\tConfigName:             pulumi.String(\"Usage Logs\"),\n\t\t\tLogType:                pulumi.String(\"BILLABLE_USAGE\"),\n\t\t\tOutputFormat:           pulumi.String(\"CSV\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var usageLogs = new MwsLogDelivery(\"usageLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(var_.databricks_account_id())\n            .credentialsId(databricks_mws_credentials.log_writer().credentials_id())\n            .storageConfigurationId(databricks_mws_storage_configurations.log_bucket().storage_configuration_id())\n            .deliveryPathPrefix(\"billable-usage\")\n            .configName(\"Usage Logs\")\n            .logType(\"BILLABLE_USAGE\")\n            .outputFormat(\"CSV\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  usageLogs:\n    type: databricks:MwsLogDelivery\n    properties:\n      accountId: ${var.databricks_account_id}\n      credentialsId: ${databricks_mws_credentials.log_writer.credentials_id}\n      storageConfigurationId: ${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}\n      deliveryPathPrefix: billable-usage\n      configName: Usage Logs\n      logType: BILLABLE_USAGE\n      outputFormat: CSV\n```\n\n## Audit Logs\n\nJSON files with [static schema](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#audit-log-schema) are delivered to `\u003cdelivery_path_prefix\u003e/workspaceId=\u003cworkspaceId\u003e/date=\u003cyyyy-mm-dd\u003e/auditlogs_\u003cinternal-id\u003e.json`. Logs are available within 15 minutes of activation for audit logs. New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. Sometimes data may arrive later than 15 minutes. Databricks can overwrite the delivered log files in your bucket at any time. If a file is overwritten, the existing content remains, but there may be additional lines for more auditable events. Overwriting ensures exactly-once semantics without requiring read or delete access to your account.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auditLogs = new databricks.MwsLogDelivery(\"auditLogs\", {\n    accountId: _var.databricks_account_id,\n    credentialsId: databricks_mws_credentials.log_writer.credentials_id,\n    storageConfigurationId: databricks_mws_storage_configurations.log_bucket.storage_configuration_id,\n    deliveryPathPrefix: \"audit-logs\",\n    configName: \"Audit Logs\",\n    logType: \"AUDIT_LOGS\",\n    outputFormat: \"JSON\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naudit_logs = databricks.MwsLogDelivery(\"auditLogs\",\n    account_id=var[\"databricks_account_id\"],\n    credentials_id=databricks_mws_credentials[\"log_writer\"][\"credentials_id\"],\n    storage_configuration_id=databricks_mws_storage_configurations[\"log_bucket\"][\"storage_configuration_id\"],\n    delivery_path_prefix=\"audit-logs\",\n    config_name=\"Audit Logs\",\n    log_type=\"AUDIT_LOGS\",\n    output_format=\"JSON\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auditLogs = new Databricks.MwsLogDelivery(\"auditLogs\", new()\n    {\n        AccountId = @var.Databricks_account_id,\n        CredentialsId = databricks_mws_credentials.Log_writer.Credentials_id,\n        StorageConfigurationId = databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id,\n        DeliveryPathPrefix = \"audit-logs\",\n        ConfigName = \"Audit Logs\",\n        LogType = \"AUDIT_LOGS\",\n        OutputFormat = \"JSON\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"auditLogs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(_var.Databricks_account_id),\n\t\t\tCredentialsId:          pulumi.Any(databricks_mws_credentials.Log_writer.Credentials_id),\n\t\t\tStorageConfigurationId: pulumi.Any(databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"audit-logs\"),\n\t\t\tConfigName:             pulumi.String(\"Audit Logs\"),\n\t\t\tLogType:                pulumi.String(\"AUDIT_LOGS\"),\n\t\t\tOutputFormat:           pulumi.String(\"JSON\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auditLogs = new MwsLogDelivery(\"auditLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(var_.databricks_account_id())\n            .credentialsId(databricks_mws_credentials.log_writer().credentials_id())\n            .storageConfigurationId(databricks_mws_storage_configurations.log_bucket().storage_configuration_id())\n            .deliveryPathPrefix(\"audit-logs\")\n            .configName(\"Audit Logs\")\n            .logType(\"AUDIT_LOGS\")\n            .outputFormat(\"JSON\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auditLogs:\n    type: databricks:MwsLogDelivery\n    properties:\n      accountId: ${var.databricks_account_id}\n      credentialsId: ${databricks_mws_credentials.log_writer.credentials_id}\n      storageConfigurationId: ${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}\n      deliveryPathPrefix: audit-logs\n      configName: Audit Logs\n      logType: AUDIT_LOGS\n      outputFormat: JSON\n```\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n"
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n"
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n"
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n"
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n"
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n"
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n"
                }
            },
            "required": [
                "accountId",
                "configId",
                "credentialsId",
                "deliveryStartTime",
                "logType",
                "outputFormat",
                "status",
                "storageConfigurationId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "willReplaceOnChanges": true
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n",
                    "willReplaceOnChanges": true
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                    "willReplaceOnChanges": true
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                    "willReplaceOnChanges": true
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsId",
                "logType",
                "outputFormat",
                "storageConfigurationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsLogDelivery resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "configId": {
                        "type": "string",
                        "description": "Databricks log delivery configuration ID.\n",
                        "willReplaceOnChanges": true
                    },
                    "configName": {
                        "type": "string",
                        "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                        "willReplaceOnChanges": true
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryPathPrefix": {
                        "type": "string",
                        "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryStartTime": {
                        "type": "string",
                        "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                        "willReplaceOnChanges": true
                    },
                    "logType": {
                        "type": "string",
                        "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "outputFormat": {
                        "type": "string",
                        "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceIdsFilters": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        },
                        "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworks:MwsNetworks": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n"
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n"
                },
                "vpcId": {
                    "type": "string",
                    "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n"
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "errorMessages",
                "networkId",
                "networkName",
                "vpcEndpoints",
                "vpcStatus",
                "workspaceId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n",
                    "willReplaceOnChanges": true
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n",
                    "willReplaceOnChanges": true
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n",
                    "willReplaceOnChanges": true
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "networkName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworks resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "errorMessages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                        }
                    },
                    "gcpNetworkInfo": {
                        "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                        "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n",
                        "willReplaceOnChanges": true
                    },
                    "networkId": {
                        "type": "string",
                        "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                    },
                    "networkName": {
                        "type": "string",
                        "description": "name under which this network is registered\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "subnetIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "vpcEndpoints": {
                        "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                        "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcStatus": {
                        "type": "string",
                        "description": "(String) VPC attachment status\n"
                    },
                    "workspaceId": {
                        "type": "integer",
                        "description": "(Integer) id of associated workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPermissionAssignment:MwsPermissionAssignment": {
            "description": "These resources are invoked in the account context. Permission Assignment Account API endpoints are restricted to account admins. Provider must have `account_id` attribute configured. Account Id that could be found in the bottom left corner of Accounts Console\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nIn account context, adding account-level group to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dataEng = new databricks.Group(\"dataEng\", {});\nconst addAdminGroup = new databricks.MwsPermissionAssignment(\"addAdminGroup\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: dataEng.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndata_eng = databricks.Group(\"dataEng\")\nadd_admin_group = databricks.MwsPermissionAssignment(\"addAdminGroup\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=data_eng.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dataEng = new Databricks.Group(\"dataEng\");\n\n    var addAdminGroup = new Databricks.MwsPermissionAssignment(\"addAdminGroup\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = dataEng.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdataEng, err := databricks.NewGroup(ctx, \"dataEng\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addAdminGroup\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: dataEng.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dataEng = new Group(\"dataEng\");\n\n        var addAdminGroup = new MwsPermissionAssignment(\"addAdminGroup\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(dataEng.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dataEng:\n    type: databricks:Group\n  addAdminGroup:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${dataEng.id}\n      permissions:\n        - ADMIN\n```\n\nIn account context, adding account-level user to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst addUser = new databricks.MwsPermissionAssignment(\"addUser\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: me.id,\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nadd_user = databricks.MwsPermissionAssignment(\"addUser\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.MwsPermissionAssignment(\"addUser\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = me.Id,\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addUser\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: me.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new MwsPermissionAssignment(\"addUser\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(me.id())\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  addUser:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${me.id}\n      permissions:\n        - USER\n```\n\nIn account context, adding account-level service principal to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\nconst addAdminSpn = new databricks.MwsPermissionAssignment(\"addAdminSpn\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: sp.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.MwsPermissionAssignment(\"addAdminSpn\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.MwsPermissionAssignment(\"addAdminSpn\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = sp.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addAdminSpn\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: sp.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new MwsPermissionAssignment(\"addAdminSpn\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(sp.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n  addAdminSpn:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.PermissionAssignment to manage permission assignment from a workspace context\n\n\n## Import\n\nThe resource `databricks_mws_permission_assignment` can be imported using the workspace id and principal id bash\n\n```sh\n $ pulumi import databricks:index/mwsPermissionAssignment:MwsPermissionAssignment this \"workspace_id|principal_id\"\n```\n\n ",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n"
                },
                "principalId": {
                    "type": "integer",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "Databricks workspace ID.\n"
                }
            },
            "required": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "Databricks workspace ID.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "integer",
                        "description": "Databricks workspace ID.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPrivateAccessSettings:MwsPrivateAccessSettings": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of Private Access Settings\n"
                }
            },
            "required": [
                "privateAccessSettingsId",
                "privateAccessSettingsName",
                "region",
                "status"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of Private Access Settings\n"
                }
            },
            "requiredInputs": [
                "privateAccessSettingsName",
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPrivateAccessSettings resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                    },
                    "allowedVpcEndpointIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                    },
                    "privateAccessLevel": {
                        "type": "string",
                        "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                    },
                    "privateAccessSettingsName": {
                        "type": "string",
                        "description": "Name of Private Access Settings in Databricks Account\n"
                    },
                    "publicAccessEnabled": {
                        "type": "boolean",
                        "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of Private Access Settings\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsStorageConfigurations:MwsStorageConfigurations": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst rootStorageBucket = new aws.s3.BucketV2(\"rootStorageBucket\", {\n    acl: \"private\",\n    versionings: [{\n        enabled: false,\n    }],\n});\nconst _this = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${_var.prefix}-storage`,\n    bucketName: rootStorageBucket.bucket,\n}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nroot_storage_bucket = aws.s3.BucketV2(\"rootStorageBucket\",\n    acl=\"private\",\n    versionings=[aws.s3.BucketV2VersioningArgs(\n        enabled=False,\n    )])\nthis = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{var['prefix']}-storage\",\n    bucket_name=root_storage_bucket.bucket,\n    opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var rootStorageBucket = new Aws.S3.BucketV2(\"rootStorageBucket\", new()\n    {\n        Acl = \"private\",\n        Versionings = new[]\n        {\n            new Aws.S3.Inputs.BucketV2VersioningArgs\n            {\n                Enabled = false,\n            },\n        },\n    });\n\n    var @this = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{@var.Prefix}-storage\",\n        BucketName = rootStorageBucket.Bucket,\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\trootStorageBucket, err := s3.NewBucketV2(ctx, \"rootStorageBucket\", \u0026s3.BucketV2Args{\n\t\t\tAcl: pulumi.String(\"private\"),\n\t\t\tVersionings: s3.BucketV2VersioningArray{\n\t\t\t\t\u0026s3.BucketV2VersioningArgs{\n\t\t\t\t\tEnabled: pulumi.Bool(false),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.String(fmt.Sprintf(\"%v-storage\", _var.Prefix)),\n\t\t\tBucketName:               rootStorageBucket.Bucket,\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.aws.s3.inputs.BucketV2VersioningArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var rootStorageBucket = new BucketV2(\"rootStorageBucket\", BucketV2Args.builder()        \n            .acl(\"private\")\n            .versionings(BucketV2VersioningArgs.builder()\n                .enabled(false)\n                .build())\n            .build());\n\n        var this_ = new MwsStorageConfigurations(\"this\", MwsStorageConfigurationsArgs.builder()        \n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", var_.prefix()))\n            .bucketName(rootStorageBucket.bucket())\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  rootStorageBucket:\n    type: aws:s3:BucketV2\n    properties:\n      acl: private\n      versionings:\n        - enabled: false\n  this:\n    type: databricks:MwsStorageConfigurations\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${var.prefix}-storage\n      bucketName: ${rootStorageBucket.bucket}\n    options:\n      provider: ${databricks.mws}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with PrivateLink guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n"
                }
            },
            "required": [
                "accountId",
                "bucketName",
                "creationTime",
                "storageConfigurationId",
                "storageConfigurationName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n",
                    "willReplaceOnChanges": true
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "bucketName",
                "storageConfigurationName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsStorageConfigurations resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "description": "name of AWS S3 bucket\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                    },
                    "storageConfigurationName": {
                        "type": "string",
                        "description": "name under which this storage configuration is stored\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsVpcEndpoint:MwsVpcEndpoint": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n"
                }
            },
            "required": [
                "awsAccountId",
                "awsEndpointServiceId",
                "awsVpcEndpointId",
                "region",
                "state",
                "useCase",
                "vpcEndpointId",
                "vpcEndpointName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n",
                    "willReplaceOnChanges": true
                },
                "state": {
                    "type": "string",
                    "description": "State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "awsVpcEndpointId",
                "region",
                "vpcEndpointName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsVpcEndpoint resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsEndpointServiceId": {
                        "type": "string",
                        "description": "The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                    },
                    "awsVpcEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "state": {
                        "type": "string",
                        "description": "State of VPC Endpoint\n"
                    },
                    "useCase": {
                        "type": "string"
                    },
                    "vpcEndpointId": {
                        "type": "string",
                        "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                    },
                    "vpcEndpointName": {
                        "type": "string",
                        "description": "Name of VPC Endpoint in Databricks Account\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsWorkspaces:MwsWorkspaces": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC\n"
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead"
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n"
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo"
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig"
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n"
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean"
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet\n"
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string"
                },
                "privateAccessSettingsId": {
                    "type": "string"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration\n"
                },
                "storageCustomerManagedKeyId": {
                    "type": "string"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "integer"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI\n"
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "required": [
                "accountId",
                "cloud",
                "creationTime",
                "pricingTier",
                "workspaceId",
                "workspaceName",
                "workspaceStatus",
                "workspaceStatusMessage",
                "workspaceUrl"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC\n",
                    "willReplaceOnChanges": true
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                    "willReplaceOnChanges": true
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                    "willReplaceOnChanges": true
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                    "willReplaceOnChanges": true
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                    "willReplaceOnChanges": true
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                    "willReplaceOnChanges": true
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet\n",
                    "willReplaceOnChanges": true
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string"
                },
                "privateAccessSettingsId": {
                    "type": "string"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration\n",
                    "willReplaceOnChanges": true
                },
                "storageCustomerManagedKeyId": {
                    "type": "string"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "integer"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI\n",
                    "willReplaceOnChanges": true
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "workspaceName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsWorkspaces resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "awsRegion": {
                        "type": "string",
                        "description": "region of VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string"
                    },
                    "cloudResourceContainer": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                        "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time when workspace was created\n"
                    },
                    "credentialsId": {
                        "type": "string"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                        "willReplaceOnChanges": true
                    },
                    "deploymentName": {
                        "type": "string",
                        "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalCustomerInfo": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                        "willReplaceOnChanges": true
                    },
                    "gcpManagedNetworkConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                        "willReplaceOnChanges": true
                    },
                    "gkeConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                        "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                        "willReplaceOnChanges": true
                    },
                    "isNoPublicIpEnabled": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "location": {
                        "type": "string",
                        "description": "region of the subnet\n",
                        "willReplaceOnChanges": true
                    },
                    "managedServicesCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                    },
                    "networkId": {
                        "type": "string",
                        "description": "`network_id` from networks.\n"
                    },
                    "pricingTier": {
                        "type": "string"
                    },
                    "privateAccessSettingsId": {
                        "type": "string"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "`storage_configuration_id` from storage configuration\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCustomerManagedKeyId": {
                        "type": "string"
                    },
                    "token": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                    },
                    "workspaceId": {
                        "type": "integer"
                    },
                    "workspaceName": {
                        "type": "string",
                        "description": "name of the workspace, will appear on UI\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceStatus": {
                        "type": "string",
                        "description": "(String) workspace status\n"
                    },
                    "workspaceStatusMessage": {
                        "type": "string",
                        "description": "(String) updates on workspace status\n"
                    },
                    "workspaceUrl": {
                        "type": "string",
                        "description": "(String) URL of the workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/notebook:Notebook": {
            "description": "\n\n\n## Import\n\nThe resource notebook can be imported using notebook path bash\n\n```sh\n $ pulumi import databricks:index/notebook:Notebook this /path/to/notebook\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n",
                    "deprecationMessage": "Use id argument to retrieve object id"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the notebook\n"
                }
            },
            "required": [
                "objectId",
                "objectType",
                "path",
                "url"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n",
                    "deprecationMessage": "Use id argument to retrieve object id"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Notebook resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "language": {
                        "type": "string",
                        "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a NOTEBOOK\n",
                        "deprecationMessage": "Use id argument to retrieve object id"
                    },
                    "objectType": {
                        "type": "string",
                        "deprecationMessage": "Always is a notebook"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the notebook\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/oboToken:OboToken": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "applicationId",
                "tokenValue"
            ],
            "inputProperties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n",
                    "willReplaceOnChanges": true
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "applicationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OboToken resources.\n",
                "properties": {
                    "applicationId": {
                        "type": "string",
                        "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Comment that describes the purpose of the token.\n",
                        "willReplaceOnChanges": true
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissionAssignment:PermissionAssignment": {
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "principalId": {
                    "type": "integer"
                }
            },
            "required": [
                "permissions",
                "principalId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering PermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissions:Permissions": {
            "description": "\n\n\n## Import\n\n### Import Example Configuration filehcl resource \"databricks_mlflow_model\" \"model\" {\n\n name\n\n\n\n\n\n\n\n= \"example_model\"\n\n description = \"MLflow registered model\" } resource \"databricks_permissions\" \"model_usage\" {\n\n registered_model_id = databricks_mlflow_model.model.registered_model_id\n\n access_control {\n\n\n\n group_name\n\n\n\n\n\n = \"users\"\n\n\n\n permission_level = \"CAN_READ\"\n\n } } Import commandbash\n\n```sh\n $ pulumi import databricks:index/permissions:Permissions model_usage /registered-models/\u003cregistered_model_id\u003e\n```\n\n ",
            "properties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string",
                    "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n"
                },
                "clusterId": {
                    "type": "string",
                    "description": "cluster id\n"
                },
                "clusterPolicyId": {
                    "type": "string",
                    "description": "cluster policy id\n"
                },
                "directoryId": {
                    "type": "string",
                    "description": "directory id\n"
                },
                "directoryPath": {
                    "type": "string",
                    "description": "path of directory\n"
                },
                "experimentId": {
                    "type": "string",
                    "description": "MLflow experiment id\n"
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "instance pool id\n"
                },
                "jobId": {
                    "type": "string",
                    "description": "job id\n"
                },
                "notebookId": {
                    "type": "string",
                    "description": "ID of notebook within workspace\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "path of notebook\n"
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "pipeline id\n"
                },
                "registeredModelId": {
                    "type": "string",
                    "description": "MLflow registered model id\n"
                },
                "repoId": {
                    "type": "string",
                    "description": "repo id\n"
                },
                "repoPath": {
                    "type": "string",
                    "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n"
                },
                "sqlAlertId": {
                    "type": "string",
                    "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n"
                },
                "sqlDashboardId": {
                    "type": "string",
                    "description": "SQL dashboard id\n"
                },
                "sqlEndpointId": {
                    "type": "string",
                    "description": "SQL endpoint id\n"
                },
                "sqlQueryId": {
                    "type": "string",
                    "description": "SQL query id\n"
                }
            },
            "required": [
                "accessControls",
                "objectType"
            ],
            "inputProperties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string",
                    "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "description": "cluster id\n",
                    "willReplaceOnChanges": true
                },
                "clusterPolicyId": {
                    "type": "string",
                    "description": "cluster policy id\n",
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "directory id\n",
                    "willReplaceOnChanges": true
                },
                "directoryPath": {
                    "type": "string",
                    "description": "path of directory\n",
                    "willReplaceOnChanges": true
                },
                "experimentId": {
                    "type": "string",
                    "description": "MLflow experiment id\n",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "instance pool id\n",
                    "willReplaceOnChanges": true
                },
                "jobId": {
                    "type": "string",
                    "description": "job id\n",
                    "willReplaceOnChanges": true
                },
                "notebookId": {
                    "type": "string",
                    "description": "ID of notebook within workspace\n",
                    "willReplaceOnChanges": true
                },
                "notebookPath": {
                    "type": "string",
                    "description": "path of notebook\n",
                    "willReplaceOnChanges": true
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "pipeline id\n",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string",
                    "description": "MLflow registered model id\n",
                    "willReplaceOnChanges": true
                },
                "repoId": {
                    "type": "string",
                    "description": "repo id\n",
                    "willReplaceOnChanges": true
                },
                "repoPath": {
                    "type": "string",
                    "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n",
                    "willReplaceOnChanges": true
                },
                "sqlAlertId": {
                    "type": "string",
                    "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n",
                    "willReplaceOnChanges": true
                },
                "sqlDashboardId": {
                    "type": "string",
                    "description": "SQL dashboard id\n",
                    "willReplaceOnChanges": true
                },
                "sqlEndpointId": {
                    "type": "string",
                    "description": "SQL endpoint id\n",
                    "willReplaceOnChanges": true
                },
                "sqlQueryId": {
                    "type": "string",
                    "description": "SQL query id\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accessControls"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Permissions resources.\n",
                "properties": {
                    "accessControls": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                        }
                    },
                    "authorization": {
                        "type": "string",
                        "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "description": "cluster id\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterPolicyId": {
                        "type": "string",
                        "description": "cluster policy id\n",
                        "willReplaceOnChanges": true
                    },
                    "directoryId": {
                        "type": "string",
                        "description": "directory id\n",
                        "willReplaceOnChanges": true
                    },
                    "directoryPath": {
                        "type": "string",
                        "description": "path of directory\n",
                        "willReplaceOnChanges": true
                    },
                    "experimentId": {
                        "type": "string",
                        "description": "MLflow experiment id\n",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "instance pool id\n",
                        "willReplaceOnChanges": true
                    },
                    "jobId": {
                        "type": "string",
                        "description": "job id\n",
                        "willReplaceOnChanges": true
                    },
                    "notebookId": {
                        "type": "string",
                        "description": "ID of notebook within workspace\n",
                        "willReplaceOnChanges": true
                    },
                    "notebookPath": {
                        "type": "string",
                        "description": "path of notebook\n",
                        "willReplaceOnChanges": true
                    },
                    "objectType": {
                        "type": "string",
                        "description": "type of permissions.\n"
                    },
                    "pipelineId": {
                        "type": "string",
                        "description": "pipeline id\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string",
                        "description": "MLflow registered model id\n",
                        "willReplaceOnChanges": true
                    },
                    "repoId": {
                        "type": "string",
                        "description": "repo id\n",
                        "willReplaceOnChanges": true
                    },
                    "repoPath": {
                        "type": "string",
                        "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlAlertId": {
                        "type": "string",
                        "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlDashboardId": {
                        "type": "string",
                        "description": "SQL dashboard id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlEndpointId": {
                        "type": "string",
                        "description": "SQL endpoint id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlQueryId": {
                        "type": "string",
                        "description": "SQL query id\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/pipeline:Pipeline": {
            "description": "Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dltDemo = new databricks.Notebook(\"dltDemo\", {});\n//...\nconst _this = new databricks.Pipeline(\"this\", {\n    storage: \"/test/first-pipeline\",\n    configuration: {\n        key1: \"value1\",\n        key2: \"value2\",\n    },\n    clusters: [\n        {\n            label: \"default\",\n            numWorkers: 2,\n            customTags: {\n                cluster_type: \"default\",\n            },\n        },\n        {\n            label: \"maintenance\",\n            numWorkers: 1,\n            customTags: {\n                cluster_type: \"maintenance\",\n            },\n        },\n    ],\n    libraries: [{\n        notebook: {\n            path: dltDemo.id,\n        },\n    }],\n    continuous: false,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndlt_demo = databricks.Notebook(\"dltDemo\")\n#...\nthis = databricks.Pipeline(\"this\",\n    storage=\"/test/first-pipeline\",\n    configuration={\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n    },\n    clusters=[\n        databricks.PipelineClusterArgs(\n            label=\"default\",\n            num_workers=2,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        ),\n        databricks.PipelineClusterArgs(\n            label=\"maintenance\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"maintenance\",\n            },\n        ),\n    ],\n    libraries=[databricks.PipelineLibraryArgs(\n        notebook=databricks.PipelineLibraryNotebookArgs(\n            path=dlt_demo.id,\n        ),\n    )],\n    continuous=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dltDemo = new Databricks.Notebook(\"dltDemo\");\n\n    //...\n    var @this = new Databricks.Pipeline(\"this\", new()\n    {\n        Storage = \"/test/first-pipeline\",\n        Configuration = \n        {\n            { \"key1\", \"value1\" },\n            { \"key2\", \"value2\" },\n        },\n        Clusters = new[]\n        {\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"default\",\n                NumWorkers = 2,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"default\" },\n                },\n            },\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"maintenance\",\n                NumWorkers = 1,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"maintenance\" },\n                },\n            },\n        },\n        Libraries = new[]\n        {\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs\n                {\n                    Path = dltDemo.Id,\n                },\n            },\n        },\n        Continuous = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdltDemo, err := databricks.NewNotebook(ctx, \"dltDemo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPipeline(ctx, \"this\", \u0026databricks.PipelineArgs{\n\t\t\tStorage: pulumi.String(\"/test/first-pipeline\"),\n\t\t\tConfiguration: pulumi.AnyMap{\n\t\t\t\t\"key1\": pulumi.Any(\"value1\"),\n\t\t\t\t\"key2\": pulumi.Any(\"value2\"),\n\t\t\t},\n\t\t\tClusters: databricks.PipelineClusterArray{\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"default\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(2),\n\t\t\t\t\tCustomTags: pulumi.AnyMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"maintenance\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(1),\n\t\t\t\t\tCustomTags: pulumi.AnyMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"maintenance\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tLibraries: databricks.PipelineLibraryArray{\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tNotebook: \u0026databricks.PipelineLibraryNotebookArgs{\n\t\t\t\t\t\tPath: dltDemo.ID(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContinuous: pulumi.Bool(false),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.Pipeline;\nimport com.pulumi.databricks.PipelineArgs;\nimport com.pulumi.databricks.inputs.PipelineClusterArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dltDemo = new Notebook(\"dltDemo\");\n\n        var this_ = new Pipeline(\"this\", PipelineArgs.builder()        \n            .storage(\"/test/first-pipeline\")\n            .configuration(Map.ofEntries(\n                Map.entry(\"key1\", \"value1\"),\n                Map.entry(\"key2\", \"value2\")\n            ))\n            .clusters(            \n                PipelineClusterArgs.builder()\n                    .label(\"default\")\n                    .numWorkers(2)\n                    .customTags(Map.of(\"cluster_type\", \"default\"))\n                    .build(),\n                PipelineClusterArgs.builder()\n                    .label(\"maintenance\")\n                    .numWorkers(1)\n                    .customTags(Map.of(\"cluster_type\", \"maintenance\"))\n                    .build())\n            .libraries(PipelineLibraryArgs.builder()\n                .notebook(PipelineLibraryNotebookArgs.builder()\n                    .path(dltDemo.id())\n                    .build())\n                .build())\n            .continuous(false)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dltDemo:\n    type: databricks:Notebook\n  this:\n    type: databricks:Pipeline\n    properties:\n      storage: /test/first-pipeline\n      configuration:\n        key1: value1\n        key2: value2\n      clusters:\n        - label: default\n          numWorkers: 2\n          customTags:\n            cluster_type: default\n        - label: maintenance\n          numWorkers: 1\n          customTags:\n            cluster_type: maintenance\n      libraries:\n        - notebook:\n            path: ${dltDemo.id}\n      continuous: false\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n\n\n## Import\n\nThe resource job can be imported using the id of the pipeline bash\n\n```sh\n $ pulumi import databricks:index/pipeline:Pipeline this \u003cpipeline-id\u003e\n```\n\n ",
            "properties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n"
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                },
                "url": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "url"
            ],
            "inputProperties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n",
                    "willReplaceOnChanges": true
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Pipeline resources.\n",
                "properties": {
                    "allowDuplicateNames": {
                        "type": "boolean"
                    },
                    "channel": {
                        "type": "string",
                        "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                    },
                    "clusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                        },
                        "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                    },
                    "configuration": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                    },
                    "continuous": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                    },
                    "development": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                    },
                    "edition": {
                        "type": "string",
                        "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                    },
                    "filters": {
                        "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                        },
                        "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                    },
                    "storage": {
                        "type": "string",
                        "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n",
                        "willReplaceOnChanges": true
                    },
                    "target": {
                        "type": "string",
                        "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                    },
                    "url": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/recipient:Recipient": {
            "description": "Within a metastore, Unity Catalog provides the ability to create a recipient to attach delta shares to.\n\nA `databricks.Recipient` is contained within databricks.Metastore and can have permissions to `SELECT` from a list of shares.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n### Databricks Sharing with non databricks recipient\n\nSetting `authentication_type` type to `TOKEN` creates a temporary url to download a credentials file. This is used to\nauthenticate to the sharing server to access data. This is for when the recipient is not using Databricks.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as random from \"@pulumi/random\";\n\nconst db2opensharecode = new random.RandomPassword(\"db2opensharecode\", {\n    length: 16,\n    special: true,\n});\nconst current = databricks.getCurrentUser({});\nconst db2open = new databricks.Recipient(\"db2open\", {\n    comment: \"made by terraform\",\n    authenticationType: \"TOKEN\",\n    sharingCode: db2opensharecode.result,\n    ipAccessList: {\n        allowedIpAddresses: [],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_random as random\n\ndb2opensharecode = random.RandomPassword(\"db2opensharecode\",\n    length=16,\n    special=True)\ncurrent = databricks.get_current_user()\ndb2open = databricks.Recipient(\"db2open\",\n    comment=\"made by terraform\",\n    authentication_type=\"TOKEN\",\n    sharing_code=db2opensharecode.result,\n    ip_access_list=databricks.RecipientIpAccessListArgs(\n        allowed_ip_addresses=[],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Random = Pulumi.Random;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var db2opensharecode = new Random.RandomPassword(\"db2opensharecode\", new()\n    {\n        Length = 16,\n        Special = true,\n    });\n\n    var current = Databricks.GetCurrentUser.Invoke();\n\n    var db2open = new Databricks.Recipient(\"db2open\", new()\n    {\n        Comment = \"made by terraform\",\n        AuthenticationType = \"TOKEN\",\n        SharingCode = db2opensharecode.Result,\n        IpAccessList = new Databricks.Inputs.RecipientIpAccessListArgs\n        {\n            AllowedIpAddresses = new[] {},\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-random/sdk/v4/go/random\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdb2opensharecode, err := random.NewRandomPassword(ctx, \"db2opensharecode\", \u0026random.RandomPasswordArgs{\n\t\t\tLength:  pulumi.Int(16),\n\t\t\tSpecial: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewRecipient(ctx, \"db2open\", \u0026databricks.RecipientArgs{\n\t\t\tComment:            pulumi.String(\"made by terraform\"),\n\t\t\tAuthenticationType: pulumi.String(\"TOKEN\"),\n\t\t\tSharingCode:        db2opensharecode.Result,\n\t\t\tIpAccessList: \u0026databricks.RecipientIpAccessListArgs{\n\t\t\t\tAllowedIpAddresses: pulumi.StringArray{},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.random.RandomPassword;\nimport com.pulumi.random.RandomPasswordArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.Recipient;\nimport com.pulumi.databricks.RecipientArgs;\nimport com.pulumi.databricks.inputs.RecipientIpAccessListArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var db2opensharecode = new RandomPassword(\"db2opensharecode\", RandomPasswordArgs.builder()        \n            .length(16)\n            .special(true)\n            .build());\n\n        final var current = DatabricksFunctions.getCurrentUser();\n\n        var db2open = new Recipient(\"db2open\", RecipientArgs.builder()        \n            .comment(\"made by terraform\")\n            .authenticationType(\"TOKEN\")\n            .sharingCode(db2opensharecode.result())\n            .ipAccessList(RecipientIpAccessListArgs.builder()\n                .allowedIpAddresses()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  db2opensharecode:\n    type: random:RandomPassword\n    properties:\n      length: 16\n      special: true\n  db2open:\n    type: databricks:Recipient\n    properties:\n      comment: made by terraform\n      authenticationType: TOKEN\n      sharingCode: ${db2opensharecode.result}\n      ipAccessList:\n        allowedIpAddresses: []\nvariables:\n  current:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Grants to manage Delta Sharing permissions.\n* databricks.getShares to read existing Delta Sharing shares.\n",
            "properties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when authentication_type is DATABRICKS.\n"
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "The one-time sharing code provided by the data recipient.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n"
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens.\n"
                }
            },
            "required": [
                "authenticationType",
                "name",
                "tokens"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when authentication_type is DATABRICKS.\n",
                    "willReplaceOnChanges": true
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "The one-time sharing code provided by the data recipient.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens.\n"
                }
            },
            "requiredInputs": [
                "authenticationType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Recipient resources.\n",
                "properties": {
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the recipient.\n"
                    },
                    "dataRecipientGlobalMetastoreId": {
                        "type": "string",
                        "description": "Required when authentication_type is DATABRICKS.\n",
                        "willReplaceOnChanges": true
                    },
                    "ipAccessList": {
                        "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                        "description": "The one-time sharing code provided by the data recipient.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of recipient. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "sharingCode": {
                        "type": "string",
                        "description": "The one-time sharing code provided by the data recipient.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "tokens": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                        },
                        "description": "List of Recipient Tokens.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/repo:Repo": {
            "description": "\n\n\n## Import\n\nThe resource Repo can be imported using the Repo ID (obtained via UI or using API) bash\n\n```sh\n $ pulumi import databricks:index/repo:Repo this repo_id\n```\n\n ",
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n"
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n"
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n"
                }
            },
            "required": [
                "branch",
                "commitHash",
                "gitProvider",
                "path",
                "url"
            ],
            "inputProperties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                    "willReplaceOnChanges": true
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Repo resources.\n",
                "properties": {
                    "branch": {
                        "type": "string",
                        "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                    },
                    "commitHash": {
                        "type": "string",
                        "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "sparseCheckout": {
                        "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                        "willReplaceOnChanges": true
                    },
                    "tag": {
                        "type": "string",
                        "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/schema:Schema": {
            "description": "Within a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, Databases (also called Schemas), and Tables / Views.\n\nA `databricks.Schema` is contained within databricks.Catalog and can contain tables \u0026 views.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: databricks_metastore[\"this\"].id,\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=databricks_metastore[\"this\"][\"id\"],\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = databricks_metastore.This.Id,\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(databricks_metastore.This.Id),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"kind\": pulumi.Any(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .metastoreId(databricks_metastore.this().id())\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.id())\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${databricks_metastore.this.id}\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/schema:Schema this \u003cname\u003e\n```\n\n ",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "catalogName",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Schema resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete schema regardless of its contents.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the schema owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Schema properties.\n"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secret:Secret": {
            "description": "With this resource you can insert a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret’s value. The server encrypts the secret using the secret scope’s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope. The secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. The maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000. You can read a secret value only from within a command on a cluster (for example, through a notebook); there is no API to read a secret value outside of a cluster. The permission applied is based on who is invoking the command and you must have at least READ permission. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst app = new databricks.SecretScope(\"app\", {});\nconst publishingApi = new databricks.Secret(\"publishingApi\", {\n    key: \"publishing_api\",\n    stringValue: data.azurerm_key_vault_secret.example.value,\n    scope: app.id,\n});\nconst _this = new databricks.Cluster(\"this\", {sparkConf: {\n    \"fs.azure.account.oauth2.client.secret\": publishingApi.configReference,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp = databricks.SecretScope(\"app\")\npublishing_api = databricks.Secret(\"publishingApi\",\n    key=\"publishing_api\",\n    string_value=data[\"azurerm_key_vault_secret\"][\"example\"][\"value\"],\n    scope=app.id)\nthis = databricks.Cluster(\"this\", spark_conf={\n    \"fs.azure.account.oauth2.client.secret\": publishing_api.config_reference,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var app = new Databricks.SecretScope(\"app\");\n\n    var publishingApi = new Databricks.Secret(\"publishingApi\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = data.Azurerm_key_vault_secret.Example.Value,\n        Scope = app.Id,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        SparkConf = \n        {\n            { \"fs.azure.account.oauth2.client.secret\", publishingApi.ConfigReference },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpublishingApi, err := databricks.NewSecret(ctx, \"publishingApi\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(data.Azurerm_key_vault_secret.Example.Value),\n\t\t\tScope:       app.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tSparkConf: pulumi.AnyMap{\n\t\t\t\t\"fs.azure.account.oauth2.client.secret\": publishingApi.ConfigReference,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var app = new SecretScope(\"app\");\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(data.azurerm_key_vault_secret().example().value())\n            .scope(app.id())\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()        \n            .sparkConf(Map.of(\"fs.azure.account.oauth2.client.secret\", publishingApi.configReference()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  app:\n    type: databricks:SecretScope\n  publishingApi:\n    type: databricks:Secret\n    properties:\n      key: publishing_api\n      stringValue: ${data.azurerm_key_vault_secret.example.value}\n      scope: ${app.id}\n  this:\n    type: databricks:Cluster\n    properties:\n      # ...\n      sparkConf:\n        fs.azure.account.oauth2.client.secret: ${publishingApi.configReference}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n\n## Import\n\nThe resource secret can be imported using `scopeName|||secretKey` combination. **This may change in future versions.** bash\n\n```sh\n $ pulumi import databricks:index/secret:Secret app `scopeName|||secretKey`\n```\n\n ",
            "properties": {
                "configReference": {
                    "type": "string",
                    "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                },
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "(Integer) time secret was updated\n"
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true
                }
            },
            "required": [
                "configReference",
                "key",
                "lastUpdatedTimestamp",
                "scope",
                "stringValue"
            ],
            "inputProperties": {
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "key",
                "scope",
                "stringValue"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Secret resources.\n",
                "properties": {
                    "configReference": {
                        "type": "string",
                        "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                    },
                    "key": {
                        "type": "string",
                        "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "(Integer) time secret was updated\n"
                    },
                    "scope": {
                        "type": "string",
                        "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "stringValue": {
                        "type": "string",
                        "description": "(String) super secret sensitive value.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretAcl:SecretAcl": {
            "description": "Create or overwrite the ACL associated with the given principal (user or group) on the specified databricks_secret_scope. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nThis way, data scientists can read the Publishing API key that is synchronized from example, Azure Key Vault.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ds = new databricks.Group(\"ds\", {});\nconst app = new databricks.SecretScope(\"app\", {});\nconst mySecretAcl = new databricks.SecretAcl(\"mySecretAcl\", {\n    principal: ds.displayName,\n    permission: \"READ\",\n    scope: app.name,\n});\nconst publishingApi = new databricks.Secret(\"publishingApi\", {\n    key: \"publishing_api\",\n    stringValue: data.azurerm_key_vault_secret.example.value,\n    scope: app.name,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nds = databricks.Group(\"ds\")\napp = databricks.SecretScope(\"app\")\nmy_secret_acl = databricks.SecretAcl(\"mySecretAcl\",\n    principal=ds.display_name,\n    permission=\"READ\",\n    scope=app.name)\npublishing_api = databricks.Secret(\"publishingApi\",\n    key=\"publishing_api\",\n    string_value=data[\"azurerm_key_vault_secret\"][\"example\"][\"value\"],\n    scope=app.name)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ds = new Databricks.Group(\"ds\");\n\n    var app = new Databricks.SecretScope(\"app\");\n\n    var mySecretAcl = new Databricks.SecretAcl(\"mySecretAcl\", new()\n    {\n        Principal = ds.DisplayName,\n        Permission = \"READ\",\n        Scope = app.Name,\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishingApi\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = data.Azurerm_key_vault_secret.Example.Value,\n        Scope = app.Name,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecretAcl(ctx, \"mySecretAcl\", \u0026databricks.SecretAclArgs{\n\t\t\tPrincipal:  ds.DisplayName,\n\t\t\tPermission: pulumi.String(\"READ\"),\n\t\t\tScope:      app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecret(ctx, \"publishingApi\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(data.Azurerm_key_vault_secret.Example.Value),\n\t\t\tScope:       app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretAcl;\nimport com.pulumi.databricks.SecretAclArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ds = new Group(\"ds\");\n\n        var app = new SecretScope(\"app\");\n\n        var mySecretAcl = new SecretAcl(\"mySecretAcl\", SecretAclArgs.builder()        \n            .principal(ds.displayName())\n            .permission(\"READ\")\n            .scope(app.name())\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(data.azurerm_key_vault_secret().example().value())\n            .scope(app.name())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ds:\n    type: databricks:Group\n  app:\n    type: databricks:SecretScope\n  mySecretAcl:\n    type: databricks:SecretAcl\n    properties:\n      principal: ${ds.displayName}\n      permission: READ\n      scope: ${app.name}\n  publishingApi:\n    type: databricks:Secret\n    properties:\n      key: publishing_api\n      # replace it with a secret management solution of your choice :-)\n      stringValue: ${data.azurerm_key_vault_secret.example.value}\n      scope: ${app.name}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n\n## Import\n\nThe resource secret acl can be imported using `scopeName|||principalName` combination. bash\n\n```sh\n $ pulumi import databricks:index/secretAcl:SecretAcl object `scopeName|||principalName`\n```\n\n ",
            "properties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n"
                },
                "principal": {
                    "type": "string",
                    "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n"
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n"
                }
            },
            "required": [
                "permission",
                "principal",
                "scope"
            ],
            "inputProperties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permission",
                "principal",
                "scope"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretAcl resources.\n",
                "properties": {
                    "permission": {
                        "type": "string",
                        "description": "`READ`, `WRITE` or `MANAGE`.\n",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n",
                        "willReplaceOnChanges": true
                    },
                    "scope": {
                        "type": "string",
                        "description": "name of the scope\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretScope:SecretScope": {
            "description": "\n\n\n## Import\n\nThe secret resource scope can be imported using the scope name. `initial_manage_principal` state won't be imported, because the underlying API doesn't include it in the response. bash\n\n```sh\n $ pulumi import databricks:index/secretScope:SecretScope object \u003cscopeName\u003e\n```\n\n ",
            "properties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n"
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata"
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                }
            },
            "required": [
                "backendType",
                "name"
            ],
            "inputProperties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                    "willReplaceOnChanges": true
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretScope resources.\n",
                "properties": {
                    "backendType": {
                        "type": "string",
                        "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                    },
                    "initialManagePrincipal": {
                        "type": "string",
                        "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                        "willReplaceOnChanges": true
                    },
                    "keyvaultMetadata": {
                        "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipal:ServicePrincipal": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nCreating regular service principal:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {applicationId: \"00000000-0000-0000-0000-000000000000\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", application_id=\"00000000-0000-0000-0000-000000000000\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n```\n\nCreating service principal with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst sp = new databricks.ServicePrincipal(\"sp\", {applicationId: \"00000000-0000-0000-0000-000000000000\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: sp.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nsp = databricks.ServicePrincipal(\"sp\", application_id=\"00000000-0000-0000-0000-000000000000\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=sp.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = sp.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  *pulumi.String(admins.Id),\n\t\t\tMemberId: sp.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(sp.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${sp.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n\nCreating service principal with cluster create permissions:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {\n    allowClusterCreate: true,\n    applicationId: \"00000000-0000-0000-0000-000000000000\",\n    displayName: \"Example service principal\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\",\n    allow_cluster_create=True,\n    application_id=\"00000000-0000-0000-0000-000000000000\",\n    display_name=\"Example service principal\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        AllowClusterCreate = true,\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n        DisplayName = \"Example service principal\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t\tApplicationId:      pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tDisplayName:        pulumi.String(\"Example service principal\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .allowClusterCreate(true)\n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .displayName(\"Example service principal\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      allowClusterCreate: true\n      applicationId: 00000000-0000-0000-0000-000000000000\n      displayName: Example service principal\n```\n\nCreating service principal in AWS Databricks account:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at account-level\nconst mws = new databricks.Provider(\"mws\", {\n    host: \"https://accounts.cloud.databricks.com\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    username: _var.databricks_account_username,\n    password: _var.databricks_account_password,\n});\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at account-level\nmws = databricks.Provider(\"mws\",\n    host=\"https://accounts.cloud.databricks.com\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    username=var[\"databricks_account_username\"],\n    password=var[\"databricks_account_password\"])\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\",\nopts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at account-level\n    var mws = new Databricks.Provider(\"mws\", new()\n    {\n        Host = \"https://accounts.cloud.databricks.com\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        Username = @var.Databricks_account_username,\n        Password = @var.Databricks_account_password,\n    });\n\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"mws\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.cloud.databricks.com\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tUsername:  pulumi.Any(_var.Databricks_account_username),\n\t\t\tPassword:  pulumi.Any(_var.Databricks_account_password),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mws = new Provider(\"mws\", ProviderArgs.builder()        \n            .host(\"https://accounts.cloud.databricks.com\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .username(var_.databricks_account_username())\n            .password(var_.databricks_account_password())\n            .build());\n\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Automation-only SP\")\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at account-level\n  mws:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.cloud.databricks.com\n      accountId: 00000000-0000-0000-0000-000000000000\n      username: ${var.databricks_account_username}\n      password: ${var.databricks_account_password}\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n    options:\n      provider: ${databricks.mws}\n```\n\nCreating service principal in Azure Databricks account:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at Azure account-level\nconst azureAccount = new databricks.Provider(\"azureAccount\", {\n    host: \"https://accounts.azuredatabricks.net\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    authType: \"azure-cli\",\n});\nconst sp = new databricks.ServicePrincipal(\"sp\", {applicationId: \"00000000-0000-0000-0000-000000000000\"}, {\n    provider: databricks.azure_account,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at Azure account-level\nazure_account = databricks.Provider(\"azureAccount\",\n    host=\"https://accounts.azuredatabricks.net\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    auth_type=\"azure-cli\")\nsp = databricks.ServicePrincipal(\"sp\", application_id=\"00000000-0000-0000-0000-000000000000\",\nopts=pulumi.ResourceOptions(provider=databricks[\"azure_account\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at Azure account-level\n    var azureAccount = new Databricks.Provider(\"azureAccount\", new()\n    {\n        Host = \"https://accounts.azuredatabricks.net\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        AuthType = \"azure-cli\",\n    });\n\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Azure_account,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"azureAccount\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.azuredatabricks.net\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tAuthType:  pulumi.String(\"azure-cli\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t}, pulumi.Provider(databricks.Azure_account))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var azureAccount = new Provider(\"azureAccount\", ProviderArgs.builder()        \n            .host(\"https://accounts.azuredatabricks.net\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .authType(\"azure-cli\")\n            .build());\n\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.azure_account())\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at Azure account-level\n  azureAccount:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.azuredatabricks.net\n      accountId: 00000000-0000-0000-0000-000000000000\n      authType: azure-cli\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n    options:\n      provider: ${databricks.azure_account}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](\u003chttps://docs.databricks\u003e.\n\n\n## Import\n\nThe resource scim service principal can be imported using its id, for example `2345678901234567`. To get the service principal ID, call [Get service principals](https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html#get-service-principals). bash\n\n```sh\n $ pulumi import databricks:index/servicePrincipal:ServicePrincipal me \u003cservice-principal-id\u003e\n```\n\n ",
            "properties": {
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "applicationId",
                "displayName",
                "home",
                "repos"
            ],
            "inputProperties": {
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n",
                    "willReplaceOnChanges": true
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipal resources.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the service principal and can be the full name of the service principal.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalRole:ServicePrincipalRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to a databricks_service_principal.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nGranting a service principal access to an instance profile\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"My Service Principal\"});\nconst myServicePrincipalInstanceProfile = new databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", {\n    servicePrincipalId: _this.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nthis = databricks.ServicePrincipal(\"this\", display_name=\"My Service Principal\")\nmy_service_principal_instance_profile = databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\",\n    service_principal_id=this.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"My Service Principal\",\n    });\n\n    var myServicePrincipalInstanceProfile = new Databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", new()\n    {\n        ServicePrincipalId = @this.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"My Service Principal\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipalRole(ctx, \"myServicePrincipalInstanceProfile\", \u0026databricks.ServicePrincipalRoleArgs{\n\t\t\tServicePrincipalId: this.ID(),\n\t\t\tRole:               instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.ServicePrincipalRole;\nimport com.pulumi.databricks.ServicePrincipalRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()        \n            .displayName(\"My Service Principal\")\n            .build());\n\n        var myServicePrincipalInstanceProfile = new ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", ServicePrincipalRoleArgs.builder()        \n            .servicePrincipalId(this_.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: My Service Principal\n  myServicePrincipalInstanceProfile:\n    type: databricks:ServicePrincipalRole\n    properties:\n      servicePrincipalId: ${this.id}\n      role: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.UserRole to attach role or databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n"
                }
            },
            "required": [
                "role",
                "servicePrincipalId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "This is the id of the role or instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "This is the id of the service principal resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalSecret:ServicePrincipalSecret": {
            "properties": {
                "secret": {
                    "type": "string",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                }
            },
            "required": [
                "secret",
                "servicePrincipalId",
                "status"
            ],
            "inputProperties": {
                "secret": {
                    "type": "string",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalSecret resources.\n",
                "properties": {
                    "secret": {
                        "type": "string",
                        "secret": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/share:Share": {
            "description": "Within a metastore, Unity Catalog provides the ability to create a share, which is a named object that contains a collection of tables in a metastore that you want to share as a group. A share can contain tables from only a single metastore. You can add or remove tables from a share at any time.\n\nA `databricks.Share` is contained within databricks.Metastore and can contain a list of shares.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nCreating a Delta Sharing share and add some existing tables to it\n\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTablesArgs;\nimport com.pulumi.databricks.Share;\nimport com.pulumi.databricks.ShareArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        var some = new Share(\"some\", ShareArgs.builder()        \n            .dynamic(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:Share\n    properties:\n      dynamic:\n        - forEach: ${things.ids}\n          content:\n            - name: ${object.value}\n              dataObjectType: TABLE\nvariables:\n  things:\n    fn::invoke:\n      Function: databricks:getTables\n      Arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n* databricks.getShares to read existing Delta Sharing shares.\n",
            "properties": {
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n"
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                }
            },
            "required": [
                "createdAt",
                "createdBy",
                "name"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Share resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of share. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlDashboard:SqlDashboard": {
            "description": "This resource is used to manage [Databricks SQL Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html). To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA dashboard may have one or more widgets.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"sharedDir\", {path: \"/Shared/Dashboards\"});\nconst d1 = new databricks.SqlDashboard(\"d1\", {\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    tags: [\n        \"some-tag\",\n        \"another-tag\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"sharedDir\", path=\"/Shared/Dashboards\")\nd1 = databricks.SqlDashboard(\"d1\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    tags=[\n        \"some-tag\",\n        \"another-tag\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"sharedDir\", new()\n    {\n        Path = \"/Shared/Dashboards\",\n    });\n\n    var d1 = new Databricks.SqlDashboard(\"d1\", new()\n    {\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        Tags = new[]\n        {\n            \"some-tag\",\n            \"another-tag\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"sharedDir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Dashboards\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlDashboard(ctx, \"d1\", \u0026databricks.SqlDashboardArgs{\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"some-tag\"),\n\t\t\t\tpulumi.String(\"another-tag\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlDashboard;\nimport com.pulumi.databricks.SqlDashboardArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()        \n            .path(\"/Shared/Dashboards\")\n            .build());\n\n        var d1 = new SqlDashboard(\"d1\", SqlDashboardArgs.builder()        \n            .parent(sharedDir.objectId().applyValue(objectId -\u003e String.format(\"folders/%s\", objectId)))\n            .tags(            \n                \"some-tag\",\n                \"another-tag\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    properties:\n      path: /Shared/Dashboards\n  d1:\n    type: databricks:SqlDashboard\n    properties:\n      parent: folders/${sharedDir.objectId}\n      tags:\n        - some-tag\n        - another-tag\n```\n\nExample permission to share dashboard with all users:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1 = new databricks.Permissions(\"d1\", {\n    sqlDashboardId: databricks_sql_dashboard.d1.id,\n    accessControls: [{\n        groupName: data.databricks_group.users.display_name,\n        permissionLevel: \"CAN_RUN\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1 = databricks.Permissions(\"d1\",\n    sql_dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    access_controls=[databricks.PermissionsAccessControlArgs(\n        group_name=data[\"databricks_group\"][\"users\"][\"display_name\"],\n        permission_level=\"CAN_RUN\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1 = new Databricks.Permissions(\"d1\", new()\n    {\n        SqlDashboardId = databricks_sql_dashboard.D1.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = data.Databricks_group.Users.Display_name,\n                PermissionLevel = \"CAN_RUN\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"d1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlDashboardId: pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(data.Databricks_group.Users.Display_name),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1 = new Permissions(\"d1\", PermissionsArgs.builder()        \n            .sqlDashboardId(databricks_sql_dashboard.d1().id())\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .groupName(data.databricks_group().users().display_name())\n                .permissionLevel(\"CAN_RUN\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1:\n    type: databricks:Permissions\n    properties:\n      sqlDashboardId: ${databricks_sql_dashboard.d1.id}\n      accessControls:\n        - groupName: ${data.databricks_group.users.display_name}\n          permissionLevel: CAN_RUN\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_dashboard` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlDashboard:SqlDashboard this \u003cdashboard-id\u003e\n```\n\n ",
            "properties": {
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "name"
            ],
            "inputProperties": {
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlDashboard resources.\n",
                "properties": {
                    "name": {
                        "type": "string"
                    },
                    "parent": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlEndpoint:SqlEndpoint": {
            "description": "This resource is used to manage [Databricks SQL Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html). To create [SQL endpoints](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.SqlEndpoint(\"this\", {\n    clusterSize: \"Small\",\n    maxNumClusters: 1,\n    tags: {\n        customTags: [{\n            key: \"City\",\n            value: \"Amsterdam\",\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.SqlEndpoint(\"this\",\n    cluster_size=\"Small\",\n    max_num_clusters=1,\n    tags=databricks.SqlEndpointTagsArgs(\n        custom_tags=[databricks.SqlEndpointTagsCustomTagArgs(\n            key=\"City\",\n            value=\"Amsterdam\",\n        )],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.SqlEndpoint(\"this\", new()\n    {\n        ClusterSize = \"Small\",\n        MaxNumClusters = 1,\n        Tags = new Databricks.Inputs.SqlEndpointTagsArgs\n        {\n            CustomTags = new[]\n            {\n                new Databricks.Inputs.SqlEndpointTagsCustomTagArgs\n                {\n                    Key = \"City\",\n                    Value = \"Amsterdam\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlEndpoint(ctx, \"this\", \u0026databricks.SqlEndpointArgs{\n\t\t\tClusterSize:    pulumi.String(\"Small\"),\n\t\t\tMaxNumClusters: pulumi.Int(1),\n\t\t\tTags: \u0026databricks.SqlEndpointTagsArgs{\n\t\t\t\tCustomTags: databricks.SqlEndpointTagsCustomTagArray{\n\t\t\t\t\t\u0026databricks.SqlEndpointTagsCustomTagArgs{\n\t\t\t\t\t\tKey:   pulumi.String(\"City\"),\n\t\t\t\t\t\tValue: pulumi.String(\"Amsterdam\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.SqlEndpoint;\nimport com.pulumi.databricks.SqlEndpointArgs;\nimport com.pulumi.databricks.inputs.SqlEndpointTagsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new SqlEndpoint(\"this\", SqlEndpointArgs.builder()        \n            .clusterSize(\"Small\")\n            .maxNumClusters(1)\n            .tags(SqlEndpointTagsArgs.builder()\n                .customTags(SqlEndpointTagsCustomTagArgs.builder()\n                    .key(\"City\")\n                    .value(\"Amsterdam\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlEndpoint\n    properties:\n      clusterSize: Small\n      maxNumClusters: 1\n      tags:\n        customTags:\n          - key: City\n            value: Amsterdam\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Can Use* or *Can Manage* SQL endpoints.\n* `databricks_sql_access` on databricks.Group or databricks_user.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_endpoint` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlEndpoint:SqlEndpoint this \u003cendpoint-id\u003e\n```\n\n ",
            "properties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                },
                "numClusters": {
                    "type": "integer"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "[SQL Warehouse Type](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless): `PRO` or `CLASSIC` (default).  If Serverless SQL is enabled, you can only specify `PRO`.\n"
                }
            },
            "required": [
                "clusterSize",
                "dataSourceId",
                "jdbcUrl",
                "name",
                "odbcParams",
                "state"
            ],
            "inputProperties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                },
                "numClusters": {
                    "type": "integer"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "[SQL Warehouse Type](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless): `PRO` or `CLASSIC` (default).  If Serverless SQL is enabled, you can only specify `PRO`.\n"
                }
            },
            "requiredInputs": [
                "clusterSize"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlEndpoint resources.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                        "description": "Databricks tags all endpoint resources with these tags.\n"
                    },
                    "warehouseType": {
                        "type": "string",
                        "description": "[SQL Warehouse Type](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless): `PRO` or `CLASSIC` (default).  If Serverless SQL is enabled, you can only specify `PRO`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlGlobalConfig:SqlGlobalConfig": {
            "description": "This resource configures the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace. *Please note that changing parameters of this resource will restart all running databricks_sql_endpoint.*  To use this resource you need to be an administrator.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n### AWS example\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    instanceProfileArn: \"arn:....\",\n    dataAccessConfig: {\n        \"spark.sql.session.timeZone\": \"UTC\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    instance_profile_arn=\"arn:....\",\n    data_access_config={\n        \"spark.sql.session.timeZone\": \"UTC\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        InstanceProfileArn = \"arn:....\",\n        DataAccessConfig = \n        {\n            { \"spark.sql.session.timeZone\", \"UTC\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy:     pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tInstanceProfileArn: pulumi.String(\"arn:....\"),\n\t\t\tDataAccessConfig: pulumi.AnyMap{\n\t\t\t\t\"spark.sql.session.timeZone\": pulumi.Any(\"UTC\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .instanceProfileArn(\"arn:....\")\n            .dataAccessConfig(Map.of(\"spark.sql.session.timeZone\", \"UTC\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      instanceProfileArn: arn:....\n      dataAccessConfig:\n        spark.sql.session.timeZone: UTC\n```\n{{% /example %}}\n{{% example %}}\n### Azure example\n\nFor Azure you should use the `data_access_config` to provide the service principal configuration. You can use the Databricks SQL Admin Console UI to help you generate the right configuration values.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    dataAccessConfig: {\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": _var.application_id,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": `{{secrets/${local.secret_scope}/${local.secret_key}}}`,\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": `https://login.microsoftonline.com/${_var.tenant_id}/oauth2/token`,\n    },\n    sqlConfigParams: {\n        ANSI_MODE: \"true\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    data_access_config={\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": var[\"application_id\"],\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": f\"{{{{secrets/{local['secret_scope']}/{local['secret_key']}}}}}\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{var['tenant_id']}/oauth2/token\",\n    },\n    sql_config_params={\n        \"ANSI_MODE\": \"true\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        DataAccessConfig = \n        {\n            { \"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\" },\n            { \"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.id\", @var.Application_id },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.secret\", $\"{{{{secrets/{local.Secret_scope}/{local.Secret_key}}}}}\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", $\"https://login.microsoftonline.com/{@var.Tenant_id}/oauth2/token\" },\n        },\n        SqlConfigParams = \n        {\n            { \"ANSI_MODE\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy: pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tDataAccessConfig: pulumi.AnyMap{\n\t\t\t\t\"spark.hadoop.fs.azure.account.auth.type\":              pulumi.Any(\"OAuth\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth.provider.type\":    pulumi.Any(\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.id\":       pulumi.Any(_var.Application_id),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.secret\":   pulumi.Any(fmt.Sprintf(\"{{secrets/%v/%v}}\", local.Secret_scope, local.Secret_key)),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": pulumi.Any(fmt.Sprintf(\"https://login.microsoftonline.com/%v/oauth2/token\", _var.Tenant_id)),\n\t\t\t},\n\t\t\tSqlConfigParams: pulumi.AnyMap{\n\t\t\t\t\"ANSI_MODE\": pulumi.Any(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .dataAccessConfig(Map.ofEntries(\n                Map.entry(\"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.id\", var_.application_id()),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.secret\", String.format(\"{{{{secrets/%s/%s}}}}\", local.secret_scope(),local.secret_key())),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", String.format(\"https://login.microsoftonline.com/%s/oauth2/token\", var_.tenant_id()))\n            ))\n            .sqlConfigParams(Map.of(\"ANSI_MODE\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      dataAccessConfig:\n        spark.hadoop.fs.azure.account.auth.type: OAuth\n        spark.hadoop.fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n        spark.hadoop.fs.azure.account.oauth2.client.id: ${var.application_id}\n        spark.hadoop.fs.azure.account.oauth2.client.secret: '{{secrets/${local.secret_scope}/${local.secret_key}}}'\n        spark.hadoop.fs.azure.account.oauth2.client.endpoint: https://login.microsoftonline.com/${var.tenant_id}/oauth2/token\n      sqlConfigParams:\n        ANSI_MODE: 'true'\n```\n\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_global_config` resource with command like the following (you need to use `global` as ID)bash\n\n```sh\n $ pulumi import databricks:index/sqlGlobalConfig:SqlGlobalConfig this global\n```\n\n ",
            "properties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Allows the possibility to create Serverless SQL warehouses. Default value: false.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "inputProperties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Allows the possibility to create Serverless SQL warehouses. Default value: false.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlGlobalConfig resources.\n",
                "properties": {
                    "dataAccessConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Allows the possibility to create Serverless SQL warehouses. Default value: false.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                    },
                    "securityPolicy": {
                        "type": "string",
                        "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                    },
                    "sqlConfigParams": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlPermissions:SqlPermissions": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nThe following resource definition will enforce access control on a table by executing the following SQL queries on a special auto-terminating cluster it would create for this operation:\n\n* ```SHOW GRANT ON TABLE `default`.`foo` ```\n* ```REVOKE ALL PRIVILEGES ON TABLE `default`.`foo` FROM ... every group and user that has access to it ...```\n* ```GRANT MODIFY, SELECT ON TABLE `default`.`foo` TO `serge@example.com` ```\n* ```GRANT SELECT ON TABLE `default`.`foo` TO `special group` ```\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fooTable = new databricks.SqlPermissions(\"fooTable\", {\n    privilegeAssignments: [\n        {\n            principal: \"serge@example.com\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            principal: \"special group\",\n            privileges: [\"SELECT\"],\n        },\n    ],\n    table: \"foo\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfoo_table = databricks.SqlPermissions(\"fooTable\",\n    privilege_assignments=[\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"serge@example.com\",\n            privileges=[\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        ),\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"special group\",\n            privileges=[\"SELECT\"],\n        ),\n    ],\n    table=\"foo\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fooTable = new Databricks.SqlPermissions(\"fooTable\", new()\n    {\n        PrivilegeAssignments = new[]\n        {\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"serge@example.com\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                    \"MODIFY\",\n                },\n            },\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"special group\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n        Table = \"foo\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlPermissions(ctx, \"fooTable\", \u0026databricks.SqlPermissionsArgs{\n\t\t\tPrivilegeAssignments: databricks.SqlPermissionsPrivilegeAssignmentArray{\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"serge@example.com\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"special group\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTable: pulumi.String(\"foo\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlPermissions;\nimport com.pulumi.databricks.SqlPermissionsArgs;\nimport com.pulumi.databricks.inputs.SqlPermissionsPrivilegeAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fooTable = new SqlPermissions(\"fooTable\", SqlPermissionsArgs.builder()        \n            .privilegeAssignments(            \n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"serge@example.com\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build(),\n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"special group\")\n                    .privileges(\"SELECT\")\n                    .build())\n            .table(\"foo\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fooTable:\n    type: databricks:SqlPermissions\n    properties:\n      privilegeAssignments:\n        - principal: serge@example.com\n          privileges:\n            - SELECT\n            - MODIFY\n        - principal: special group\n          privileges:\n            - SELECT\n      table: foo\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Grants to manage data access in Unity Catalog.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are* `table/default.foo` - table `foo` in a `default` database. Database is always mandatory. * `view/bar.foo` - view `foo` in `bar` database. * `database/bar` - `bar` database. * `catalog/` - entire catalog. `/` suffix is mandatory. * `any file/` - direct access to any file. `/` suffix is mandatory. * `anonymous function/` - anonymous function. `/` suffix is mandatory. bash\n\n```sh\n $ pulumi import databricks:index/sqlPermissions:SqlPermissions foo /\u003cobject-type\u003e/\u003cobject-name\u003e\n```\n\n ",
            "properties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n"
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading any file. Defaults to `false`.\n"
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n"
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n"
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading any file. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n",
                    "willReplaceOnChanges": true
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlPermissions resources.\n",
                "properties": {
                    "anonymousFunction": {
                        "type": "boolean",
                        "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "anyFile": {
                        "type": "boolean",
                        "description": "If this access control for reading any file. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "catalog": {
                        "type": "boolean",
                        "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "database": {
                        "type": "string",
                        "description": "Name of the database. Has default value of `default`.\n",
                        "willReplaceOnChanges": true
                    },
                    "privilegeAssignments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                        }
                    },
                    "table": {
                        "type": "string",
                        "description": "Name of the table. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "description": "Name of the view. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlQuery:SqlQuery": {
            "description": "\n\n\n## Import\n\nYou can import a `databricks_sql_query` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlQuery:SqlQuery this \u003cquery-id\u003e\n```\n\n ",
            "properties": {
                "dataSourceId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string"
                },
                "query": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "dataSourceId",
                "name",
                "query"
            ],
            "inputProperties": {
                "dataSourceId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "query": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "requiredInputs": [
                "dataSourceId",
                "query"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlQuery resources.\n",
                "properties": {
                    "dataSourceId": {
                        "type": "string"
                    },
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                        }
                    },
                    "parent": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "query": {
                        "type": "string"
                    },
                    "runAsRole": {
                        "type": "string"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlVisualization:SqlVisualization": {
            "description": "\n\n\n## Import\n\nYou can import a `databricks_sql_visualization` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlVisualization:SqlVisualization this \u003cquery-id\u003e/\u003cvisualization-id\u003e\n```\n\n ",
            "properties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string"
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "options",
                "queryId",
                "type",
                "visualizationId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "options",
                "queryId",
                "type"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlVisualization resources.\n",
                "properties": {
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "options": {
                        "type": "string"
                    },
                    "queryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "queryPlan": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlWidget:SqlWidget": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA widget is always tied to a dashboard. Every dashboard may have one or more widgets.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1w1 = new databricks.SqlWidget(\"d1w1\", {\n    dashboardId: databricks_sql_dashboard.d1.id,\n    text: \"Hello! I'm a **text widget**!\",\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 0,\n        posY: 0,\n    },\n});\nconst d1w2 = new databricks.SqlWidget(\"d1w2\", {\n    dashboardId: databricks_sql_dashboard.d1.id,\n    visualizationId: databricks_sql_visualization.q1v1.id,\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 3,\n        posY: 0,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1w1 = databricks.SqlWidget(\"d1w1\",\n    dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    text=\"Hello! I'm a **text widget**!\",\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=0,\n        pos_y=0,\n    ))\nd1w2 = databricks.SqlWidget(\"d1w2\",\n    dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    visualization_id=databricks_sql_visualization[\"q1v1\"][\"id\"],\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=3,\n        pos_y=0,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1w1 = new Databricks.SqlWidget(\"d1w1\", new()\n    {\n        DashboardId = databricks_sql_dashboard.D1.Id,\n        Text = \"Hello! I'm a **text widget**!\",\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 0,\n            PosY = 0,\n        },\n    });\n\n    var d1w2 = new Databricks.SqlWidget(\"d1w2\", new()\n    {\n        DashboardId = databricks_sql_dashboard.D1.Id,\n        VisualizationId = databricks_sql_visualization.Q1v1.Id,\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 3,\n            PosY = 0,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlWidget(ctx, \"d1w1\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId: pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tText:        pulumi.String(\"Hello! I'm a **text widget**!\"),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(0),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlWidget(ctx, \"d1w2\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId:     pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tVisualizationId: pulumi.Any(databricks_sql_visualization.Q1v1.Id),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(3),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlWidget;\nimport com.pulumi.databricks.SqlWidgetArgs;\nimport com.pulumi.databricks.inputs.SqlWidgetPositionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1w1 = new SqlWidget(\"d1w1\", SqlWidgetArgs.builder()        \n            .dashboardId(databricks_sql_dashboard.d1().id())\n            .text(\"Hello! I'm a **text widget**!\")\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(0)\n                .posY(0)\n                .build())\n            .build());\n\n        var d1w2 = new SqlWidget(\"d1w2\", SqlWidgetArgs.builder()        \n            .dashboardId(databricks_sql_dashboard.d1().id())\n            .visualizationId(databricks_sql_visualization.q1v1().id())\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(3)\n                .posY(0)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1w1:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${databricks_sql_dashboard.d1.id}\n      text: Hello! I'm a **text widget**!\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 0\n        posY: 0\n  d1w2:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${databricks_sql_dashboard.d1.id}\n      visualizationId: ${databricks_sql_visualization.q1v1.id}\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 3\n        posY: 0\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_widget` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlWidget:SqlWidget this \u003cdashboard-id\u003e/\u003cwidget-id\u003e\n```\n\n ",
            "properties": {
                "dashboardId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                },
                "widgetId": {
                    "type": "string"
                }
            },
            "required": [
                "dashboardId",
                "widgetId"
            ],
            "inputProperties": {
                "dashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "widgetId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "dashboardId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlWidget resources.\n",
                "properties": {
                    "dashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "description": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                        }
                    },
                    "position": {
                        "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                    },
                    "text": {
                        "type": "string"
                    },
                    "title": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "widgetId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/storageCredential:StorageCredential": {
            "description": "To work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- `databricks.StorageCredential` represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- databricks.ExternalLocation are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nFor AWS\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    awsIamRole: {\n        roleArn: aws_iam_role.external_data_access.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"externalCreds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    aws_iam_role=databricks.StorageCredentialAwsIamRoleArgs(\n        role_arn=aws_iam_role[\"external_data_access\"][\"arn\"],\n    ),\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"externalCreds\",\n    storage_credential=external.id,\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\"CREATE_TABLE\"],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = aws_iam_role.External_data_access.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"externalCreds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(aws_iam_role.External_data_access.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"externalCreds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()        \n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(aws_iam_role.external_data_access().arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()        \n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      awsIamRole:\n        roleArn: ${aws_iam_role.external_data_access.arn}\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_TABLE\n```\n\nFor Azure\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as azure from \"@pulumi/azure\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = azure.core.getResourceGroup({\n    name: \"example-rg\",\n});\nconst example = new azure.databricks.AccessConnector(\"example\", {\n    resourceGroupName: azurerm_resource_group[\"this\"].name,\n    location: azurerm_resource_group[\"this\"].location,\n    identity: {\n        type: \"SystemAssigned\",\n    },\n    tags: {\n        Environment: \"Production\",\n    },\n});\nconst externalMi = new databricks.StorageCredential(\"externalMi\", {\n    azureManagedIdentity: {\n        accessConnectorId: example.id,\n    },\n    comment: \"Managed identity credential managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"externalCreds\", {\n    storageCredential: databricks_storage_credential.external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_azure as azure\nimport pulumi_databricks as databricks\n\nthis = azure.core.get_resource_group(name=\"example-rg\")\nexample = azure.databricks.AccessConnector(\"example\",\n    resource_group_name=azurerm_resource_group[\"this\"][\"name\"],\n    location=azurerm_resource_group[\"this\"][\"location\"],\n    identity=azure.databricks.AccessConnectorIdentityArgs(\n        type=\"SystemAssigned\",\n    ),\n    tags={\n        \"Environment\": \"Production\",\n    })\nexternal_mi = databricks.StorageCredential(\"externalMi\",\n    azure_managed_identity=databricks.StorageCredentialAzureManagedIdentityArgs(\n        access_connector_id=example.id,\n    ),\n    comment=\"Managed identity credential managed by TF\")\nexternal_creds = databricks.Grants(\"externalCreds\",\n    storage_credential=databricks_storage_credential[\"external\"][\"id\"],\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\"CREATE_TABLE\"],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Azure = Pulumi.Azure;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Azure.Core.GetResourceGroup.Invoke(new()\n    {\n        Name = \"example-rg\",\n    });\n\n    var example = new Azure.DataBricks.AccessConnector(\"example\", new()\n    {\n        ResourceGroupName = azurerm_resource_group.This.Name,\n        Location = azurerm_resource_group.This.Location,\n        Identity = new Azure.DataBricks.Inputs.AccessConnectorIdentityArgs\n        {\n            Type = \"SystemAssigned\",\n        },\n        Tags = \n        {\n            { \"Environment\", \"Production\" },\n        },\n    });\n\n    var externalMi = new Databricks.StorageCredential(\"externalMi\", new()\n    {\n        AzureManagedIdentity = new Databricks.Inputs.StorageCredentialAzureManagedIdentityArgs\n        {\n            AccessConnectorId = example.Id,\n        },\n        Comment = \"Managed identity credential managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"externalCreds\", new()\n    {\n        StorageCredential = databricks_storage_credential.External.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/core\"\n\t\"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/databricks\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := core.LookupResourceGroup(ctx, \u0026core.LookupResourceGroupArgs{\n\t\t\tName: \"example-rg\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\texample, err := databricks.NewAccessConnector(ctx, \"example\", \u0026databricks.AccessConnectorArgs{\n\t\t\tResourceGroupName: pulumi.Any(azurerm_resource_group.This.Name),\n\t\t\tLocation:          pulumi.Any(azurerm_resource_group.This.Location),\n\t\t\tIdentity: \u0026databricks.AccessConnectorIdentityArgs{\n\t\t\t\tType: pulumi.String(\"SystemAssigned\"),\n\t\t\t},\n\t\t\tTags: pulumi.StringMap{\n\t\t\t\t\"Environment\": pulumi.String(\"Production\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewStorageCredential(ctx, \"externalMi\", \u0026databricks.StorageCredentialArgs{\n\t\t\tAzureManagedIdentity: \u0026databricks.StorageCredentialAzureManagedIdentityArgs{\n\t\t\t\tAccessConnectorId: example.ID(),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed identity credential managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"externalCreds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: pulumi.Any(databricks_storage_credential.External.Id),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.azure.core.CoreFunctions;\nimport com.pulumi.azure.core.inputs.GetResourceGroupArgs;\nimport com.pulumi.azure.databricks.AccessConnector;\nimport com.pulumi.azure.databricks.AccessConnectorArgs;\nimport com.pulumi.azure.databricks.inputs.AccessConnectorIdentityArgs;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAzureManagedIdentityArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = CoreFunctions.getResourceGroup(GetResourceGroupArgs.builder()\n            .name(\"example-rg\")\n            .build());\n\n        var example = new AccessConnector(\"example\", AccessConnectorArgs.builder()        \n            .resourceGroupName(azurerm_resource_group.this().name())\n            .location(azurerm_resource_group.this().location())\n            .identity(AccessConnectorIdentityArgs.builder()\n                .type(\"SystemAssigned\")\n                .build())\n            .tags(Map.of(\"Environment\", \"Production\"))\n            .build());\n\n        var externalMi = new StorageCredential(\"externalMi\", StorageCredentialArgs.builder()        \n            .azureManagedIdentity(StorageCredentialAzureManagedIdentityArgs.builder()\n                .accessConnectorId(example.id())\n                .build())\n            .comment(\"Managed identity credential managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()        \n            .storageCredential(databricks_storage_credential.external().id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  example:\n    type: azure:databricks:AccessConnector\n    properties:\n      resourceGroupName: ${azurerm_resource_group.this.name}\n      location: ${azurerm_resource_group.this.location}\n      identity:\n        type: SystemAssigned\n      tags:\n        Environment: Production\n  externalMi:\n    type: databricks:StorageCredential\n    properties:\n      azureManagedIdentity:\n        accessConnectorId: ${example.id}\n      comment: Managed identity credential managed by TF\n  externalCreds:\n    type: databricks:Grants\n    properties:\n      storageCredential: ${databricks_storage_credential.external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_TABLE\nvariables:\n  this:\n    fn::invoke:\n      Function: azure:core:getResourceGroup\n      Arguments:\n        name: example-rg\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/storageCredential:StorageCredential this \u003cname\u003e\n```\n\n ",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                }
            },
            "required": [
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering StorageCredential resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                    },
                    "comment": {
                        "type": "string"
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/table:Table": {
            "properties": {
                "catalogName": {
                    "type": "string"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "schemaName": {
                    "type": "string"
                },
                "storageCredentialName": {
                    "type": "string"
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string"
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "required": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "name",
                "owner",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "schemaName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Table resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                        }
                    },
                    "comment": {
                        "type": "string"
                    },
                    "dataSourceFormat": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "owner": {
                        "type": "string"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        }
                    },
                    "schemaName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string"
                    },
                    "tableType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/token:Token": {
            "description": "This resource creates [Personal Access Tokens](https://docs.databricks.com/sql/user/security/personal-access-tokens.html) for the same user that is authenticated with the provider. Most likely you should use databricks.OboToken to create [On-Behalf-Of tokens](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-personal-access-tokens-for-a-service-principal) for a databricks.ServicePrincipal in Databricks workspaces on AWS. Databricks workspaces on other clouds use their own native OAuth token flows.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider in normal mode\nconst createdWorkspace = new databricks.Provider(\"createdWorkspace\", {host: databricks_mws_workspaces[\"this\"].workspace_url});\n// create PAT token to provision entities within workspace\nconst pat = new databricks.Token(\"pat\", {\n    comment: \"Terraform Provisioning\",\n    lifetimeSeconds: 8640000,\n}, {\n    provider: databricks.created_workspace,\n});\nexport const databricksToken = pat.tokenValue;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider in normal mode\ncreated_workspace = databricks.Provider(\"createdWorkspace\", host=databricks_mws_workspaces[\"this\"][\"workspace_url\"])\n# create PAT token to provision entities within workspace\npat = databricks.Token(\"pat\",\n    comment=\"Terraform Provisioning\",\n    lifetime_seconds=8640000,\n    opts=pulumi.ResourceOptions(provider=databricks[\"created_workspace\"]))\npulumi.export(\"databricksToken\", pat.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider in normal mode\n    var createdWorkspace = new Databricks.Provider(\"createdWorkspace\", new()\n    {\n        Host = databricks_mws_workspaces.This.Workspace_url,\n    });\n\n    // create PAT token to provision entities within workspace\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = \"Terraform Provisioning\",\n        LifetimeSeconds = 8640000,\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Created_workspace,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = pat.TokenValue,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"createdWorkspace\", \u0026databricks.ProviderArgs{\n\t\t\tHost: pulumi.Any(databricks_mws_workspaces.This.Workspace_url),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpat, err := databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"Terraform Provisioning\"),\n\t\t\tLifetimeSeconds: pulumi.Int(8640000),\n\t\t}, pulumi.Provider(databricks.Created_workspace))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", pat.TokenValue)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var createdWorkspace = new Provider(\"createdWorkspace\", ProviderArgs.builder()        \n            .host(databricks_mws_workspaces.this().workspace_url())\n            .build());\n\n        var pat = new Token(\"pat\", TokenArgs.builder()        \n            .comment(\"Terraform Provisioning\")\n            .lifetimeSeconds(8640000)\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.created_workspace())\n                .build());\n\n        ctx.export(\"databricksToken\", pat.tokenValue());\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider in normal mode\n  createdWorkspace:\n    type: pulumi:providers:databricks\n    properties:\n      host: ${databricks_mws_workspaces.this.workspace_url}\n  # create PAT token to provision entities within workspace\n  pat:\n    type: databricks:Token\n    properties:\n      comment: Terraform Provisioning\n      # 100 day token\n      lifetimeSeconds: 8.64e+06\n    options:\n      provider: ${databricks.created_workspace}\noutputs:\n  # output token for other modules\n  databricksToken: ${pat.tokenValue}\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "creationTime",
                "expiryTime",
                "tokenId",
                "tokenValue"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                },
                "tokenId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Token resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "expiryTime": {
                        "type": "integer"
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenId": {
                        "type": "string"
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/user:User": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nCreating regular user:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n```\n\nCreating user with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  *pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n\nCreating user with cluster create permissions:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {\n    allowClusterCreate: true,\n    displayName: \"Example user\",\n    userName: \"me@example.com\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\",\n    allow_cluster_create=True,\n    display_name=\"Example user\",\n    user_name=\"me@example.com\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        AllowClusterCreate = true,\n        DisplayName = \"Example user\",\n        UserName = \"me@example.com\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t\tDisplayName:        pulumi.String(\"Example user\"),\n\t\t\tUserName:           pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .allowClusterCreate(true)\n            .displayName(\"Example user\")\n            .userName(\"me@example.com\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      allowClusterCreate: true\n      displayName: Example user\n      userName: me@example.com\n```\n\nCreating user in AWS Databricks account:\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at account-level\nconst mws = new databricks.Provider(\"mws\", {\n    host: \"https://accounts.cloud.databricks.com\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    username: _var.databricks_account_username,\n    password: _var.databricks_account_password,\n});\nconst accountUser = new databricks.User(\"accountUser\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at account-level\nmws = databricks.Provider(\"mws\",\n    host=\"https://accounts.cloud.databricks.com\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    username=var[\"databricks_account_username\"],\n    password=var[\"databricks_account_password\"])\naccount_user = databricks.User(\"accountUser\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\",\n    opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at account-level\n    var mws = new Databricks.Provider(\"mws\", new()\n    {\n        Host = \"https://accounts.cloud.databricks.com\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        Username = @var.Databricks_account_username,\n        Password = @var.Databricks_account_password,\n    });\n\n    var accountUser = new Databricks.User(\"accountUser\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"mws\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.cloud.databricks.com\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tUsername:  pulumi.Any(_var.Databricks_account_username),\n\t\t\tPassword:  pulumi.Any(_var.Databricks_account_password),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUser(ctx, \"accountUser\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mws = new Provider(\"mws\", ProviderArgs.builder()        \n            .host(\"https://accounts.cloud.databricks.com\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .username(var_.databricks_account_username())\n            .password(var_.databricks_account_password())\n            .build());\n\n        var accountUser = new User(\"accountUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at account-level\n  mws:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.cloud.databricks.com\n      accountId: 00000000-0000-0000-0000-000000000000\n      username: ${var.databricks_account_username}\n      password: ${var.databricks_account_password}\n  accountUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n      displayName: Example user\n    options:\n      provider: ${databricks.mws}\n```\n\nCreating user in Azure Databricks account:\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider at Azure account-level\nconst azureAccount = new databricks.Provider(\"azureAccount\", {\n    host: \"https://accounts.azuredatabricks.net\",\n    accountId: \"00000000-0000-0000-0000-000000000000\",\n    authType: \"azure-cli\",\n});\nconst accountUser = new databricks.User(\"accountUser\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider at Azure account-level\nazure_account = databricks.Provider(\"azureAccount\",\n    host=\"https://accounts.azuredatabricks.net\",\n    account_id=\"00000000-0000-0000-0000-000000000000\",\n    auth_type=\"azure-cli\")\naccount_user = databricks.User(\"accountUser\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\",\n    opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider at Azure account-level\n    var azureAccount = new Databricks.Provider(\"azureAccount\", new()\n    {\n        Host = \"https://accounts.azuredatabricks.net\",\n        AccountId = \"00000000-0000-0000-0000-000000000000\",\n        AuthType = \"azure-cli\",\n    });\n\n    var accountUser = new Databricks.User(\"accountUser\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"azureAccount\", \u0026databricks.ProviderArgs{\n\t\t\tHost:      pulumi.String(\"https://accounts.azuredatabricks.net\"),\n\t\t\tAccountId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tAuthType:  pulumi.String(\"azure-cli\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUser(ctx, \"accountUser\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Provider;\nimport com.pulumi.databricks.ProviderArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var azureAccount = new Provider(\"azureAccount\", ProviderArgs.builder()        \n            .host(\"https://accounts.azuredatabricks.net\")\n            .accountId(\"00000000-0000-0000-0000-000000000000\")\n            .authType(\"azure-cli\")\n            .build());\n\n        var accountUser = new User(\"accountUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider at Azure account-level\n  azureAccount:\n    type: pulumi:providers:databricks\n    properties:\n      host: https://accounts.azuredatabricks.net\n      accountId: 00000000-0000-0000-0000-000000000000\n      authType: azure-cli\n  accountUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n      displayName: Example user\n    options:\n      provider: ${databricks.mws}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\nThe resource scim user can be imported using idbash\n\n```sh\n $ pulumi import databricks:index/user:User me \u003cuser-id\u003e\n```\n\n ",
            "properties": {
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.\n"
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "required": [
                "displayName",
                "home",
                "repos",
                "userName"
            ],
            "inputProperties": {
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "requiredInputs": [
                "userName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering User resources.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the username that can be the full name of the user.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                    },
                    "userName": {
                        "type": "string",
                        "description": "This is the username of the given user and will be their form of access and identity.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userInstanceProfile:UserInstanceProfile": {
            "description": "\u003e **Deprecated** Please rewrite with databricks_user_role. This resource will be removed in v0.5.x\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserInstanceProfile = new databricks.UserInstanceProfile(\"myUserInstanceProfile\", {\n    userId: myUser.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_instance_profile = databricks.UserInstanceProfile(\"myUserInstanceProfile\",\n    user_id=my_user.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserInstanceProfile = new Databricks.UserInstanceProfile(\"myUserInstanceProfile\", new()\n    {\n        UserId = myUser.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserInstanceProfile(ctx, \"myUserInstanceProfile\", \u0026databricks.UserInstanceProfileArgs{\n\t\t\tUserId:            myUser.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserInstanceProfile;\nimport com.pulumi.databricks.UserInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserInstanceProfile = new UserInstanceProfile(\"myUserInstanceProfile\", UserInstanceProfileArgs.builder()        \n            .userId(myUser.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserInstanceProfile:\n    type: databricks:UserInstanceProfile\n    properties:\n      userId: ${myUser.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "instanceProfileId",
                "userId"
            ],
            "inputProperties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "instanceProfileId",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserInstanceProfile resources.\n",
                "properties": {
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userRole:UserRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAdding AWS instance profile to a user\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserRole = new databricks.UserRole(\"myUserRole\", {\n    userId: myUser.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_role = databricks.UserRole(\"myUserRole\",\n    user_id=my_user.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserRole = new Databricks.UserRole(\"myUserRole\", new()\n    {\n        UserId = myUser.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"myUserRole\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserRole = new UserRole(\"myUserRole\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserRole:\n    type: databricks:UserRole\n    properties:\n      userId: ${myUser.id}\n      role: ${instanceProfile.id}\n```\n\nAdding user as administrator to Databricks Account\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserAccountAdmin = new databricks.UserRole(\"myUserAccountAdmin\", {\n    userId: myUser.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_account_admin = databricks.UserRole(\"myUserAccountAdmin\",\n    user_id=my_user.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserAccountAdmin = new Databricks.UserRole(\"myUserAccountAdmin\", new()\n    {\n        UserId = myUser.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"myUserAccountAdmin\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserAccountAdmin = new UserRole(\"myUserAccountAdmin\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserAccountAdmin:\n    type: databricks:UserRole\n    properties:\n      userId: ${myUser.id}\n      role: account_admin\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "role",
                "userId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceConf:WorkspaceConf": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAllows specification of custom configuration properties for expert usage:\n\n * `enableIpAccessLists` - enables the use of databricks.IpAccessList resources\n * `maxTokenLifetimeDays` - (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. **WARNING:** This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set. \n * `enableTokensConfig` - (boolean) Enable or disable personal access tokens for this workspace.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.AnyMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "inputProperties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceConf resources.\n",
                "properties": {
                    "customConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                    }
                },
                "type": "object"
            }
        }
    },
    "functions": {
        "databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy": {
            "inputs": {
                "description": "A collection of arguments for invoking getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                    },
                    "forLogDelivery": {
                        "type": "boolean",
                        "description": "Either or not this assume role policy should be created for usage log delivery. Defaults to false.\n"
                    }
                },
                "type": "object",
                "required": [
                    "externalId"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "forLogDelivery": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "AWS IAM Policy JSON document\n"
                    }
                },
                "type": "object",
                "required": [
                    "externalId",
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getAwsBucketPolicy:getAwsBucketPolicy": {
            "description": "This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it. \n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* End to end workspace management guide\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string",
                        "description": "AWS S3 Bucket name for which to generate the policy document.\n"
                    },
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "databricksE2AccountId": {
                        "type": "string",
                        "description": "Your Databricks E2 account ID. Used to generate  restrictive IAM policies that will increase the security of your root bucket\n"
                    },
                    "fullAccessRole": {
                        "type": "string",
                        "description": "Data access role that can have full access for this bucket\n"
                    }
                },
                "type": "object",
                "required": [
                    "bucket"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string"
                    },
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "databricksE2AccountId": {
                        "type": "string"
                    },
                    "fullAccessRole": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "(Read-only) AWS IAM Policy JSON document to grant Databricks full access to bucket.\n"
                    }
                },
                "type": "object",
                "required": [
                    "bucket",
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.\n\nThis data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nFor more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks.AwsS3Mount pages.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getAwsCrossAccountPolicy({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_cross_account_policy()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsCrossAccountPolicy.\n",
                "properties": {
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of Data IAM role ARNs that are explicitly granted `iam:PassRole` action.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getAwsCrossAccountPolicy.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "AWS IAM Policy JSON document\n"
                    },
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object",
                "required": [
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getCatalogs:getCatalogs": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nListing all catalogs:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getCatalogs({});\nexport const allCatalogs = all;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_catalogs()\npulumi.export(\"allCatalogs\", all)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetCatalogs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allCatalogs\"] = all.Apply(getCatalogsResult =\u003e getCatalogsResult),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetCatalogs(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allCatalogs\", all)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCatalogsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getCatalogs();\n\n        ctx.export(\"allCatalogs\", all.applyValue(getCatalogsResult -\u003e getCatalogsResult));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getCatalogs\n      Arguments: {}\noutputs:\n  allCatalogs: ${all}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCatalogs.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCatalogs.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getCluster:getCluster": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allClusters = databricks.getClusters({});\nconst allCluster = .map(([, ]) =\u003e databricks.getCluster({\n    clusterId: __value,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_clusters = databricks.get_clusters()\nall_cluster = [databricks.get_cluster(cluster_id=__value) for __key, __value in data[\"databricks_clusters\"][\"ids\"]]\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "description": "The id of the cluster\n"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "The exact name of the cluster to search\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "cluster ID\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "Cluster name, which doesn’t have to be unique.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "cluster ID\n"
                    }
                },
                "type": "object",
                "required": [
                    "clusterId",
                    "clusterInfo",
                    "clusterName",
                    "id"
                ]
            }
        },
        "databricks:index/getClusterPolicy:getClusterPolicy": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nReferring to a cluster policy by name:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst personal = databricks.getClusterPolicy({\n    name: \"Personal Compute\",\n});\nconst myCluster = new databricks.Cluster(\"myCluster\", {policyId: personal.then(personal =\u003e personal.id)});\n// ...\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npersonal = databricks.get_cluster_policy(name=\"Personal Compute\")\nmy_cluster = databricks.Cluster(\"myCluster\", policy_id=personal.id)\n# ...\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var personal = Databricks.GetClusterPolicy.Invoke(new()\n    {\n        Name = \"Personal Compute\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"myCluster\", new()\n    {\n        PolicyId = personal.Apply(getClusterPolicyResult =\u003e getClusterPolicyResult.Id),\n    });\n\n    // ...\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpersonal, err := databricks.LookupClusterPolicy(ctx, \u0026databricks.LookupClusterPolicyArgs{\n\t\t\tName: \"Personal Compute\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"myCluster\", \u0026databricks.ClusterArgs{\n\t\t\tPolicyId: *pulumi.String(personal.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClusterPolicyArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()\n            .name(\"Personal Compute\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()        \n            .policyId(personal.applyValue(getClusterPolicyResult -\u003e getClusterPolicyResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    properties:\n      policyId: ${personal.id}\nvariables:\n  personal:\n    fn::invoke:\n      Function: databricks:getClusterPolicy\n      Arguments:\n        name: Personal Compute\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getClusterPolicy.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the cluster policy. The cluster policy must exist before this resource can be planned.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getClusterPolicy.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "maxClustersPerUser": {
                        "type": "integer"
                    },
                    "name": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "definition",
                    "maxClustersPerUser",
                    "name",
                    "id"
                ]
            }
        },
        "databricks:index/getClusters:getClusters": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve all clusters on this workspace on AWS or GCP:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getClusters\n      Arguments: {}\n```\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getClusters({\n    clusterNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_clusters(cluster_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetClusters.Invoke(new()\n    {\n        ClusterNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tClusterNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .clusterNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      Function: databricks:getClusters\n      Arguments:\n        clusterNameContains: shared\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string",
                        "description": "Only return databricks.Cluster ids that match the given name string.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Cluster ids\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getCurrentUser:getCurrentUser": {
            "description": "## Exported attributes\n\nData source exposes the following attributes:\n\n* `id` -  The id of the calling user.\n* `external_id` - ID of the user in an external identity provider.\n* `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`\n* `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n* `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n* `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.\n* `workspace_url` - URL of the current Databricks workspace.\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n",
            "outputs": {
                "description": "A collection of values returned by getCurrentUser.\n",
                "properties": {
                    "alphanumeric": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "home": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "repos": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string"
                    },
                    "workspaceUrl": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "alphanumeric",
                    "externalId",
                    "home",
                    "repos",
                    "userName",
                    "workspaceUrl",
                    "id"
                ]
            }
        },
        "databricks:index/getDbfsFile:getDbfsFile": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst report = databricks.getDbfsFile({\n    limitFileSize: 10240,\n    path: \"dbfs:/reports/some.csv\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nreport = databricks.get_dbfs_file(limit_file_size=10240,\n    path=\"dbfs:/reports/some.csv\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var report = Databricks.GetDbfsFile.Invoke(new()\n    {\n        LimitFileSize = 10240,\n        Path = \"dbfs:/reports/some.csv\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDbfsFile(ctx, \u0026databricks.LookupDbfsFileArgs{\n\t\t\tLimitFileSize: 10240,\n\t\t\tPath:          \"dbfs:/reports/some.csv\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()\n            .limitFileSize(10240)\n            .path(\"dbfs:/reports/some.csv\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  report:\n    fn::invoke:\n      Function: databricks:getDbfsFile\n      Arguments:\n        limitFileSize: 10240\n        path: dbfs:/reports/some.csv\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFile.\n",
                "properties": {
                    "limitFileSize": {
                        "type": "boolean",
                        "description": "Do lot load content for files smaller than this in bytes\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to get content of\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "limitFileSize",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFile.\n",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "base64-encoded file contents\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "size of the file in bytes\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "limitFileSize": {
                        "type": "boolean"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "content",
                    "fileSize",
                    "limitFileSize",
                    "path",
                    "id"
                ]
            }
        },
        "databricks:index/getDbfsFilePaths:getDbfsFilePaths": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst partitions = databricks.getDbfsFilePaths({\n    path: \"dbfs:/user/hive/default.db/table\",\n    recursive: false,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npartitions = databricks.get_dbfs_file_paths(path=\"dbfs:/user/hive/default.db/table\",\n    recursive=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var partitions = Databricks.GetDbfsFilePaths.Invoke(new()\n    {\n        Path = \"dbfs:/user/hive/default.db/table\",\n        Recursive = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetDbfsFilePaths(ctx, \u0026databricks.GetDbfsFilePathsArgs{\n\t\t\tPath:      \"dbfs:/user/hive/default.db/table\",\n\t\t\tRecursive: false,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()\n            .path(\"dbfs:/user/hive/default.db/table\")\n            .recursive(false)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  partitions:\n    fn::invoke:\n      Function: databricks:getDbfsFilePaths\n      Arguments:\n        path: dbfs:/user/hive/default.db/table\n        recursive: false\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFilePaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to perform listing\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or not recursively list all files\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFilePaths.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "path": {
                        "type": "string"
                    },
                    "pathLists": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList"
                        },
                        "description": "returns list of objects with `path` and `file_size` attributes in each\n"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "pathLists",
                    "recursive",
                    "id"
                ]
            }
        },
        "databricks:index/getDirectory:getDirectory": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getDirectory({\n    path: \"/Production\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_directory(path=\"/Production\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetDirectory.Invoke(new()\n    {\n        Path = \"/Production\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDirectory(ctx, \u0026databricks.LookupDirectoryArgs{\n\t\t\tPath: \"/Production\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDirectoryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()\n            .path(\"/Production\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      Function: databricks:getDirectory\n      Arguments:\n        path: /Production\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getDirectory.\n",
                "properties": {
                    "objectId": {
                        "type": "integer",
                        "description": "directory object ID\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Path to a directory in the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDirectory.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "directory object ID\n"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "objectId",
                    "path",
                    "id"
                ]
            }
        },
        "databricks:index/getGroup:getGroup": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAdding user to administrative group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst myMemberA = new databricks.GroupMember(\"myMemberA\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"myMemberA\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"myMemberA\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"myMemberA\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  *pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myMemberA:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getGroup.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n"
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the group. The group must exist before this resource can be planned.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Collect information for all nested groups. *Defaults to true.*\n"
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "displayName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getGroup.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n"
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean"
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "childGroups",
                    "displayName",
                    "externalId",
                    "groups",
                    "instanceProfiles",
                    "members",
                    "servicePrincipals",
                    "users",
                    "id"
                ]
            }
        },
        "databricks:index/getInstancePool:getInstancePool": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nReferring to an instance pool by name:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst pool = databricks.getInstancePool({\n    name: \"All spot\",\n});\nconst myCluster = new databricks.Cluster(\"myCluster\", {instancePoolId: data.databricks_instance_pool.pool.id});\n// ...\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npool = databricks.get_instance_pool(name=\"All spot\")\nmy_cluster = databricks.Cluster(\"myCluster\", instance_pool_id=data[\"databricks_instance_pool\"][\"pool\"][\"id\"])\n# ...\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var pool = Databricks.GetInstancePool.Invoke(new()\n    {\n        Name = \"All spot\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"myCluster\", new()\n    {\n        InstancePoolId = data.Databricks_instance_pool.Pool.Id,\n    });\n\n    // ...\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupInstancePool(ctx, \u0026databricks.LookupInstancePoolArgs{\n\t\t\tName: \"All spot\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"myCluster\", \u0026databricks.ClusterArgs{\n\t\t\tInstancePoolId: pulumi.Any(data.Databricks_instance_pool.Pool.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetInstancePoolArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()\n            .name(\"All spot\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()        \n            .instancePoolId(data.databricks_instance_pool().pool().id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    properties:\n      instancePoolId: ${data.databricks_instance_pool.pool.id}\nvariables:\n  pool:\n    fn::invoke:\n      Function: databricks:getInstancePool\n      Arguments:\n        name: All spot\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getInstancePool.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the instance pool. The instance pool must exist before this resource can be planned.\n"
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getInstancePool.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "name": {
                        "type": "string"
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name",
                    "poolInfo",
                    "id"
                ]
            }
        },
        "databricks:index/getJob:getJob": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nGetting the existing cluster id of specific databricks.Job by name or by id:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getJob({\n    jobName: \"My job\",\n});\nexport const jobNumWorkers = _this.then(_this =\u003e _this.jobSettings?.settings?.newCluster?.numWorkers);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_job(job_name=\"My job\")\npulumi.export(\"jobNumWorkers\", this.job_settings.settings.new_cluster.num_workers)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetJob.Invoke(new()\n    {\n        JobName = \"My job\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"jobNumWorkers\"] = @this.Apply(getJobResult =\u003e getJobResult).Apply(@this =\u003e @this.Apply(getJobResult =\u003e getJobResult.JobSettings?.Settings?.NewCluster?.NumWorkers)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupJob(ctx, \u0026databricks.LookupJobArgs{\n\t\t\tJobName: pulumi.StringRef(\"My job\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"jobNumWorkers\", this.JobSettings.Settings.NewCluster.NumWorkers)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJob(GetJobArgs.builder()\n            .jobName(\"My job\")\n            .build());\n\n        ctx.export(\"jobNumWorkers\", this_.jobSettings().settings().newCluster().numWorkers());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getJob\n      Arguments:\n        jobName: My job\noutputs:\n  jobNumWorkers: ${this.jobSettings.settings.newCluster.numWorkers}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getJobs data to get all jobs and their names from a workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJob.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "the id of databricks.Job if the resource was matched by name.\n"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "the job name of databricks.Job if the resource was matched by id.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJob.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "the id of databricks.Job if the resource was matched by name.\n"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "the job name of databricks.Job if the resource was matched by id.\n"
                    }
                },
                "type": "object",
                "required": [
                    "id",
                    "jobId",
                    "jobName",
                    "jobSettings",
                    "name"
                ]
            }
        },
        "databricks:index/getJobs:getJobs": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJobs.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJobs.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getMwsCredentials:getMwsCredentials": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.\n\nLists all databricks.MwsCredentials in Databricks Account.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nListing all workspaces in \n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMwsCredentials({});\nexport const allMwsCredentials = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_mws_credentials()\npulumi.export(\"allMwsCredentials\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMwsCredentials.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMwsCredentials\"] = all.Apply(getMwsCredentialsResult =\u003e getMwsCredentialsResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.LookupMwsCredentials(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMwsCredentials\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMwsCredentials();\n\n        ctx.export(\"allMwsCredentials\", all.applyValue(getMwsCredentialsResult -\u003e getMwsCredentialsResult.ids()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getMwsCredentials\n      Arguments: {}\noutputs:\n  allMwsCredentials: ${all.ids}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsCredentials.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsCredentials.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getMwsWorkspaces:getMwsWorkspaces": {
            "inputs": {
                "description": "A collection of arguments for invoking getMwsWorkspaces.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsWorkspaces.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getNodeType:getNodeType": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           *pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             *pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string",
                        "description": "Node category, which can be one of (depending on the cloud environment, could be checked with `databricks clusters list-node-types|jq '.node_types[]|.category'|sort |uniq`):\n* `General Purpose` (all clouds)\n* `General Purpose (HDD)` (Azure)\n* `Compute Optimized` (all clouds)\n* `Memory Optimized` (all clouds)\n* `Memory Optimized (Remote HDD)` (Azure)\n* `Storage Optimized` (AWS, Azure)\n* `GPU Accelerated` (AWS, Azure)\n"
                    },
                    "gbPerCore": {
                        "type": "integer",
                        "description": "Number of gigabytes per core available on instance. Conflicts with `min_memory_gb`. Defaults to *0*.\n"
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to nodes with AWS Graviton CPUs. Default to *false*.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean",
                        "description": ". Pick only nodes that have IO Cache. Defaults to *false*.\n"
                    },
                    "localDisk": {
                        "type": "boolean",
                        "description": "Pick only nodes with local storage. Defaults to *false*.\n"
                    },
                    "localDiskMinSize": {
                        "type": "integer",
                        "description": "Pick only nodes that have size local storage greater or equal to given value. Defaults to *0*.\n"
                    },
                    "minCores": {
                        "type": "integer",
                        "description": "Minimum number of CPU cores available on instance. Defaults to *0*.\n"
                    },
                    "minGpus": {
                        "type": "integer",
                        "description": "Minimum number of GPU's attached to instance. Defaults to *0*.\n"
                    },
                    "minMemoryGb": {
                        "type": "integer",
                        "description": "Minimum amount of memory per node in gigabytes. Defaults to *0*.\n"
                    },
                    "photonDriverCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon driver. Defaults to *false*.\n"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon workers. Defaults to *false*.\n"
                    },
                    "supportPortForwarding": {
                        "type": "boolean",
                        "description": "Pick only nodes that support port forwarding. Defaults to *false*.\n"
                    },
                    "vcpu": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string"
                    },
                    "gbPerCore": {
                        "type": "integer"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean"
                    },
                    "localDisk": {
                        "type": "boolean"
                    },
                    "localDiskMinSize": {
                        "type": "integer"
                    },
                    "minCores": {
                        "type": "integer"
                    },
                    "minGpus": {
                        "type": "integer"
                    },
                    "minMemoryGb": {
                        "type": "integer"
                    },
                    "photonDriverCapable": {
                        "type": "boolean"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean"
                    },
                    "supportPortForwarding": {
                        "type": "boolean"
                    },
                    "vcpu": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            }
        },
        "databricks:index/getNotebook:getNotebook": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst features = databricks.getNotebook({\n    format: \"SOURCE\",\n    path: \"/Production/Features\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfeatures = databricks.get_notebook(format=\"SOURCE\",\n    path=\"/Production/Features\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var features = Databricks.GetNotebook.Invoke(new()\n    {\n        Format = \"SOURCE\",\n        Path = \"/Production/Features\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupNotebook(ctx, \u0026databricks.LookupNotebookArgs{\n\t\t\tFormat: \"SOURCE\",\n\t\t\tPath:   \"/Production/Features\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()\n            .format(\"SOURCE\")\n            .path(\"/Production/Features\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  features:\n    fn::invoke:\n      Function: databricks:getNotebook\n      Arguments:\n        format: SOURCE\n        path: /Production/Features\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebook.\n",
                "properties": {
                    "format": {
                        "type": "string",
                        "description": "Notebook format to export. Either `SOURCE`, `HTML`, `JUPYTER`, or `DBC`.\n",
                        "willReplaceOnChanges": true
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Notebook path on the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "format",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebook.\n",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "notebook content in selected format\n"
                    },
                    "format": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "content",
                    "format",
                    "language",
                    "objectId",
                    "objectType",
                    "path",
                    "id"
                ]
            }
        },
        "databricks:index/getNotebookPaths:getNotebookPaths": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getNotebookPaths({\n    path: \"/Production\",\n    recursive: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_notebook_paths(path=\"/Production\",\n    recursive=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetNotebookPaths.Invoke(new()\n    {\n        Path = \"/Production\",\n        Recursive = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetNotebookPaths(ctx, \u0026databricks.GetNotebookPathsArgs{\n\t\t\tPath:      \"/Production\",\n\t\t\tRecursive: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookPathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()\n            .path(\"/Production\")\n            .recursive(true)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      Function: databricks:getNotebookPaths\n      Arguments:\n        path: /Production\n        recursive: true\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebookPaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path to workspace directory\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or recursively walk given path\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebookPaths.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "notebookPathLists": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList"
                        },
                        "description": "list of objects with `path` and `language` attributes\n"
                    },
                    "path": {
                        "type": "string"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "notebookPathLists",
                    "path",
                    "recursive",
                    "id"
                ]
            }
        },
        "databricks:index/getSchemas:getSchemas": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nListing all schemas in a _sandbox_ databricks_catalog:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = databricks.getSchemas({\n    catalogName: \"sandbox\",\n});\nexport const allSandboxSchemas = sandbox;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.get_schemas(catalog_name=\"sandbox\")\npulumi.export(\"allSandboxSchemas\", sandbox)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = Databricks.GetSchemas.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allSandboxSchemas\"] = sandbox.Apply(getSchemasResult =\u003e getSchemasResult),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.GetSchemas(ctx, \u0026databricks.GetSchemasArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allSandboxSchemas\", sandbox)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSchemasArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()\n            .catalogName(\"sandbox\")\n            .build());\n\n        ctx.export(\"allSandboxSchemas\", sandbox.applyValue(getSchemasResult -\u003e getSchemasResult));\n    }\n}\n```\n```yaml\nvariables:\n  sandbox:\n    fn::invoke:\n      Function: databricks:getSchemas\n      Arguments:\n        catalogName: sandbox\noutputs:\n  allSandboxSchemas: ${sandbox}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getServicePrincipal:getServicePrincipal": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAdding service principal `11111111-2222-3333-4444-555666777888` to administrative group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst spn = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst myMemberA = new databricks.GroupMember(\"myMemberA\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: spn.then(spn =\u003e spn.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nspn = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nmy_member_a = databricks.GroupMember(\"myMemberA\",\n    group_id=admins.id,\n    member_id=spn.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var spn = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"myMemberA\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = spn.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tspn, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"myMemberA\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  *pulumi.String(admins.Id),\n\t\t\tMemberId: *pulumi.String(spn.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(spn.applyValue(getServicePrincipalResult -\u003e getServicePrincipalResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${spn.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n  spn:\n    fn::invoke:\n      Function: databricks:getServicePrincipal\n      Arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipal.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "ID of the service principal. The service principal must exist before this resource can be retrieved.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipal.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "active",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "id",
                    "repos",
                    "spId"
                ]
            }
        },
        "databricks:index/getServicePrincipals:getServicePrincipals": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string",
                        "description": "Only return databricks.ServicePrincipal display name that match the given name string\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    }
                },
                "type": "object",
                "required": [
                    "applicationIds",
                    "displayNameContains",
                    "id"
                ]
            }
        },
        "databricks:index/getShare:getShare": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nGetting details of an existing share in the metastore\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getShare({\n    name: \"this\",\n});\nexport const createdBy = _this.then(_this =\u003e _this.createdBy);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_share(name=\"this\")\npulumi.export(\"createdBy\", this.created_by)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShare.Invoke(new()\n    {\n        Name = \"this\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"createdBy\"] = @this.Apply(getShareResult =\u003e getShareResult).Apply(@this =\u003e @this.Apply(getShareResult =\u003e getShareResult.CreatedBy)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupShare(ctx, \u0026databricks.LookupShareArgs{\n\t\t\tName: pulumi.StringRef(\"this\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"createdBy\", this.CreatedBy)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetShareArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShare(GetShareArgs.builder()\n            .name(\"this\")\n            .build());\n\n        ctx.export(\"createdBy\", this_.createdBy());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getShare\n      Arguments:\n        name: this\noutputs:\n  createdBy: ${this.createdBy}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShare.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the share\n"
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "description": "arrays containing details of each object in the share.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShare.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Full name of the object being shared.\n"
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "description": "arrays containing details of each object in the share.\n"
                    }
                },
                "type": "object",
                "required": [
                    "createdAt",
                    "createdBy",
                    "name",
                    "objects",
                    "id"
                ]
            }
        },
        "databricks:index/getShares:getShares": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nGetting all existing shares in the metastore\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getShares({});\nexport const shareName = _this.then(_this =\u003e _this.shares);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_shares()\npulumi.export(\"shareName\", this.shares)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShares.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"shareName\"] = @this.Apply(getSharesResult =\u003e getSharesResult).Apply(@this =\u003e @this.Apply(getSharesResult =\u003e getSharesResult.Shares)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetShares(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"shareName\", this.Shares)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSharesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShares();\n\n        ctx.export(\"shareName\", this_.shares());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getShares\n      Arguments: {}\noutputs:\n  shareName: ${this.shares}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShares.\n",
                "properties": {
                    "shares": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Share names.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShares.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "shares": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Share names.\n"
                    }
                },
                "type": "object",
                "required": [
                    "shares",
                    "id"
                ]
            }
        },
        "databricks:index/getSparkVersion:getSparkVersion": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           *pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             *pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that are in Beta stage. Default to `false`.\n"
                    },
                    "genomics": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.\n"
                    },
                    "gpu": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that support GPUs. Default to `false`.\n"
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`.\n"
                    },
                    "latest": {
                        "type": "boolean",
                        "description": "if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.\n"
                    },
                    "longTermSupport": {
                        "type": "boolean",
                        "description": "if we should limit the search only to LTS (long term support) \u0026 ESR (extended support) versions. Default to `false`.\n"
                    },
                    "ml": {
                        "type": "boolean",
                        "description": "if we should limit the search only to ML runtimes. Default to `false`.\n"
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Photon runtimes. Default to `false`.\n"
                    },
                    "scala": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean"
                    },
                    "genomics": {
                        "type": "boolean"
                    },
                    "gpu": {
                        "type": "boolean"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "latest": {
                        "type": "boolean"
                    },
                    "longTermSupport": {
                        "type": "boolean"
                    },
                    "ml": {
                        "type": "boolean"
                    },
                    "photon": {
                        "type": "boolean"
                    },
                    "scala": {
                        "type": "string"
                    },
                    "sparkVersion": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            }
        },
        "databricks:index/getSqlWarehouse:getSqlWarehouse": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allSqlWarehouses = databricks.getSqlWarehouses({});\nconst allSqlWarehouse = .map(([, ]) =\u003e databricks.getSqlWarehouse({\n    id: __value,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_sql_warehouses = databricks.get_sql_warehouses()\nall_sql_warehouse = [databricks.get_sql_warehouse(id=__value) for __key, __value in data[\"databricks_sql\"][\"warehouses\"][\"ids\"]]\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine).\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a Serverless warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL warehouses for the workspace.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the SQL warehouse\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "Databricks tags all warehouse resources with these tags.\n"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine).\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a Serverless warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL warehouses for the workspace.\n"
                    },
                    "id": {
                        "type": "string"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "Databricks tags all warehouse resources with these tags.\n"
                    }
                },
                "type": "object",
                "required": [
                    "autoStopMins",
                    "channel",
                    "clusterSize",
                    "dataSourceId",
                    "enablePhoton",
                    "enableServerlessCompute",
                    "id",
                    "instanceProfileArn",
                    "jdbcUrl",
                    "maxNumClusters",
                    "minNumClusters",
                    "name",
                    "numClusters",
                    "odbcParams",
                    "spotInstancePolicy",
                    "state",
                    "tags"
                ]
            }
        },
        "databricks:index/getSqlWarehouses:getSqlWarehouses": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve all SQL warehouses on this workspace on AWS or GCP:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouses();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments: {}\n```\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getSqlWarehouses({\n    warehouseNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_sql_warehouses(warehouse_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetSqlWarehouses.Invoke(new()\n    {\n        WarehouseNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, \u0026databricks.GetSqlWarehousesArgs{\n\t\t\tWarehouseNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()\n            .warehouseNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments:\n        warehouseNameContains: shared\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouses.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string",
                        "description": "Only return databricks.SqlEndpoint ids that match the given name string.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouses.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getTables:getTables": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ]
            }
        },
        "databricks:index/getUser:getUser": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAdding user to administrative group\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst myMemberA = new databricks.GroupMember(\"myMemberA\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.then(me =\u003e me.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.get_user(user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"myMemberA\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"myMemberA\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Apply(getUserResult =\u003e getUserResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"myMemberA\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  *pulumi.String(admins.Id),\n\t\t\tMemberId: *pulumi.String(me.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.applyValue(getUserResult -\u003e getUserResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n  me:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: me@example.com\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getUser.\n",
                "properties": {
                    "userId": {
                        "type": "string",
                        "description": "ID of the user.\n"
                    },
                    "userName": {
                        "type": "string",
                        "description": "User name of the user. The user must exist before this resource can be planned.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getUser.\n",
                "properties": {
                    "alphanumeric": {
                        "type": "string",
                        "description": "Alphanumeric representation of user local name. e.g. `mr_foo`.\n"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the user, e.g. `Mr Foo`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                    },
                    "userId": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string",
                        "description": "Name of the user, e.g. `mr.foo@example.com`.\n"
                    }
                },
                "type": "object",
                "required": [
                    "alphanumeric",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "repos",
                    "id"
                ]
            }
        },
        "databricks:index/getViews:getViews": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ]
            }
        },
        "databricks:index/getZones:getZones": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst zones = databricks.getZones({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nzones = databricks.get_zones()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var zones = Databricks.GetZones.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetZones(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var zones = DatabricksFunctions.getZones();\n\n    }\n}\n```\n```yaml\nvariables:\n  zones:\n    fn::invoke:\n      Function: databricks:getZones\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}",
            "outputs": {
                "description": "A collection of values returned by getZones.\n",
                "properties": {
                    "defaultZone": {
                        "type": "string",
                        "description": "This is the default zone that gets assigned to your workspace. This is the zone used by default for clusters and instance pools.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "zones": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "This is a list of all the zones available for your subnets in your Databricks workspace.\n"
                    }
                },
                "type": "object",
                "required": [
                    "defaultZone",
                    "zones",
                    "id"
                ]
            }
        }
    }
}