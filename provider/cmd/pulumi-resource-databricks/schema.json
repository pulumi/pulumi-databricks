{
    "name": "databricks",
    "displayName": "Databricks",
    "description": "A Pulumi package for creating and managing databricks cloud resources.",
    "keywords": [
        "pulumi",
        "databricks",
        "category/Infrastructure"
    ],
    "homepage": "https://www.pulumi.com",
    "license": "Apache-2.0",
    "attribution": "This Pulumi package is based on the [`databricks` Terraform Provider](https://github.com/databricks/terraform-provider-databricks).",
    "repository": "https://github.com/pulumi/pulumi-databricks",
    "logoUrl": "https://databricks.com/wp-content/uploads/2021/10/db-nav-logo.svg",
    "publisher": "Pulumi",
    "meta": {
        "moduleFormat": "(.*)(?:/[^/]*)"
    },
    "language": {
        "csharp": {
            "compatibility": "tfbridge20",
            "namespaces": null,
            "packageReferences": {
                "Pulumi": "3.*"
            }
        },
        "go": {
            "generateExtraInputTypes": true,
            "generateResourceContainerTypes": true,
            "importBasePath": "github.com/pulumi/pulumi-databricks/sdk/go/databricks"
        },
        "nodejs": {
            "compatibility": "tfbridge20",
            "dependencies": {
                "@pulumi/pulumi": "^3.0.0"
            },
            "devDependencies": {
                "@types/mime": "^2.0.0",
                "@types/node": "^10.0.0"
            },
            "disableUnionOutputTypes": true,
            "packageDescription": "A Pulumi package for creating and managing databricks cloud resources.",
            "packageName": "",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "typescriptVersion": ""
        },
        "python": {
            "compatibility": "tfbridge20",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "requires": {
                "pulumi": "\u003e=3.0.0,\u003c4.0.0"
            }
        }
    },
    "config": {
        "variables": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string"
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string"
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string"
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        }
    },
    "types": {
        "databricks:index/ClusterAutoscale:ClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAwsAttributes:ClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributes:ClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConf:ClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterDockerImage:ClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/ClusterGcpAttributes:ClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScript:ClusterInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptFile:ClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptS3:ClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptFile:ClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptS3:ClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterLibrary:ClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterLibraryCran:ClusterLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterLibraryMaven:ClusterLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterLibraryPypi:ClusterLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterLibraryCran:ClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterLibraryMaven:ClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterLibraryPypi:ClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterWorkloadType:ClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/GrantsGrant:GrantsGrant": {
            "properties": {
                "principal": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "(Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "(String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `\"us-west-2a\"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `\"us-west-2a\"` is not a valid zone ID if the Databricks deployment resides in the `\"us-east-1\"` region. This is an optional field. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max price for Azure spot instances.  Use `-1` to specify the lowest price.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer",
                    "description": "(Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskSize": {
                    "type": "integer",
                    "description": "(Integer) The size of each disk (in GiB) to attach.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instanceType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobEmailNotifications:JobEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobGitSource:JobGitSource": {
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `tag` and `commit`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "commit": {
                    "type": "string",
                    "description": "hash of Git commit to use. Conflicts with `branch` and `tag`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "provider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tag": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `branch` and `commit`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobCluster:JobJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibrary:JobLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobLibraryCran:JobLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobLibraryMaven:JobLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobLibraryPypi:JobLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibraryCran:JobLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobLibraryMaven:JobLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobLibraryPypi:JobLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewCluster:JobNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterInitScript:JobNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScript:JobNewClusterInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobNotebookTask:JobNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The absolute path of the databricks.Notebook to be run in the Databricks workspace. This path must begin with a slash. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobPipelineTask:JobPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobPythonWheelTask:JobPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobSchedule:JobSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this schedule is paused or not. Either “PAUSED” or “UNPAUSED”. When the pause_status field is omitted and a schedule is provided, the server will default to using \"UNPAUSED\" as a value for pause_status.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timezoneId": {
                    "type": "string",
                    "description": "A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pauseStatus",
                        "quartzCronExpression",
                        "timezoneId"
                    ]
                }
            }
        },
        "databricks:index/JobSparkJarTask:JobSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobSparkPythonTask:JobSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks.DbfsFile and S3 paths are supported. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobSparkSubmitTask:JobSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTask:JobTask": {
            "properties": {
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskDbtTask:JobTaskDbtTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskDependsOn:JobTaskDependsOn"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "description": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begin and complete and when this job is deleted. The default behavior is to not send any emails. This field is a block and is documented below.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "existingClusterId": {
                    "type": "string",
                    "description": "If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use `new_cluster` for greater reliability.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskLibrary:JobTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED result_state or INTERNAL_ERROR life_cycle_state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskNewCluster:JobTaskNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskNotebookTask:JobTaskNotebookTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskPipelineTask:JobTaskPipelineTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTask:JobTaskSqlTask",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout"
                    ]
                }
            }
        },
        "databricks:index/JobTaskDbtTask:JobTaskDbtTask": {
            "properties": {
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "projectDirectory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "schema": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskDependsOn:JobTaskDependsOn": {
            "properties": {
                "taskKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify on failure\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibrary:JobTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryCran:JobTaskLibraryCran",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "egg": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibraryCran:JobTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewCluster:JobTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "idempotencyToken": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebooks": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNotebookTask:JobTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The absolute path of the databricks.Notebook to be run in the Databricks workspace. This path must begin with a slash. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskPipelineTask:JobTaskPipelineTask": {
            "properties": {
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks.DbfsFile and S3 paths are supported. This field is required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTask:JobTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Parameters for the task\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "warehouseId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard": {
            "properties": {
                "dashboardId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/LibraryCran:LibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/LibraryMaven:LibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/LibraryPypi:LibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole": {
            "properties": {
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/MlflowModelTag:MlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec": {
            "properties": {
                "authorization": {
                    "type": "string",
                    "description": "Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `\u003cauth type\u003e \u003ccredentials\u003e`, e.g. `Bearer \u003caccess_token\u003e`. If set to an empty string, no authorization header will be included in the request.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableSslVerification": {
                    "type": "boolean",
                    "description": "Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "string": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "description": "External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec": {
            "properties": {
                "accessToken": {
                    "type": "string",
                    "description": "The personal access token used to authorize webhook's job runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jobId": {
                    "type": "string",
                    "description": "ID of the Databricks job that the webhook runs.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "accessToken",
                "jobId"
            ]
        },
        "databricks:index/MountAbfs:MountAbfs": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "initializeFileSystem": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "initializeFileSystem"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "containerName",
                        "initializeFileSystem",
                        "storageAccountName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountAdl:MountAdl": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "sparkConfPrefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageResourceName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "storageResourceName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountGs:MountGs": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "serviceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountS3:MountS3": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "instanceProfile": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountWasb:MountWasb": {
            "properties": {
                "authType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tokenSecretKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "tokenSecretScope": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "authType",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "authType",
                        "containerName",
                        "storageAccountName",
                        "tokenSecretKey",
                        "tokenSecretScope"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo": {
            "properties": {
                "keyAlias": {
                    "type": "string",
                    "description": "The AWS KMS key alias.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "keyArn": {
                    "type": "string",
                    "description": "The AWS KMS key's Amazon Resource Name (ARN).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "keyRegion": {
                    "type": "string",
                    "description": "(Computed) The AWS region in which KMS key is deployed to. This is not required.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "keyAlias",
                "keyArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "keyAlias",
                        "keyArn",
                        "keyRegion"
                    ]
                }
            }
        },
        "databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage": {
            "properties": {
                "errorMessage": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "errorType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints": {
            "properties": {
                "dataplaneRelays": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "restApis": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dataplaneRelays",
                "restApis"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceBucket:MwsWorkspacesCloudResourceBucket": {
            "properties": {
                "gcp": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceBucketGcp:MwsWorkspacesCloudResourceBucketGcp",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "gcp"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceBucketGcp:MwsWorkspacesCloudResourceBucketGcp": {
            "properties": {
                "projectId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "projectId"
            ]
        },
        "databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo": {
            "properties": {
                "authoritativeUserEmail": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "authoritativeUserFullName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customerName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "authoritativeUserEmail",
                "authoritativeUserFullName",
                "customerName"
            ]
        },
        "databricks:index/MwsWorkspacesNetwork:MwsWorkspacesNetwork": {
            "properties": {
                "gcpCommonNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesNetworkGcpCommonNetworkConfig:MwsWorkspacesNetworkGcpCommonNetworkConfig",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesNetworkGcpManagedNetworkConfig:MwsWorkspacesNetworkGcpManagedNetworkConfig",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "networkId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "gcpCommonNetworkConfig"
            ]
        },
        "databricks:index/MwsWorkspacesNetworkGcpCommonNetworkConfig:MwsWorkspacesNetworkGcpCommonNetworkConfig": {
            "properties": {
                "gkeClusterMasterIpRange": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gkeConnectivityType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "gkeClusterMasterIpRange",
                "gkeConnectivityType"
            ]
        },
        "databricks:index/MwsWorkspacesNetworkGcpManagedNetworkConfig:MwsWorkspacesNetworkGcpManagedNetworkConfig": {
            "properties": {
                "gkeClusterPodIpRange": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gkeClusterServiceIpRange": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "subnetCidr": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "gkeClusterPodIpRange",
                "gkeClusterServiceIpRange",
                "subnetCidr"
            ]
        },
        "databricks:index/MwsWorkspacesToken:MwsWorkspacesToken": {
            "properties": {
                "comment": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tokenId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "tokenValue": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "tokenId",
                        "tokenValue"
                    ]
                }
            }
        },
        "databricks:index/PermissionsAccessControl:PermissionsAccessControl": {
            "properties": {
                "groupName": {
                    "type": "string",
                    "description": "name of the group. We recommend setting permissions on groups.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "permissionLevel": {
                    "type": "string",
                    "description": "permission level according to specific resource. See examples above for the reference.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "Application ID of the service_principal.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "userName": {
                    "type": "string",
                    "description": "name of the user.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "permissionLevel"
            ]
        },
        "databricks:index/PipelineCluster:PipelineCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineClusterInitScript:PipelineClusterInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "label": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverNodeTypeId",
                        "nodeTypeId"
                    ]
                }
            }
        },
        "databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes": {
            "properties": {
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes": {
            "properties": {
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScript:PipelineClusterInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "file": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineFilters:PipelineFilters": {
            "properties": {
                "excludes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "includes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibrary:PipelineLibrary": {
            "properties": {
                "jar": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "maven": {
                    "$ref": "#/types/databricks:index/PipelineLibraryMaven:PipelineLibraryMaven",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "notebook": {
                    "$ref": "#/types/databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "whl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibraryMaven:PipelineLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "repo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook": {
            "properties": {
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata": {
            "properties": {
                "dnsName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "dnsName",
                "resourceId"
            ]
        },
        "databricks:index/SqlEndpointChannel:SqlEndpointChannel": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams": {
            "properties": {
                "host": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "hostname": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "port": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "protocol": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path",
                "port",
                "protocol"
            ]
        },
        "databricks:index/SqlEndpointTags:SqlEndpointTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "customTags"
            ]
        },
        "databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment": {
            "properties": {
                "principal": {
                    "type": "string",
                    "description": "`display_name` for a databricks.Group or databricks_user, `application_id` for a databricks_service_principal.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "set of available privilege names in upper case.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/SqlQueryParameter:SqlQueryParameter": {
            "properties": {
                "date": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDate:SqlQueryParameterDate",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dateRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetime": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimeRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimesec": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "datetimesecRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enum": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "number": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "text": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterText:SqlQueryParameterText",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "title": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/SqlQueryParameterDate:SqlQueryParameterDate": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "options": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "options"
            ]
        },
        "databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "separator": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "suffix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "prefix",
                "separator",
                "suffix"
            ]
        },
        "databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber": {
            "properties": {
                "value": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "queryId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "separator": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "suffix": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "prefix",
                "separator",
                "suffix"
            ]
        },
        "databricks:index/SqlQueryParameterText:SqlQueryParameterText": {
            "properties": {
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQuerySchedule:SqlQuerySchedule": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "daily": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "weekly": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous": {
            "properties": {
                "intervalSeconds": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "intervalSeconds"
            ]
        },
        "databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily": {
            "properties": {
                "intervalDays": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeOfDay": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "intervalDays",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly": {
            "properties": {
                "dayOfWeek": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "intervalWeeks": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "timeOfDay": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "untilDate": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "dayOfWeek",
                "intervalWeeks",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlWidgetParameter:SqlWidgetParameter": {
            "properties": {
                "mapTo": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "title": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "type": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "type"
            ]
        },
        "databricks:index/SqlWidgetPosition:SqlWidgetPosition": {
            "properties": {
                "autoHeight": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "posX": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "posY": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sizeX": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sizeY": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "sizeX",
                "sizeY"
            ]
        },
        "databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole": {
            "properties": {
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/TableColumn:TableColumn": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "name": {
                    "type": "string",
                    "description": "User-visible name of column\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nullable": {
                    "type": "boolean",
                    "description": "Whether field is nullable (Default: `true`)\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "partitionIndex": {
                    "type": "integer",
                    "description": "Partition ID\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "position": {
                    "type": "integer",
                    "description": "Ordinal position of column, starting at 0.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeIntervalType": {
                    "type": "string",
                    "description": "Format of `INTERVAL` columns\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeJson": {
                    "type": "string",
                    "description": "Column type spec (with metadata) as JSON string\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeName": {
                    "type": "string",
                    "description": "Name of (outer) type\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typePrecision": {
                    "type": "integer",
                    "description": "Digits of precision; applies to `DECIMAL` columns\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeScale": {
                    "type": "integer",
                    "description": "Digits to right of decimal; applies to `DECIMAL` columns\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "typeText": {
                    "type": "string",
                    "description": "Column type spec (with metadata) as SQL text\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/getClusterClusterInfo:getClusterClusterInfo": {
            "properties": {
                "autoscale": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterCores": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterId": {
                    "type": "string",
                    "description": "The id of the cluster\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterLogStatus": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterMemoryMb": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "clusterSource": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "creatorUserName": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driver": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "Use autoscaling local storage.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Enable local disk encryption.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "executors": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "jdbcPort": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastActivityTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastStateLossTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id.\n* `instance_pool_id` The pool of idle instances the cluster is attached to.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "numWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkContextId": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "state": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "stateMessage": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "terminateTime": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "terminationReason": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "defaultTags",
                "driverInstancePoolId",
                "sparkVersion",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "defaultTags",
                        "sparkVersion",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "minWorkers": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "ebsVolumeType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceProfileArn": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "firstOnDemand": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus": {
            "properties": {
                "lastAttempted": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "lastException": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "url": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "username": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "publicDns": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTimestamp": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "instanceId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "nodeId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "privateIp": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "publicDns": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "startTimestamp": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "bootDiskSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "googleServiceAccount": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "zoneId": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "destination": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "enableEncryption": {
                    "type": "boolean",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "encryptionType": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "endpoint": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "kmsKey": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "region": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason": {
            "properties": {
                "code": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "type": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList": {
            "properties": {
                "fileSize": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "description": "Path on DBFS for the file to perform listing\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList": {
            "properties": {
                "language": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "description": "Path to workspace directory\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams": {
            "properties": {
                "host": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "hostname": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "path": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "port": {
                    "type": "integer",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "protocol": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "path",
                "port",
                "protocol"
            ]
        },
        "databricks:index/getSqlWarehouseTags:getSqlWarehouseTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag"
                    },
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "customTags"
            ]
        },
        "databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                },
                "value": {
                    "type": "string",
                    "language": {
                        "python": {
                            "mapCase": false
                        }
                    }
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        }
    },
    "provider": {
        "description": "The provider type for the databricks package. By default, resources use package-wide configuration\nsettings, however an explicit `Provider` instance may be created and passed during resource\nconstruction to achieve fine-grained programmatic control over provider settings. See the\n[documentation](https://www.pulumi.com/docs/reference/programming-model/#providers) for more information.\n",
        "properties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string"
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string"
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string"
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        },
        "inputProperties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string"
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string"
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "password": {
                "type": "string"
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string"
            },
            "username": {
                "type": "string"
            }
        }
    },
    "resources": {
        "databricks:index/catalog:Catalog": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: databricks_metastore[\"this\"].id,\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=databricks_metastore[\"this\"][\"id\"],\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = databricks_metastore.This.Id,\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(databricks_metastore.This.Id),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .metastoreId(databricks_metastore.this().id())\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${databricks_metastore.this.id}\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/catalog:Catalog this \u003cname\u003e\n```\n\n ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                }
            },
            "required": [
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Catalog resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Catalog relative to parent metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the catalog owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Catalog properties.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/cluster:Cluster": {
            "description": "\n\n\n## Import\n\nThe resource cluster can be imported using cluster id. bash\n\n```sh\n $ pulumi import databricks:index/cluster:Cluster this \u003ccluster-id\u003e\n```\n\n ",
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  _We highly recommend having this setting present for Interactive/BI clusters._\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. _Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access._\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "- To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "state": {
                    "type": "string",
                    "description": "(string) State of the cluster.\n"
                },
                "url": {
                    "type": "string"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "required": [
                "clusterId",
                "defaultTags",
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "sparkVersion",
                "state",
                "url"
            ],
            "inputProperties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  _We highly recommend having this setting present for Interactive/BI clusters._\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. _Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access._\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "- To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "requiredInputs": [
                "sparkVersion"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Cluster resources.\n",
                "properties": {
                    "applyPolicyDefaultValues": {
                        "type": "boolean",
                        "description": "Whether to use policy default values for missing cluster attributes.\n"
                    },
                    "autoscale": {
                        "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                    },
                    "autoterminationMinutes": {
                        "type": "integer",
                        "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  _We highly recommend having this setting present for Interactive/BI clusters._\n"
                    },
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterLogConf": {
                        "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Additional tags for cluster resources. Databricks will tag all cluster resources (e.g., AWS EC2 instances and EBS volumes) with these tags in addition to `default_tags`.\n"
                    },
                    "dataSecurityMode": {
                        "type": "string",
                        "description": "Select the security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                    },
                    "defaultTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(map) Tags that are added by Databricks by default, regardless of any custom_tags that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e\n"
                    },
                    "dockerImage": {
                        "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                    },
                    "driverInstancePoolId": {
                        "type": "string",
                        "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                    },
                    "driverNodeTypeId": {
                        "type": "string",
                        "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                    },
                    "enableLocalDiskEncryption": {
                        "type": "boolean",
                        "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. _Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access._\n"
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                    },
                    "idempotencyToken": {
                        "type": "string",
                        "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "initScripts": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                        }
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "- To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                    },
                    "isPinned": {
                        "type": "boolean",
                        "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 70](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that.\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                        }
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                    },
                    "numWorkers": {
                        "type": "integer"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.\n"
                    },
                    "singleUserName": {
                        "type": "string",
                        "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                    },
                    "sparkConf": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map with key-value pairs to fine-tune Spark clusters, where you can provide custom [Spark configuration properties](https://spark.apache.org/docs/latest/configuration.html) in a cluster configuration.\n"
                    },
                    "sparkEnvVars": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                    },
                    "sshPublicKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "(string) State of the cluster.\n"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workloadType": {
                        "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/clusterPolicy:ClusterPolicy": {
            "description": "This resource creates a cluster policy, which limits the ability to create clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. cluster policies have ACLs that limit their use to specific users and groups. Only admin users can create, edit, and delete policies. Admin users also have access to all policies.\n\nCluster policies let you:\n\n* Limit users to create clusters with prescribed settings.\n* Simplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).\n* Control cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).\n\nCluster policy permissions limit which policies a user can select in the Policy drop-down when the user creates a cluster:\n\n* If no policies have been created in the workspace, the Policy drop-down does not display.\n* A user who has cluster create permission can select the `Free form` policy and create fully-configurable clusters.\n* A user who has both cluster create permission and access to cluster policies can select the Free form policy and policies they have access to.\n* A user that has access to only cluster policies, can select the policies they have access to.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* Dynamic Passthrough Clusters for a Group guide\n* End to end workspace management guide\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.getNodeType data to get the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n\n## Import\n\nThe resource cluster policy can be imported using the policy idbash\n\n```sh\n $ pulumi import databricks:index/clusterPolicy:ClusterPolicy this \u003ccluster-policy-id\u003e\n```\n\n ",
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the cluster policy.\n"
                }
            },
            "required": [
                "name",
                "policyId"
            ],
            "inputProperties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ClusterPolicy resources.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the cluster policy.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/dbfsFile:DbfsFile": {
            "description": "\n\n\n## Import\n\nThe resource dbfs file can be imported using the path of the file bash\n\n```sh\n $ pulumi import databricks:index/dbfsFile:DbfsFile this \u003cpath\u003e\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "dbfsPath": {
                    "type": "string",
                    "description": "Path, but with `dbfs:` prefix\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "dbfsPath",
                "fileSize",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "md5": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DbfsFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "dbfsPath": {
                        "type": "string",
                        "description": "Path, but with `dbfs:` prefix\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save.\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/directory:Directory": {
            "description": "\n\n\n## Import\n\nThe resource directory can be imported using directory path bash\n\n```sh\n $ pulumi import databricks:index/directory:Directory this /path/to/directory\n```\n\n ",
            "properties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n"
                }
            },
            "required": [
                "objectId",
                "path"
            ],
            "inputProperties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Directory resources.\n",
                "properties": {
                    "deleteRecursive": {
                        "type": "boolean"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a DIRECTORY\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/externalLocation:ExternalLocation": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access.\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- databricks.StorageCredential represent authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- `databricks.ExternalLocation` are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/externalLocation:ExternalLocation this \u003cname\u003e\n```\n\n ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external Location owner.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                }
            },
            "required": [
                "credentialName",
                "metastoreId",
                "name",
                "owner",
                "url"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external Location owner.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                }
            },
            "requiredInputs": [
                "credentialName",
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ExternalLocation resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "credentialName": {
                        "type": "string",
                        "description": "Name of the databricks.StorageCredential to use with this External Location.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the external Location owner.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the external location\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/gitCredential:GitCredential": {
            "description": "\n\n\n## Import\n\nThe resource cluster can be imported using ID of Git credential that could be obtained via REST APIbash\n\n```sh\n $ pulumi import databricks:index/gitCredential:GitCredential this \u003cgit-credential-id\u003e\n```\n\n ",
            "properties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string"
                }
            },
            "required": [
                "gitProvider",
                "gitUsername",
                "personalAccessToken"
            ],
            "inputProperties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "gitProvider",
                "gitUsername",
                "personalAccessToken"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GitCredential resources.\n",
                "properties": {
                    "force": {
                        "type": "boolean",
                        "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                    },
                    "gitUsername": {
                        "type": "string",
                        "description": "user name at Git provider.\n"
                    },
                    "personalAccessToken": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/globalInitScript:GlobalInitScript": {
            "description": "\n\n\n## Import\n\nThe resource global init script can be imported using script IDbash\n\n```sh\n $ pulumi import databricks:index/globalInitScript:GlobalInitScript this script_id\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "- the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "- the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "required": [
                "name",
                "position"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "- the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "- the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GlobalInitScript resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "enabled": {
                        "type": "boolean",
                        "description": "specifies if the script is enabled for execution, or not\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "- the name of the script.  It should be unique\n"
                    },
                    "position": {
                        "type": "integer",
                        "description": "- the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grants:Grants": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "view": {
                    "type": "string"
                }
            },
            "required": [
                "grants"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "grants"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grants resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "grants": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                        },
                        "language": {
                            "csharp": {
                                "name": "GrantDetails"
                            }
                        }
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/group:Group": {
            "description": "This resource allows you to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html), [Databricks Account Console](https://accounts.cloud.databricks.com/) or [Azure Databricks Account Console](https://accounts.azuredatabricks.net). You can also associate Databricks users and service principals to groups. This is useful if you are using an application to sync users \u0026 groups with SCIM API.\n\nTo create groups in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments\n\nRecommended to use along with Identity Provider SCIM provisioning to populate users into those groups:\n\n* [Azure Active Directory](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad)\n* [Okta](https://docs.databricks.com/administration-guide/users-groups/scim/okta.html)\n* [OneLogin](https://docs.databricks.com/administration-guide/users-groups/scim/onelogin.html)\n\n\n## Import\n\nYou can import a `databricks_group` resource with the name `my_group` like the followingbash\n\n```sh\n $ pulumi import databricks:index/group:Group my_group \u003cgroup_id\u003e\n```\n\n ",
            "properties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "displayName",
                "url"
            ],
            "inputProperties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n",
                    "willReplaceOnChanges": true
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Group resources.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is the display name for the given group.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n",
                        "willReplaceOnChanges": true
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupInstanceProfile:GroupInstanceProfile": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider.\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_group.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"myGroup\", {});\nconst myGroupInstanceProfile = new databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", {\n    groupId: myGroup.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"myGroup\")\nmy_group_instance_profile = databricks.GroupInstanceProfile(\"myGroupInstanceProfile\",\n    group_id=my_group.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"myGroup\");\n\n    var myGroupInstanceProfile = new Databricks.GroupInstanceProfile(\"myGroupInstanceProfile\", new()\n    {\n        GroupId = myGroup.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"myGroup\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"myGroupInstanceProfile\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           myGroup.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\");\n\n        var myGroupInstanceProfile = new GroupInstanceProfile(\"myGroupInstanceProfile\", GroupInstanceProfileArgs.builder()        \n            .groupId(myGroup.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n  myGroupInstanceProfile:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${myGroup.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks_group_member to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "instanceProfileId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "instanceProfileId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupInstanceProfile resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instancePool:InstancePool": {
            "description": "\n\n\n## Import\n\nThe resource instance pool can be imported using it's idbash\n\n```sh\n $ pulumi import databricks:index/instancePool:InstancePool this \u003cinstance-pool-id\u003e\n```\n\n ",
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n"
                }
            },
            "required": [
                "awsAttributes",
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "inputProperties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                    "willReplaceOnChanges": true
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                    "willReplaceOnChanges": true
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n",
                    "willReplaceOnChanges": true
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                    "willReplaceOnChanges": true
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                    "willReplaceOnChanges": true
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                    "willReplaceOnChanges": true
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                    "willReplaceOnChanges": true
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    },
                    "willReplaceOnChanges": true
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstancePool resources.\n",
                "properties": {
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                        "willReplaceOnChanges": true
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                        "willReplaceOnChanges": true
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). *Databricks allows at most 43 custom tags.*\n",
                        "willReplaceOnChanges": true
                    },
                    "diskSpec": {
                        "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                        "willReplaceOnChanges": true
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                        "willReplaceOnChanges": true
                    },
                    "idleInstanceAutoterminationMinutes": {
                        "type": "integer",
                        "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                    },
                    "instancePoolFleetAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string"
                    },
                    "instancePoolName": {
                        "type": "string",
                        "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                    },
                    "maxCapacity": {
                        "type": "integer",
                        "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling.\n"
                    },
                    "minIdleInstances": {
                        "type": "integer",
                        "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                        "willReplaceOnChanges": true
                    },
                    "preloadedDockerImages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                        },
                        "willReplaceOnChanges": true
                    },
                    "preloadedSparkVersions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instanceProfile:InstanceProfile": {
            "description": "This resource allows you to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount. The following example demonstrates how to create an instance profile and create a cluster with it. When creating a new `databricks.InstanceProfile`, Databricks validates that it has sufficient permissions to launch instances with the instance profile. This validation uses AWS dry-run mode for the [AWS EC2 RunInstances API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html).\n\n\u003e **Note** Please switch to databricks.StorageCredential with Unity Catalog to manage storage credentials, which provides a better and faster way for managing credential security.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst crossaccountRoleName = config.require(\"crossaccountRoleName\");\nconst assumeRoleForEc2 = aws.iam.getPolicyDocument({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            identifiers: [\"ec2.amazonaws.com\"],\n            type: \"Service\",\n        }],\n    }],\n});\nconst roleForS3Access = new aws.iam.Role(\"roleForS3Access\", {\n    description: \"Role for shared access\",\n    assumeRolePolicy: assumeRoleForEc2.then(assumeRoleForEc2 =\u003e assumeRoleForEc2.json),\n});\nconst passRoleForS3AccessPolicyDocument = aws.iam.getPolicyDocumentOutput({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"iam:PassRole\"],\n        resources: [roleForS3Access.arn],\n    }],\n});\nconst passRoleForS3AccessPolicy = new aws.iam.Policy(\"passRoleForS3AccessPolicy\", {\n    path: \"/\",\n    policy: passRoleForS3AccessPolicyDocument.apply(passRoleForS3AccessPolicyDocument =\u003e passRoleForS3AccessPolicyDocument.json),\n});\nconst crossAccount = new aws.iam.RolePolicyAttachment(\"crossAccount\", {\n    policyArn: passRoleForS3AccessPolicy.arn,\n    role: crossaccountRoleName,\n});\nconst sharedInstanceProfile = new aws.iam.InstanceProfile(\"sharedInstanceProfile\", {role: roleForS3Access.name});\nconst sharedIndex_instanceProfileInstanceProfile = new databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", {instanceProfileArn: sharedInstanceProfile.arn});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Cluster(\"this\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    awsAttributes: {\n        instanceProfileArn: sharedIndex / instanceProfileInstanceProfile.id,\n        availability: \"SPOT\",\n        zoneId: \"us-east-1\",\n        firstOnDemand: 1,\n        spotBidPricePercent: 100,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ncrossaccount_role_name = config.require(\"crossaccountRoleName\")\nassume_role_for_ec2 = aws.iam.get_policy_document(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"sts:AssumeRole\"],\n    principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n        identifiers=[\"ec2.amazonaws.com\"],\n        type=\"Service\",\n    )],\n)])\nrole_for_s3_access = aws.iam.Role(\"roleForS3Access\",\n    description=\"Role for shared access\",\n    assume_role_policy=assume_role_for_ec2.json)\npass_role_for_s3_access_policy_document = aws.iam.get_policy_document_output(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"iam:PassRole\"],\n    resources=[role_for_s3_access.arn],\n)])\npass_role_for_s3_access_policy = aws.iam.Policy(\"passRoleForS3AccessPolicy\",\n    path=\"/\",\n    policy=pass_role_for_s3_access_policy_document.json)\ncross_account = aws.iam.RolePolicyAttachment(\"crossAccount\",\n    policy_arn=pass_role_for_s3_access_policy.arn,\n    role=crossaccount_role_name)\nshared_instance_profile = aws.iam.InstanceProfile(\"sharedInstanceProfile\", role=role_for_s3_access.name)\nshared_index_instance_profile_instance_profile = databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", instance_profile_arn=shared_instance_profile.arn)\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Cluster(\"this\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    aws_attributes=databricks.ClusterAwsAttributesArgs(\n        instance_profile_arn=shared_index / instance_profile_instance_profile[\"id\"],\n        availability=\"SPOT\",\n        zone_id=\"us-east-1\",\n        first_on_demand=1,\n        spot_bid_price_percent=100,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var crossaccountRoleName = config.Require(\"crossaccountRoleName\");\n    var assumeRoleForEc2 = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Identifiers = new[]\n                        {\n                            \"ec2.amazonaws.com\",\n                        },\n                        Type = \"Service\",\n                    },\n                },\n            },\n        },\n    });\n\n    var roleForS3Access = new Aws.Iam.Role(\"roleForS3Access\", new()\n    {\n        Description = \"Role for shared access\",\n        AssumeRolePolicy = assumeRoleForEc2.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var passRoleForS3AccessPolicyDocument = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"iam:PassRole\",\n                },\n                Resources = new[]\n                {\n                    roleForS3Access.Arn,\n                },\n            },\n        },\n    });\n\n    var passRoleForS3AccessPolicy = new Aws.Iam.Policy(\"passRoleForS3AccessPolicy\", new()\n    {\n        Path = \"/\",\n        PolicyDocument = passRoleForS3AccessPolicyDocument.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var crossAccount = new Aws.Iam.RolePolicyAttachment(\"crossAccount\", new()\n    {\n        PolicyArn = passRoleForS3AccessPolicy.Arn,\n        Role = crossaccountRoleName,\n    });\n\n    var sharedInstanceProfile = new Aws.Iam.InstanceProfile(\"sharedInstanceProfile\", new()\n    {\n        Role = roleForS3Access.Name,\n    });\n\n    var sharedIndex_instanceProfileInstanceProfile = new Databricks.InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", new()\n    {\n        InstanceProfileArn = sharedInstanceProfile.Arn,\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        AwsAttributes = new Databricks.Inputs.ClusterAwsAttributesArgs\n        {\n            InstanceProfileArn = sharedIndex / instanceProfileInstanceProfile.Id,\n            Availability = \"SPOT\",\n            ZoneId = \"us-east-1\",\n            FirstOnDemand = 1,\n            SpotBidPricePercent = 100,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tcrossaccountRoleName := cfg.Require(\"crossaccountRoleName\")\n\t\tassumeRoleForEc2, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\tiam.GetPolicyDocumentStatement{\n\t\t\t\t\tEffect: pulumi.StringRef(\"Allow\"),\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\tiam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"ec2.amazonaws.com\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tType: \"Service\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\troleForS3Access, err := iam.NewRole(ctx, \"roleForS3Access\", \u0026iam.RoleArgs{\n\t\t\tDescription:      pulumi.String(\"Role for shared access\"),\n\t\t\tAssumeRolePolicy: pulumi.String(assumeRoleForEc2.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpassRoleForS3AccessPolicyDocument := iam.GetPolicyDocumentOutput(ctx, iam.GetPolicyDocumentOutputArgs{\n\t\t\tStatements: iam.GetPolicyDocumentStatementArray{\n\t\t\t\t\u0026iam.GetPolicyDocumentStatementArgs{\n\t\t\t\t\tEffect: pulumi.String(\"Allow\"),\n\t\t\t\t\tActions: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"iam:PassRole\"),\n\t\t\t\t\t},\n\t\t\t\t\tResources: pulumi.StringArray{\n\t\t\t\t\t\troleForS3Access.Arn,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tpassRoleForS3AccessPolicy, err := iam.NewPolicy(ctx, \"passRoleForS3AccessPolicy\", \u0026iam.PolicyArgs{\n\t\t\tPath: pulumi.String(\"/\"),\n\t\t\tPolicy: passRoleForS3AccessPolicyDocument.ApplyT(func(passRoleForS3AccessPolicyDocument iam.GetPolicyDocumentResult) (string, error) {\n\t\t\t\treturn passRoleForS3AccessPolicyDocument.Json, nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"crossAccount\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: passRoleForS3AccessPolicy.Arn,\n\t\t\tRole:      pulumi.String(crossaccountRoleName),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedInstanceProfile, err := iam.NewInstanceProfile(ctx, \"sharedInstanceProfile\", \u0026iam.InstanceProfileArgs{\n\t\t\tRole: roleForS3Access.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstanceProfile(ctx, \"sharedIndex/instanceProfileInstanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: sharedInstanceProfile.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tAwsAttributes: \u0026ClusterAwsAttributesArgs{\n\t\t\t\tInstanceProfileArn:  sharedIndex / instanceProfileInstanceProfile.Id,\n\t\t\t\tAvailability:        pulumi.String(\"SPOT\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1\"),\n\t\t\t\tFirstOnDemand:       pulumi.Int(1),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.aws.iam.InstanceProfile;\nimport com.pulumi.aws.iam.InstanceProfileArgs;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport com.pulumi.databricks.inputs.ClusterAwsAttributesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var crossaccountRoleName = config.get(\"crossaccountRoleName\");\n        final var assumeRoleForEc2 = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .identifiers(\"ec2.amazonaws.com\")\n                    .type(\"Service\")\n                    .build())\n                .build())\n            .build());\n\n        var roleForS3Access = new Role(\"roleForS3Access\", RoleArgs.builder()        \n            .description(\"Role for shared access\")\n            .assumeRolePolicy(assumeRoleForEc2.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        final var passRoleForS3AccessPolicyDocument = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"iam:PassRole\")\n                .resources(roleForS3Access.arn())\n                .build())\n            .build());\n\n        var passRoleForS3AccessPolicy = new Policy(\"passRoleForS3AccessPolicy\", PolicyArgs.builder()        \n            .path(\"/\")\n            .policy(passRoleForS3AccessPolicyDocument.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult).applyValue(passRoleForS3AccessPolicyDocument -\u003e passRoleForS3AccessPolicyDocument.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json())))\n            .build());\n\n        var crossAccount = new RolePolicyAttachment(\"crossAccount\", RolePolicyAttachmentArgs.builder()        \n            .policyArn(passRoleForS3AccessPolicy.arn())\n            .role(crossaccountRoleName)\n            .build());\n\n        var sharedInstanceProfile = new InstanceProfile(\"sharedInstanceProfile\", InstanceProfileArgs.builder()        \n            .role(roleForS3Access.name())\n            .build());\n\n        var sharedIndex_instanceProfileInstanceProfile = new InstanceProfile(\"sharedIndex/instanceProfileInstanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(sharedInstanceProfile.arn())\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion();\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latest.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .awsAttributes(ClusterAwsAttributesArgs.builder()\n                .instanceProfileArn(sharedIndex / instanceProfileInstanceProfile.id())\n                .availability(\"SPOT\")\n                .zoneId(\"us-east-1\")\n                .firstOnDemand(1)\n                .spotBidPricePercent(100)\n                .build())\n            .build());\n\n    }\n}\n```\n\n## Usage with Cluster Policies\n\nIt is advised to keep all common configurations in Cluster Policies to maintain control of the environments launched, so `databricks.Cluster` above could be replaced with `databricks.ClusterPolicy`:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ClusterPolicy(\"this\", {definition: JSON.stringify({\n    \"aws_attributes.instance_profile_arn\": {\n        type: \"fixed\",\n        value: databricks_instance_profile.shared.arn,\n    },\n})});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nthis = databricks.ClusterPolicy(\"this\", definition=json.dumps({\n    \"aws_attributes.instance_profile_arn\": {\n        \"type\": \"fixed\",\n        \"value\": databricks_instance_profile[\"shared\"][\"arn\"],\n    },\n}))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ClusterPolicy(\"this\", new()\n    {\n        Definition = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"aws_attributes.instance_profile_arn\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"fixed\",\n                [\"value\"] = databricks_instance_profile.Shared.Arn,\n            },\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"aws_attributes.instance_profile_arn\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": databricks_instance_profile.Shared.Arn,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"this\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tDefinition: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ClusterPolicy(\"this\", ClusterPolicyArgs.builder()        \n            .definition(serializeJson(\n                jsonObject(\n                    jsonProperty(\"aws_attributes.instance_profile_arn\", jsonObject(\n                        jsonProperty(\"type\", \"fixed\"),\n                        jsonProperty(\"value\", databricks_instance_profile.shared().arn())\n                    ))\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ClusterPolicy\n    properties:\n      definition:\n        Fn::ToJSON:\n          aws_attributes.instance_profile_arn:\n            type: fixed\n            value: ${databricks_instance_profile.shared.arn}\n```\n\n## Granting access to all users\n\nYou can make instance profile available to all users by associating it with the special group called `users` through databricks.Group data source.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.InstanceProfile(\"this\", {instanceProfileArn: aws_iam_instance_profile.shared.arn});\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst all = new databricks.GroupInstanceProfile(\"all\", {\n    groupId: users.then(users =\u003e users.id),\n    instanceProfileId: _this.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.InstanceProfile(\"this\", instance_profile_arn=aws_iam_instance_profile[\"shared\"][\"arn\"])\nusers = databricks.get_group(display_name=\"users\")\nall = databricks.GroupInstanceProfile(\"all\",\n    group_id=users.id,\n    instance_profile_id=this.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = aws_iam_instance_profile.Shared.Arn,\n    });\n\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var all = new Databricks.GroupInstanceProfile(\"all\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        InstanceProfileId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.Any(aws_iam_instance_profile.Shared.Arn),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026GetGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"all\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           pulumi.String(users.Id),\n\t\t\tInstanceProfileId: this.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new InstanceProfile(\"this\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(aws_iam_instance_profile.shared().arn())\n            .build());\n\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var all = new GroupInstanceProfile(\"all\", GroupInstanceProfileArgs.builder()        \n            .groupId(users.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .instanceProfileId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: ${aws_iam_instance_profile.shared.arn}\n  all:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${users.id}\n      instanceProfileId: ${this.id}\nvariables:\n  users:\n    Fn::Invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: users\n```\n\n\n## Import\n\nThe resource instance profile can be imported using the ARN of it bash\n\n```sh\n $ pulumi import databricks:index/instanceProfile:InstanceProfile this \u003cinstance-profile-arn\u003e\n```\n\n ",
            "properties": {
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "required": [
                "skipValidation"
            ],
            "inputProperties": {
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n",
                    "willReplaceOnChanges": true
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n",
                    "willReplaceOnChanges": true
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstanceProfile resources.\n",
                "properties": {
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n",
                        "willReplaceOnChanges": true
                    },
                    "isMetaInstanceProfile": {
                        "type": "boolean",
                        "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n",
                        "willReplaceOnChanges": true
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. “Your requested instance type is not supported in your requested availability zone”), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/ipAccessList:IpAccessList": {
            "description": "Security-conscious enterprises that use cloud SaaS applications need to restrict access to their own employees. Authentication helps to prove user identity, but that does not enforce network location of the users. Accessing a cloud service from an unsecured network can pose security risks to an enterprise, especially when the user may have authorized access to sensitive or personal data. Enterprise network perimeters apply security policies and limit access to external services (for example, firewalls, proxies, DLP, and logging), so access beyond these controls are assumed to be untrusted. Please see [IP Access List](https://docs.databricks.com/security/network/ip-access-list.html) for full feature documentation.\n\n\u003e **Note** The total number of IP addresses and CIDR scopes provided across all ACL Lists in a workspace can not exceed 1000.  Refer to the docs above for specifics.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\nconst allowed_list = new databricks.IpAccessList(\"allowed-list\", {\n    label: \"allow_in\",\n    listType: \"ALLOW\",\n    ipAddresses: [\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n}, {\n    dependsOn: [_this],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\nallowed_list = databricks.IpAccessList(\"allowed-list\",\n    label=\"allow_in\",\n    list_type=\"ALLOW\",\n    ip_addresses=[\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n    opts=pulumi.ResourceOptions(depends_on=[this]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n    var allowed_list = new Databricks.IpAccessList(\"allowed-list\", new()\n    {\n        Label = \"allow_in\",\n        ListType = \"ALLOW\",\n        IpAddresses = new[]\n        {\n            \"1.2.3.0/24\",\n            \"1.2.5.0/24\",\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn = new[]\n        {\n            @this,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.AnyMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewIpAccessList(ctx, \"allowed-list\", \u0026databricks.IpAccessListArgs{\n\t\t\tLabel:    pulumi.String(\"allow_in\"),\n\t\t\tListType: pulumi.String(\"ALLOW\"),\n\t\t\tIpAddresses: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"1.2.3.0/24\"),\n\t\t\t\tpulumi.String(\"1.2.5.0/24\"),\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthis,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport com.pulumi.databricks.IpAccessList;\nimport com.pulumi.databricks.IpAccessListArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n        var allowed_list = new IpAccessList(\"allowed-list\", IpAccessListArgs.builder()        \n            .label(\"allow_in\")\n            .listType(\"ALLOW\")\n            .ipAddresses(            \n                \"1.2.3.0/24\",\n                \"1.2.5.0/24\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(this_)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n  allowed-list:\n    type: databricks:IpAccessList\n    properties:\n      label: allow_in\n      listType: ALLOW\n      ipAddresses:\n        - 1.2.3.0/24\n        - 1.2.5.0/24\n    options:\n      dependson:\n        - ${this}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsPrivateAccessSettings to create a [Private Access Setting](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-5-create-a-private-access-settings-configuration-using-the-databricks-account-api) that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nThe databricks_ip_access_list can be imported using idbash\n\n```sh\n $ pulumi import databricks:index/ipAccessList:IpAccessList this \u003clist-id\u003e\n```\n\n ",
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "This is a field to allow the group to have instance pool create privileges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                }
            },
            "required": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "inputProperties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "This is a field to allow the group to have instance pool create privileges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                }
            },
            "requiredInputs": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering IpAccessList resources.\n",
                "properties": {
                    "enabled": {
                        "type": "boolean",
                        "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                    },
                    "ipAddresses": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "This is a field to allow the group to have instance pool create privileges.\n"
                    },
                    "label": {
                        "type": "string",
                        "description": "This is the display name for the given IP ACL List.\n"
                    },
                    "listType": {
                        "type": "string",
                        "description": "Can only be \"ALLOW\" or \"BLOCK\"\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/job:Job": {
            "description": "\n\n\n## Import\n\nThe resource job can be imported using the id of the job bash\n\n```sh\n $ pulumi import databricks:index/job:Job this \u003cjob-id\u003e\n```\n\n ",
            "properties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begin and complete and when this job is deleted. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "existingClusterId": {
                    "type": "string",
                    "description": "If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use `new_cluster` for greater reliability.\n"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED result_state or INTERNAL_ERROR life_cycle_state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "required": [
                "format",
                "name",
                "url"
            ],
            "inputProperties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begin and complete and when this job is deleted. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "existingClusterId": {
                    "type": "string",
                    "description": "If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use `new_cluster` for greater reliability.\n"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED result_state or INTERNAL_ERROR life_cycle_state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Job resources.\n",
                "properties": {
                    "alwaysRunning": {
                        "type": "boolean",
                        "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n"
                    },
                    "emailNotifications": {
                        "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                        "description": "(List) An optional set of email addresses notified when runs of this job begin and complete and when this job is deleted. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                    },
                    "existingClusterId": {
                        "type": "string",
                        "description": "If existing_cluster_id, the ID of an existing cluster that will be used for all runs of this job. When running jobs on an existing cluster, you may need to manually restart the cluster if it stops responding. We strongly suggest to use `new_cluster` for greater reliability.\n"
                    },
                    "format": {
                        "type": "string"
                    },
                    "gitSource": {
                        "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                    },
                    "jobClusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                        }
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                        },
                        "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.\n"
                    },
                    "maxConcurrentRuns": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                    },
                    "maxRetries": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a FAILED result_state or INTERNAL_ERROR life_cycle_state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.\n"
                    },
                    "minRetryIntervalMillis": {
                        "type": "integer",
                        "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "An optional name for the job. The default value is Untitled.\n"
                    },
                    "newCluster": {
                        "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster",
                        "description": "Same set of parameters as for databricks.Cluster resource.\n"
                    },
                    "notebookTask": {
                        "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask"
                    },
                    "pipelineTask": {
                        "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask"
                    },
                    "pythonWheelTask": {
                        "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask"
                    },
                    "retryOnTimeout": {
                        "type": "boolean",
                        "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                        "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                    },
                    "sparkJarTask": {
                        "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask"
                    },
                    "sparkPythonTask": {
                        "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask"
                    },
                    "sparkSubmitTask": {
                        "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask"
                    },
                    "tags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(Map) An optional map of the tags associated with the job. Specified tags will be used as cluster tags for job clusters.\n"
                    },
                    "tasks": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobTask:JobTask"
                        }
                    },
                    "timeoutSeconds": {
                        "type": "integer",
                        "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the job on the given workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/library:Library": {
            "description": "Installs a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster. Each different type of library has a slightly different syntax. It's possible to set only one type of library within one resource. Otherwise, the plan will fail with an error. \n\n\u003e **Note** `databricks.Library` resource would always start the associated cluster if it's not running, so make sure to have auto-termination configured. It's not possible to atomically change the version of the same library without cluster restart. Libraries are fully removed from the cluster only after restart.\n\n## Java/Scala JAR\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/app-0.0.1.jar`,\n    path: \"/FileStore/app-0.0.1.jar\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    jar: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/app-0.0.1.jar\",\n    path=\"/FileStore/app-0.0.1.jar\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    jar=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/app-0.0.1.jar\",\n        Path = \"/FileStore/app-0.0.1.jar\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Jar = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/app-0.0.1.jar\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/app-0.0.1.jar\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tJar:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/app-0.0.1.jar\", path.module()))\n            .path(\"/FileStore/app-0.0.1.jar\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .jar(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/app-0.0.1.jar\n      path: /FileStore/app-0.0.1.jar\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      jar: ${appDbfsFile.dbfsPath}\n```\n\n## Java/Scala Maven\n\nInstalling artifacts from Maven repository. You can also optionally specify a `repo` parameter for a custom Maven-style repository, that should be accessible without any authentication. Maven libraries are resolved in Databricks Control Plane, so repo should be accessible from it. It can even be properly configured [maven s3 wagon](https://github.com/seahen/maven-s3-wagon), [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) or [Azure Artifacts](https://azure.microsoft.com/en-us/services/devops/artifacts/).\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst deequ = new databricks.Library(\"deequ\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    maven: {\n        coordinates: \"com.amazon.deequ:deequ:1.0.4\",\n        exclusions: [\"org.apache.avro:avro\"],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndeequ = databricks.Library(\"deequ\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    maven=databricks.LibraryMavenArgs(\n        coordinates=\"com.amazon.deequ:deequ:1.0.4\",\n        exclusions=[\"org.apache.avro:avro\"],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var deequ = new Databricks.Library(\"deequ\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Maven = new Databricks.Inputs.LibraryMavenArgs\n        {\n            Coordinates = \"com.amazon.deequ:deequ:1.0.4\",\n            Exclusions = new[]\n            {\n                \"org.apache.avro:avro\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"deequ\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tMaven: \u0026LibraryMavenArgs{\n\t\t\t\tCoordinates: pulumi.String(\"com.amazon.deequ:deequ:1.0.4\"),\n\t\t\t\tExclusions: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"org.apache.avro:avro\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryMavenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var deequ = new Library(\"deequ\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .maven(LibraryMavenArgs.builder()\n                .coordinates(\"com.amazon.deequ:deequ:1.0.4\")\n                .exclusions(\"org.apache.avro:avro\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  deequ:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      maven:\n        coordinates: com.amazon.deequ:deequ:1.0.4\n        exclusions:\n          - org.apache.avro:avro\n```\n\n## Python Wheel\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/baz.whl`,\n    path: \"/FileStore/baz.whl\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    whl: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/baz.whl\",\n    path=\"/FileStore/baz.whl\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    whl=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/baz.whl\",\n        Path = \"/FileStore/baz.whl\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Whl = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/baz.whl\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/baz.whl\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tWhl:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/baz.whl\", path.module()))\n            .path(\"/FileStore/baz.whl\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .whl(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/baz.whl\n      path: /FileStore/baz.whl\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      whl: ${appDbfsFile.dbfsPath}\n```\n\n## Python PyPI\n\nInstalling Python PyPI artifacts. You can optionally also specify the `repo` parameter for a custom PyPI mirror, which should be accessible without any authentication for the network that cluster runs in.\n\n\u003e **Note** `repo` host should be accessible from the Internet by Databricks control plane. If connectivity to custom PyPI repositories is required, please modify cluster-node `/etc/pip.conf` through databricks_global_init_script.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fbprophet = new databricks.Library(\"fbprophet\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    pypi: {\n        \"package\": \"fbprophet==0.6\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfbprophet = databricks.Library(\"fbprophet\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    pypi=databricks.LibraryPypiArgs(\n        package=\"fbprophet==0.6\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fbprophet = new Databricks.Library(\"fbprophet\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Pypi = new Databricks.Inputs.LibraryPypiArgs\n        {\n            Package = \"fbprophet==0.6\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"fbprophet\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tPypi: \u0026LibraryPypiArgs{\n\t\t\t\tPackage: pulumi.String(\"fbprophet==0.6\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fbprophet = new Library(\"fbprophet\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .pypi(LibraryPypiArgs.builder()\n                .package_(\"fbprophet==0.6\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fbprophet:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      pypi:\n        package: fbprophet==0.6\n```\n\n## Python EGG\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst appDbfsFile = new databricks.DbfsFile(\"appDbfsFile\", {\n    source: `${path.module}/foo.egg`,\n    path: \"/FileStore/foo.egg\",\n});\nconst appLibrary = new databricks.Library(\"appLibrary\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    egg: appDbfsFile.dbfsPath,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp_dbfs_file = databricks.DbfsFile(\"appDbfsFile\",\n    source=f\"{path['module']}/foo.egg\",\n    path=\"/FileStore/foo.egg\")\napp_library = databricks.Library(\"appLibrary\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    egg=app_dbfs_file.dbfs_path)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var appDbfsFile = new Databricks.DbfsFile(\"appDbfsFile\", new()\n    {\n        Source = $\"{path.Module}/foo.egg\",\n        Path = \"/FileStore/foo.egg\",\n    });\n\n    var appLibrary = new Databricks.Library(\"appLibrary\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Egg = appDbfsFile.DbfsPath,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tappDbfsFile, err := databricks.NewDbfsFile(ctx, \"appDbfsFile\", \u0026databricks.DbfsFileArgs{\n\t\t\tSource: pulumi.String(fmt.Sprintf(\"%v/foo.egg\", path.Module)),\n\t\t\tPath:   pulumi.String(\"/FileStore/foo.egg\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLibrary(ctx, \"appLibrary\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tEgg:       appDbfsFile.DbfsPath,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DbfsFile;\nimport com.pulumi.databricks.DbfsFileArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var appDbfsFile = new DbfsFile(\"appDbfsFile\", DbfsFileArgs.builder()        \n            .source(String.format(\"%s/foo.egg\", path.module()))\n            .path(\"/FileStore/foo.egg\")\n            .build());\n\n        var appLibrary = new Library(\"appLibrary\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .egg(appDbfsFile.dbfsPath())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  appDbfsFile:\n    type: databricks:DbfsFile\n    properties:\n      source: ${path.module}/foo.egg\n      path: /FileStore/foo.egg\n  appLibrary:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      egg: ${appDbfsFile.dbfsPath}\n```\n\n## R CRan\n\nInstalling artifacts from CRan. You can also optionally specify a `repo` parameter for a custom cran mirror.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst rkeops = new databricks.Library(\"rkeops\", {\n    clusterId: databricks_cluster[\"this\"].id,\n    cran: {\n        \"package\": \"rkeops\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nrkeops = databricks.Library(\"rkeops\",\n    cluster_id=databricks_cluster[\"this\"][\"id\"],\n    cran=databricks.LibraryCranArgs(\n        package=\"rkeops\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var rkeops = new Databricks.Library(\"rkeops\", new()\n    {\n        ClusterId = databricks_cluster.This.Id,\n        Cran = new Databricks.Inputs.LibraryCranArgs\n        {\n            Package = \"rkeops\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"rkeops\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(databricks_cluster.This.Id),\n\t\t\tCran: \u0026LibraryCranArgs{\n\t\t\t\tPackage: pulumi.String(\"rkeops\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryCranArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var rkeops = new Library(\"rkeops\", LibraryArgs.builder()        \n            .clusterId(databricks_cluster.this().id())\n            .cran(LibraryCranArgs.builder()\n                .package_(\"rkeops\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  rkeops:\n    type: databricks:Library\n    properties:\n      clusterId: ${databricks_cluster.this.id}\n      cran:\n        package: rkeops\n```\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "clusterId": {
                    "type": "string"
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                    "willReplaceOnChanges": true
                },
                "egg": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "jar": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                    "willReplaceOnChanges": true
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                    "willReplaceOnChanges": true
                },
                "whl": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "clusterId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Library resources.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "cran": {
                        "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                        "willReplaceOnChanges": true
                    },
                    "egg": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "jar": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "maven": {
                        "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                        "willReplaceOnChanges": true
                    },
                    "pypi": {
                        "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                        "willReplaceOnChanges": true
                    },
                    "whl": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastore:Metastore": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access. \n\nA metastore is the top-level container of objects in Unity Catalog. It stores data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create metastores and assign them to Databricks workspaces in order to control which workloads use each metastore.\n\nUnity Catalog offers a new metastore with built in security and auditing. This is distinct to the metastore used in previous versions of Databricks (based on the Hive Metastore).\n\n\n## Import\n\nThis resource can be imported by IDbash\n\n```sh\n $ pulumi import databricks:index/metastore:Metastore this \u003cid\u003e\n```\n\n ",
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed databricks.Table are stored. Change forces creation of a new resource.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "required": [
                "cloud",
                "createdAt",
                "createdBy",
                "globalMetastoreId",
                "name",
                "owner",
                "region",
                "storageRoot",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed databricks.Table are stored. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "storageRoot"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Metastore resources.\n",
                "properties": {
                    "cloud": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer"
                    },
                    "createdBy": {
                        "type": "string"
                    },
                    "defaultDataAccessConfigId": {
                        "type": "string"
                    },
                    "deltaSharingOrganizationName": {
                        "type": "string",
                        "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                    },
                    "deltaSharingRecipientTokenLifetimeInSeconds": {
                        "type": "integer",
                        "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                    },
                    "deltaSharingScope": {
                        "type": "string",
                        "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy metastore regardless of its contents.\n"
                    },
                    "globalMetastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of metastore.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the metastore owner.\n"
                    },
                    "region": {
                        "type": "string"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Path on cloud storage account, where managed databricks.Table are stored. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreAssignment:MetastoreAssignment": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access. \n\nA single databricks.Metastore can be shared across Databricks workspaces, and each linked workspace has a consistent view of the data and a single set of access policies. It is only recommended to have multiple metastores when organizations wish to have hard isolation boundaries between data (note that data cannot be easily joined/queried across metastores).\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisMetastore = new databricks.Metastore(\"thisMetastore\", {\n    storageRoot: `s3://${aws_s3_bucket.metastore.id}/metastore`,\n    owner: \"uc admins\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"thisMetastoreAssignment\", {\n    metastoreId: thisMetastore.id,\n    workspaceId: local.workspace_id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_metastore = databricks.Metastore(\"thisMetastore\",\n    storage_root=f\"s3://{aws_s3_bucket['metastore']['id']}/metastore\",\n    owner=\"uc admins\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"thisMetastoreAssignment\",\n    metastore_id=this_metastore.id,\n    workspace_id=local[\"workspace_id\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisMetastore = new Databricks.Metastore(\"thisMetastore\", new()\n    {\n        StorageRoot = $\"s3://{aws_s3_bucket.Metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"thisMetastoreAssignment\", new()\n    {\n        MetastoreId = thisMetastore.Id,\n        WorkspaceId = local.Workspace_id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisMetastore, err := databricks.NewMetastore(ctx, \"thisMetastore\", \u0026databricks.MetastoreArgs{\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", aws_s3_bucket.Metastore.Id)),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"thisMetastoreAssignment\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: thisMetastore.ID(),\n\t\t\tWorkspaceId: pulumi.Any(local.Workspace_id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisMetastore = new Metastore(\"thisMetastore\", MetastoreArgs.builder()        \n            .storageRoot(String.format(\"s3://%s/metastore\", aws_s3_bucket.metastore().id()))\n            .owner(\"uc admins\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()        \n            .metastoreId(thisMetastore.id())\n            .workspaceId(local.workspace_id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisMetastore:\n    type: databricks:Metastore\n    properties:\n      storageRoot: s3://${aws_s3_bucket.metastore.id}/metastore\n      owner: uc admins\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    properties:\n      metastoreId: ${thisMetastore.id}\n      workspaceId: ${local.workspace_id}\n```\n{{% /example %}}\n{{% /examples %}}",
            "properties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "id of the workspace for the assignment\n"
                }
            },
            "required": [
                "metastoreId",
                "workspaceId"
            ],
            "inputProperties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "id of the workspace for the assignment\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreAssignment resources.\n",
                "properties": {
                    "defaultCatalogName": {
                        "type": "string",
                        "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n"
                    },
                    "workspaceId": {
                        "type": "integer",
                        "description": "id of the workspace for the assignment\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreDataAccess:MetastoreDataAccess": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access.\n\nEach databricks.Metastore requires an IAM role that will be assumed by Unity Catalog to access data. `databricks.MetastoreDataAccess` defines this\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal"
                },
                "configurationType": {
                    "type": "string"
                },
                "isDefault": {
                    "type": "boolean"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "configurationType",
                "metastoreId",
                "name"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                    "willReplaceOnChanges": true
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                    "willReplaceOnChanges": true
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                    "willReplaceOnChanges": true
                },
                "configurationType": {
                    "type": "string"
                },
                "isDefault": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreDataAccess resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                        "willReplaceOnChanges": true
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                        "willReplaceOnChanges": true
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                        "willReplaceOnChanges": true
                    },
                    "configurationType": {
                        "type": "string"
                    },
                    "isDefault": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Data Access Configuration, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowExperiment:MlflowExperiment": {
            "description": "This resource allows you to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = pulumi.output(databricks.getCurrentUser());\nconst thisMlflowExperiment = new databricks.MlflowExperiment(\"this\", {\n    artifactLocation: \"dbfs:/tmp/my-experiment\",\n    description: \"My MLflow experiment description\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.MlflowExperiment(\"this\",\n    artifact_location=\"dbfs:/tmp/my-experiment\",\n    description=\"My MLflow experiment description\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.MlflowExperiment(\"this\", new()\n    {\n        ArtifactLocation = \"dbfs:/tmp/my-experiment\",\n        Description = \"My MLflow experiment description\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowExperiment(ctx, \"this\", \u0026databricks.MlflowExperimentArgs{\n\t\t\tArtifactLocation: pulumi.String(\"dbfs:/tmp/my-experiment\"),\n\t\t\tDescription:      pulumi.String(\"My MLflow experiment description\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.MlflowExperiment;\nimport com.pulumi.databricks.MlflowExperimentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new MlflowExperiment(\"this\", MlflowExperimentArgs.builder()        \n            .artifactLocation(\"dbfs:/tmp/my-experiment\")\n            .description(\"My MLflow experiment description\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowExperiment\n    properties:\n      artifactLocation: dbfs:/tmp/my-experiment\n      description: My MLflow experiment description\nvariables:\n  me:\n    Fn::Invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, or *Manage* individual experiments.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\nThe experiment resource can be imported using the id of the experiment bash\n\n```sh\n $ pulumi import databricks:index/mlflowExperiment:MlflowExperiment this \u003cexperiment-id\u003e\n```\n\n ",
            "properties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "required": [
                "creationTime",
                "experimentId",
                "lastUpdateTime",
                "lifecycleStage",
                "name"
            ],
            "inputProperties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowExperiment resources.\n",
                "properties": {
                    "artifactLocation": {
                        "type": "string",
                        "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow experiment.\n"
                    },
                    "experimentId": {
                        "type": "string"
                    },
                    "lastUpdateTime": {
                        "type": "integer"
                    },
                    "lifecycleStage": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowModel:MlflowModel": {
            "description": "This resource allows you to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst test = new databricks.MlflowModel(\"test\", {\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest = databricks.MlflowModel(\"test\",\n    description=\"My MLflow model description\",\n    tags=[\n        databricks.MlflowModelTagArgs(\n            key=\"key1\",\n            value=\"value1\",\n        ),\n        databricks.MlflowModelTagArgs(\n            key=\"key2\",\n            value=\"value2\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var test = new Databricks.MlflowModel(\"test\", new()\n    {\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"test\", \u0026databricks.MlflowModelArgs{\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: MlflowModelTagArray{\n\t\t\t\t\u0026MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var test = new MlflowModel(\"test\", MlflowModelArgs.builder()        \n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  test:\n    type: databricks:MlflowModel\n    properties:\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, *Manage Staging Versions*, *Manage Production Versions*, and *Manage* individual models.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\nThe model resource can be imported using the name bash\n\n```sh\n $ pulumi import databricks:index/mlflowModel:MlflowModel this \u003cname\u003e\n```\n\n ",
            "properties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                },
                "userId": {
                    "type": "string"
                }
            },
            "required": [
                "creationTimestamp",
                "lastUpdatedTimestamp",
                "name",
                "registeredModelId",
                "userId"
            ],
            "inputProperties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                },
                "userId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowModel resources.\n",
                "properties": {
                    "creationTimestamp": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow model.\n"
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow model. Change of name triggers new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                        },
                        "description": "Tags for the MLflow model.\n"
                    },
                    "userId": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowWebhook:MlflowWebhook": {
            "description": "This resource allows you to create [MLflow Model Registry Webhooks](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) in Databricks.  Webhooks enable you to listen for Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. Webhooks allow trigger execution of a Databricks job or call a web service on specific event(s) that is generated in the MLflow Registry - stage transitioning, creation of registered model, creation of transition request, etc.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n### POSTing to URL\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst url = new databricks.MlflowWebhook(\"url\", {\n    description: \"URL webhook trigger\",\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    httpUrlSpec: {\n        url: \"https://my_cool_host/webhook\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nurl = databricks.MlflowWebhook(\"url\",\n    description=\"URL webhook trigger\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    http_url_spec=databricks.MlflowWebhookHttpUrlSpecArgs(\n        url=\"https://my_cool_host/webhook\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var url = new Databricks.MlflowWebhook(\"url\", new()\n    {\n        Description = \"URL webhook trigger\",\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        HttpUrlSpec = new Databricks.Inputs.MlflowWebhookHttpUrlSpecArgs\n        {\n            Url = \"https://my_cool_host/webhook\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowWebhook(ctx, \"url\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tDescription: pulumi.String(\"URL webhook trigger\"),\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tHttpUrlSpec: \u0026MlflowWebhookHttpUrlSpecArgs{\n\t\t\t\tUrl: pulumi.String(\"https://my_cool_host/webhook\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookHttpUrlSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var url = new MlflowWebhook(\"url\", MlflowWebhookArgs.builder()        \n            .description(\"URL webhook trigger\")\n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .httpUrlSpec(MlflowWebhookHttpUrlSpecArgs.builder()\n                .url(\"https://my_cool_host/webhook\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  url:\n    type: databricks:MlflowWebhook\n    properties:\n      description: URL webhook trigger\n      events:\n        - TRANSITION_REQUEST_CREATED\n      httpUrlSpec:\n        url: https://my_cool_host/webhook\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* MLflow webhooks could be configured only by workspace admins.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "required": [
                "events"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "requiredInputs": [
                "events"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowWebhook resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Optional description of the MLflow webhook.\n"
                    },
                    "events": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n"
                    },
                    "httpUrlSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                    },
                    "jobSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                    },
                    "modelName": {
                        "type": "string",
                        "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mount:Mount": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs"
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl"
                },
                "clusterId": {
                    "type": "string"
                },
                "encryptionType": {
                    "type": "string"
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs"
                },
                "name": {
                    "type": "string"
                },
                "resourceId": {
                    "type": "string"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3"
                },
                "source": {
                    "type": "string",
                    "description": "(String) HDFS-compatible url\n"
                },
                "uri": {
                    "type": "string"
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb"
                }
            },
            "required": [
                "clusterId",
                "name",
                "source"
            ],
            "inputProperties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                    "willReplaceOnChanges": true
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "encryptionType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "willReplaceOnChanges": true
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3",
                    "willReplaceOnChanges": true
                },
                "uri": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Mount resources.\n",
                "properties": {
                    "abfs": {
                        "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                        "willReplaceOnChanges": true
                    },
                    "adl": {
                        "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "encryptionType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "extraConfigs": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "willReplaceOnChanges": true
                    },
                    "gs": {
                        "$ref": "#/types/databricks:index/MountGs:MountGs",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "s3": {
                        "$ref": "#/types/databricks:index/MountS3:MountS3",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "(String) HDFS-compatible url\n"
                    },
                    "uri": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "wasb": {
                        "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCredentials:MwsCredentials": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n\u003e **Note** This resource has an evolving API, which may change in future versions of the provider.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst thisAwsAssumeRolePolicy = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccountRole = new aws.iam.Role(\"crossAccountRole\", {\n    assumeRolePolicy: thisAwsAssumeRolePolicy.then(thisAwsAssumeRolePolicy =\u003e thisAwsAssumeRolePolicy.json),\n    tags: _var.tags,\n});\nconst thisAwsCrossAccountPolicy = databricks.getAwsCrossAccountPolicy({});\nconst thisRolePolicy = new aws.iam.RolePolicy(\"thisRolePolicy\", {\n    role: crossAccountRole.id,\n    policy: thisAwsCrossAccountPolicy.then(thisAwsCrossAccountPolicy =\u003e thisAwsCrossAccountPolicy.json),\n});\nconst thisMwsCredentials = new databricks.MwsCredentials(\"thisMwsCredentials\", {\n    accountId: databricksAccountId,\n    credentialsName: `${local.prefix}-creds`,\n    roleArn: crossAccountRole.arn,\n}, {\n    provider: databricks.mws,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nthis_aws_assume_role_policy = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account_role = aws.iam.Role(\"crossAccountRole\",\n    assume_role_policy=this_aws_assume_role_policy.json,\n    tags=var[\"tags\"])\nthis_aws_cross_account_policy = databricks.get_aws_cross_account_policy()\nthis_role_policy = aws.iam.RolePolicy(\"thisRolePolicy\",\n    role=cross_account_role.id,\n    policy=this_aws_cross_account_policy.json)\nthis_mws_credentials = databricks.MwsCredentials(\"thisMwsCredentials\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{local['prefix']}-creds\",\n    role_arn=cross_account_role.arn,\n    opts=pulumi.ResourceOptions(provider=databricks[\"mws\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var thisAwsAssumeRolePolicy = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccountRole = new Aws.Iam.Role(\"crossAccountRole\", new()\n    {\n        AssumeRolePolicy = thisAwsAssumeRolePolicy.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json),\n        Tags = @var.Tags,\n    });\n\n    var thisAwsCrossAccountPolicy = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var thisRolePolicy = new Aws.Iam.RolePolicy(\"thisRolePolicy\", new()\n    {\n        Role = crossAccountRole.Id,\n        Policy = thisAwsCrossAccountPolicy.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json),\n    });\n\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"thisMwsCredentials\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{local.Prefix}-creds\",\n        RoleArn = crossAccountRole.Arn,\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Mws,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v5/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tthisAwsAssumeRolePolicy, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountRole, err := iam.NewRole(ctx, \"crossAccountRole\", \u0026iam.RoleArgs{\n\t\t\tAssumeRolePolicy: pulumi.String(thisAwsAssumeRolePolicy.Json),\n\t\t\tTags:             pulumi.Any(_var.Tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisAwsCrossAccountPolicy, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicy(ctx, \"thisRolePolicy\", \u0026iam.RolePolicyArgs{\n\t\t\tRole:   crossAccountRole.ID(),\n\t\t\tPolicy: pulumi.String(thisAwsCrossAccountPolicy.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsCredentials(ctx, \"thisMwsCredentials\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.String(fmt.Sprintf(\"%v-creds\", local.Prefix)),\n\t\t\tRoleArn:         crossAccountRole.Arn,\n\t\t}, pulumi.Provider(databricks.Mws))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.RolePolicy;\nimport com.pulumi.aws.iam.RolePolicyArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var thisAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccountRole = new Role(\"crossAccountRole\", RoleArgs.builder()        \n            .assumeRolePolicy(thisAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -\u003e getAwsAssumeRolePolicyResult.json()))\n            .tags(var_.tags())\n            .build());\n\n        final var thisAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n        var thisRolePolicy = new RolePolicy(\"thisRolePolicy\", RolePolicyArgs.builder()        \n            .role(crossAccountRole.id())\n            .policy(thisAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -\u003e getAwsCrossAccountPolicyResult.json()))\n            .build());\n\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", local.prefix()))\n            .roleArn(crossAccountRole.arn())\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  crossAccountRole:\n    type: aws:iam:Role\n    properties:\n      assumeRolePolicy: ${thisAwsAssumeRolePolicy.json}\n      tags: ${var.tags}\n  thisRolePolicy:\n    type: aws:iam:RolePolicy\n    properties:\n      role: ${crossAccountRole.id}\n      policy: ${thisAwsCrossAccountPolicy.json}\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${local.prefix}-creds\n      roleArn: ${crossAccountRole.arn}\n    options:\n      provider: ${databricks.mws}\nvariables:\n  thisAwsAssumeRolePolicy:\n    Fn::Invoke:\n      Function: databricks:getAwsAssumeRolePolicy\n      Arguments:\n        externalId: ${databricksAccountId}\n  thisAwsCrossAccountPolicy:\n    Fn::Invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n"
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "credentialsId",
                "credentialsName",
                "externalId",
                "roleArn"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n",
                    "willReplaceOnChanges": true
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsName",
                "roleArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCredentials resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time of credentials registration\n"
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "(String) identifier of credentials\n"
                    },
                    "credentialsName": {
                        "type": "string",
                        "description": "name of credentials to register\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "roleArn": {
                        "type": "string",
                        "description": "ARN of cross-account role\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCustomerManagedKeys:MwsCustomerManagedKeys": {
            "description": "{{% examples %}}\n## Example Usage\n\n\u003e **Note** If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.\n{{% example %}}\n### Customer-managed key for managed services\n\nYou must configure this during workspace creation\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst current = aws.getCallerIdentity({});\nconst databricksManagedServicesCmk = current.then(current =\u003e aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for control plane managed services\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources: [\"*\"],\n        },\n    ],\n}));\nconst managedServicesCustomerManagedKey = new aws.kms.Key(\"managedServicesCustomerManagedKey\", {policy: databricksManagedServicesCmk.then(databricksManagedServicesCmk =\u003e databricksManagedServicesCmk.json)});\nconst managedServicesCustomerManagedKeyAlias = new aws.kms.Alias(\"managedServicesCustomerManagedKeyAlias\", {targetKeyId: managedServicesCustomerManagedKey.keyId});\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managedServices\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: managedServicesCustomerManagedKey.arn,\n        keyAlias: managedServicesCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ncurrent = aws.get_caller_identity()\ndatabricks_managed_services_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[current.account_id],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for control plane managed services\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources=[\"*\"],\n        ),\n    ])\nmanaged_services_customer_managed_key = aws.kms.Key(\"managedServicesCustomerManagedKey\", policy=databricks_managed_services_cmk.json)\nmanaged_services_customer_managed_key_alias = aws.kms.Alias(\"managedServicesCustomerManagedKeyAlias\", target_key_id=managed_services_customer_managed_key.key_id)\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managedServices\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=managed_services_customer_managed_key.arn,\n        key_alias=managed_services_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var current = Aws.GetCallerIdentity.Invoke();\n\n    var databricksManagedServicesCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.Apply(getCallerIdentityResult =\u003e getCallerIdentityResult.AccountId),\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for control plane managed services\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n        },\n    });\n\n    var managedServicesCustomerManagedKey = new Aws.Kms.Key(\"managedServicesCustomerManagedKey\", new()\n    {\n        Policy = databricksManagedServicesCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var managedServicesCustomerManagedKeyAlias = new Aws.Kms.Alias(\"managedServicesCustomerManagedKeyAlias\", new()\n    {\n        TargetKeyId = managedServicesCustomerManagedKey.KeyId,\n    });\n\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managedServices\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = managedServicesCustomerManagedKey.Arn,\n            KeyAlias = managedServicesCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.AwsFunctions;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var current = AwsFunctions.getCallerIdentity();\n\n        final var databricksManagedServicesCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.applyValue(getCallerIdentityResult -\u003e getCallerIdentityResult.accountId()))\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for control plane managed services\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\")\n                    .resources(\"*\")\n                    .build())\n            .build());\n\n        var managedServicesCustomerManagedKey = new Key(\"managedServicesCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksManagedServicesCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var managedServicesCustomerManagedKeyAlias = new Alias(\"managedServicesCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .targetKeyId(managedServicesCustomerManagedKey.keyId())\n            .build());\n\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(managedServicesCustomerManagedKey.arn())\n                .keyAlias(managedServicesCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  managedServicesCustomerManagedKey:\n    type: aws:kms:Key\n    properties:\n      policy: ${databricksManagedServicesCmk.json}\n  managedServicesCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    properties:\n      targetKeyId: ${managedServicesCustomerManagedKey.keyId}\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${managedServicesCustomerManagedKey.arn}\n        keyAlias: ${managedServicesCustomerManagedKeyAlias.name}\n      useCases:\n        - MANAGED_SERVICES\nvariables:\n  current:\n    Fn::Invoke:\n      Function: aws:getCallerIdentity\n      Arguments: {}\n  databricksManagedServicesCmk:\n    Fn::Invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for control plane managed services\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n            resources:\n              - '*'\n```\n{{% /example %}}\n{{% example %}}\n### Customer-managed key for workspace storage\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst databricksCrossAccountRole = config.requireObject(\"databricksCrossAccountRole\");\nconst databricksStorageCmk = aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [data.aws_caller_identity.current.account_id],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"Bool\",\n                variable: \"kms:GrantIsForAWSResource\",\n                values: [\"true\"],\n            }],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for EBS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [databricksCrossAccountRole],\n            }],\n            actions: [\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"ForAnyValue:StringLike\",\n                variable: \"kms:ViaService\",\n                values: [\"ec2.*.amazonaws.com\"],\n            }],\n        },\n    ],\n});\nconst storageCustomerManagedKey = new aws.kms.Key(\"storageCustomerManagedKey\", {policy: databricksStorageCmk.then(databricksStorageCmk =\u003e databricksStorageCmk.json)});\nconst storageCustomerManagedKeyAlias = new aws.kms.Alias(\"storageCustomerManagedKeyAlias\", {targetKeyId: storageCustomerManagedKey.keyId});\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: storageCustomerManagedKey.arn,\n        keyAlias: storageCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndatabricks_cross_account_role = config.require_object(\"databricksCrossAccountRole\")\ndatabricks_storage_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[data[\"aws_caller_identity\"][\"current\"][\"account_id\"]],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"Bool\",\n                variable=\"kms:GrantIsForAWSResource\",\n                values=[\"true\"],\n            )],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for EBS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[databricks_cross_account_role],\n            )],\n            actions=[\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"ForAnyValue:StringLike\",\n                variable=\"kms:ViaService\",\n                values=[\"ec2.*.amazonaws.com\"],\n            )],\n        ),\n    ])\nstorage_customer_managed_key = aws.kms.Key(\"storageCustomerManagedKey\", policy=databricks_storage_cmk.json)\nstorage_customer_managed_key_alias = aws.kms.Alias(\"storageCustomerManagedKeyAlias\", target_key_id=storage_customer_managed_key.key_id)\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=storage_customer_managed_key.arn,\n        key_alias=storage_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var databricksCrossAccountRole = config.RequireObject\u003cdynamic\u003e(\"databricksCrossAccountRole\");\n    var databricksStorageCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            data.Aws_caller_identity.Current.Account_id,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                    \"kms:ReEncrypt*\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS (Grants)\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:CreateGrant\",\n                    \"kms:ListGrants\",\n                    \"kms:RevokeGrant\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"Bool\",\n                        Variable = \"kms:GrantIsForAWSResource\",\n                        Values = new[]\n                        {\n                            \"true\",\n                        },\n                    },\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for EBS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            databricksCrossAccountRole,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Decrypt\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:CreateGrant\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"ForAnyValue:StringLike\",\n                        Variable = \"kms:ViaService\",\n                        Values = new[]\n                        {\n                            \"ec2.*.amazonaws.com\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var storageCustomerManagedKey = new Aws.Kms.Key(\"storageCustomerManagedKey\", new()\n    {\n        Policy = databricksStorageCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var storageCustomerManagedKeyAlias = new Aws.Kms.Alias(\"storageCustomerManagedKeyAlias\", new()\n    {\n        TargetKeyId = storageCustomerManagedKey.KeyId,\n    });\n\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = storageCustomerManagedKey.Arn,\n            KeyAlias = storageCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksCrossAccountRole = config.get(\"databricksCrossAccountRole\");\n        final var databricksStorageCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(data.aws_caller_identity().current().account_id())\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\",\n                        \"kms:ReEncrypt*\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS (Grants)\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:CreateGrant\",\n                        \"kms:ListGrants\",\n                        \"kms:RevokeGrant\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"Bool\")\n                        .variable(\"kms:GrantIsForAWSResource\")\n                        .values(\"true\")\n                        .build())\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for EBS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(databricksCrossAccountRole)\n                        .build())\n                    .actions(                    \n                        \"kms:Decrypt\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:CreateGrant\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"ForAnyValue:StringLike\")\n                        .variable(\"kms:ViaService\")\n                        .values(\"ec2.*.amazonaws.com\")\n                        .build())\n                    .build())\n            .build());\n\n        var storageCustomerManagedKey = new Key(\"storageCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksStorageCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var storageCustomerManagedKeyAlias = new Alias(\"storageCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .targetKeyId(storageCustomerManagedKey.keyId())\n            .build());\n\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(storageCustomerManagedKey.arn())\n                .keyAlias(storageCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksCrossAccountRole:\n    type: dynamic\nresources:\n  storageCustomerManagedKey:\n    type: aws:kms:Key\n    properties:\n      policy: ${databricksStorageCmk.json}\n  storageCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    properties:\n      targetKeyId: ${storageCustomerManagedKey.keyId}\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${storageCustomerManagedKey.arn}\n        keyAlias: ${storageCustomerManagedKeyAlias.name}\n      useCases:\n        - STORAGE\nvariables:\n  databricksStorageCmk:\n    Fn::Invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${data.aws_caller_identity.current.account_id}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n              - kms:ReEncrypt*\n              - kms:GenerateDataKey*\n              - kms:DescribeKey\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS (Grants)\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:CreateGrant\n              - kms:ListGrants\n              - kms:RevokeGrant\n            resources:\n              - '*'\n            conditions:\n              - test: Bool\n                variable: kms:GrantIsForAWSResource\n                values:\n                  - true\n          - sid: Allow Databricks to use KMS key for EBS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${databricksCrossAccountRole}\n            actions:\n              - kms:Decrypt\n              - kms:GenerateDataKey*\n              - kms:CreateGrant\n              - kms:DescribeKey\n            resources:\n              - '*'\n            conditions:\n              - test: ForAnyValue:StringLike\n                variable: kms:ViaService\n                values:\n                  - ec2.*.amazonaws.com\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below.\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n"
                }
            },
            "required": [
                "accountId",
                "awsKeyInfo",
                "creationTime",
                "customerManagedKeyId",
                "useCases"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "awsKeyInfo",
                "useCases"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCustomerManagedKeys resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                        "description": "This field is a block and is documented below.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "description": "(String) ID of the encryption key configuration object.\n"
                    },
                    "useCases": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsLogDelivery:MwsLogDelivery": {
            "description": "\u003e **Note** This resource has an evolving API, which will change in the upcoming versions of the provider in order to simplify user experience.\n\nMake sure you have authenticated with username and password for Accounts Console. This resource configures the delivery of the two supported log types from Databricks workspaces: [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html). \n\nYou cannot delete a log delivery configuration, but you can disable it when you no longer need it. This fact is important because there is a limit to the number of enabled log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.\n\n## Billable Usage\n\nCSV files are delivered to `\u003cdelivery_path_prefix\u003e/billable-usage/csv/` and are named `workspaceId=\u003cworkspace-id\u003e-usageMonth=\u003cmonth\u003e.csv`, which are delivered daily by overwriting the month's CSV file for each workspace. Format of CSV file, as well as some usage examples, can be found [here](https://docs.databricks.com/administration-guide/account-settings/usage.html#download-usage-as-a-csv-file).\n\nCommon processing scenario is to apply [cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html), that could be enforced by setting custom_tags on a cluster or through cluster policy. Report contains `clusterId` field, that could be joined with data from AWS [cost and usage reports](https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html), that can be joined with `user:ClusterId` tag from AWS usage report.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst usageLogs = new databricks.MwsLogDelivery(\"usageLogs\", {\n    accountId: _var.databricks_account_id,\n    credentialsId: databricks_mws_credentials.log_writer.credentials_id,\n    storageConfigurationId: databricks_mws_storage_configurations.log_bucket.storage_configuration_id,\n    deliveryPathPrefix: \"billable-usage\",\n    configName: \"Usage Logs\",\n    logType: \"BILLABLE_USAGE\",\n    outputFormat: \"CSV\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusage_logs = databricks.MwsLogDelivery(\"usageLogs\",\n    account_id=var[\"databricks_account_id\"],\n    credentials_id=databricks_mws_credentials[\"log_writer\"][\"credentials_id\"],\n    storage_configuration_id=databricks_mws_storage_configurations[\"log_bucket\"][\"storage_configuration_id\"],\n    delivery_path_prefix=\"billable-usage\",\n    config_name=\"Usage Logs\",\n    log_type=\"BILLABLE_USAGE\",\n    output_format=\"CSV\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var usageLogs = new Databricks.MwsLogDelivery(\"usageLogs\", new()\n    {\n        AccountId = @var.Databricks_account_id,\n        CredentialsId = databricks_mws_credentials.Log_writer.Credentials_id,\n        StorageConfigurationId = databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id,\n        DeliveryPathPrefix = \"billable-usage\",\n        ConfigName = \"Usage Logs\",\n        LogType = \"BILLABLE_USAGE\",\n        OutputFormat = \"CSV\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"usageLogs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(_var.Databricks_account_id),\n\t\t\tCredentialsId:          pulumi.Any(databricks_mws_credentials.Log_writer.Credentials_id),\n\t\t\tStorageConfigurationId: pulumi.Any(databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"billable-usage\"),\n\t\t\tConfigName:             pulumi.String(\"Usage Logs\"),\n\t\t\tLogType:                pulumi.String(\"BILLABLE_USAGE\"),\n\t\t\tOutputFormat:           pulumi.String(\"CSV\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var usageLogs = new MwsLogDelivery(\"usageLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(var_.databricks_account_id())\n            .credentialsId(databricks_mws_credentials.log_writer().credentials_id())\n            .storageConfigurationId(databricks_mws_storage_configurations.log_bucket().storage_configuration_id())\n            .deliveryPathPrefix(\"billable-usage\")\n            .configName(\"Usage Logs\")\n            .logType(\"BILLABLE_USAGE\")\n            .outputFormat(\"CSV\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  usageLogs:\n    type: databricks:MwsLogDelivery\n    properties:\n      accountId: ${var.databricks_account_id}\n      credentialsId: ${databricks_mws_credentials.log_writer.credentials_id}\n      storageConfigurationId: ${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}\n      deliveryPathPrefix: billable-usage\n      configName: Usage Logs\n      logType: BILLABLE_USAGE\n      outputFormat: CSV\n```\n\n## Audit Logs\n\nJSON files with [static schema](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#audit-log-schema) are delivered to `\u003cdelivery_path_prefix\u003e/workspaceId=\u003cworkspaceId\u003e/date=\u003cyyyy-mm-dd\u003e/auditlogs_\u003cinternal-id\u003e.json`. Logs are available within 15 minutes of activation for audit logs. New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. Sometimes data may arrive later than 15 minutes. Databricks can overwrite the delivered log files in your bucket at any time. If a file is overwritten, the existing content remains, but there may be additional lines for more auditable events. Overwriting ensures exactly-once semantics without requiring read or delete access to your account.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auditLogs = new databricks.MwsLogDelivery(\"auditLogs\", {\n    accountId: _var.databricks_account_id,\n    credentialsId: databricks_mws_credentials.log_writer.credentials_id,\n    storageConfigurationId: databricks_mws_storage_configurations.log_bucket.storage_configuration_id,\n    deliveryPathPrefix: \"audit-logs\",\n    configName: \"Audit Logs\",\n    logType: \"AUDIT_LOGS\",\n    outputFormat: \"JSON\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naudit_logs = databricks.MwsLogDelivery(\"auditLogs\",\n    account_id=var[\"databricks_account_id\"],\n    credentials_id=databricks_mws_credentials[\"log_writer\"][\"credentials_id\"],\n    storage_configuration_id=databricks_mws_storage_configurations[\"log_bucket\"][\"storage_configuration_id\"],\n    delivery_path_prefix=\"audit-logs\",\n    config_name=\"Audit Logs\",\n    log_type=\"AUDIT_LOGS\",\n    output_format=\"JSON\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auditLogs = new Databricks.MwsLogDelivery(\"auditLogs\", new()\n    {\n        AccountId = @var.Databricks_account_id,\n        CredentialsId = databricks_mws_credentials.Log_writer.Credentials_id,\n        StorageConfigurationId = databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id,\n        DeliveryPathPrefix = \"audit-logs\",\n        ConfigName = \"Audit Logs\",\n        LogType = \"AUDIT_LOGS\",\n        OutputFormat = \"JSON\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"auditLogs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(_var.Databricks_account_id),\n\t\t\tCredentialsId:          pulumi.Any(databricks_mws_credentials.Log_writer.Credentials_id),\n\t\t\tStorageConfigurationId: pulumi.Any(databricks_mws_storage_configurations.Log_bucket.Storage_configuration_id),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"audit-logs\"),\n\t\t\tConfigName:             pulumi.String(\"Audit Logs\"),\n\t\t\tLogType:                pulumi.String(\"AUDIT_LOGS\"),\n\t\t\tOutputFormat:           pulumi.String(\"JSON\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auditLogs = new MwsLogDelivery(\"auditLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(var_.databricks_account_id())\n            .credentialsId(databricks_mws_credentials.log_writer().credentials_id())\n            .storageConfigurationId(databricks_mws_storage_configurations.log_bucket().storage_configuration_id())\n            .deliveryPathPrefix(\"audit-logs\")\n            .configName(\"Audit Logs\")\n            .logType(\"AUDIT_LOGS\")\n            .outputFormat(\"JSON\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auditLogs:\n    type: databricks:MwsLogDelivery\n    properties:\n      accountId: ${var.databricks_account_id}\n      credentialsId: ${databricks_mws_credentials.log_writer.credentials_id}\n      storageConfigurationId: ${databricks_mws_storage_configurations.log_bucket.storage_configuration_id}\n      deliveryPathPrefix: audit-logs\n      configName: Audit Logs\n      logType: AUDIT_LOGS\n      outputFormat: JSON\n```\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n"
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n"
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n"
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n"
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n"
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n"
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n"
                }
            },
            "required": [
                "accountId",
                "configId",
                "credentialsId",
                "deliveryStartTime",
                "logType",
                "outputFormat",
                "status",
                "storageConfigurationId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "willReplaceOnChanges": true
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n",
                    "willReplaceOnChanges": true
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                    "willReplaceOnChanges": true
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                    "willReplaceOnChanges": true
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsId",
                "logType",
                "outputFormat",
                "storageConfigurationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsLogDelivery resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "configId": {
                        "type": "string",
                        "description": "Databricks log delivery configuration ID.\n",
                        "willReplaceOnChanges": true
                    },
                    "configName": {
                        "type": "string",
                        "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                        "willReplaceOnChanges": true
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryPathPrefix": {
                        "type": "string",
                        "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryStartTime": {
                        "type": "string",
                        "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                        "willReplaceOnChanges": true
                    },
                    "logType": {
                        "type": "string",
                        "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "outputFormat": {
                        "type": "string",
                        "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceIdsFilters": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        },
                        "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworks:MwsNetworks": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks_mws_workspace resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n"
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n"
                },
                "vpcId": {
                    "type": "string"
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "errorMessages",
                "networkId",
                "networkName",
                "securityGroupIds",
                "subnetIds",
                "vpcEndpoints",
                "vpcId",
                "vpcStatus",
                "workspaceId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks_mws_workspace resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n",
                    "willReplaceOnChanges": true
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "integer",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "networkName",
                "securityGroupIds",
                "subnetIds",
                "vpcId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworks resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "errorMessages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                        }
                    },
                    "networkId": {
                        "type": "string",
                        "description": "(String) id of network to be used for databricks_mws_workspace resource.\n"
                    },
                    "networkName": {
                        "type": "string",
                        "description": "name under which this network is registered\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "subnetIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "vpcEndpoints": {
                        "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                        "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink connections\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "vpcStatus": {
                        "type": "string",
                        "description": "(String) VPC attachment status\n"
                    },
                    "workspaceId": {
                        "type": "integer",
                        "description": "(Integer) id of associated workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPermissionAssignment:MwsPermissionAssignment": {
            "description": "These resources are invoked in the account context. Provider must have `account_id` attribute configured.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nIn account context, adding account-level group to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dataEng = new databricks.Group(\"dataEng\", {});\nconst addAdminGroup = new databricks.MwsPermissionAssignment(\"addAdminGroup\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: dataEng.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndata_eng = databricks.Group(\"dataEng\")\nadd_admin_group = databricks.MwsPermissionAssignment(\"addAdminGroup\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=data_eng.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dataEng = new Databricks.Group(\"dataEng\");\n\n    var addAdminGroup = new Databricks.MwsPermissionAssignment(\"addAdminGroup\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = dataEng.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdataEng, err := databricks.NewGroup(ctx, \"dataEng\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addAdminGroup\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: dataEng.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dataEng = new Group(\"dataEng\");\n\n        var addAdminGroup = new MwsPermissionAssignment(\"addAdminGroup\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(dataEng.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dataEng:\n    type: databricks:Group\n  addAdminGroup:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${dataEng.id}\n      permissions:\n        - ADMIN\n```\n\nIn account context, adding account-level user to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst addUser = new databricks.MwsPermissionAssignment(\"addUser\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: me.id,\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nadd_user = databricks.MwsPermissionAssignment(\"addUser\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.MwsPermissionAssignment(\"addUser\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = me.Id,\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addUser\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: me.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new MwsPermissionAssignment(\"addUser\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(me.id())\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  addUser:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${me.id}\n      permissions:\n        - USER\n```\n\nIn account context, adding account-level service principal to a workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\nconst addAdminSpn = new databricks.MwsPermissionAssignment(\"addAdminSpn\", {\n    workspaceId: databricks_mws_workspaces[\"this\"].workspace_id,\n    principalId: sp.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.MwsPermissionAssignment(\"addAdminSpn\",\n    workspace_id=databricks_mws_workspaces[\"this\"][\"workspace_id\"],\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.MwsPermissionAssignment(\"addAdminSpn\", new()\n    {\n        WorkspaceId = databricks_mws_workspaces.This.Workspace_id,\n        PrincipalId = sp.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"addAdminSpn\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(databricks_mws_workspaces.This.Workspace_id),\n\t\t\tPrincipalId: sp.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new MwsPermissionAssignment(\"addAdminSpn\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(databricks_mws_workspaces.this().workspace_id())\n            .principalId(sp.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n  addAdminSpn:\n    type: databricks:MwsPermissionAssignment\n    properties:\n      workspaceId: ${databricks_mws_workspaces.this.workspace_id}\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\n```\n{{% /example %}}\n{{% /examples %}}",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "principalId": {
                    "type": "integer"
                },
                "workspaceId": {
                    "type": "integer"
                }
            },
            "required": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "integer",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "integer",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPrivateAccessSettings:MwsPrivateAccessSettings": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ANY` level access _(default)_ lets any databricks.MwsVpcEndpoint connect to your databricks_mws_workspaces. `ACCOUNT` level access lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "- If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of Private Access Settings\n"
                }
            },
            "required": [
                "privateAccessSettingsId",
                "privateAccessSettingsName",
                "region",
                "status"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ANY` level access _(default)_ lets any databricks.MwsVpcEndpoint connect to your databricks_mws_workspaces. `ACCOUNT` level access lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "- If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of Private Access Settings\n"
                }
            },
            "requiredInputs": [
                "privateAccessSettingsName",
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPrivateAccessSettings resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                    },
                    "allowedVpcEndpointIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                    },
                    "privateAccessLevel": {
                        "type": "string",
                        "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ANY` level access _(default)_ lets any databricks.MwsVpcEndpoint connect to your databricks_mws_workspaces. `ACCOUNT` level access lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                    },
                    "privateAccessSettingsName": {
                        "type": "string",
                        "description": "Name of Private Access Settings in Databricks Account\n"
                    },
                    "publicAccessEnabled": {
                        "type": "boolean",
                        "description": "- If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false` (default), the workspace can be accessed only over VPC endpoints, and not over the public network.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of Private Access Settings\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsStorageConfigurations:MwsStorageConfigurations": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var rootStorageBucket = new BucketV2(\"rootStorageBucket\", BucketV2Args.builder()        \n            .acl(\"private\")\n            .versionings(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference))\n            .build());\n\n        var this_ = new MwsStorageConfigurations(\"this\", MwsStorageConfigurationsArgs.builder()        \n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", var_.prefix()))\n            .bucketName(rootStorageBucket.bucket())\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.mws())\n                .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  rootStorageBucket:\n    type: aws:s3:BucketV2\n    properties:\n      acl: private\n      versionings:\n        - enabled: false\n  this:\n    type: databricks:MwsStorageConfigurations\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${var.prefix}-storage\n      bucketName: ${rootStorageBucket.bucket}\n    options:\n      provider: ${databricks.mws}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with PrivateLink guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n"
                }
            },
            "required": [
                "accountId",
                "bucketName",
                "creationTime",
                "storageConfigurationId",
                "storageConfigurationName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n",
                    "willReplaceOnChanges": true
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "bucketName",
                "storageConfigurationName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsStorageConfigurations resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "description": "name of AWS S3 bucket\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                    },
                    "storageConfigurationName": {
                        "type": "string",
                        "description": "name under which this storage configuration is stored\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsVpcEndpoint:MwsVpcEndpoint": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "ID of Databricks VPC endpoint service to connect to. Please contact your Databricks representative to request mapping\n"
                },
                "awsVpcEndpointId": {
                    "type": "string"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n"
                }
            },
            "required": [
                "awsAccountId",
                "awsEndpointServiceId",
                "awsVpcEndpointId",
                "region",
                "state",
                "useCase",
                "vpcEndpointId",
                "vpcEndpointName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "ID of Databricks VPC endpoint service to connect to. Please contact your Databricks representative to request mapping\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n",
                    "willReplaceOnChanges": true
                },
                "state": {
                    "type": "string",
                    "description": "State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "awsVpcEndpointId",
                "region",
                "vpcEndpointName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsVpcEndpoint resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsEndpointServiceId": {
                        "type": "string",
                        "description": "ID of Databricks VPC endpoint service to connect to. Please contact your Databricks representative to request mapping\n"
                    },
                    "awsVpcEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "state": {
                        "type": "string",
                        "description": "State of VPC Endpoint\n"
                    },
                    "useCase": {
                        "type": "string"
                    },
                    "vpcEndpointId": {
                        "type": "string",
                        "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                    },
                    "vpcEndpointName": {
                        "type": "string",
                        "description": "Name of VPC Endpoint in Databricks Account\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsWorkspaces:MwsWorkspaces": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                },
                "awsRegion": {
                    "type": "string",
                    "description": "AWS region of VPC\n"
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceBucket": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceBucket:MwsWorkspacesCloudResourceBucket"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead"
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n"
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo"
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean"
                },
                "location": {
                    "type": "string"
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "network": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesNetwork:MwsWorkspacesNetwork"
                },
                "networkId": {
                    "type": "string"
                },
                "pricingTier": {
                    "type": "string"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration\n"
                },
                "storageCustomerManagedKeyId": {
                    "type": "string"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "integer"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI\n"
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "required": [
                "accountId",
                "cloud",
                "creationTime",
                "pricingTier",
                "workspaceId",
                "workspaceName",
                "workspaceStatus",
                "workspaceStatusMessage",
                "workspaceUrl"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "willReplaceOnChanges": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "AWS region of VPC\n",
                    "willReplaceOnChanges": true
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceBucket": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceBucket:MwsWorkspacesCloudResourceBucket",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                    "willReplaceOnChanges": true
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                    "willReplaceOnChanges": true
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                    "willReplaceOnChanges": true
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "location": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n",
                    "willReplaceOnChanges": true
                },
                "network": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesNetwork:MwsWorkspacesNetwork",
                    "willReplaceOnChanges": true
                },
                "networkId": {
                    "type": "string"
                },
                "pricingTier": {
                    "type": "string"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account\n",
                    "willReplaceOnChanges": true
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration\n",
                    "willReplaceOnChanges": true
                },
                "storageCustomerManagedKeyId": {
                    "type": "string"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "integer"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI\n",
                    "willReplaceOnChanges": true
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "workspaceName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsWorkspaces resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "awsRegion": {
                        "type": "string",
                        "description": "AWS region of VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string"
                    },
                    "cloudResourceBucket": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceBucket:MwsWorkspacesCloudResourceBucket",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time when workspace was created\n"
                    },
                    "credentialsId": {
                        "type": "string"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                        "willReplaceOnChanges": true
                    },
                    "deploymentName": {
                        "type": "string",
                        "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalCustomerInfo": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                        "willReplaceOnChanges": true
                    },
                    "isNoPublicIpEnabled": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "location": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "managedServicesCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n",
                        "willReplaceOnChanges": true
                    },
                    "network": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesNetwork:MwsWorkspacesNetwork",
                        "willReplaceOnChanges": true
                    },
                    "networkId": {
                        "type": "string"
                    },
                    "pricingTier": {
                        "type": "string"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account\n",
                        "willReplaceOnChanges": true
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "`storage_configuration_id` from storage configuration\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCustomerManagedKeyId": {
                        "type": "string"
                    },
                    "token": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                    },
                    "workspaceId": {
                        "type": "integer"
                    },
                    "workspaceName": {
                        "type": "string",
                        "description": "name of the workspace, will appear on UI\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceStatus": {
                        "type": "string",
                        "description": "(String) workspace status\n"
                    },
                    "workspaceStatusMessage": {
                        "type": "string",
                        "description": "(String) updates on workspace status\n"
                    },
                    "workspaceUrl": {
                        "type": "string",
                        "description": "(String) URL of the workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/notebook:Notebook": {
            "description": "\n\n\n## Import\n\nThe resource notebook can be imported using notebook path bash\n\n```sh\n $ pulumi import databricks:index/notebook:Notebook this /path/to/notebook\n```\n\n ",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n",
                    "deprecationMessage": "Use id argument to retrieve object id"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the notebook\n"
                }
            },
            "required": [
                "objectId",
                "objectType",
                "path",
                "url"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n",
                    "deprecationMessage": "Use id argument to retrieve object id"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Notebook resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "language": {
                        "type": "string",
                        "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a NOTEBOOK\n",
                        "deprecationMessage": "Use id argument to retrieve object id"
                    },
                    "objectType": {
                        "type": "string",
                        "deprecationMessage": "Always is a notebook"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the notebook\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/oboToken:OboToken": {
            "description": "\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires.\n"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n"
                }
            },
            "required": [
                "applicationId",
                "comment",
                "lifetimeSeconds",
                "tokenValue"
            ],
            "inputProperties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n",
                    "willReplaceOnChanges": true
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "applicationId",
                "comment",
                "lifetimeSeconds"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OboToken resources.\n",
                "properties": {
                    "applicationId": {
                        "type": "string",
                        "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Comment that describes the purpose of the token.\n",
                        "willReplaceOnChanges": true
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "The number of seconds before the token expires. Token resource is re-created when it expires.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissionAssignment:PermissionAssignment": {
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "principalId": {
                    "type": "integer"
                }
            },
            "required": [
                "permissions",
                "principalId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering PermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissions:Permissions": {
            "description": "\n\n\n## Import\n\n### Import Example Configuration filehcl resource \"databricks_mlflow_model\" \"model\" {\n\n name\n\n\n\n\n\n\n\n= \"example_model\"\n\n description = \"MLflow registered model\" } resource \"databricks_permissions\" \"model_usage\" {\n\n registered_model_id = databricks_mlflow_model.model.registered_model_id\n\n access_control {\n\n\n\n group_name\n\n\n\n\n\n = \"users\"\n\n\n\n permission_level = \"CAN_READ\"\n\n } } Import commandbash\n\n```sh\n $ pulumi import databricks:index/permissions:Permissions model_usage /registered-models/\u003cregistered_model_id\u003e\n```\n\n ",
            "properties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string",
                    "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n"
                },
                "clusterId": {
                    "type": "string",
                    "description": "cluster id\n"
                },
                "clusterPolicyId": {
                    "type": "string",
                    "description": "cluster policy id\n"
                },
                "directoryId": {
                    "type": "string",
                    "description": "directory id\n"
                },
                "directoryPath": {
                    "type": "string",
                    "description": "path of directory\n"
                },
                "experimentId": {
                    "type": "string",
                    "description": "MLflow experiment id\n"
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "instance pool id\n"
                },
                "jobId": {
                    "type": "string",
                    "description": "job id\n"
                },
                "notebookId": {
                    "type": "string",
                    "description": "ID of notebook within workspace\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "path of notebook\n"
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "pipeline id\n"
                },
                "registeredModelId": {
                    "type": "string",
                    "description": "MLflow registered model id\n"
                },
                "repoId": {
                    "type": "string",
                    "description": "repo id\n"
                },
                "repoPath": {
                    "type": "string",
                    "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n"
                },
                "sqlAlertId": {
                    "type": "string",
                    "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n"
                },
                "sqlDashboardId": {
                    "type": "string",
                    "description": "SQL dashboard id\n"
                },
                "sqlEndpointId": {
                    "type": "string",
                    "description": "SQL endpoint id\n"
                },
                "sqlQueryId": {
                    "type": "string",
                    "description": "SQL query id\n"
                }
            },
            "required": [
                "accessControls",
                "objectType"
            ],
            "inputProperties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string",
                    "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "description": "cluster id\n",
                    "willReplaceOnChanges": true
                },
                "clusterPolicyId": {
                    "type": "string",
                    "description": "cluster policy id\n",
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "directory id\n",
                    "willReplaceOnChanges": true
                },
                "directoryPath": {
                    "type": "string",
                    "description": "path of directory\n",
                    "willReplaceOnChanges": true
                },
                "experimentId": {
                    "type": "string",
                    "description": "MLflow experiment id\n",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "instance pool id\n",
                    "willReplaceOnChanges": true
                },
                "jobId": {
                    "type": "string",
                    "description": "job id\n",
                    "willReplaceOnChanges": true
                },
                "notebookId": {
                    "type": "string",
                    "description": "ID of notebook within workspace\n",
                    "willReplaceOnChanges": true
                },
                "notebookPath": {
                    "type": "string",
                    "description": "path of notebook\n",
                    "willReplaceOnChanges": true
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "pipeline id\n",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string",
                    "description": "MLflow registered model id\n",
                    "willReplaceOnChanges": true
                },
                "repoId": {
                    "type": "string",
                    "description": "repo id\n",
                    "willReplaceOnChanges": true
                },
                "repoPath": {
                    "type": "string",
                    "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n",
                    "willReplaceOnChanges": true
                },
                "sqlAlertId": {
                    "type": "string",
                    "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n",
                    "willReplaceOnChanges": true
                },
                "sqlDashboardId": {
                    "type": "string",
                    "description": "SQL dashboard id\n",
                    "willReplaceOnChanges": true
                },
                "sqlEndpointId": {
                    "type": "string",
                    "description": "SQL endpoint id\n",
                    "willReplaceOnChanges": true
                },
                "sqlQueryId": {
                    "type": "string",
                    "description": "SQL query id\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accessControls"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Permissions resources.\n",
                "properties": {
                    "accessControls": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                        }
                    },
                    "authorization": {
                        "type": "string",
                        "description": "either [`tokens`](https://docs.databricks.com/administration-guide/access-control/tokens.html) or [`passwords`](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#configure-password-permission).\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "description": "cluster id\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterPolicyId": {
                        "type": "string",
                        "description": "cluster policy id\n",
                        "willReplaceOnChanges": true
                    },
                    "directoryId": {
                        "type": "string",
                        "description": "directory id\n",
                        "willReplaceOnChanges": true
                    },
                    "directoryPath": {
                        "type": "string",
                        "description": "path of directory\n",
                        "willReplaceOnChanges": true
                    },
                    "experimentId": {
                        "type": "string",
                        "description": "MLflow experiment id\n",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "instance pool id\n",
                        "willReplaceOnChanges": true
                    },
                    "jobId": {
                        "type": "string",
                        "description": "job id\n",
                        "willReplaceOnChanges": true
                    },
                    "notebookId": {
                        "type": "string",
                        "description": "ID of notebook within workspace\n",
                        "willReplaceOnChanges": true
                    },
                    "notebookPath": {
                        "type": "string",
                        "description": "path of notebook\n",
                        "willReplaceOnChanges": true
                    },
                    "objectType": {
                        "type": "string",
                        "description": "type of permissions.\n"
                    },
                    "pipelineId": {
                        "type": "string",
                        "description": "pipeline id\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string",
                        "description": "MLflow registered model id\n",
                        "willReplaceOnChanges": true
                    },
                    "repoId": {
                        "type": "string",
                        "description": "repo id\n",
                        "willReplaceOnChanges": true
                    },
                    "repoPath": {
                        "type": "string",
                        "description": "path of databricks repo directory(`/Repos/\u003cusername\u003e/...`)\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlAlertId": {
                        "type": "string",
                        "description": "[SQL alert](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlDashboardId": {
                        "type": "string",
                        "description": "SQL dashboard id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlEndpointId": {
                        "type": "string",
                        "description": "SQL endpoint id\n",
                        "willReplaceOnChanges": true
                    },
                    "sqlQueryId": {
                        "type": "string",
                        "description": "SQL query id\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/pipeline:Pipeline": {
            "description": "Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dltDemo = new databricks.Notebook(\"dltDemo\", {});\n//...\nconst _this = new databricks.Pipeline(\"this\", {\n    storage: \"/test/first-pipeline\",\n    configuration: {\n        key1: \"value1\",\n        key2: \"value2\",\n    },\n    clusters: [\n        {\n            label: \"default\",\n            numWorkers: 2,\n            customTags: {\n                cluster_type: \"default\",\n            },\n        },\n        {\n            label: \"maintenance\",\n            numWorkers: 1,\n            customTags: {\n                cluster_type: \"maintenance\",\n            },\n        },\n    ],\n    libraries: [{\n        notebook: {\n            path: dltDemo.id,\n        },\n    }],\n    continuous: false,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndlt_demo = databricks.Notebook(\"dltDemo\")\n#...\nthis = databricks.Pipeline(\"this\",\n    storage=\"/test/first-pipeline\",\n    configuration={\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n    },\n    clusters=[\n        databricks.PipelineClusterArgs(\n            label=\"default\",\n            num_workers=2,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        ),\n        databricks.PipelineClusterArgs(\n            label=\"maintenance\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"maintenance\",\n            },\n        ),\n    ],\n    libraries=[databricks.PipelineLibraryArgs(\n        notebook=databricks.PipelineLibraryNotebookArgs(\n            path=dlt_demo.id,\n        ),\n    )],\n    continuous=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dltDemo = new Databricks.Notebook(\"dltDemo\");\n\n    //...\n    var @this = new Databricks.Pipeline(\"this\", new()\n    {\n        Storage = \"/test/first-pipeline\",\n        Configuration = \n        {\n            { \"key1\", \"value1\" },\n            { \"key2\", \"value2\" },\n        },\n        Clusters = new[]\n        {\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"default\",\n                NumWorkers = 2,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"default\" },\n                },\n            },\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"maintenance\",\n                NumWorkers = 1,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"maintenance\" },\n                },\n            },\n        },\n        Libraries = new[]\n        {\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs\n                {\n                    Path = dltDemo.Id,\n                },\n            },\n        },\n        Continuous = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdltDemo, err := databricks.NewNotebook(ctx, \"dltDemo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPipeline(ctx, \"this\", \u0026databricks.PipelineArgs{\n\t\t\tStorage: pulumi.String(\"/test/first-pipeline\"),\n\t\t\tConfiguration: pulumi.AnyMap{\n\t\t\t\t\"key1\": pulumi.Any(\"value1\"),\n\t\t\t\t\"key2\": pulumi.Any(\"value2\"),\n\t\t\t},\n\t\t\tClusters: PipelineClusterArray{\n\t\t\t\t\u0026PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"default\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(2),\n\t\t\t\t\tCustomTags: pulumi.AnyMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"maintenance\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(1),\n\t\t\t\t\tCustomTags: pulumi.AnyMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"maintenance\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tLibraries: PipelineLibraryArray{\n\t\t\t\t\u0026PipelineLibraryArgs{\n\t\t\t\t\tNotebook: \u0026PipelineLibraryNotebookArgs{\n\t\t\t\t\t\tPath: dltDemo.ID(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContinuous: pulumi.Bool(false),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.Pipeline;\nimport com.pulumi.databricks.PipelineArgs;\nimport com.pulumi.databricks.inputs.PipelineClusterArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dltDemo = new Notebook(\"dltDemo\");\n\n        var this_ = new Pipeline(\"this\", PipelineArgs.builder()        \n            .storage(\"/test/first-pipeline\")\n            .configuration(Map.ofEntries(\n                Map.entry(\"key1\", \"value1\"),\n                Map.entry(\"key2\", \"value2\")\n            ))\n            .clusters(            \n                PipelineClusterArgs.builder()\n                    .label(\"default\")\n                    .numWorkers(2)\n                    .customTags(Map.of(\"cluster_type\", \"default\"))\n                    .build(),\n                PipelineClusterArgs.builder()\n                    .label(\"maintenance\")\n                    .numWorkers(1)\n                    .customTags(Map.of(\"cluster_type\", \"maintenance\"))\n                    .build())\n            .libraries(PipelineLibraryArgs.builder()\n                .notebook(PipelineLibraryNotebookArgs.builder()\n                    .path(dltDemo.id())\n                    .build())\n                .build())\n            .continuous(false)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dltDemo:\n    type: databricks:Notebook\n  this:\n    type: databricks:Pipeline\n    properties:\n      storage: /test/first-pipeline\n      configuration:\n        key1: value1\n        key2: value2\n      clusters:\n        - label: default\n          numWorkers: 2\n          customTags:\n            cluster_type: default\n        - label: maintenance\n          numWorkers: 1\n          customTags:\n            cluster_type: maintenance\n      libraries:\n        - notebook:\n            path: ${dltDemo.id}\n      continuous: false\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n\n\n## Import\n\nThe resource job can be imported using the id of the pipeline bash\n\n```sh\n $ pulumi import databricks:index/pipeline:Pipeline this \u003cpipeline-id\u003e\n```\n\n ",
            "properties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n"
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                },
                "url": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "url"
            ],
            "inputProperties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n",
                    "willReplaceOnChanges": true
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Pipeline resources.\n",
                "properties": {
                    "allowDuplicateNames": {
                        "type": "boolean"
                    },
                    "channel": {
                        "type": "string",
                        "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `current` (default) and `preview`.\n"
                    },
                    "clusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                        },
                        "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*\n"
                    },
                    "configuration": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                    },
                    "continuous": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                    },
                    "development": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                    },
                    "edition": {
                        "type": "string",
                        "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `core`, `pro`, `advanced` (default).\n"
                    },
                    "filters": {
                        "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                        },
                        "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` type of library that should have the `path` attribute. *Right now only the `notebook` type is supported.*\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                    },
                    "storage": {
                        "type": "string",
                        "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.*\n",
                        "willReplaceOnChanges": true
                    },
                    "target": {
                        "type": "string",
                        "description": "The name of a database for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                    },
                    "url": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/repo:Repo": {
            "description": "\n\n\n## Import\n\nThe resource Repo can be imported using the Repo ID (obtained via UI or using API) bash\n\n```sh\n $ pulumi import databricks:index/repo:Repo this repo_id\n```\n\n ",
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n"
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n"
                }
            },
            "required": [
                "branch",
                "commitHash",
                "gitProvider",
                "path",
                "url"
            ],
            "inputProperties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Repo resources.\n",
                "properties": {
                    "branch": {
                        "type": "string",
                        "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                    },
                    "commitHash": {
                        "type": "string",
                        "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, , `awsCodeCommit`.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "tag": {
                        "type": "string",
                        "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/schema:Schema": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access. \n\nWithin a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, Databases (also called Schemas), and Tables / Views.\n\nA `databricks.Schema` is contained within databricks.Catalog and can contain tables \u0026 views.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: databricks_metastore[\"this\"].id,\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=databricks_metastore[\"this\"][\"id\"],\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = databricks_metastore.This.Id,\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(databricks_metastore.This.Id),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"kind\": pulumi.Any(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .metastoreId(databricks_metastore.this().id())\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.id())\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${databricks_metastore.this.id}\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/schema:Schema this \u003cname\u003e\n```\n\n ",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                }
            },
            "required": [
                "catalogName",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                }
            },
            "requiredInputs": [
                "catalogName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Schema resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the schema owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Schema properties.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secret:Secret": {
            "description": "With this resource you can insert a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret’s value. The server encrypts the secret using the secret scope’s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope. The secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. The maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000. You can read a secret value only from within a command on a cluster (for example, through a notebook); there is no API to read a secret value outside of a cluster. The permission applied is based on who is invoking the command and you must have at least READ permission. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst app = new databricks.SecretScope(\"app\", {});\nconst publishingApi = new databricks.Secret(\"publishingApi\", {\n    key: \"publishing_api\",\n    stringValue: data.azurerm_key_vault_secret.example.value,\n    scope: app.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp = databricks.SecretScope(\"app\")\npublishing_api = databricks.Secret(\"publishingApi\",\n    key=\"publishing_api\",\n    string_value=data[\"azurerm_key_vault_secret\"][\"example\"][\"value\"],\n    scope=app.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var app = new Databricks.SecretScope(\"app\");\n\n    var publishingApi = new Databricks.Secret(\"publishingApi\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = data.Azurerm_key_vault_secret.Example.Value,\n        Scope = app.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecret(ctx, \"publishingApi\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(data.Azurerm_key_vault_secret.Example.Value),\n\t\t\tScope:       app.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var app = new SecretScope(\"app\");\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(data.azurerm_key_vault_secret().example().value())\n            .scope(app.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  app:\n    type: databricks:SecretScope\n  publishingApi:\n    type: databricks:Secret\n    properties:\n      key: publishing_api\n      stringValue: ${data.azurerm_key_vault_secret.example.value}\n      scope: ${app.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n\n## Import\n\nThe resource secret can be imported using `scopeName|||secretKey` combination. **This may change in future versions.** bash\n\n```sh\n $ pulumi import databricks:index/secret:Secret app `scopeName|||secretKey`\n```\n\n ",
            "properties": {
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "(Integer) time secret was updated\n"
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n"
                }
            },
            "required": [
                "key",
                "lastUpdatedTimestamp",
                "scope",
                "stringValue"
            ],
            "inputProperties": {
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "key",
                "scope",
                "stringValue"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Secret resources.\n",
                "properties": {
                    "key": {
                        "type": "string",
                        "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "(Integer) time secret was updated\n"
                    },
                    "scope": {
                        "type": "string",
                        "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "stringValue": {
                        "type": "string",
                        "description": "(String) super secret sensitive value.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretAcl:SecretAcl": {
            "description": "Create or overwrite the ACL associated with the given principal (user or group) on the specified databricks_secret_scope. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nThis way, data scientists can read the Publishing API key that is synchronized from example, Azure Key Vault.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ds = new databricks.Group(\"ds\", {});\nconst app = new databricks.SecretScope(\"app\", {});\nconst mySecretAcl = new databricks.SecretAcl(\"mySecretAcl\", {\n    principal: ds.displayName,\n    permission: \"READ\",\n    scope: app.name,\n});\nconst publishingApi = new databricks.Secret(\"publishingApi\", {\n    key: \"publishing_api\",\n    stringValue: data.azurerm_key_vault_secret.example.value,\n    scope: app.name,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nds = databricks.Group(\"ds\")\napp = databricks.SecretScope(\"app\")\nmy_secret_acl = databricks.SecretAcl(\"mySecretAcl\",\n    principal=ds.display_name,\n    permission=\"READ\",\n    scope=app.name)\npublishing_api = databricks.Secret(\"publishingApi\",\n    key=\"publishing_api\",\n    string_value=data[\"azurerm_key_vault_secret\"][\"example\"][\"value\"],\n    scope=app.name)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ds = new Databricks.Group(\"ds\");\n\n    var app = new Databricks.SecretScope(\"app\");\n\n    var mySecretAcl = new Databricks.SecretAcl(\"mySecretAcl\", new()\n    {\n        Principal = ds.DisplayName,\n        Permission = \"READ\",\n        Scope = app.Name,\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishingApi\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = data.Azurerm_key_vault_secret.Example.Value,\n        Scope = app.Name,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecretAcl(ctx, \"mySecretAcl\", \u0026databricks.SecretAclArgs{\n\t\t\tPrincipal:  ds.DisplayName,\n\t\t\tPermission: pulumi.String(\"READ\"),\n\t\t\tScope:      app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecret(ctx, \"publishingApi\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(data.Azurerm_key_vault_secret.Example.Value),\n\t\t\tScope:       app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretAcl;\nimport com.pulumi.databricks.SecretAclArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ds = new Group(\"ds\");\n\n        var app = new SecretScope(\"app\");\n\n        var mySecretAcl = new SecretAcl(\"mySecretAcl\", SecretAclArgs.builder()        \n            .principal(ds.displayName())\n            .permission(\"READ\")\n            .scope(app.name())\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(data.azurerm_key_vault_secret().example().value())\n            .scope(app.name())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ds:\n    type: databricks:Group\n  app:\n    type: databricks:SecretScope\n  mySecretAcl:\n    type: databricks:SecretAcl\n    properties:\n      principal: ${ds.displayName}\n      permission: READ\n      scope: ${app.name}\n  publishingApi:\n    type: databricks:Secret\n    properties:\n      key: publishing_api\n      # replace it with a secret management solution of your choice :-)\n      stringValue: ${data.azurerm_key_vault_secret.example.value}\n      scope: ${app.name}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n\n## Import\n\nThe resource secret acl can be imported using `scopeName|||principalName` combination. bash\n\n```sh\n $ pulumi import databricks:index/secretAcl:SecretAcl object `scopeName|||principalName`\n```\n\n ",
            "properties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n"
                },
                "principal": {
                    "type": "string",
                    "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n"
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n"
                }
            },
            "required": [
                "permission",
                "principal",
                "scope"
            ],
            "inputProperties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permission",
                "principal",
                "scope"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretAcl resources.\n",
                "properties": {
                    "permission": {
                        "type": "string",
                        "description": "`READ`, `WRITE` or `MANAGE`.\n",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "description": "name of the principals. It can be `users` for all users or name or `display_name` of databricks_group\n",
                        "willReplaceOnChanges": true
                    },
                    "scope": {
                        "type": "string",
                        "description": "name of the scope\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretScope:SecretScope": {
            "description": "\n\n\n## Import\n\nThe secret resource scope can be imported using the scope name. `initial_manage_principal` state won't be imported, because the underlying API doesn't include it in the response. bash\n\n```sh\n $ pulumi import databricks:index/secretScope:SecretScope object \u003cscopeName\u003e\n```\n\n ",
            "properties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n"
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata"
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                }
            },
            "required": [
                "backendType",
                "name"
            ],
            "inputProperties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                    "willReplaceOnChanges": true
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretScope resources.\n",
                "properties": {
                    "backendType": {
                        "type": "string",
                        "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                    },
                    "initialManagePrincipal": {
                        "type": "string",
                        "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                        "willReplaceOnChanges": true
                    },
                    "keyvaultMetadata": {
                        "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipal:ServicePrincipal": {
            "description": "Directly manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) that could be added to databricks.Group in Databricks workspace or account.\n\nTo create service principals in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks_group_member to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.\n\n\n## Import\n\nThe resource scim service principal can be imported using idbash\n\n```sh\n $ pulumi import databricks:index/servicePrincipal:ServicePrincipal me \u003cservice-principal-id\u003e\n```\n\n ",
            "properties": {
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the application id of the given service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "applicationId",
                "displayName"
            ],
            "inputProperties": {
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the application id of the given service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n",
                    "willReplaceOnChanges": true
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipal resources.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "This is the application id of the given service principal and will be their form of access and identity. On other clouds than Azure this value is auto-generated.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalRole:ServicePrincipalRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to a databricks_service_principal.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nGranting a service principal access to an instance profile\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"My Service Principal\"});\nconst myServicePrincipalInstanceProfile = new databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", {\n    servicePrincipalId: _this.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nthis = databricks.ServicePrincipal(\"this\", display_name=\"My Service Principal\")\nmy_service_principal_instance_profile = databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\",\n    service_principal_id=this.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"My Service Principal\",\n    });\n\n    var myServicePrincipalInstanceProfile = new Databricks.ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", new()\n    {\n        ServicePrincipalId = @this.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"My Service Principal\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipalRole(ctx, \"myServicePrincipalInstanceProfile\", \u0026databricks.ServicePrincipalRoleArgs{\n\t\t\tServicePrincipalId: this.ID(),\n\t\t\tRole:               instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.ServicePrincipalRole;\nimport com.pulumi.databricks.ServicePrincipalRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()        \n            .displayName(\"My Service Principal\")\n            .build());\n\n        var myServicePrincipalInstanceProfile = new ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", ServicePrincipalRoleArgs.builder()        \n            .servicePrincipalId(this_.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: My Service Principal\n  myServicePrincipalInstanceProfile:\n    type: databricks:ServicePrincipalRole\n    properties:\n      servicePrincipalId: ${this.id}\n      role: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.UserRole to attach role or databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n"
                }
            },
            "required": [
                "role",
                "servicePrincipalId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "This is the id of the role or instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "This is the id of the service principal resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlDashboard:SqlDashboard": {
            "description": "This resource is used to manage [Databricks SQL Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html). To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA dashboard may have one or more widgets.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1 = new databricks.SqlDashboard(\"d1\", {\n    tags: [\n        \"some-tag\",\n        \"another-tag\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1 = databricks.SqlDashboard(\"d1\", tags=[\n    \"some-tag\",\n    \"another-tag\",\n])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1 = new Databricks.SqlDashboard(\"d1\", new()\n    {\n        Tags = new[]\n        {\n            \"some-tag\",\n            \"another-tag\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlDashboard(ctx, \"d1\", \u0026databricks.SqlDashboardArgs{\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"some-tag\"),\n\t\t\t\tpulumi.String(\"another-tag\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlDashboard;\nimport com.pulumi.databricks.SqlDashboardArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1 = new SqlDashboard(\"d1\", SqlDashboardArgs.builder()        \n            .tags(            \n                \"some-tag\",\n                \"another-tag\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1:\n    type: databricks:SqlDashboard\n    properties:\n      tags:\n        - some-tag\n        - another-tag\n```\n\nExample permission to share dashboard with all users:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1 = new databricks.Permissions(\"d1\", {\n    sqlDashboardId: databricks_sql_dashboard.d1.id,\n    accessControls: [{\n        groupName: data.databricks_group.users.display_name,\n        permissionLevel: \"CAN_RUN\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1 = databricks.Permissions(\"d1\",\n    sql_dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    access_controls=[databricks.PermissionsAccessControlArgs(\n        group_name=data[\"databricks_group\"][\"users\"][\"display_name\"],\n        permission_level=\"CAN_RUN\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1 = new Databricks.Permissions(\"d1\", new()\n    {\n        SqlDashboardId = databricks_sql_dashboard.D1.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = data.Databricks_group.Users.Display_name,\n                PermissionLevel = \"CAN_RUN\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"d1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlDashboardId: pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tAccessControls: PermissionsAccessControlArray{\n\t\t\t\t\u0026PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(data.Databricks_group.Users.Display_name),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1 = new Permissions(\"d1\", PermissionsArgs.builder()        \n            .sqlDashboardId(databricks_sql_dashboard.d1().id())\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .groupName(data.databricks_group().users().display_name())\n                .permissionLevel(\"CAN_RUN\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1:\n    type: databricks:Permissions\n    properties:\n      sqlDashboardId: ${databricks_sql_dashboard.d1.id}\n      accessControls:\n        - groupName: ${data.databricks_group.users.display_name}\n          permissionLevel: CAN_RUN\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_dashboard` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlDashboard:SqlDashboard this \u003cdashboard-id\u003e\n```\n\n ",
            "properties": {
                "name": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "name"
            ],
            "inputProperties": {
                "name": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlDashboard resources.\n",
                "properties": {
                    "name": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlEndpoint:SqlEndpoint": {
            "description": "This resource is used to manage [Databricks SQL Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html). To create [SQL endpoints](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = pulumi.output(databricks.getCurrentUser());\nconst thisSqlEndpoint = new databricks.SqlEndpoint(\"this\", {\n    clusterSize: \"Small\",\n    maxNumClusters: 1,\n    tags: {\n        customTags: [{\n            key: \"City\",\n            value: \"Amsterdam\",\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.SqlEndpoint(\"this\",\n    cluster_size=\"Small\",\n    max_num_clusters=1,\n    tags=databricks.SqlEndpointTagsArgs(\n        custom_tags=[databricks.SqlEndpointTagsCustomTagArgs(\n            key=\"City\",\n            value=\"Amsterdam\",\n        )],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.SqlEndpoint(\"this\", new()\n    {\n        ClusterSize = \"Small\",\n        MaxNumClusters = 1,\n        Tags = new Databricks.Inputs.SqlEndpointTagsArgs\n        {\n            CustomTags = new[]\n            {\n                new Databricks.Inputs.SqlEndpointTagsCustomTagArgs\n                {\n                    Key = \"City\",\n                    Value = \"Amsterdam\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlEndpoint(ctx, \"this\", \u0026databricks.SqlEndpointArgs{\n\t\t\tClusterSize:    pulumi.String(\"Small\"),\n\t\t\tMaxNumClusters: pulumi.Int(1),\n\t\t\tTags: \u0026SqlEndpointTagsArgs{\n\t\t\t\tCustomTags: SqlEndpointTagsCustomTagArray{\n\t\t\t\t\t\u0026SqlEndpointTagsCustomTagArgs{\n\t\t\t\t\t\tKey:   pulumi.String(\"City\"),\n\t\t\t\t\t\tValue: pulumi.String(\"Amsterdam\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.SqlEndpoint;\nimport com.pulumi.databricks.SqlEndpointArgs;\nimport com.pulumi.databricks.inputs.SqlEndpointTagsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new SqlEndpoint(\"this\", SqlEndpointArgs.builder()        \n            .clusterSize(\"Small\")\n            .maxNumClusters(1)\n            .tags(SqlEndpointTagsArgs.builder()\n                .customTags(SqlEndpointTagsCustomTagArgs.builder()\n                    .key(\"City\")\n                    .value(\"Amsterdam\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlEndpoint\n    properties:\n      clusterSize: Small\n      maxNumClusters: 1\n      tags:\n        customTags:\n          - key: City\n            value: Amsterdam\nvariables:\n  me:\n    Fn::Invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Can Use* or *Can Manage* SQL endpoints.\n* `databricks_sql_access` on databricks.Group or databricks_user.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_endpoint` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlEndpoint:SqlEndpoint this \u003cendpoint-id\u003e\n```\n\n ",
            "properties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                },
                "numClusters": {
                    "type": "integer"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                }
            },
            "required": [
                "clusterSize",
                "dataSourceId",
                "jdbcUrl",
                "name",
                "odbcParams",
                "state"
            ],
            "inputProperties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                },
                "numClusters": {
                    "type": "integer"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                }
            },
            "requiredInputs": [
                "clusterSize"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlEndpoint resources.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL endpoint terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL endpoint is a Serverless endpoint. To use a Serverless SQL endpoint, you must enable Serverless SQL endpoints for the workspace.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL endpoint is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL endpoint is running. The default is `1`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                        "description": "Databricks tags all endpoint resources with these tags.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlGlobalConfig:SqlGlobalConfig": {
            "description": "This resource configures the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace. *Please note that changing parameters of this resource will restart all running databricks_sql_endpoint.*  To use this resource you need to be an administrator.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n### AWS example\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    instanceProfileArn: \"arn:....\",\n    dataAccessConfig: {\n        \"spark.sql.session.timeZone\": \"UTC\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    instance_profile_arn=\"arn:....\",\n    data_access_config={\n        \"spark.sql.session.timeZone\": \"UTC\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        InstanceProfileArn = \"arn:....\",\n        DataAccessConfig = \n        {\n            { \"spark.sql.session.timeZone\", \"UTC\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy:     pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tInstanceProfileArn: pulumi.String(\"arn:....\"),\n\t\t\tDataAccessConfig: pulumi.AnyMap{\n\t\t\t\t\"spark.sql.session.timeZone\": pulumi.Any(\"UTC\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .instanceProfileArn(\"arn:....\")\n            .dataAccessConfig(Map.of(\"spark.sql.session.timeZone\", \"UTC\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      instanceProfileArn: arn:....\n      dataAccessConfig:\n        spark.sql.session.timeZone: UTC\n```\n{{% /example %}}\n{{% example %}}\n### Azure example\n\nFor Azure you should use the `data_access_config` to provide the service principal configuration. You can use the Databricks SQL Admin Console UI to help you generate the right configuration values.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    dataAccessConfig: {\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": _var.tenant_id,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": `{{secrets/${local.secret_scope}/${local.secret_key}}}`,\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": `https://login.microsoftonline.com/${_var.tenant_id}/oauth2/token`,\n    },\n    sqlConfigParams: {\n        ANSI_MODE: \"true\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    data_access_config={\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": var[\"tenant_id\"],\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": f\"{{{{secrets/{local['secret_scope']}/{local['secret_key']}}}}}\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{var['tenant_id']}/oauth2/token\",\n    },\n    sql_config_params={\n        \"ANSI_MODE\": \"true\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        DataAccessConfig = \n        {\n            { \"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\" },\n            { \"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.id\", @var.Tenant_id },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.secret\", $\"{{{{secrets/{local.Secret_scope}/{local.Secret_key}}}}}\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", $\"https://login.microsoftonline.com/{@var.Tenant_id}/oauth2/token\" },\n        },\n        SqlConfigParams = \n        {\n            { \"ANSI_MODE\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy: pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tDataAccessConfig: pulumi.AnyMap{\n\t\t\t\t\"spark.hadoop.fs.azure.account.auth.type\":              pulumi.Any(\"OAuth\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth.provider.type\":    pulumi.Any(\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.id\":       pulumi.Any(_var.Tenant_id),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.secret\":   pulumi.Any(fmt.Sprintf(\"{{secrets/%v/%v}}\", local.Secret_scope, local.Secret_key)),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": pulumi.Any(fmt.Sprintf(\"https://login.microsoftonline.com/%v/oauth2/token\", _var.Tenant_id)),\n\t\t\t},\n\t\t\tSqlConfigParams: pulumi.AnyMap{\n\t\t\t\t\"ANSI_MODE\": pulumi.Any(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .dataAccessConfig(Map.ofEntries(\n                Map.entry(\"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.id\", var_.tenant_id()),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.secret\", String.format(\"{{{{secrets/%s/%s}}}}\", local.secret_scope(),local.secret_key())),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", String.format(\"https://login.microsoftonline.com/%s/oauth2/token\", var_.tenant_id()))\n            ))\n            .sqlConfigParams(Map.of(\"ANSI_MODE\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      dataAccessConfig:\n        spark.hadoop.fs.azure.account.auth.type: OAuth\n        spark.hadoop.fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n        spark.hadoop.fs.azure.account.oauth2.client.id: ${var.tenant_id}\n        spark.hadoop.fs.azure.account.oauth2.client.secret: '{{secrets/${local.secret_scope}/${local.secret_key}}}'\n        spark.hadoop.fs.azure.account.oauth2.client.endpoint: https://login.microsoftonline.com/${var.tenant_id}/oauth2/token\n      sqlConfigParams:\n        ANSI_MODE: true\n```\n\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_global_config` resource with command like the following (you need to use `global` as ID)bash\n\n```sh\n $ pulumi import databricks:index/sqlGlobalConfig:SqlGlobalConfig this global\n```\n\n ",
            "properties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "- data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "- databricks.InstanceProfile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "- The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "- SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "inputProperties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "- data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "- databricks.InstanceProfile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "- The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "- SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlGlobalConfig resources.\n",
                "properties": {
                    "dataAccessConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "- data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "- databricks.InstanceProfile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                    },
                    "securityPolicy": {
                        "type": "string",
                        "description": "- The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                    },
                    "sqlConfigParams": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "- SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlPermissions:SqlPermissions": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nThe following resource definition will enforce access control on a table by executing the following SQL queries on a special auto-terminating cluster it would create for this operation:\n\n* ```SHOW GRANT ON TABLE `default`.`foo` ```\n* ```REVOKE ALL PRIVILEGES ON TABLE `default`.`foo` FROM ... every group and user that has access to it ...```\n* ```GRANT MODIFY, SELECT ON TABLE `default`.`foo` TO `serge@example.com` ```\n* ```GRANT SELECT ON TABLE `default`.`foo` TO `special group` ```\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fooTable = new databricks.SqlPermissions(\"foo_table\", {\n    privilegeAssignments: [\n        {\n            principal: \"serge@example.com\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            principal: \"special group\",\n            privileges: [\"SELECT\"],\n        },\n    ],\n    table: \"foo\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfoo_table = databricks.SqlPermissions(\"fooTable\",\n    privilege_assignments=[\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"serge@example.com\",\n            privileges=[\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        ),\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"special group\",\n            privileges=[\"SELECT\"],\n        ),\n    ],\n    table=\"foo\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fooTable = new Databricks.SqlPermissions(\"fooTable\", new()\n    {\n        PrivilegeAssignments = new[]\n        {\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"serge@example.com\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                    \"MODIFY\",\n                },\n            },\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"special group\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n        Table = \"foo\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlPermissions(ctx, \"fooTable\", \u0026databricks.SqlPermissionsArgs{\n\t\t\tPrivilegeAssignments: SqlPermissionsPrivilegeAssignmentArray{\n\t\t\t\t\u0026SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"serge@example.com\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"special group\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTable: pulumi.String(\"foo\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlPermissions;\nimport com.pulumi.databricks.SqlPermissionsArgs;\nimport com.pulumi.databricks.inputs.SqlPermissionsPrivilegeAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fooTable = new SqlPermissions(\"fooTable\", SqlPermissionsArgs.builder()        \n            .privilegeAssignments(            \n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"serge@example.com\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build(),\n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"special group\")\n                    .privileges(\"SELECT\")\n                    .build())\n            .table(\"foo\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fooTable:\n    type: databricks:SqlPermissions\n    properties:\n      privilegeAssignments:\n        - principal: serge@example.com\n          privileges:\n            - SELECT\n            - MODIFY\n        - principal: special group\n          privileges:\n            - SELECT\n      table: foo\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Grants to manage data access in Unity Catalog.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are* `table/default.foo` - table `foo` in a `default` database. Database is always mandatory. * `view/bar.foo` - view `foo` in `bar` database. * `database/bar` - `bar` database. * `catalog/` - entire catalog. `/` suffix is mandatory. * `any file/` - direct access to any file. `/` suffix is mandatory. * `anonymous function/` - anonymous function. `/` suffix is mandatory. bash\n\n```sh\n $ pulumi import databricks:index/sqlPermissions:SqlPermissions foo /\u003cobject-type\u003e/\u003cobject-name\u003e\n```\n\n ",
            "properties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n"
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading any file. Defaults to `false`.\n"
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n"
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n"
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading any file. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n",
                    "willReplaceOnChanges": true
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlPermissions resources.\n",
                "properties": {
                    "anonymousFunction": {
                        "type": "boolean",
                        "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "anyFile": {
                        "type": "boolean",
                        "description": "If this access control for reading any file. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "catalog": {
                        "type": "boolean",
                        "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "database": {
                        "type": "string",
                        "description": "Name of the database. Has default value of `default`.\n",
                        "willReplaceOnChanges": true
                    },
                    "privilegeAssignments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                        }
                    },
                    "table": {
                        "type": "string",
                        "description": "Name of the table. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "description": "Name of the view. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlQuery:SqlQuery": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA query may have one or more visualizations.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst q1 = new databricks.SqlQuery(\"q1\", {\n    dataSourceId: databricks_sql_endpoint.example.data_source_id,\n    query: \"SELECT {{ p1 }} AS p1, 2 as p2\",\n    runAsRole: \"viewer\",\n    schedule: {\n        continuous: {\n            intervalSeconds: 5 * 60,\n        },\n    },\n    parameters: [{\n        name: \"p1\",\n        title: \"Title for p1\",\n        text: {\n            value: \"default\",\n        },\n    }],\n    tags: [\n        \"t1\",\n        \"t2\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nq1 = databricks.SqlQuery(\"q1\",\n    data_source_id=databricks_sql_endpoint[\"example\"][\"data_source_id\"],\n    query=\"SELECT {{ p1 }} AS p1, 2 as p2\",\n    run_as_role=\"viewer\",\n    schedule=databricks.SqlQueryScheduleArgs(\n        continuous=databricks.SqlQueryScheduleContinuousArgs(\n            interval_seconds=5 * 60,\n        ),\n    ),\n    parameters=[databricks.SqlQueryParameterArgs(\n        name=\"p1\",\n        title=\"Title for p1\",\n        text=databricks.SqlQueryParameterTextArgs(\n            value=\"default\",\n        ),\n    )],\n    tags=[\n        \"t1\",\n        \"t2\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var q1 = new Databricks.SqlQuery(\"q1\", new()\n    {\n        DataSourceId = databricks_sql_endpoint.Example.Data_source_id,\n        Query = \"SELECT {{ p1 }} AS p1, 2 as p2\",\n        RunAsRole = \"viewer\",\n        Schedule = new Databricks.Inputs.SqlQueryScheduleArgs\n        {\n            Continuous = new Databricks.Inputs.SqlQueryScheduleContinuousArgs\n            {\n                IntervalSeconds = 5 * 60,\n            },\n        },\n        Parameters = new[]\n        {\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p1\",\n                Title = \"Title for p1\",\n                Text = new Databricks.Inputs.SqlQueryParameterTextArgs\n                {\n                    Value = \"default\",\n                },\n            },\n        },\n        Tags = new[]\n        {\n            \"t1\",\n            \"t2\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlQuery(ctx, \"q1\", \u0026databricks.SqlQueryArgs{\n\t\t\tDataSourceId: pulumi.Any(databricks_sql_endpoint.Example.Data_source_id),\n\t\t\tQuery:        pulumi.String(\"SELECT {{ p1 }} AS p1, 2 as p2\"),\n\t\t\tRunAsRole:    pulumi.String(\"viewer\"),\n\t\t\tSchedule: \u0026SqlQueryScheduleArgs{\n\t\t\t\tContinuous: \u0026SqlQueryScheduleContinuousArgs{\n\t\t\t\t\tIntervalSeconds: 5 * 60,\n\t\t\t\t},\n\t\t\t},\n\t\t\tParameters: SqlQueryParameterArray{\n\t\t\t\t\u0026SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p1\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p1\"),\n\t\t\t\t\tText: \u0026SqlQueryParameterTextArgs{\n\t\t\t\t\t\tValue: pulumi.String(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"t1\"),\n\t\t\t\tpulumi.String(\"t2\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlQuery;\nimport com.pulumi.databricks.SqlQueryArgs;\nimport com.pulumi.databricks.inputs.SqlQueryScheduleArgs;\nimport com.pulumi.databricks.inputs.SqlQueryScheduleContinuousArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterTextArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var q1 = new SqlQuery(\"q1\", SqlQueryArgs.builder()        \n            .dataSourceId(databricks_sql_endpoint.example().data_source_id())\n            .query(\"SELECT {{ p1 }} AS p1, 2 as p2\")\n            .runAsRole(\"viewer\")\n            .schedule(SqlQueryScheduleArgs.builder()\n                .continuous(SqlQueryScheduleContinuousArgs.builder()\n                    .intervalSeconds(5 * 60)\n                    .build())\n                .build())\n            .parameters(SqlQueryParameterArgs.builder()\n                .name(\"p1\")\n                .title(\"Title for p1\")\n                .text(SqlQueryParameterTextArgs.builder()\n                    .value(\"default\")\n                    .build())\n                .build())\n            .tags(            \n                \"t1\",\n                \"t2\")\n            .build());\n\n    }\n}\n```\n\nExample permission to share query with all users:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst q1 = new databricks.Permissions(\"q1\", {\n    sqlQueryId: databricks_sql_query.q1.id,\n    accessControls: [\n        {\n            groupName: data.databricks_group.users.display_name,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: data.databricks_group.team.display_name,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nq1 = databricks.Permissions(\"q1\",\n    sql_query_id=databricks_sql_query[\"q1\"][\"id\"],\n    access_controls=[\n        databricks.PermissionsAccessControlArgs(\n            group_name=data[\"databricks_group\"][\"users\"][\"display_name\"],\n            permission_level=\"CAN_RUN\",\n        ),\n        databricks.PermissionsAccessControlArgs(\n            group_name=data[\"databricks_group\"][\"team\"][\"display_name\"],\n            permission_level=\"CAN_EDIT\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var q1 = new Databricks.Permissions(\"q1\", new()\n    {\n        SqlQueryId = databricks_sql_query.Q1.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = data.Databricks_group.Users.Display_name,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = data.Databricks_group.Team.Display_name,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"q1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlQueryId: pulumi.Any(databricks_sql_query.Q1.Id),\n\t\t\tAccessControls: PermissionsAccessControlArray{\n\t\t\t\t\u0026PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(data.Databricks_group.Users.Display_name),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(data.Databricks_group.Team.Display_name),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var q1 = new Permissions(\"q1\", PermissionsArgs.builder()        \n            .sqlQueryId(databricks_sql_query.q1().id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(data.databricks_group().users().display_name())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(data.databricks_group().team().display_name())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  q1:\n    type: databricks:Permissions\n    properties:\n      sqlQueryId: ${databricks_sql_query.q1.id}\n      accessControls:\n        - groupName: ${data.databricks_group.users.display_name}\n          permissionLevel: CAN_RUN\n        - groupName: ${data.databricks_group.team.display_name}\n          permissionLevel: CAN_EDIT\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_query` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlQuery:SqlQuery this \u003cquery-id\u003e\n```\n\n ",
            "properties": {
                "dataSourceId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "query": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "dataSourceId",
                "name",
                "query"
            ],
            "inputProperties": {
                "dataSourceId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "query": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "requiredInputs": [
                "dataSourceId",
                "query"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlQuery resources.\n",
                "properties": {
                    "dataSourceId": {
                        "type": "string"
                    },
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                        }
                    },
                    "query": {
                        "type": "string"
                    },
                    "runAsRole": {
                        "type": "string"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlVisualization:SqlVisualization": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA visualization is always tied to a query. Every query may have one or more visualizations.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst q1v1 = new databricks.SqlVisualization(\"q1v1\", {\n    queryId: databricks_sql_query.q1.id,\n    type: \"table\",\n    description: \"Some Description\",\n    options: JSON.stringify({\n        itemsPerPage: 25,\n        columns: [\n            {\n                name: \"p1\",\n                type: \"string\",\n                title: \"Parameter 1\",\n                displayAs: \"string\",\n            },\n            {\n                name: \"p2\",\n                type: \"string\",\n                title: \"Parameter 2\",\n                displayAs: \"link\",\n                highlightLinks: true,\n            },\n        ],\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nq1v1 = databricks.SqlVisualization(\"q1v1\",\n    query_id=databricks_sql_query[\"q1\"][\"id\"],\n    type=\"table\",\n    description=\"Some Description\",\n    options=json.dumps({\n        \"itemsPerPage\": 25,\n        \"columns\": [\n            {\n                \"name\": \"p1\",\n                \"type\": \"string\",\n                \"title\": \"Parameter 1\",\n                \"displayAs\": \"string\",\n            },\n            {\n                \"name\": \"p2\",\n                \"type\": \"string\",\n                \"title\": \"Parameter 2\",\n                \"displayAs\": \"link\",\n                \"highlightLinks\": True,\n            },\n        ],\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var q1v1 = new Databricks.SqlVisualization(\"q1v1\", new()\n    {\n        QueryId = databricks_sql_query.Q1.Id,\n        Type = \"table\",\n        Description = \"Some Description\",\n        Options = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"itemsPerPage\"] = 25,\n            [\"columns\"] = new[]\n            {\n                new Dictionary\u003cstring, object?\u003e\n                {\n                    [\"name\"] = \"p1\",\n                    [\"type\"] = \"string\",\n                    [\"title\"] = \"Parameter 1\",\n                    [\"displayAs\"] = \"string\",\n                },\n                new Dictionary\u003cstring, object?\u003e\n                {\n                    [\"name\"] = \"p2\",\n                    [\"type\"] = \"string\",\n                    [\"title\"] = \"Parameter 2\",\n                    [\"displayAs\"] = \"link\",\n                    [\"highlightLinks\"] = true,\n                },\n            },\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"itemsPerPage\": 25,\n\t\t\t\"columns\": []map[string]interface{}{\n\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\"name\":      \"p1\",\n\t\t\t\t\t\"type\":      \"string\",\n\t\t\t\t\t\"title\":     \"Parameter 1\",\n\t\t\t\t\t\"displayAs\": \"string\",\n\t\t\t\t},\n\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\"name\":           \"p2\",\n\t\t\t\t\t\"type\":           \"string\",\n\t\t\t\t\t\"title\":          \"Parameter 2\",\n\t\t\t\t\t\"displayAs\":      \"link\",\n\t\t\t\t\t\"highlightLinks\": true,\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewSqlVisualization(ctx, \"q1v1\", \u0026databricks.SqlVisualizationArgs{\n\t\t\tQueryId:     pulumi.Any(databricks_sql_query.Q1.Id),\n\t\t\tType:        pulumi.String(\"table\"),\n\t\t\tDescription: pulumi.String(\"Some Description\"),\n\t\t\tOptions:     pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlVisualization;\nimport com.pulumi.databricks.SqlVisualizationArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var q1v1 = new SqlVisualization(\"q1v1\", SqlVisualizationArgs.builder()        \n            .queryId(databricks_sql_query.q1().id())\n            .type(\"table\")\n            .description(\"Some Description\")\n            .options(serializeJson(\n                jsonObject(\n                    jsonProperty(\"itemsPerPage\", 25),\n                    jsonProperty(\"columns\", jsonArray(\n                        jsonObject(\n                            jsonProperty(\"name\", \"p1\"),\n                            jsonProperty(\"type\", \"string\"),\n                            jsonProperty(\"title\", \"Parameter 1\"),\n                            jsonProperty(\"displayAs\", \"string\")\n                        ), \n                        jsonObject(\n                            jsonProperty(\"name\", \"p2\"),\n                            jsonProperty(\"type\", \"string\"),\n                            jsonProperty(\"title\", \"Parameter 2\"),\n                            jsonProperty(\"displayAs\", \"link\"),\n                            jsonProperty(\"highlightLinks\", true)\n                        )\n                    ))\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  q1v1:\n    type: databricks:SqlVisualization\n    properties:\n      queryId: ${databricks_sql_query.q1.id}\n      type: table\n      description: Some Description\n      # The options encoded in this field are passed verbatim to the SQLA API.\n      options:\n        Fn::ToJSON:\n          itemsPerPage: 25\n          columns:\n            - name: p1\n              type: string\n              title: Parameter 1\n              displayAs: string\n            - name: p2\n              type: string\n              title: Parameter 2\n              displayAs: link\n              highlightLinks: true\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_visualization` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlVisualization:SqlVisualization this \u003cquery-id\u003e/\u003cvisualization-id\u003e\n```\n\n ",
            "properties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "options",
                "queryId",
                "type",
                "visualizationId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "options",
                "queryId",
                "type"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlVisualization resources.\n",
                "properties": {
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "options": {
                        "type": "string"
                    },
                    "queryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "type": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlWidget:SqlWidget": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA widget is always tied to a dashboard. Every dashboard may have one or more widgets.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1w1 = new databricks.SqlWidget(\"d1w1\", {\n    dashboardId: databricks_sql_dashboard.d1.id,\n    text: \"Hello! I'm a **text widget**!\",\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 0,\n        posY: 0,\n    },\n});\nconst d1w2 = new databricks.SqlWidget(\"d1w2\", {\n    dashboardId: databricks_sql_dashboard.d1.id,\n    visualizationId: databricks_sql_visualization.q1v1.id,\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 3,\n        posY: 0,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1w1 = databricks.SqlWidget(\"d1w1\",\n    dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    text=\"Hello! I'm a **text widget**!\",\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=0,\n        pos_y=0,\n    ))\nd1w2 = databricks.SqlWidget(\"d1w2\",\n    dashboard_id=databricks_sql_dashboard[\"d1\"][\"id\"],\n    visualization_id=databricks_sql_visualization[\"q1v1\"][\"id\"],\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=3,\n        pos_y=0,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1w1 = new Databricks.SqlWidget(\"d1w1\", new()\n    {\n        DashboardId = databricks_sql_dashboard.D1.Id,\n        Text = \"Hello! I'm a **text widget**!\",\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 0,\n            PosY = 0,\n        },\n    });\n\n    var d1w2 = new Databricks.SqlWidget(\"d1w2\", new()\n    {\n        DashboardId = databricks_sql_dashboard.D1.Id,\n        VisualizationId = databricks_sql_visualization.Q1v1.Id,\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 3,\n            PosY = 0,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlWidget(ctx, \"d1w1\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId: pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tText:        pulumi.String(\"Hello! I'm a **text widget**!\"),\n\t\t\tPosition: \u0026SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(0),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlWidget(ctx, \"d1w2\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId:     pulumi.Any(databricks_sql_dashboard.D1.Id),\n\t\t\tVisualizationId: pulumi.Any(databricks_sql_visualization.Q1v1.Id),\n\t\t\tPosition: \u0026SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(3),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlWidget;\nimport com.pulumi.databricks.SqlWidgetArgs;\nimport com.pulumi.databricks.inputs.SqlWidgetPositionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1w1 = new SqlWidget(\"d1w1\", SqlWidgetArgs.builder()        \n            .dashboardId(databricks_sql_dashboard.d1().id())\n            .text(\"Hello! I'm a **text widget**!\")\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(0)\n                .posY(0)\n                .build())\n            .build());\n\n        var d1w2 = new SqlWidget(\"d1w2\", SqlWidgetArgs.builder()        \n            .dashboardId(databricks_sql_dashboard.d1().id())\n            .visualizationId(databricks_sql_visualization.q1v1().id())\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(3)\n                .posY(0)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1w1:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${databricks_sql_dashboard.d1.id}\n      text: Hello! I'm a **text widget**!\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 0\n        posY: 0\n  d1w2:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${databricks_sql_dashboard.d1.id}\n      visualizationId: ${databricks_sql_visualization.q1v1.id}\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 3\n        posY: 0\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n\n## Import\n\nYou can import a `databricks_sql_widget` resource with ID like the followingbash\n\n```sh\n $ pulumi import databricks:index/sqlWidget:SqlWidget this \u003cdashboard-id\u003e/\u003cwidget-id\u003e\n```\n\n ",
            "properties": {
                "dashboardId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                },
                "widgetId": {
                    "type": "string"
                }
            },
            "required": [
                "dashboardId",
                "widgetId"
            ],
            "inputProperties": {
                "dashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "widgetId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "dashboardId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlWidget resources.\n",
                "properties": {
                    "dashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "description": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                        }
                    },
                    "position": {
                        "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                    },
                    "text": {
                        "type": "string"
                    },
                    "title": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "widgetId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/storageCredential:StorageCredential": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access.\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- `databricks.StorageCredential` represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- databricks.ExternalLocation are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n\n## Import\n\nThis resource can be imported by namebash\n\n```sh\n $ pulumi import databricks:index/storageCredential:StorageCredential this \u003cname\u003e\n```\n\n ",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                }
            },
            "required": [
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering StorageCredential resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                    },
                    "comment": {
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/table:Table": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html). Contact your Databricks representative to request access. \n\nWithin a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, databases (also called schemas), and tables / views.\n\n\u003e **Note** This resource has an evolving API, which will change in the upcoming versions of the provider in order to simplify user experience.\n\nA `databricks.Table` is contained within databricks_schema.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: databricks_metastore[\"this\"].id,\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst thing = new databricks.Table(\"thing\", {\n    catalogName: sandbox.id,\n    schemaName: things.name,\n    tableType: \"MANAGED\",\n    dataSourceFormat: \"DELTA\",\n    columns: [\n        {\n            name: \"id\",\n            position: 0,\n            typeName: \"INT\",\n            typeText: \"int\",\n            typeJson: \"{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n        },\n        {\n            name: \"name\",\n            position: 1,\n            typeName: \"STRING\",\n            typeText: \"varchar(64)\",\n            typeJson: \"{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"varchar(64)\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n        },\n    ],\n    comment: \"this table is managed by terraform\",\n}, {\n    provider: databricks.workspace,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=databricks_metastore[\"this\"][\"id\"],\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nthing = databricks.Table(\"thing\",\n    catalog_name=sandbox.id,\n    schema_name=things.name,\n    table_type=\"MANAGED\",\n    data_source_format=\"DELTA\",\n    columns=[\n        databricks.TableColumnArgs(\n            name=\"id\",\n            position=0,\n            type_name=\"INT\",\n            type_text=\"int\",\n            type_json=\"{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n        ),\n        databricks.TableColumnArgs(\n            name=\"name\",\n            position=1,\n            type_name=\"STRING\",\n            type_text=\"varchar(64)\",\n            type_json=\"{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"varchar(64)\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n        ),\n    ],\n    comment=\"this table is managed by terraform\",\n    opts=pulumi.ResourceOptions(provider=databricks[\"workspace\"]))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = databricks_metastore.This.Id,\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var thing = new Databricks.Table(\"thing\", new()\n    {\n        CatalogName = sandbox.Id,\n        SchemaName = things.Name,\n        TableType = \"MANAGED\",\n        DataSourceFormat = \"DELTA\",\n        Columns = new[]\n        {\n            new Databricks.Inputs.TableColumnArgs\n            {\n                Name = \"id\",\n                Position = 0,\n                TypeName = \"INT\",\n                TypeText = \"int\",\n                TypeJson = \"{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n            },\n            new Databricks.Inputs.TableColumnArgs\n            {\n                Name = \"name\",\n                Position = 1,\n                TypeName = \"STRING\",\n                TypeText = \"varchar(64)\",\n                TypeJson = \"{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"varchar(64)\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\",\n            },\n        },\n        Comment = \"this table is managed by terraform\",\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Workspace,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(databricks_metastore.This.Id),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.AnyMap{\n\t\t\t\t\"kind\": pulumi.Any(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewTable(ctx, \"thing\", \u0026databricks.TableArgs{\n\t\t\tCatalogName:      sandbox.ID(),\n\t\t\tSchemaName:       things.Name,\n\t\t\tTableType:        pulumi.String(\"MANAGED\"),\n\t\t\tDataSourceFormat: pulumi.String(\"DELTA\"),\n\t\t\tColumns: TableColumnArray{\n\t\t\t\t\u0026TableColumnArgs{\n\t\t\t\t\tName:     pulumi.String(\"id\"),\n\t\t\t\t\tPosition: pulumi.Int(0),\n\t\t\t\t\tTypeName: pulumi.String(\"INT\"),\n\t\t\t\t\tTypeText: pulumi.String(\"int\"),\n\t\t\t\t\tTypeJson: pulumi.String(\"{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\"),\n\t\t\t\t},\n\t\t\t\t\u0026TableColumnArgs{\n\t\t\t\t\tName:     pulumi.String(\"name\"),\n\t\t\t\t\tPosition: pulumi.Int(1),\n\t\t\t\t\tTypeName: pulumi.String(\"STRING\"),\n\t\t\t\t\tTypeText: pulumi.String(\"varchar(64)\"),\n\t\t\t\t\tTypeJson: pulumi.String(\"{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"varchar(64)\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\"),\n\t\t\t\t},\n\t\t\t},\n\t\t\tComment: pulumi.String(\"this table is managed by terraform\"),\n\t\t}, pulumi.Provider(databricks.Workspace))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.Table;\nimport com.pulumi.databricks.TableArgs;\nimport com.pulumi.databricks.inputs.TableColumnArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .metastoreId(databricks_metastore.this().id())\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.id())\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var thing = new Table(\"thing\", TableArgs.builder()        \n            .catalogName(sandbox.id())\n            .schemaName(things.name())\n            .tableType(\"MANAGED\")\n            .dataSourceFormat(\"DELTA\")\n            .columns(            \n                TableColumnArgs.builder()\n                    .name(\"id\")\n                    .position(0)\n                    .typeName(\"INT\")\n                    .typeText(\"int\")\n                    .typeJson(\"{\\\"name\\\":\\\"id\\\",\\\"type\\\":\\\"integer\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\")\n                    .build(),\n                TableColumnArgs.builder()\n                    .name(\"name\")\n                    .position(1)\n                    .typeName(\"STRING\")\n                    .typeText(\"varchar(64)\")\n                    .typeJson(\"{\\\"name\\\":\\\"name\\\",\\\"type\\\":\\\"varchar(64)\\\",\\\"nullable\\\":true,\\\"metadata\\\":{}}\")\n                    .build())\n            .comment(\"this table is managed by terraform\")\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.workspace())\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${databricks_metastore.this.id}\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n  thing:\n    type: databricks:Table\n    properties:\n      catalogName: ${sandbox.id}\n      schemaName: ${things.name}\n      tableType: MANAGED\n      dataSourceFormat: DELTA\n      columns:\n        - name: id\n          position: 0\n          typeName: INT\n          typeText: int\n          typeJson: '{\"name\":\"id\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}'\n        - name: name\n          position: 1\n          typeName: STRING\n          typeText: varchar(64)\n          typeJson: '{\"name\":\"name\",\"type\":\"varchar(64)\",\"nullable\":true,\"metadata\":{}}'\n      comment: this table is managed by terraform\n    options:\n      provider: ${databricks.workspace}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table data to list tables within Unity Catalog.\n* databricks.Schema data to list schemas within Unity Catalog.\n* databricks.Catalog data to list catalogs within Unity Catalog.\n\n\n## Import\n\nThis resource can be imported by full name*`catalog`.`schema`.`table`*bash\n\n```sh\n $ pulumi import databricks:index/table:Table this \u003cfull-name\u003e\n```\n\n ",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`\n"
                },
                "name": {
                    "type": "string",
                    "description": "User-visible name of column\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the table owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog\n"
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. This cannot be updated\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables. For Managed Tables, if the path is provided it needs to be a Staging Table path that has been generated through the Staging Table API, otherwise should be empty)\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`\n"
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`)\n"
                }
            },
            "required": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "name",
                "owner",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog\n",
                    "willReplaceOnChanges": true
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`\n"
                },
                "name": {
                    "type": "string",
                    "description": "User-visible name of column\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the table owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog\n",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. This cannot be updated\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables. For Managed Tables, if the path is provided it needs to be a Staging Table path that has been generated through the Staging Table API, otherwise should be empty)\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`\n",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`)\n"
                }
            },
            "requiredInputs": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Table resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                        }
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "dataSourceFormat": {
                        "type": "string",
                        "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "User-visible name of column\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the table owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Table properties.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of parent Schema relative to parent Catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "description": "For EXTERNAL Tables only: the name of storage credential to use. This cannot be updated\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "URL of storage location for Table data (required for EXTERNAL Tables. For Managed Tables, if the path is provided it needs to be a Staging Table path that has been generated through the Staging Table API, otherwise should be empty)\n"
                    },
                    "tableType": {
                        "type": "string",
                        "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`\n",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string",
                        "description": "SQL text defining the view (for `table_type == \"VIEW\"`)\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/token:Token": {
            "description": "This resource creates [Personal Access Tokens](https://docs.databricks.com/sql/user/security/personal-access-tokens.html) for the same user that is authenticated with the provider. Most likely you should use databricks.OboToken to create [On-Behalf-Of tokens](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-personal-access-tokens-for-a-service-principal) for a databricks.ServicePrincipal in Databricks workspaces on AWS. Databricks workspaces on other clouds use their own native OAuth token flows.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// initialize provider in normal mode\nconst createdWorkspace = new databricks.Provider(\"createdWorkspace\", {host: databricks_mws_workspaces[\"this\"].workspace_url});\n// create PAT token to provision entities within workspace\nconst pat = new databricks.Token(\"pat\", {\n    comment: \"Terraform Provisioning\",\n    lifetimeSeconds: 8640000,\n}, {\n    provider: databricks.created_workspace,\n});\nexport const databricksToken = pat.tokenValue;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# initialize provider in normal mode\ncreated_workspace = databricks.Provider(\"createdWorkspace\", host=databricks_mws_workspaces[\"this\"][\"workspace_url\"])\n# create PAT token to provision entities within workspace\npat = databricks.Token(\"pat\",\n    comment=\"Terraform Provisioning\",\n    lifetime_seconds=8640000,\n    opts=pulumi.ResourceOptions(provider=databricks[\"created_workspace\"]))\npulumi.export(\"databricksToken\", pat.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // initialize provider in normal mode\n    var createdWorkspace = new Databricks.Provider(\"createdWorkspace\", new()\n    {\n        Host = databricks_mws_workspaces.This.Workspace_url,\n    });\n\n    // create PAT token to provision entities within workspace\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = \"Terraform Provisioning\",\n        LifetimeSeconds = 8640000,\n    }, new CustomResourceOptions\n    {\n        Provider = databricks.Created_workspace,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = pat.TokenValue,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewProvider(ctx, \"createdWorkspace\", \u0026databricks.ProviderArgs{\n\t\t\tHost: pulumi.Any(databricks_mws_workspaces.This.Workspace_url),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpat, err := databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"Terraform Provisioning\"),\n\t\t\tLifetimeSeconds: pulumi.Int(8640000),\n\t\t}, pulumi.Provider(databricks.Created_workspace))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", pat.TokenValue)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.pulumi.providers.databricks;\nimport com.pulumi.pulumi.providers.ProviderArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var createdWorkspace = new Provider(\"createdWorkspace\", ProviderArgs.builder()        \n            .host(databricks_mws_workspaces.this().workspace_url())\n            .build());\n\n        var pat = new Token(\"pat\", TokenArgs.builder()        \n            .comment(\"Terraform Provisioning\")\n            .lifetimeSeconds(8640000)\n            .build(), CustomResourceOptions.builder()\n                .provider(databricks.created_workspace())\n                .build());\n\n        ctx.export(\"databricksToken\", pat.tokenValue());\n    }\n}\n```\n```yaml\nresources:\n  # initialize provider in normal mode\n  createdWorkspace:\n    type: pulumi:providers:databricks\n    properties:\n      host: ${databricks_mws_workspaces.this.workspace_url}\n  # create PAT token to provision entities within workspace\n  pat:\n    type: databricks:Token\n    properties:\n      comment: Terraform Provisioning\n      # 100 day token\n      lifetimeSeconds: 8.64e+06\n    options:\n      provider: ${databricks.created_workspace}\noutputs:\n  # output token for other modules\n  databricksToken: ${pat.tokenValue}\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n"
                }
            },
            "required": [
                "creationTime",
                "expiryTime",
                "tokenId",
                "tokenValue"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                },
                "tokenId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Token resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "expiryTime": {
                        "type": "integer"
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenId": {
                        "type": "string"
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/user:User": {
            "description": "This resource allows you to manage [users in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/users.html), [Databricks Account Console](https://accounts.cloud.databricks.com/) or [Azure Databricks Account Console](https://accounts.azuredatabricks.net). You can also associate Databricks users to databricks_group. Upon user creation the user will receive a password reset email. You can also get information about caller identity using databricks.getCurrentUser data source.\n\nTo create users in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\nThe resource scim user can be imported using idbash\n\n```sh\n $ pulumi import databricks:index/user:User me \u003cuser-id\u003e\n```\n\n ",
            "properties": {
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.\n"
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "required": [
                "displayName",
                "userName"
            ],
            "inputProperties": {
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "requiredInputs": [
                "userName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering User resources.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the username that can be the full name of the user.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "userName": {
                        "type": "string",
                        "description": "This is the username of the given user and will be their form of access and identity.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userInstanceProfile:UserInstanceProfile": {
            "description": "\u003e **Deprecated** Please rewrite with databricks_user_role. This resource will be removed in v0.5.x\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserInstanceProfile = new databricks.UserInstanceProfile(\"myUserInstanceProfile\", {\n    userId: myUser.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_instance_profile = databricks.UserInstanceProfile(\"myUserInstanceProfile\",\n    user_id=my_user.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserInstanceProfile = new Databricks.UserInstanceProfile(\"myUserInstanceProfile\", new()\n    {\n        UserId = myUser.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserInstanceProfile(ctx, \"myUserInstanceProfile\", \u0026databricks.UserInstanceProfileArgs{\n\t\t\tUserId:            myUser.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserInstanceProfile;\nimport com.pulumi.databricks.UserInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserInstanceProfile = new UserInstanceProfile(\"myUserInstanceProfile\", UserInstanceProfileArgs.builder()        \n            .userId(myUser.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserInstanceProfile:\n    type: databricks:UserInstanceProfile\n    properties:\n      userId: ${myUser.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "instanceProfileId",
                "userId"
            ],
            "inputProperties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "instanceProfileId",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserInstanceProfile resources.\n",
                "properties": {
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userRole:UserRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to databricks_user.\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAdding AWS instance profile to a user\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instanceProfile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserRole = new databricks.UserRole(\"myUserRole\", {\n    userId: myUser.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instanceProfile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_role = databricks.UserRole(\"myUserRole\",\n    user_id=my_user.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instanceProfile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserRole = new Databricks.UserRole(\"myUserRole\", new()\n    {\n        UserId = myUser.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instanceProfile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"myUserRole\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserRole = new UserRole(\"myUserRole\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserRole:\n    type: databricks:UserRole\n    properties:\n      userId: ${myUser.id}\n      role: ${instanceProfile.id}\n```\n\nAdding user as administrator to Databricks Account\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myUser = new databricks.User(\"myUser\", {userName: \"me@example.com\"});\nconst myUserAccountAdmin = new databricks.UserRole(\"myUserAccountAdmin\", {\n    userId: myUser.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_user = databricks.User(\"myUser\", user_name=\"me@example.com\")\nmy_user_account_admin = databricks.UserRole(\"myUserAccountAdmin\",\n    user_id=my_user.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myUser = new Databricks.User(\"myUser\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserAccountAdmin = new Databricks.UserRole(\"myUserAccountAdmin\", new()\n    {\n        UserId = myUser.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyUser, err := databricks.NewUser(ctx, \"myUser\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"myUserAccountAdmin\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserAccountAdmin = new UserRole(\"myUserAccountAdmin\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myUser:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myUserAccountAdmin:\n    type: databricks:UserRole\n    properties:\n      userId: ${myUser.id}\n      role: account_admin\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the id of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "role",
                "userId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceConf:WorkspaceConf": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nAllows specification of custom configuration properties for expert usage:\n\n * `enableIpAccessLists` - enables the use of databricks.IpAccessList resources\n * `maxTokenLifetimeDays` - (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. **WARNING:** This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set. \n * `enableTokensConfig` - (boolean) Enable or disable personal access tokens for this workspace.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.AnyMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n```\n{{% /example %}}\n{{% /examples %}}\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported. ",
            "properties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "inputProperties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceConf resources.\n",
                "properties": {
                    "customConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                    }
                },
                "type": "object"
            }
        }
    },
    "functions": {
        "databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy": {
            "inputs": {
                "description": "A collection of arguments for invoking getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "Account Id that could be found in the bottom left corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                    },
                    "forLogDelivery": {
                        "type": "boolean",
                        "description": "Either or not this assume role policy should be created for usage log delivery. Defaults to false.\n"
                    }
                },
                "type": "object",
                "required": [
                    "externalId"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "forLogDelivery": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "AWS IAM Policy JSON document\n"
                    }
                },
                "type": "object",
                "required": [
                    "externalId",
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getAwsBucketPolicy:getAwsBucketPolicy": {
            "description": "This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it. \n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* End to end workspace management guide\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string",
                        "description": "AWS S3 Bucket name for which to generate the policy document.\n"
                    },
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "fullAccessRole": {
                        "type": "string",
                        "description": "Data access role that can have full access for this bucket\n"
                    }
                },
                "type": "object",
                "required": [
                    "bucket"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string"
                    },
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "fullAccessRole": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "(Read-only) AWS IAM Policy JSON document to grant Databricks full access to bucket.\n"
                    }
                },
                "type": "object",
                "required": [
                    "bucket",
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.\n\nThis data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n\n{{% examples %}}\n## Example Usage\n{{% example %}}\n\nFor more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisAwsCrossAccountPolicy = pulumi.output(databricks.getAwsCrossAccountPolicy());\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_cross_account_policy()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    Fn::Invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsCrossAccountPolicy.\n",
                "properties": {
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of Data IAM role ARNs that are explicitly granted `iam:PassRole` action.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getAwsCrossAccountPolicy.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "json": {
                        "type": "string",
                        "description": "AWS IAM Policy JSON document\n"
                    },
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object",
                "required": [
                    "json",
                    "id"
                ]
            }
        },
        "databricks:index/getCatalogs:getCatalogs": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nListing all catalogs:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getCatalogs({});\nexport const allCatalogs = all;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_catalogs()\npulumi.export(\"allCatalogs\", all)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetCatalogs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allCatalogs\"] = all.Apply(getCatalogsResult =\u003e getCatalogsResult),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetCatalogs(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allCatalogs\", all)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCatalogsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getCatalogs();\n\n        ctx.export(\"allCatalogs\", all.applyValue(getCatalogsResult -\u003e getCatalogsResult));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    Fn::Invoke:\n      Function: databricks:getCatalogs\n      Arguments: {}\noutputs:\n  allCatalogs: ${all}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table to manage tables within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCatalogs.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCatalogs.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getCluster:getCluster": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allClusters = databricks.getClusters({});\nconst allCluster = .map(([, ]) =\u003e databricks.getCluster({\n    clusterId: __value,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_clusters = databricks.get_clusters()\nall_cluster = [databricks.get_cluster(cluster_id=__value) for __key, __value in data[\"databricks_clusters\"][\"ids\"]]\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "description": "The id of the cluster\n"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo"
                    }
                },
                "type": "object",
                "required": [
                    "clusterId"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    }
                },
                "type": "object",
                "required": [
                    "clusterId",
                    "clusterInfo",
                    "id"
                ]
            }
        },
        "databricks:index/getClusters:getClusters": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve all clusters on this workspace on AWS or GCP:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    Fn::Invoke:\n      Function: databricks:getClusters\n      Arguments: {}\n```\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getClusters({\n    clusterNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_clusters(cluster_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetClusters.Invoke(new()\n    {\n        ClusterNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026GetClustersArgs{\n\t\t\tClusterNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .clusterNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    Fn::Invoke:\n      Function: databricks:getClusters\n      Arguments:\n        clusterNameContains: shared\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string",
                        "description": "Only return databricks.Cluster ids that match the given name string.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Cluster ids\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getCurrentUser:getCurrentUser": {
            "description": "## Exported attributes\n\nData source exposes the following attributes:\n\n* `id` -  The id of the calling user.\n* `external_id` - ID of the user in an external identity provider.\n* `user_name` - Name of the user, e.g. `mr.foo@example.com`. If the currently logged-in identity is a service principal, returns the application ID, e.g. `11111111-2222-3333-4444-555666777888`\n* `home` - Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n* `repos` - Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n* `alphanumeric` - Alphanumeric representation of user local name. e.g. `mr_foo`.\n* `workspace_url` - URL of the current Databricks workspace.\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n",
            "outputs": {
                "description": "A collection of values returned by getCurrentUser.\n",
                "properties": {
                    "alphanumeric": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "home": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "repos": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string"
                    },
                    "workspaceUrl": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "alphanumeric",
                    "externalId",
                    "home",
                    "repos",
                    "userName",
                    "workspaceUrl",
                    "id"
                ]
            }
        },
        "databricks:index/getDbfsFile:getDbfsFile": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst report = pulumi.output(databricks.getDbfsFile({\n    limitFileSize: 10240,\n    path: \"dbfs:/reports/some.csv\",\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nreport = databricks.get_dbfs_file(limit_file_size=10240,\n    path=\"dbfs:/reports/some.csv\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var report = Databricks.GetDbfsFile.Invoke(new()\n    {\n        LimitFileSize = 10240,\n        Path = \"dbfs:/reports/some.csv\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDbfsFile(ctx, \u0026GetDbfsFileArgs{\n\t\t\tLimitFileSize: 10240,\n\t\t\tPath:          \"dbfs:/reports/some.csv\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()\n            .limitFileSize(10240)\n            .path(\"dbfs:/reports/some.csv\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  report:\n    Fn::Invoke:\n      Function: databricks:getDbfsFile\n      Arguments:\n        limitFileSize: 10240\n        path: dbfs:/reports/some.csv\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFile.\n",
                "properties": {
                    "limitFileSize": {
                        "type": "boolean",
                        "description": "Do lot load content for files smaller than this in bytes\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to get content of\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "limitFileSize",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFile.\n",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "base64-encoded file contents\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "size of the file in bytes\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "limitFileSize": {
                        "type": "boolean"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "content",
                    "fileSize",
                    "limitFileSize",
                    "path",
                    "id"
                ]
            }
        },
        "databricks:index/getDbfsFilePaths:getDbfsFilePaths": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst partitions = pulumi.output(databricks.getDbfsFilePaths({\n    path: \"dbfs:/user/hive/default.db/table\",\n    recursive: false,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npartitions = databricks.get_dbfs_file_paths(path=\"dbfs:/user/hive/default.db/table\",\n    recursive=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var partitions = Databricks.GetDbfsFilePaths.Invoke(new()\n    {\n        Path = \"dbfs:/user/hive/default.db/table\",\n        Recursive = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetDbfsFilePaths(ctx, \u0026GetDbfsFilePathsArgs{\n\t\t\tPath:      \"dbfs:/user/hive/default.db/table\",\n\t\t\tRecursive: false,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()\n            .path(\"dbfs:/user/hive/default.db/table\")\n            .recursive(false)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  partitions:\n    Fn::Invoke:\n      Function: databricks:getDbfsFilePaths\n      Arguments:\n        path: dbfs:/user/hive/default.db/table\n        recursive: false\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFilePaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to perform listing\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or not recursively list all files\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFilePaths.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "path": {
                        "type": "string"
                    },
                    "pathLists": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList"
                        },
                        "description": "returns list of objects with `path` and `file_size` attributes in each\n"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "pathLists",
                    "recursive",
                    "id"
                ]
            }
        },
        "databricks:index/getGroup:getGroup": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks_group_member to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getGroup.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n"
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the group. The group must exist before this resource can be planned.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Collect information for all nested groups. *Defaults to true.*\n"
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "displayName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getGroup.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n"
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean"
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks_group_member resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "childGroups",
                    "displayName",
                    "externalId",
                    "groups",
                    "instanceProfiles",
                    "members",
                    "servicePrincipals",
                    "users",
                    "id"
                ]
            }
        },
        "databricks:index/getJobs:getJobs": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJobs.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJobs.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getMwsWorkspaces:getMwsWorkspaces": {
            "inputs": {
                "description": "A collection of arguments for invoking getMwsWorkspaces.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsWorkspaces.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getNodeType:getNodeType": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    Fn::Invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    Fn::Invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string",
                        "description": "Node category, which can be one of (depending on the cloud environment, could be checked with `databricks clusters list-node-types|jq '.node_types[]|.category'|sort |uniq`):\n* `General Purpose` (all clouds)\n* `General Purpose (HDD)` (Azure)\n* `Compute Optimized` (all clouds)\n* `Memory Optimized` (all clouds)\n* `Memory Optimized (Remote HDD)` (Azure)\n* `Storage Optimized` (AWS, Azure)\n* `GPU Accelerated` (AWS, Azure)\n"
                    },
                    "gbPerCore": {
                        "type": "integer",
                        "description": "Number of gigabytes per core available on instance. Conflicts with `min_memory_gb`. Defaults to *0*.\n"
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to nodes with AWS Graviton CPUs. Default to *false*.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean",
                        "description": ". Pick only nodes that have IO Cache. Defaults to *false*.\n"
                    },
                    "localDisk": {
                        "type": "boolean",
                        "description": "Pick only nodes with local storage. Defaults to *false*.\n"
                    },
                    "minCores": {
                        "type": "integer",
                        "description": "Minimum number of CPU cores available on instance. Defaults to *0*.\n"
                    },
                    "minGpus": {
                        "type": "integer",
                        "description": "Minimum number of GPU's attached to instance. Defaults to *0*.\n"
                    },
                    "minMemoryGb": {
                        "type": "integer",
                        "description": "Minimum amount of memory per node in gigabytes. Defaults to *0*.\n"
                    },
                    "photonDriverCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon driver. Defaults to *false*.\n"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon workers. Defaults to *false*.\n"
                    },
                    "supportPortForwarding": {
                        "type": "boolean",
                        "description": "Pick only nodes that support port forwarding. Defaults to *false*.\n"
                    },
                    "vcpu": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string"
                    },
                    "gbPerCore": {
                        "type": "integer"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean"
                    },
                    "localDisk": {
                        "type": "boolean"
                    },
                    "minCores": {
                        "type": "integer"
                    },
                    "minGpus": {
                        "type": "integer"
                    },
                    "minMemoryGb": {
                        "type": "integer"
                    },
                    "photonDriverCapable": {
                        "type": "boolean"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean"
                    },
                    "supportPortForwarding": {
                        "type": "boolean"
                    },
                    "vcpu": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            }
        },
        "databricks:index/getNotebook:getNotebook": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst features = pulumi.output(databricks.getNotebook({\n    format: \"SOURCE\",\n    path: \"/Production/Features\",\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfeatures = databricks.get_notebook(format=\"SOURCE\",\n    path=\"/Production/Features\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var features = Databricks.GetNotebook.Invoke(new()\n    {\n        Format = \"SOURCE\",\n        Path = \"/Production/Features\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupNotebook(ctx, \u0026GetNotebookArgs{\n\t\t\tFormat: \"SOURCE\",\n\t\t\tPath:   \"/Production/Features\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()\n            .format(\"SOURCE\")\n            .path(\"/Production/Features\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  features:\n    Fn::Invoke:\n      Function: databricks:getNotebook\n      Arguments:\n        format: SOURCE\n        path: /Production/Features\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebook.\n",
                "properties": {
                    "format": {
                        "type": "string",
                        "description": "Notebook format to export. Either `SOURCE`, `HTML`, `JUPYTER`, or `DBC`.\n",
                        "willReplaceOnChanges": true
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Notebook path on the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "format",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebook.\n",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "notebook content in selected format\n"
                    },
                    "format": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "content",
                    "format",
                    "language",
                    "objectId",
                    "objectType",
                    "path",
                    "id"
                ]
            }
        },
        "databricks:index/getNotebookPaths:getNotebookPaths": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = pulumi.output(databricks.getNotebookPaths({\n    path: \"/Production\",\n    recursive: true,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_notebook_paths(path=\"/Production\",\n    recursive=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetNotebookPaths.Invoke(new()\n    {\n        Path = \"/Production\",\n        Recursive = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetNotebookPaths(ctx, \u0026GetNotebookPathsArgs{\n\t\t\tPath:      \"/Production\",\n\t\t\tRecursive: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookPathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()\n            .path(\"/Production\")\n            .recursive(true)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    Fn::Invoke:\n      Function: databricks:getNotebookPaths\n      Arguments:\n        path: /Production\n        recursive: true\n```\n{{% /example %}}\n{{% /examples %}}",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebookPaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path to workspace directory\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or recursively walk given path\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebookPaths.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "notebookPathLists": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList"
                        },
                        "description": "list of objects with `path` and `language` attributes\n"
                    },
                    "path": {
                        "type": "string"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "type": "object",
                "required": [
                    "notebookPathLists",
                    "path",
                    "recursive",
                    "id"
                ]
            }
        },
        "databricks:index/getSchemas:getSchemas": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nListing all schemas in a _sandbox_ databricks_catalog:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = databricks.getSchemas({\n    catalogName: \"sandbox\",\n});\nexport const allSandboxSchemas = sandbox;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.get_schemas(catalog_name=\"sandbox\")\npulumi.export(\"allSandboxSchemas\", sandbox)\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = Databricks.GetSchemas.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allSandboxSchemas\"] = sandbox.Apply(getSchemasResult =\u003e getSchemasResult),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.GetSchemas(ctx, \u0026GetSchemasArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allSandboxSchemas\", sandbox)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSchemasArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()\n            .catalogName(\"sandbox\")\n            .build());\n\n        ctx.export(\"allSandboxSchemas\", sandbox.applyValue(getSchemasResult -\u003e getSchemasResult));\n    }\n}\n```\n```yaml\nvariables:\n  sandbox:\n    Fn::Invoke:\n      Function: databricks:getSchemas\n      Arguments:\n        catalogName: sandbox\noutputs:\n  allSandboxSchemas: ${sandbox}\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table to manage tables within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getServicePrincipal:getServicePrincipal": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipal.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "ID of the service principal. The service principal must exist before this resource can be retrieved.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipal.\n",
                "properties": {
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    }
                },
                "type": "object",
                "required": [
                    "active",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "repos",
                    "spId",
                    "id"
                ]
            }
        },
        "databricks:index/getServicePrincipals:getServicePrincipals": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string",
                        "description": "Only return databricks.ServicePrincipal display name that match the given name string\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    }
                },
                "type": "object",
                "required": [
                    "applicationIds",
                    "displayNameContains",
                    "id"
                ]
            }
        },
        "databricks:index/getSparkVersion:getSparkVersion": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    Fn::Invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    Fn::Invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that are in Beta stage. Default to `false`.\n"
                    },
                    "genomics": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.\n"
                    },
                    "gpu": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that support GPUs. Default to `false`.\n"
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`.\n"
                    },
                    "latest": {
                        "type": "boolean",
                        "description": "if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.\n"
                    },
                    "longTermSupport": {
                        "type": "boolean",
                        "description": "if we should limit the search only to LTS (long term support) \u0026 ESR (extended support) versions. Default to `false`.\n"
                    },
                    "ml": {
                        "type": "boolean",
                        "description": "if we should limit the search only to ML runtimes. Default to `false`.\n"
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Photon runtimes. Default to `false`.\n"
                    },
                    "scala": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean"
                    },
                    "genomics": {
                        "type": "boolean"
                    },
                    "gpu": {
                        "type": "boolean"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "latest": {
                        "type": "boolean"
                    },
                    "longTermSupport": {
                        "type": "boolean"
                    },
                    "ml": {
                        "type": "boolean"
                    },
                    "photon": {
                        "type": "boolean"
                    },
                    "scala": {
                        "type": "string"
                    },
                    "sparkVersion": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            }
        },
        "databricks:index/getSqlWarehouse:getSqlWarehouse": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allSqlWarehouses = databricks.getSqlWarehouses({});\nconst allSqlWarehouse = .map(([, ]) =\u003e databricks.getSqlWarehouse({\n    id: __value,\n}));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_sql_warehouses = databricks.get_sql_warehouses()\nall_sql_warehouse = [databricks.get_sql_warehouse(id=__value) for __key, __value in data[\"databricks_sql\"][\"warehouses\"][\"ids\"]]\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine).\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a Serverless warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL warehouses for the workspace.\n* `channel` block, consisting of following fields:\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the SQL warehouse\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "Databricks tags all warehouse resources with these tags.\n"
                    }
                },
                "type": "object",
                "required": [
                    "id"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine).\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a Serverless warehouse. To use a Serverless SQL warehouse, you must enable Serverless SQL warehouses for the workspace.\n* `channel` block, consisting of following fields:\n"
                    },
                    "id": {
                        "type": "string"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                    },
                    "numClusters": {
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "Databricks tags all warehouse resources with these tags.\n"
                    }
                },
                "type": "object",
                "required": [
                    "autoStopMins",
                    "channel",
                    "clusterSize",
                    "dataSourceId",
                    "enablePhoton",
                    "enableServerlessCompute",
                    "id",
                    "instanceProfileArn",
                    "jdbcUrl",
                    "maxNumClusters",
                    "minNumClusters",
                    "name",
                    "numClusters",
                    "odbcParams",
                    "spotInstancePolicy",
                    "state",
                    "tags"
                ]
            }
        },
        "databricks:index/getSqlWarehouses:getSqlWarehouses": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\nRetrieve all SQL warehouses on this workspace on AWS or GCP:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouses();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    Fn::Invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments: {}\n```\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getSqlWarehouses({\n    warehouseNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_sql_warehouses(warehouse_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetSqlWarehouses.Invoke(new()\n    {\n        WarehouseNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, \u0026GetSqlWarehousesArgs{\n\t\t\tWarehouseNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()\n            .warehouseNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    Fn::Invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments:\n        warehouseNameContains: shared\n```\n{{% /example %}}\n{{% /examples %}}\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouses.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string",
                        "description": "Only return databricks.SqlEndpoint ids that match the given name string.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouses.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "ids",
                    "id"
                ]
            }
        },
        "databricks:index/getTables:getTables": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table to manage tables within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ]
            }
        },
        "databricks:index/getUser:getUser": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks_group_member to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getUser.\n",
                "properties": {
                    "userId": {
                        "type": "string",
                        "description": "ID of the user.\n"
                    },
                    "userName": {
                        "type": "string",
                        "description": "User name of the user. The user must exist before this resource can be planned.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getUser.\n",
                "properties": {
                    "alphanumeric": {
                        "type": "string",
                        "description": "Alphanumeric representation of user local name. e.g. `mr_foo`.\n"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the user, e.g. `Mr Foo`.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                    },
                    "userId": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string",
                        "description": "Name of the user, e.g. `mr.foo@example.com`.\n"
                    }
                },
                "type": "object",
                "required": [
                    "alphanumeric",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "repos",
                    "id"
                ]
            }
        },
        "databricks:index/getViews:getViews": {
            "description": "## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Table to manage tables within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ]
            }
        },
        "databricks:index/getZones:getZones": {
            "description": "{{% examples %}}\n## Example Usage\n{{% example %}}\n\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst zones = pulumi.output(databricks.getZones());\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nzones = databricks.get_zones()\n```\n```csharp\nusing System.Collections.Generic;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var zones = Databricks.GetZones.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetZones(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var zones = DatabricksFunctions.getZones();\n\n    }\n}\n```\n```yaml\nvariables:\n  zones:\n    Fn::Invoke:\n      Function: databricks:getZones\n      Arguments: {}\n```\n{{% /example %}}\n{{% /examples %}}",
            "outputs": {
                "description": "A collection of values returned by getZones.\n",
                "properties": {
                    "defaultZone": {
                        "type": "string",
                        "description": "This is the default zone that gets assigned to your workspace. This is the zone used by default for clusters and instance pools.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The provider-assigned unique ID for this managed resource.\n"
                    },
                    "zones": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "This is a list of all the zones available for your subnets in your Databricks workspace.\n"
                    }
                },
                "type": "object",
                "required": [
                    "defaultZone",
                    "zones",
                    "id"
                ]
            }
        }
    }
}