{
    "name": "databricks",
    "displayName": "Databricks",
    "description": "A Pulumi package for creating and managing databricks cloud resources.",
    "keywords": [
        "pulumi",
        "databricks",
        "category/infrastructure"
    ],
    "homepage": "https://www.pulumi.com",
    "license": "Apache-2.0",
    "attribution": "This Pulumi package is based on the [`databricks` Terraform Provider](https://github.com/databricks/terraform-provider-databricks).",
    "repository": "https://github.com/pulumi/pulumi-databricks",
    "publisher": "Pulumi",
    "meta": {
        "moduleFormat": "(.*)(?:/[^/]*)"
    },
    "language": {
        "csharp": {
            "packageReferences": {
                "Pulumi": "3.*"
            },
            "compatibility": "tfbridge20",
            "respectSchemaVersion": true
        },
        "go": {
            "importBasePath": "github.com/pulumi/pulumi-databricks/sdk/go/databricks",
            "generateResourceContainerTypes": true,
            "generateExtraInputTypes": true,
            "respectSchemaVersion": true
        },
        "nodejs": {
            "packageDescription": "A Pulumi package for creating and managing databricks cloud resources.",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "devDependencies": {
                "@types/mime": "^2.0.0",
                "@types/node": "^10.0.0"
            },
            "compatibility": "tfbridge20",
            "disableUnionOutputTypes": true,
            "respectSchemaVersion": true
        },
        "python": {
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "compatibility": "tfbridge20",
            "respectSchemaVersion": true,
            "pyproject": {
                "enabled": true
            }
        }
    },
    "config": {
        "variables": {
            "accountId": {
                "type": "string"
            },
            "actionsIdTokenRequestToken": {
                "type": "string"
            },
            "actionsIdTokenRequestUrl": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "serverlessComputeId": {
                "type": "string"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        }
    },
    "types": {
        "databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule": {
            "properties": {
                "principals": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a list of principals who are granted a role. The following format is supported:\n* `users/{username}` (also exposed as `acl_principal_id` attribute of `databricks.User` resource).\n* `groups/{groupname}` (also exposed as `acl_principal_id` attribute of `databricks.Group` resource).\n* `servicePrincipals/{applicationId}` (also exposed as `acl_principal_id` attribute of `databricks.ServicePrincipal` resource).\n"
                },
                "role": {
                    "type": "string",
                    "description": "Role to be granted. The supported roles are listed below. For more information about these roles, refer to [service principal roles](https://docs.databricks.com/security/auth-authz/access-control/service-principal-acl.html#service-principal-roles), [group roles](https://docs.databricks.com/en/administration-guide/users-groups/groups.html#manage-roles-on-an-account-group-using-the-workspace-admin-settings-page), [marketplace roles](https://docs.databricks.com/en/marketplace/get-started-provider.html#assign-the-marketplace-admin-role) or [budget policy permissions](https://docs.databricks.com/aws/en/admin/usage/budget-policies#manage-budget-policy-permissions), depending on the `name` defined:\n* `accounts/{account_id}/ruleSets/default`\n* `roles/marketplace.admin` - Databricks Marketplace administrator.\n* `roles/billing.admin` - Billing administrator.\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default`\n* `roles/servicePrincipal.manager` - Manager of a service principal.\n* `roles/servicePrincipal.user` - User of a service principal.\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default`\n* `roles/group.manager` - Manager of a group.\n* `accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default`\n* `roles/budgetPolicy.manager` - Manager of a budget policy.\n* `roles/budgetPolicy.user` - User of a budget policy.\n"
                }
            },
            "type": "object",
            "required": [
                "role"
            ]
        },
        "databricks:index/AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy:AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy": {
            "properties": {
                "accessPolicyType": {
                    "type": "string",
                    "description": "Configured embedding policy. Possible values are `ALLOW_ALL_DOMAINS`, `ALLOW_APPROVED_DOMAINS`, `DENY_ALL_DOMAINS`.\n"
                }
            },
            "type": "object",
            "required": [
                "accessPolicyType"
            ]
        },
        "databricks:index/AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains:AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains": {
            "properties": {
                "approvedDomains": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "the list of approved domains. To allow all subdomains for a given domain, use a wildcard symbol (`*`) before the domain name, i.e., `*.databricks.com` will allow to embed into any site under the `databricks.com`.\n"
                }
            },
            "type": "object",
            "required": [
                "approvedDomains"
            ]
        },
        "databricks:index/AlertCondition:AlertCondition": {
            "properties": {
                "emptyResultState": {
                    "type": "string",
                    "description": "Alert state if the result is empty (`UNKNOWN`, `OK`, `TRIGGERED`)\n"
                },
                "op": {
                    "type": "string",
                    "description": "Operator used for comparison in alert evaluation. (Enum: `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `EQUAL`, `NOT_EQUAL`, `IS_NULL`)\n"
                },
                "operand": {
                    "$ref": "#/types/databricks:index/AlertConditionOperand:AlertConditionOperand",
                    "description": "Name of the column from the query result to use for comparison in alert evaluation:\n"
                },
                "threshold": {
                    "$ref": "#/types/databricks:index/AlertConditionThreshold:AlertConditionThreshold",
                    "description": "Threshold value used for comparison in alert evaluation:\n"
                }
            },
            "type": "object",
            "required": [
                "op",
                "operand"
            ]
        },
        "databricks:index/AlertConditionOperand:AlertConditionOperand": {
            "properties": {
                "column": {
                    "$ref": "#/types/databricks:index/AlertConditionOperandColumn:AlertConditionOperandColumn",
                    "description": "Block describing the column from the query result to use for comparison in alert evaluation:\n"
                }
            },
            "type": "object",
            "required": [
                "column"
            ]
        },
        "databricks:index/AlertConditionOperandColumn:AlertConditionOperandColumn": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the column.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/AlertConditionThreshold:AlertConditionThreshold": {
            "properties": {
                "value": {
                    "$ref": "#/types/databricks:index/AlertConditionThresholdValue:AlertConditionThresholdValue",
                    "description": "actual value used in comparison (one of the attributes is required):\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/AlertConditionThresholdValue:AlertConditionThresholdValue": {
            "properties": {
                "boolValue": {
                    "type": "boolean",
                    "description": "boolean value (`true` or `false`) to compare against boolean results.\n"
                },
                "doubleValue": {
                    "type": "number",
                    "description": "double value to compare against integer and double results.\n"
                },
                "stringValue": {
                    "type": "string",
                    "description": "string value to compare against string results.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/AppActiveDeployment:AppActiveDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/AppActiveDeploymentDeploymentArtifacts:AppActiveDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/AppActiveDeploymentStatus:AppActiveDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "createTime",
                        "creator",
                        "deploymentArtifacts",
                        "status",
                        "updateTime"
                    ]
                }
            }
        },
        "databricks:index/AppActiveDeploymentDeploymentArtifacts:AppActiveDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/AppActiveDeploymentStatus:AppActiveDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "message",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/AppAppStatus:AppAppStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "message",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/AppComputeStatus:AppComputeStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "message",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/AppPendingDeployment:AppPendingDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/AppPendingDeploymentDeploymentArtifacts:AppPendingDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/AppPendingDeploymentStatus:AppPendingDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "createTime",
                        "creator",
                        "deploymentArtifacts",
                        "status",
                        "updateTime"
                    ]
                }
            }
        },
        "databricks:index/AppPendingDeploymentDeploymentArtifacts:AppPendingDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/AppPendingDeploymentStatus:AppPendingDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "message",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/AppResource:AppResource": {
            "properties": {
                "description": {
                    "type": "string",
                    "description": "The description of the resource.\n\nExactly one of the following attributes must be provided:\n"
                },
                "job": {
                    "$ref": "#/types/databricks:index/AppResourceJob:AppResourceJob",
                    "description": "attribute\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the resource.\n"
                },
                "secret": {
                    "$ref": "#/types/databricks:index/AppResourceSecret:AppResourceSecret",
                    "description": "attribute\n"
                },
                "servingEndpoint": {
                    "$ref": "#/types/databricks:index/AppResourceServingEndpoint:AppResourceServingEndpoint",
                    "description": "attribute\n"
                },
                "sqlWarehouse": {
                    "$ref": "#/types/databricks:index/AppResourceSqlWarehouse:AppResourceSqlWarehouse",
                    "description": "attribute\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/AppResourceJob:AppResourceJob": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ]
        },
        "databricks:index/AppResourceSecret:AppResourceSecret": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "Key of the secret to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permission to grant on the secret scope. For secrets, only one permission is allowed. Permission must be one of: `READ`, `WRITE`, `MANAGE`.\n"
                },
                "scope": {
                    "type": "string",
                    "description": "Scope of the secret to grant permission on.\n"
                }
            },
            "type": "object",
            "required": [
                "key",
                "permission",
                "scope"
            ]
        },
        "databricks:index/AppResourceServingEndpoint:AppResourceServingEndpoint": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the serving endpoint to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permission to grant on the serving endpoint. Supported permissions are: `CAN_MANAGE`, `CAN_QUERY`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "permission"
            ]
        },
        "databricks:index/AppResourceSqlWarehouse:AppResourceSqlWarehouse": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the SQL warehouse to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permission to grant on the SQL warehouse. Supported permissions are: `CAN_MANAGE`, `CAN_USE`, `IS_OWNER`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ]
        },
        "databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher": {
            "properties": {
                "artifact": {
                    "type": "string",
                    "description": "The artifact path or maven coordinate.\n"
                },
                "matchType": {
                    "type": "string",
                    "description": "The pattern matching type of the artifact. Only `PREFIX_MATCH` is supported.\n"
                }
            },
            "type": "object",
            "required": [
                "artifact",
                "matchType"
            ]
        },
        "databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace": {
            "properties": {
                "canToggle": {
                    "type": "boolean"
                },
                "enabled": {
                    "type": "boolean"
                },
                "enablementDetails": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails"
                },
                "maintenanceWindow": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow"
                },
                "restartEvenIfNoUpdatesAvailable": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "enabled",
                        "enablementDetails"
                    ]
                }
            }
        },
        "databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails": {
            "properties": {
                "forcedForComplianceMode": {
                    "type": "boolean"
                },
                "unavailableForDisabledEntitlement": {
                    "type": "boolean"
                },
                "unavailableForNonEnterpriseTier": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow": {
            "properties": {
                "weekDayBasedSchedule": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule"
                }
            },
            "type": "object"
        },
        "databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule": {
            "properties": {
                "dayOfWeek": {
                    "type": "string"
                },
                "frequency": {
                    "type": "string"
                },
                "windowStartTime": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime"
                }
            },
            "type": "object",
            "required": [
                "dayOfWeek",
                "frequency"
            ]
        },
        "databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime": {
            "properties": {
                "hours": {
                    "type": "integer"
                },
                "minutes": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "hours",
                "minutes"
            ]
        },
        "databricks:index/BudgetAlertConfiguration:BudgetAlertConfiguration": {
            "properties": {
                "actionConfigurations": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetAlertConfigurationActionConfiguration:BudgetAlertConfigurationActionConfiguration"
                    },
                    "description": "List of action configurations to take when the budget alert is triggered. Consists of the following fields:\n"
                },
                "alertConfigurationId": {
                    "type": "string"
                },
                "quantityThreshold": {
                    "type": "string",
                    "description": "The threshold for the budget alert to determine if it is in a triggered state. The number is evaluated based on `quantity_type`.\n"
                },
                "quantityType": {
                    "type": "string",
                    "description": "The way to calculate cost for this budget alert. This is what quantity_threshold is measured in. (Enum: `LIST_PRICE_DOLLARS_USD`)\n"
                },
                "timePeriod": {
                    "type": "string",
                    "description": "The time window of usage data for the budget. (Enum: `MONTH`)\n"
                },
                "triggerType": {
                    "type": "string",
                    "description": "The evaluation method to determine when this budget alert is in a triggered state. (Enum: `CUMULATIVE_SPENDING_EXCEEDED`)\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "alertConfigurationId"
                    ]
                }
            }
        },
        "databricks:index/BudgetAlertConfigurationActionConfiguration:BudgetAlertConfigurationActionConfiguration": {
            "properties": {
                "actionConfigurationId": {
                    "type": "string"
                },
                "actionType": {
                    "type": "string",
                    "description": "The type of action to take when the budget alert is triggered. (Enum: `EMAIL_NOTIFICATION`)\n"
                },
                "target": {
                    "type": "string",
                    "description": "The target of the action. For `EMAIL_NOTIFICATION`, this is the email address to send the notification to.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "actionConfigurationId"
                    ]
                }
            }
        },
        "databricks:index/BudgetFilter:BudgetFilter": {
            "properties": {
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetFilterTag:BudgetFilterTag"
                    },
                    "description": "List of tags to filter by. Consists of the following fields:\n"
                },
                "workspaceId": {
                    "$ref": "#/types/databricks:index/BudgetFilterWorkspaceId:BudgetFilterWorkspaceId",
                    "description": "Filter by workspace ID (if empty, include usage all usage for this account). Consists of the following fields:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/BudgetFilterTag:BudgetFilterTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "The key of the tag.\n"
                },
                "value": {
                    "$ref": "#/types/databricks:index/BudgetFilterTagValue:BudgetFilterTagValue",
                    "description": "Consists of the following fields:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/BudgetFilterTagValue:BudgetFilterTagValue": {
            "properties": {
                "operator": {
                    "type": "string",
                    "description": "The operator to use for the filter. (Enum: `IN`)\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The values to filter by.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/BudgetFilterWorkspaceId:BudgetFilterWorkspaceId": {
            "properties": {
                "operator": {
                    "type": "string",
                    "description": "The operator to use for the filter. (Enum: `IN`)\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "The values to filter by.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/BudgetPolicyCustomTag:BudgetPolicyCustomTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "The key of the tag. - Must be unique among all custom tags of the same policy. Cannot be “budget-policy-name”, “budget-policy-id” or \"budget-policy-resolution-result\" as these tags are preserved.\n"
                },
                "value": {
                    "type": "string",
                    "description": "The value of the tag.\n"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/ClusterAutoscale:ClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "description": "The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.\n\nWhen using a [Single Node cluster](https://docs.databricks.com/clusters/single-node.html), `num_workers` needs to be `0`. It can be set to `0` explicitly, or simply not specified, as it defaults to `0`.  When `num_workers` is `0`, provider checks for presence of the required Spark configurations:\n\n* `spark.master` must have prefix `local`, like `local[*]`\n* `spark.databricks.cluster.profile` must have value `singleNode`\n\nand also `custom_tag` entry:\n\n* `\"ResourceClass\" = \"SingleNode\"`\n\nThe following example demonstrates how to create an single node cluster:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst singleNode = new databricks.Cluster(\"single_node\", {\n    clusterName: \"Single Node\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.cluster.profile\": \"singleNode\",\n        \"spark.master\": \"local[*]\",\n    },\n    customTags: {\n        ResourceClass: \"SingleNode\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nsingle_node = databricks.Cluster(\"single_node\",\n    cluster_name=\"Single Node\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.cluster.profile\": \"singleNode\",\n        \"spark.master\": \"local[*]\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"SingleNode\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var singleNode = new Databricks.Cluster(\"single_node\", new()\n    {\n        ClusterName = \"Single Node\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.cluster.profile\", \"singleNode\" },\n            { \"spark.master\", \"local[*]\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"SingleNode\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"single_node\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Single Node\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.cluster.profile\": pulumi.String(\"singleNode\"),\n\t\t\t\t\"spark.master\":                     pulumi.String(\"local[*]\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\"ResourceClass\": pulumi.String(\"SingleNode\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var singleNode = new Cluster(\"singleNode\", ClusterArgs.builder()\n            .clusterName(\"Single Node\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.cluster.profile\", \"singleNode\"),\n                Map.entry(\"spark.master\", \"local[*]\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"SingleNode\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  singleNode:\n    type: databricks:Cluster\n    name: single_node\n    properties:\n      clusterName: Single Node\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.cluster.profile: singleNode\n        spark.master: local[*]\n      customTags:\n        ResourceClass: SingleNode\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "minWorkers": {
                    "type": "integer",
                    "description": "The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAwsAttributes:ClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Valid values are `SPOT`, `SPOT_WITH_FALLBACK` and `ON_DEMAND`. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster. Backend default value is `SPOT_WITH_FALLBACK` and could change in the future\n"
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "description": "The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.\n"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "description": "The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).\n"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string",
                    "description": "The type of EBS volumes that will be launched with this cluster. Valid values are `GENERAL_PURPOSE_SSD` or `THROUGHPUT_OPTIMIZED_HDD`. Use this option only if you're not picking *Delta Optimized `i3.*`* node types.\n"
                },
                "firstOnDemand": {
                    "type": "integer",
                    "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. If unspecified, the default value is 0.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks.InstanceProfile resource documentation for extended examples on adding a valid instance profile using Pulumi.\n"
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new `i3.xlarge` spot instance, then the max price is half of the price of on-demand `i3.xlarge` instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand `i3.xlarge` instances. If not specified, the default value is `100`. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than `10000`.\n"
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-west-2a`. The provided availability zone must be in the same region as the Databricks deployment. For example, `us-west-2a` is not a valid zone ID if the Databricks deployment resides in the `us-east-1` region. Enable automatic availability zone selection (\"Auto-AZ\"), by setting the value `auto`. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributes:ClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Valid values are `SPOT_AZURE`, `SPOT_WITH_FALLBACK_AZURE`, and `ON_DEMAND_AZURE`. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster.\n"
                },
                "firstOnDemand": {
                    "type": "integer",
                    "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.\n"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributesLogAnalyticsInfo:ClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to `-1`, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributesLogAnalyticsInfo:ClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConf:ClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfVolumes:ClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "description": "Set canned access control list, e.g. `bucket-owner-full-control`. If `canned_cal` is set, the cluster instance profile must have `s3:PutObjectAcl` permission on the destination bucket and prefix. The full list of possible canned ACLs can be found [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl). By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set `bucket-owner-full-control` to make bucket owners able to read the logs.\n"
                },
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                },
                "enableEncryption": {
                    "type": "boolean",
                    "description": "Enable server-side encryption, false by default.\n"
                },
                "encryptionType": {
                    "type": "string",
                    "description": "The encryption type, it could be `sse-s3` or `sse-kms`. It is used only when encryption is enabled, and the default type is `sse-s3`.\n"
                },
                "endpoint": {
                    "type": "string",
                    "description": "S3 endpoint, e.g. \u003chttps://s3-us-west-2.amazonaws.com\u003e. Either `region` or `endpoint` needs to be set. If both are set, the endpoint is used.\n"
                },
                "kmsKey": {
                    "type": "string",
                    "description": "KMS key used if encryption is enabled and encryption type is set to `sse-kms`.\n"
                },
                "region": {
                    "type": "string",
                    "description": "S3 region, e.g. `us-west-2`. Either `region` or `endpoint` must be set. If both are set, the endpoint is used.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterLogConfVolumes:ClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "description": "path inside the Spark container.\n\nFor example, you can mount Azure Data Lake Storage container using the following code:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst storageAccount = \"ewfw3ggwegwg\";\nconst storageContainer = \"test\";\nconst withNfs = new databricks.Cluster(\"with_nfs\", {clusterMountInfos: [{\n    networkFilesystemInfo: {\n        serverAddress: `${storageAccount}.blob.core.windows.net`,\n        mountOptions: \"sec=sys,vers=3,nolock,proto=tcp\",\n    },\n    remoteMountDirPath: `${storageAccount}/${storageContainer}`,\n    localMountDirPath: \"/mnt/nfs-test\",\n}]});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nstorage_account = \"ewfw3ggwegwg\"\nstorage_container = \"test\"\nwith_nfs = databricks.Cluster(\"with_nfs\", cluster_mount_infos=[{\n    \"network_filesystem_info\": {\n        \"server_address\": f\"{storage_account}.blob.core.windows.net\",\n        \"mount_options\": \"sec=sys,vers=3,nolock,proto=tcp\",\n    },\n    \"remote_mount_dir_path\": f\"{storage_account}/{storage_container}\",\n    \"local_mount_dir_path\": \"/mnt/nfs-test\",\n}])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var storageAccount = \"ewfw3ggwegwg\";\n\n    var storageContainer = \"test\";\n\n    var withNfs = new Databricks.Cluster(\"with_nfs\", new()\n    {\n        ClusterMountInfos = new[]\n        {\n            new Databricks.Inputs.ClusterClusterMountInfoArgs\n            {\n                NetworkFilesystemInfo = new Databricks.Inputs.ClusterClusterMountInfoNetworkFilesystemInfoArgs\n                {\n                    ServerAddress = $\"{storageAccount}.blob.core.windows.net\",\n                    MountOptions = \"sec=sys,vers=3,nolock,proto=tcp\",\n                },\n                RemoteMountDirPath = $\"{storageAccount}/{storageContainer}\",\n                LocalMountDirPath = \"/mnt/nfs-test\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tstorageAccount := \"ewfw3ggwegwg\"\n\t\tstorageContainer := \"test\"\n\t\t_, err := databricks.NewCluster(ctx, \"with_nfs\", \u0026databricks.ClusterArgs{\n\t\t\tClusterMountInfos: databricks.ClusterClusterMountInfoArray{\n\t\t\t\t\u0026databricks.ClusterClusterMountInfoArgs{\n\t\t\t\t\tNetworkFilesystemInfo: \u0026databricks.ClusterClusterMountInfoNetworkFilesystemInfoArgs{\n\t\t\t\t\t\tServerAddress: pulumi.Sprintf(\"%v.blob.core.windows.net\", storageAccount),\n\t\t\t\t\t\tMountOptions:  pulumi.String(\"sec=sys,vers=3,nolock,proto=tcp\"),\n\t\t\t\t\t},\n\t\t\t\t\tRemoteMountDirPath: pulumi.Sprintf(\"%v/%v\", storageAccount, storageContainer),\n\t\t\t\t\tLocalMountDirPath:  pulumi.String(\"/mnt/nfs-test\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterClusterMountInfoArgs;\nimport com.pulumi.databricks.inputs.ClusterClusterMountInfoNetworkFilesystemInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var storageAccount = \"ewfw3ggwegwg\";\n\n        final var storageContainer = \"test\";\n\n        var withNfs = new Cluster(\"withNfs\", ClusterArgs.builder()\n            .clusterMountInfos(ClusterClusterMountInfoArgs.builder()\n                .networkFilesystemInfo(ClusterClusterMountInfoNetworkFilesystemInfoArgs.builder()\n                    .serverAddress(String.format(\"%s.blob.core.windows.net\", storageAccount))\n                    .mountOptions(\"sec=sys,vers=3,nolock,proto=tcp\")\n                    .build())\n                .remoteMountDirPath(String.format(\"%s/%s\", storageAccount,storageContainer))\n                .localMountDirPath(\"/mnt/nfs-test\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  withNfs:\n    type: databricks:Cluster\n    name: with_nfs\n    properties:\n      clusterMountInfos:\n        - networkFilesystemInfo:\n            serverAddress: ${storageAccount}.blob.core.windows.net\n            mountOptions: sec=sys,vers=3,nolock,proto=tcp\n          remoteMountDirPath: ${storageAccount}/${storageContainer}\n          localMountDirPath: /mnt/nfs-test\nvariables:\n  storageAccount: ewfw3ggwegwg\n  storageContainer: test\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo",
                    "description": "block specifying connection. It consists of:\n"
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "description": "string specifying path to mount on the remote service.\n"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "description": "string that will be passed as options passed to the `mount` command.\n"
                },
                "serverAddress": {
                    "type": "string",
                    "description": "host name.\n"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/ClusterDockerImage:ClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth",
                    "description": "`basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.\n\nExample usage with azurerm_container_registry and docker_registry_image, that you can adapt to your specific use-case:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as docker from \"@pulumi/docker\";\n\nconst _this = new docker.index.RegistryImage(\"this\", {\n    build: [{}],\n    name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,\n});\nconst thisCluster = new databricks.Cluster(\"this\", {dockerImage: {\n    url: _this.name,\n    basicAuth: {\n        username: thisAzurermContainerRegistry.adminUsername,\n        password: thisAzurermContainerRegistry.adminPassword,\n    },\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_docker as docker\n\nthis = docker.index.RegistryImage(\"this\",\n    build=[{}],\n    name=f{this_azurerm_container_registry.login_server}/sample:latest)\nthis_cluster = databricks.Cluster(\"this\", docker_image={\n    \"url\": this[\"name\"],\n    \"basic_auth\": {\n        \"username\": this_azurerm_container_registry[\"adminUsername\"],\n        \"password\": this_azurerm_container_registry[\"adminPassword\"],\n    },\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Docker = Pulumi.Docker;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Docker.Index.RegistryImage(\"this\", new()\n    {\n        Build = new[]\n        {\n            null,\n        },\n        Name = $\"{thisAzurermContainerRegistry.LoginServer}/sample:latest\",\n    });\n\n    var thisCluster = new Databricks.Cluster(\"this\", new()\n    {\n        DockerImage = new Databricks.Inputs.ClusterDockerImageArgs\n        {\n            Url = @this.Name,\n            BasicAuth = new Databricks.Inputs.ClusterDockerImageBasicAuthArgs\n            {\n                Username = thisAzurermContainerRegistry.AdminUsername,\n                Password = thisAzurermContainerRegistry.AdminPassword,\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-docker/sdk/v4/go/docker\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := docker.NewRegistryImage(ctx, \"this\", \u0026docker.RegistryImageArgs{\n\t\t\tBuild: []map[string]interface{}{\n\t\t\t\tmap[string]interface{}{},\n\t\t\t},\n\t\t\tName: fmt.Sprintf(\"%v/sample:latest\", thisAzurermContainerRegistry.LoginServer),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tDockerImage: \u0026databricks.ClusterDockerImageArgs{\n\t\t\t\tUrl: this.Name,\n\t\t\t\tBasicAuth: \u0026databricks.ClusterDockerImageBasicAuthArgs{\n\t\t\t\t\tUsername: pulumi.Any(thisAzurermContainerRegistry.AdminUsername),\n\t\t\t\t\tPassword: pulumi.Any(thisAzurermContainerRegistry.AdminPassword),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.docker.registryImage;\nimport com.pulumi.docker.registryImageArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterDockerImageArgs;\nimport com.pulumi.databricks.inputs.ClusterDockerImageBasicAuthArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegistryImage(\"this\", RegistryImageArgs.builder()\n            .build(List.of(Map.ofEntries(\n            )))\n            .name(String.format(\"%s/sample:latest\", thisAzurermContainerRegistry.loginServer()))\n            .build());\n\n        var thisCluster = new Cluster(\"thisCluster\", ClusterArgs.builder()\n            .dockerImage(ClusterDockerImageArgs.builder()\n                .url(this_.name())\n                .basicAuth(ClusterDockerImageBasicAuthArgs.builder()\n                    .username(thisAzurermContainerRegistry.adminUsername())\n                    .password(thisAzurermContainerRegistry.adminPassword())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: docker:registryImage\n    properties:\n      build:\n        - {}\n      name: ${thisAzurermContainerRegistry.loginServer}/sample:latest\n  thisCluster:\n    type: databricks:Cluster\n    name: this\n    properties:\n      dockerImage:\n        url: ${this.name}\n        basicAuth:\n          username: ${thisAzurermContainerRegistry.adminUsername}\n          password: ${thisAzurermContainerRegistry.adminPassword}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL for the Docker image\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/ClusterGcpAttributes:ClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n"
                },
                "bootDiskSize": {
                    "type": "integer",
                    "description": "Boot disk size in GB\n"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.\n"
                },
                "localSsdCount": {
                    "type": "integer",
                    "description": "Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.\n"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "description": "if we should use preemptible executors ([GCP documentation](https://cloud.google.com/compute/docs/instances/preemptible)). *Warning: this field is deprecated in favor of `availability`, and will be removed soon.*\n"
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone in which the cluster resides. This can be one of the following:\n* `HA` (default): High availability, spread nodes across availability zones for a Databricks deployment region.\n* `AUTO`: Databricks picks an availability zone to schedule the cluster on.\n* name of a GCP availability zone: pick one of the available zones from the [list of available availability zones](https://cloud.google.com/compute/docs/regions-zones#available).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScript:ClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptFile:ClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptS3:ClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptVolumes:ClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptWorkspace:ClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptFile:ClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptS3:ClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string",
                    "description": "Set canned access control list, e.g. `bucket-owner-full-control`. If `canned_cal` is set, the cluster instance profile must have `s3:PutObjectAcl` permission on the destination bucket and prefix. The full list of possible canned ACLs can be found [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl). By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set `bucket-owner-full-control` to make bucket owners able to read the logs.\n"
                },
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                },
                "enableEncryption": {
                    "type": "boolean",
                    "description": "Enable server-side encryption, false by default.\n"
                },
                "encryptionType": {
                    "type": "string",
                    "description": "The encryption type, it could be `sse-s3` or `sse-kms`. It is used only when encryption is enabled, and the default type is `sse-s3`.\n"
                },
                "endpoint": {
                    "type": "string",
                    "description": "S3 endpoint, e.g. \u003chttps://s3-us-west-2.amazonaws.com\u003e. Either `region` or `endpoint` needs to be set. If both are set, the endpoint is used.\n"
                },
                "kmsKey": {
                    "type": "string",
                    "description": "KMS key used if encryption is enabled and encryption type is set to `sse-kms`.\n"
                },
                "region": {
                    "type": "string",
                    "description": "S3 region, e.g. `us-west-2`. Either `region` or `endpoint` must be set. If both are set, the endpoint is used.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptVolumes:ClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptWorkspace:ClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string",
                    "description": "S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.\n"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterLibrary:ClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterLibraryCran:ClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterLibraryMaven:ClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterLibraryPypi:ClusterLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterLibraryCran:ClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterLibraryMaven:ClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterLibraryPypi:ClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryCran:ClusterPolicyLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryMaven:ClusterPolicyLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryPypi:ClusterPolicyLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterPolicyLibraryCran:ClusterPolicyLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterPolicyLibraryMaven:ClusterPolicyLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterPolicyLibraryPypi:ClusterPolicyLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterWorkloadType:ClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "description": "boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: `true`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withNfs = new databricks.Cluster(\"with_nfs\", {workloadType: {\n    clients: {\n        jobs: false,\n        notebooks: true,\n    },\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_nfs = databricks.Cluster(\"with_nfs\", workload_type={\n    \"clients\": {\n        \"jobs\": False,\n        \"notebooks\": True,\n    },\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withNfs = new Databricks.Cluster(\"with_nfs\", new()\n    {\n        WorkloadType = new Databricks.Inputs.ClusterWorkloadTypeArgs\n        {\n            Clients = new Databricks.Inputs.ClusterWorkloadTypeClientsArgs\n            {\n                Jobs = false,\n                Notebooks = true,\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"with_nfs\", \u0026databricks.ClusterArgs{\n\t\t\tWorkloadType: \u0026databricks.ClusterWorkloadTypeArgs{\n\t\t\t\tClients: \u0026databricks.ClusterWorkloadTypeClientsArgs{\n\t\t\t\t\tJobs:      pulumi.Bool(false),\n\t\t\t\t\tNotebooks: pulumi.Bool(true),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterWorkloadTypeArgs;\nimport com.pulumi.databricks.inputs.ClusterWorkloadTypeClientsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var withNfs = new Cluster(\"withNfs\", ClusterArgs.builder()\n            .workloadType(ClusterWorkloadTypeArgs.builder()\n                .clients(ClusterWorkloadTypeClientsArgs.builder()\n                    .jobs(false)\n                    .notebooks(true)\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  withNfs:\n    type: databricks:Cluster\n    name: with_nfs\n    properties:\n      workloadType:\n        clients:\n          jobs: false\n          notebooks: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "notebooks": {
                    "type": "boolean",
                    "description": "boolean flag defining if it's possible to run notebooks on this cluster. Default: `true`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace:ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace": {
            "properties": {
                "complianceStandards": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "isEnabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "complianceStandards",
                "isEnabled"
            ]
        },
        "databricks:index/ConnectionProvisioningInfo:ConnectionProvisioningInfo": {
            "properties": {
                "state": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/CredentialAwsIamRole:CredentialAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role you want to use to setup the trust policy, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n\n`azure_managed_identity` optional configuration block for using managed identity as credential details for Azure (recommended over `azure_service_principal`):\n"
                },
                "unityCatalogIamArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "externalId",
                        "unityCatalogIamArn"
                    ]
                }
            }
        },
        "databricks:index/CredentialAzureManagedIdentity:CredentialAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.\n"
                },
                "credentialId": {
                    "type": "string",
                    "description": "Unique ID of the credential.\n"
                },
                "managedIdentityId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.\n\n`azure_service_principal` optional configuration block to use service principal as credential details for Azure. Only applicable when purpose is `STORAGE` (Legacy):\n"
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "accessConnectorId",
                        "credentialId"
                    ]
                }
            }
        },
        "databricks:index/CredentialAzureServicePrincipal:CredentialAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n"
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n\n`databricks_gcp_service_account` optional configuration block for creating a Databricks-managed GCP Service Account:\n",
                    "secret": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n"
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/CredentialDatabricksGcpServiceAccount:CredentialDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string",
                    "description": "Unique ID of the credential.\n"
                },
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n"
                },
                "privateKeyId": {
                    "type": "string"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "credentialId",
                        "email",
                        "privateKeyId"
                    ]
                }
            }
        },
        "databricks:index/CustomAppIntegrationTokenAccessPolicy:CustomAppIntegrationTokenAccessPolicy": {
            "properties": {
                "accessTokenTtlInMinutes": {
                    "type": "integer",
                    "description": "access token time to live (TTL) in minutes.\n"
                },
                "refreshTokenTtlInMinutes": {
                    "type": "integer",
                    "description": "refresh token TTL in minutes. The TTL of refresh token cannot be lower than TTL of access token.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The value for the setting.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/DisableLegacyAccessSettingDisableLegacyAccess:DisableLegacyAccessSettingDisableLegacyAccess": {
            "properties": {
                "value": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace:EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace": {
            "properties": {
                "isEnabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "isEnabled"
            ]
        },
        "databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetailsSseEncryptionDetails:ExternalLocationEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/ExternalLocationEncryptionDetailsSseEncryptionDetails:ExternalLocationEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/GrantsGrant:GrantsGrant": {
            "properties": {
                "principal": {
                    "type": "string"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "(String) Availability type used for all instances in the pool. Only `ON_DEMAND` and `SPOT` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "(Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*\n",
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "(String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `\"us-west-2a\"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `\"us-west-2a\"` is not a valid zone ID if the Databricks deployment resides in the `\"us-east-1\"` region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `SPOT_AZURE` and `ON_DEMAND_AZURE`.\n",
                    "willReplaceOnChanges": true
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to `-1`, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer",
                    "description": "(Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.\n"
                },
                "diskSize": {
                    "type": "integer",
                    "description": "(Integer) The size of each disk (in GiB) to attach.\n"
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType"
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "ebsVolumeType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "willReplaceOnChanges": true
                },
                "localSsdCount": {
                    "type": "integer",
                    "description": "Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.\n",
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-central1-a`. The provided availability zone must be in the same region as the Databricks workspace.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "localSsdCount",
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption",
                    "willReplaceOnChanges": true
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption",
                    "willReplaceOnChanges": true
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride"
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instanceType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth",
                    "description": "`basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.\n\nExample usage with azurerm_container_registry and docker_registry_image, that you can adapt to your specific use-case:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as docker from \"@pulumi/docker\";\n\nconst _this = new docker.index.RegistryImage(\"this\", {\n    build: [{}],\n    name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,\n});\nconst thisInstancePool = new databricks.InstancePool(\"this\", {preloadedDockerImages: [{\n    url: _this.name,\n    basicAuth: {\n        username: thisAzurermContainerRegistry.adminUsername,\n        password: thisAzurermContainerRegistry.adminPassword,\n    },\n}]});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_docker as docker\n\nthis = docker.index.RegistryImage(\"this\",\n    build=[{}],\n    name=f{this_azurerm_container_registry.login_server}/sample:latest)\nthis_instance_pool = databricks.InstancePool(\"this\", preloaded_docker_images=[{\n    \"url\": this[\"name\"],\n    \"basic_auth\": {\n        \"username\": this_azurerm_container_registry[\"adminUsername\"],\n        \"password\": this_azurerm_container_registry[\"adminPassword\"],\n    },\n}])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Docker = Pulumi.Docker;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Docker.Index.RegistryImage(\"this\", new()\n    {\n        Build = new[]\n        {\n            null,\n        },\n        Name = $\"{thisAzurermContainerRegistry.LoginServer}/sample:latest\",\n    });\n\n    var thisInstancePool = new Databricks.InstancePool(\"this\", new()\n    {\n        PreloadedDockerImages = new[]\n        {\n            new Databricks.Inputs.InstancePoolPreloadedDockerImageArgs\n            {\n                Url = @this.Name,\n                BasicAuth = new Databricks.Inputs.InstancePoolPreloadedDockerImageBasicAuthArgs\n                {\n                    Username = thisAzurermContainerRegistry.AdminUsername,\n                    Password = thisAzurermContainerRegistry.AdminPassword,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-docker/sdk/v4/go/docker\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := docker.NewRegistryImage(ctx, \"this\", \u0026docker.RegistryImageArgs{\n\t\t\tBuild: []map[string]interface{}{\n\t\t\t\tmap[string]interface{}{},\n\t\t\t},\n\t\t\tName: fmt.Sprintf(\"%v/sample:latest\", thisAzurermContainerRegistry.LoginServer),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstancePool(ctx, \"this\", \u0026databricks.InstancePoolArgs{\n\t\t\tPreloadedDockerImages: databricks.InstancePoolPreloadedDockerImageArray{\n\t\t\t\t\u0026databricks.InstancePoolPreloadedDockerImageArgs{\n\t\t\t\t\tUrl: this.Name,\n\t\t\t\t\tBasicAuth: \u0026databricks.InstancePoolPreloadedDockerImageBasicAuthArgs{\n\t\t\t\t\t\tUsername: pulumi.Any(thisAzurermContainerRegistry.AdminUsername),\n\t\t\t\t\t\tPassword: pulumi.Any(thisAzurermContainerRegistry.AdminPassword),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.docker.registryImage;\nimport com.pulumi.docker.registryImageArgs;\nimport com.pulumi.databricks.InstancePool;\nimport com.pulumi.databricks.InstancePoolArgs;\nimport com.pulumi.databricks.inputs.InstancePoolPreloadedDockerImageArgs;\nimport com.pulumi.databricks.inputs.InstancePoolPreloadedDockerImageBasicAuthArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegistryImage(\"this\", RegistryImageArgs.builder()\n            .build(List.of(Map.ofEntries(\n            )))\n            .name(String.format(\"%s/sample:latest\", thisAzurermContainerRegistry.loginServer()))\n            .build());\n\n        var thisInstancePool = new InstancePool(\"thisInstancePool\", InstancePoolArgs.builder()\n            .preloadedDockerImages(InstancePoolPreloadedDockerImageArgs.builder()\n                .url(this_.name())\n                .basicAuth(InstancePoolPreloadedDockerImageBasicAuthArgs.builder()\n                    .username(thisAzurermContainerRegistry.adminUsername())\n                    .password(thisAzurermContainerRegistry.adminPassword())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: docker:registryImage\n    properties:\n      build:\n        - {}\n      name: ${thisAzurermContainerRegistry.loginServer}/sample:latest\n  thisInstancePool:\n    type: databricks:InstancePool\n    name: this\n    properties:\n      preloadedDockerImages:\n        - url: ${this.name}\n          basicAuth:\n            username: ${thisAzurermContainerRegistry.adminUsername}\n            password: ${thisAzurermContainerRegistry.adminPassword}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
                    "willReplaceOnChanges": true
                },
                "url": {
                    "type": "string",
                    "description": "URL for the Docker image\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "username": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobContinuous:JobContinuous": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this continuous job is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobDbtTask:JobDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobDeployment:JobDeployment": {
            "properties": {
                "kind": {
                    "type": "string"
                },
                "metadataFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "kind"
            ]
        },
        "databricks:index/JobEmailNotifications:JobEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notification_settings` configuration block).\n"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nThe following parameter is only available for the job level configuration.\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run fails.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run starts.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run completes successfully.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobEnvironment:JobEnvironment": {
            "properties": {
                "environmentKey": {
                    "type": "string",
                    "description": "an unique identifier of the Environment.  It will be referenced from `environment_key` attribute of corresponding task.\n"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/JobEnvironmentSpec:JobEnvironmentSpec",
                    "description": "block describing the Environment. Consists of following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "environmentKey"
            ]
        },
        "databricks:index/JobEnvironmentSpec:JobEnvironmentSpec": {
            "properties": {
                "client": {
                    "type": "string",
                    "description": "client version used by the environment.\n"
                },
                "dependencies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See [API docs](https://docs.databricks.com/api/workspace/jobs/create#environments-spec-dependencies) for more information.\n\n"
                }
            },
            "type": "object",
            "required": [
                "client"
            ]
        },
        "databricks:index/JobGitSource:JobGitSource": {
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `tag` and `commit`.\n"
                },
                "commit": {
                    "type": "string",
                    "description": "hash of Git commit to use. Conflicts with `branch` and `tag`.\n"
                },
                "gitSnapshot": {
                    "$ref": "#/types/databricks:index/JobGitSourceGitSnapshot:JobGitSourceGitSnapshot"
                },
                "jobSource": {
                    "$ref": "#/types/databricks:index/JobGitSourceJobSource:JobGitSourceJobSource"
                },
                "provider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.\n"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `branch` and `commit`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobGitSourceGitSnapshot:JobGitSourceGitSnapshot": {
            "properties": {
                "usedCommit": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobGitSourceJobSource:JobGitSourceJobSource": {
            "properties": {
                "dirtyState": {
                    "type": "string"
                },
                "importFromGitBranch": {
                    "type": "string"
                },
                "jobConfigPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "importFromGitBranch",
                "jobConfigPath"
            ]
        },
        "databricks:index/JobHealth:JobHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobHealthRule:JobHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobHealthRule:JobHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/JobJobCluster:JobJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster",
                    "description": "Block with almost the same set of parameters as for databricks.Cluster resource, except following (check the [REST API documentation for full list of supported parameters](https://docs.databricks.com/api/workspace/jobs/create#job_clusters-new_cluster)):\n"
                }
            },
            "type": "object",
            "required": [
                "jobClusterKey",
                "newCluster"
            ]
        },
        "databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "kind": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterLibrary:JobJobClusterNewClusterLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType",
                    "description": "isn't supported\n"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo:JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo:JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfVolumes:JobJobClusterNewClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfVolumes:JobJobClusterNewClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile",
                    "description": "block consisting of single string fields:\n"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptVolumes:JobJobClusterNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptWorkspace:JobJobClusterNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptVolumes:JobJobClusterNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptWorkspace:JobJobClusterNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterLibrary:JobJobClusterNewClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterLibraryCran:JobJobClusterNewClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterLibraryMaven:JobJobClusterNewClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterLibraryPypi:JobJobClusterNewClusterLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterLibraryCran:JobJobClusterNewClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobJobClusterNewClusterLibraryMaven:JobJobClusterNewClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobJobClusterNewClusterLibraryPypi:JobJobClusterNewClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibrary:JobLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobLibraryCran:JobLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobLibraryMaven:JobLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobLibraryPypi:JobLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibraryCran:JobLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobLibraryMaven:JobLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobLibraryPypi:JobLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewCluster:JobNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterInitScript:JobNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "kind": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterLibrary:JobNewClusterLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType",
                    "description": "isn't supported\n"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/JobNewClusterAzureAttributesLogAnalyticsInfo:JobNewClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAzureAttributesLogAnalyticsInfo:JobNewClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfVolumes:JobNewClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterLogConfVolumes:JobNewClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScript:JobNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile",
                    "description": "block consisting of single string fields:\n"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptVolumes:JobNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptWorkspace:JobNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptVolumes:JobNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptWorkspace:JobNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterLibrary:JobNewClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobNewClusterLibraryCran:JobNewClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobNewClusterLibraryMaven:JobNewClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobNewClusterLibraryPypi:JobNewClusterLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterLibraryCran:JobNewClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewClusterLibraryMaven:JobNewClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobNewClusterLibraryPypi:JobNewClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNotebookTask:JobNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobNotificationSettings:JobNotificationSettings": {
            "properties": {
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n\nThe following parameter is only available on task level.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobParameter:JobParameter": {
            "properties": {
                "default": {
                    "type": "string",
                    "description": "Default value of the parameter.\n\n*You can use this block only together with `task` blocks, not with the legacy tasks specification!*\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the defined parameter. May only contain alphanumeric characters, `_`, `-`, and `.`.\n"
                }
            },
            "type": "object",
            "required": [
                "default",
                "name"
            ]
        },
        "databricks:index/JobPipelineTask:JobPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobPythonWheelTask:JobPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobQueue:JobQueue": {
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "If true, enable queueing for the job.\n"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/JobRunAs:JobRunAs": {
            "properties": {
                "servicePrincipalName": {
                    "type": "string",
                    "description": "The application ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.\n\nExample:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Job(\"this\", {runAs: {\n    servicePrincipalName: \"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Job(\"this\", run_as={\n    \"service_principal_name\": \"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Job(\"this\", new()\n    {\n        RunAs = new Databricks.Inputs.JobRunAsArgs\n        {\n            ServicePrincipalName = \"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tRunAs: \u0026databricks.JobRunAsArgs{\n\t\t\t\tServicePrincipalName: pulumi.String(\"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobRunAsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Job(\"this\", JobArgs.builder()\n            .runAs(JobRunAsArgs.builder()\n                .servicePrincipalName(\"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Job\n    properties:\n      runAs:\n        servicePrincipalName: 8d23ae77-912e-4a19-81e4-b9c3f5cc9349\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobRunJobTask:JobRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Job parameters for the task\n"
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobSchedule:JobSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this schedule is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted and a schedule is provided, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.\n"
                },
                "timezoneId": {
                    "type": "string",
                    "description": "A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.\n"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ]
        },
        "databricks:index/JobSparkJarTask:JobSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobSparkPythonTask:JobSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobSparkSubmitTask:JobSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTask:JobTask": {
            "properties": {
                "cleanRoomsNotebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskCleanRoomsNotebookTask:JobTaskCleanRoomsNotebookTask"
                },
                "conditionTask": {
                    "$ref": "#/types/databricks:index/JobTaskConditionTask:JobTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskDbtTask:JobTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskDependsOn:JobTaskDependsOn"
                    },
                    "description": "block specifying dependency(-ies) for a given task.\n"
                },
                "description": {
                    "type": "string",
                    "description": "description for this task.\n"
                },
                "disableAutoOptimization": {
                    "type": "boolean",
                    "description": "A flag to disable auto optimization in serverless tasks.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications",
                    "description": "An optional block to specify a set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This block is documented below.\n"
                },
                "environmentKey": {
                    "type": "string",
                    "description": "identifier of an `environment` block that is used to specify libraries.  Required for some tasks (`spark_python_task`, `python_wheel_task`, ...) running on serverless compute.\n"
                },
                "existingClusterId": {
                    "type": "string",
                    "description": "Identifier of the interactive cluster to run job on.  *Note: running tasks on interactive clusters may lead to increased costs!*\n"
                },
                "forEachTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTask:JobTaskForEachTask"
                },
                "genAiComputeTask": {
                    "$ref": "#/types/databricks:index/JobTaskGenAiComputeTask:JobTaskGenAiComputeTask"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobTaskHealth:JobTaskHealth",
                    "description": "block described below that specifies health conditions for a given task.\n"
                },
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier of the Job cluster specified in the `job_cluster` block.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskLibrary:JobTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskNewCluster:JobTaskNewCluster",
                    "description": "Task will run on a dedicated cluster.  See databricks.Cluster documentation for specification. *Some parameters, such as `autotermination_minutes`, `is_pinned`, `workload_type` aren't supported!*\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskNotebookTask:JobTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobTaskNotificationSettings:JobTaskNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level documented below.\n"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskPipelineTask:JobTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "runIf": {
                    "type": "string",
                    "description": "An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of `ALL_SUCCESS`, `AT_LEAST_ONE_SUCCESS`, `NONE_FAILED`, `ALL_DONE`, `AT_LEAST_ONE_FAILED` or `ALL_FAILED`. When omitted, defaults to `ALL_SUCCESS`.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobTaskRunJobTask:JobTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTask:JobTaskSqlTask"
                },
                "taskKey": {
                    "type": "string",
                    "description": "string specifying an unique key for a given task.\n* `*_task` - (Required) one of the specific task blocks described below:\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskWebhookNotifications:JobTaskWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n\n\u003e If no `job_cluster_key`, `existing_cluster_id`, or `new_cluster` were specified in task definition, then task will executed using serverless compute.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout",
                        "taskKey"
                    ]
                }
            }
        },
        "databricks:index/JobTaskCleanRoomsNotebookTask:JobTaskCleanRoomsNotebookTask": {
            "properties": {
                "cleanRoomName": {
                    "type": "string",
                    "description": "The clean room that the notebook belongs to.\n"
                },
                "etag": {
                    "type": "string",
                    "description": "Checksum to validate the freshness of the notebook resource.\n"
                },
                "notebookBaseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Base parameters to be used for the clean room notebook job.\n"
                },
                "notebookName": {
                    "type": "string",
                    "description": "Name of the notebook being run.\n"
                }
            },
            "type": "object",
            "required": [
                "cleanRoomName",
                "notebookName"
            ]
        },
        "databricks:index/JobTaskConditionTask:JobTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string",
                    "description": "The left operand of the condition task. It could be a string value, job state, or a parameter reference.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n\nThis task does not require a cluster to execute and does not support retries or notifications.\n"
                },
                "right": {
                    "type": "string",
                    "description": "The right operand of the condition task. It could be a string value, job state, or parameter reference.\n"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/JobTaskDbtTask:JobTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskDependsOn:JobTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string",
                    "description": "Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `\"true\"` or `\"false\"`.\n\n\u003e Similar to the tasks themselves, each dependency inside the task need to be declared in alphabetical order with respect to task_key in order to get consistent Pulumi diffs.\n"
                },
                "taskKey": {
                    "type": "string",
                    "description": "The name of the task this task depends on.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notification_settings` configuration block).\n"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nThe following parameter is only available for the job level configuration.\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run fails.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run starts.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run completes successfully.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTask:JobTaskForEachTask": {
            "properties": {
                "concurrency": {
                    "type": "integer",
                    "description": "Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.\n"
                },
                "inputs": {
                    "type": "string",
                    "description": "(String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.\n"
                },
                "task": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTask:JobTaskForEachTaskTask",
                    "description": "Task to run against the `inputs` list.\n"
                }
            },
            "type": "object",
            "required": [
                "inputs",
                "task"
            ]
        },
        "databricks:index/JobTaskForEachTaskTask:JobTaskForEachTaskTask": {
            "properties": {
                "cleanRoomsNotebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskCleanRoomsNotebookTask:JobTaskForEachTaskTaskCleanRoomsNotebookTask"
                },
                "conditionTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskConditionTask:JobTaskForEachTaskTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskDbtTask:JobTaskForEachTaskTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskDependsOn:JobTaskForEachTaskTaskDependsOn"
                    },
                    "description": "block specifying dependency(-ies) for a given task.\n"
                },
                "description": {
                    "type": "string",
                    "description": "description for this task.\n"
                },
                "disableAutoOptimization": {
                    "type": "boolean",
                    "description": "A flag to disable auto optimization in serverless tasks.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskEmailNotifications:JobTaskForEachTaskTaskEmailNotifications",
                    "description": "An optional block to specify a set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This block is documented below.\n"
                },
                "environmentKey": {
                    "type": "string",
                    "description": "identifier of an `environment` block that is used to specify libraries.  Required for some tasks (`spark_python_task`, `python_wheel_task`, ...) running on serverless compute.\n"
                },
                "existingClusterId": {
                    "type": "string",
                    "description": "Identifier of the interactive cluster to run job on.  *Note: running tasks on interactive clusters may lead to increased costs!*\n"
                },
                "genAiComputeTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskGenAiComputeTask:JobTaskForEachTaskTaskGenAiComputeTask"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskHealth:JobTaskForEachTaskTaskHealth",
                    "description": "block described below that specifies health conditions for a given task.\n"
                },
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier of the Job cluster specified in the `job_cluster` block.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibrary:JobTaskForEachTaskTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewCluster:JobTaskForEachTaskTaskNewCluster",
                    "description": "Task will run on a dedicated cluster.  See databricks.Cluster documentation for specification. *Some parameters, such as `autotermination_minutes`, `is_pinned`, `workload_type` aren't supported!*\n"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNotebookTask:JobTaskForEachTaskTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNotificationSettings:JobTaskForEachTaskTaskNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level documented below.\n"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskPipelineTask:JobTaskForEachTaskTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskPythonWheelTask:JobTaskForEachTaskTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "runIf": {
                    "type": "string",
                    "description": "An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of `ALL_SUCCESS`, `AT_LEAST_ONE_SUCCESS`, `NONE_FAILED`, `ALL_DONE`, `AT_LEAST_ONE_FAILED` or `ALL_FAILED`. When omitted, defaults to `ALL_SUCCESS`.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskRunJobTask:JobTaskForEachTaskTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkJarTask:JobTaskForEachTaskTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkPythonTask:JobTaskForEachTaskTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkSubmitTask:JobTaskForEachTaskTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTask:JobTaskForEachTaskTaskSqlTask"
                },
                "taskKey": {
                    "type": "string",
                    "description": "string specifying an unique key for a given task.\n* `*_task` - (Required) one of the specific task blocks described below:\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotifications:JobTaskForEachTaskTaskWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n\n\u003e If no `job_cluster_key`, `existing_cluster_id`, or `new_cluster` were specified in task definition, then task will executed using serverless compute.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout",
                        "taskKey"
                    ]
                }
            }
        },
        "databricks:index/JobTaskForEachTaskTaskCleanRoomsNotebookTask:JobTaskForEachTaskTaskCleanRoomsNotebookTask": {
            "properties": {
                "cleanRoomName": {
                    "type": "string",
                    "description": "The clean room that the notebook belongs to.\n"
                },
                "etag": {
                    "type": "string",
                    "description": "Checksum to validate the freshness of the notebook resource.\n"
                },
                "notebookBaseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Base parameters to be used for the clean room notebook job.\n"
                },
                "notebookName": {
                    "type": "string",
                    "description": "Name of the notebook being run.\n"
                }
            },
            "type": "object",
            "required": [
                "cleanRoomName",
                "notebookName"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskConditionTask:JobTaskForEachTaskTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string",
                    "description": "The left operand of the condition task. It could be a string value, job state, or a parameter reference.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n\nThis task does not require a cluster to execute and does not support retries or notifications.\n"
                },
                "right": {
                    "type": "string",
                    "description": "The right operand of the condition task. It could be a string value, job state, or parameter reference.\n"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskDbtTask:JobTaskForEachTaskTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskDependsOn:JobTaskForEachTaskTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string",
                    "description": "Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `\"true\"` or `\"false\"`.\n\n\u003e Similar to the tasks themselves, each dependency inside the task need to be declared in alphabetical order with respect to task_key in order to get consistent Pulumi diffs.\n"
                },
                "taskKey": {
                    "type": "string",
                    "description": "The name of the task this task depends on.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskEmailNotifications:JobTaskForEachTaskTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notification_settings` configuration block).\n"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nThe following parameter is only available for the job level configuration.\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run fails.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run starts.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) list of emails to notify when the run completes successfully.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskGenAiComputeTask:JobTaskForEachTaskTaskGenAiComputeTask": {
            "properties": {
                "command": {
                    "type": "string"
                },
                "compute": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskGenAiComputeTaskCompute:JobTaskForEachTaskTaskGenAiComputeTaskCompute"
                },
                "dlRuntimeImage": {
                    "type": "string"
                },
                "mlflowExperimentName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "trainingScriptPath": {
                    "type": "string"
                },
                "yamlParameters": {
                    "type": "string"
                },
                "yamlParametersFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "dlRuntimeImage"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskGenAiComputeTaskCompute:JobTaskForEachTaskTaskGenAiComputeTaskCompute": {
            "properties": {
                "gpuNodePoolId": {
                    "type": "string"
                },
                "gpuType": {
                    "type": "string"
                },
                "numGpus": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "gpuNodePoolId",
                "numGpus"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskHealth:JobTaskForEachTaskTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskHealthRule:JobTaskForEachTaskTaskHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskHealthRule:JobTaskForEachTaskTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskLibrary:JobTaskForEachTaskTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryCran:JobTaskForEachTaskTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryMaven:JobTaskForEachTaskTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryPypi:JobTaskForEachTaskTaskLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryCran:JobTaskForEachTaskTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryMaven:JobTaskForEachTaskTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryPypi:JobTaskForEachTaskTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewCluster:JobTaskForEachTaskTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAutoscale:JobTaskForEachTaskTaskNewClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAwsAttributes:JobTaskForEachTaskTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributes:JobTaskForEachTaskTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConf:JobTaskForEachTaskTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterDockerImage:JobTaskForEachTaskTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterGcpAttributes:JobTaskForEachTaskTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScript:JobTaskForEachTaskTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "kind": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterLibrary:JobTaskForEachTaskTaskNewClusterLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadType:JobTaskForEachTaskTaskNewClusterWorkloadType",
                    "description": "isn't supported\n"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAutoscale:JobTaskForEachTaskTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAwsAttributes:JobTaskForEachTaskTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributes:JobTaskForEachTaskTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo:JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo:JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConf:JobTaskForEachTaskTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs:JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfS3:JobTaskForEachTaskTaskNewClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes:JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs:JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfS3:JobTaskForEachTaskTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes:JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterDockerImage:JobTaskForEachTaskTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth:JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth:JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterGcpAttributes:JobTaskForEachTaskTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScript:JobTaskForEachTaskTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptAbfss:JobTaskForEachTaskTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptDbfs:JobTaskForEachTaskTaskNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptFile:JobTaskForEachTaskTaskNewClusterInitScriptFile",
                    "description": "block consisting of single string fields:\n"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptGcs:JobTaskForEachTaskTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptS3:JobTaskForEachTaskTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptVolumes:JobTaskForEachTaskTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptWorkspace:JobTaskForEachTaskTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptAbfss:JobTaskForEachTaskTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptDbfs:JobTaskForEachTaskTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptFile:JobTaskForEachTaskTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptGcs:JobTaskForEachTaskTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptS3:JobTaskForEachTaskTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptVolumes:JobTaskForEachTaskTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptWorkspace:JobTaskForEachTaskTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterLibrary:JobTaskForEachTaskTaskNewClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterLibraryCran:JobTaskForEachTaskTaskNewClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterLibraryMaven:JobTaskForEachTaskTaskNewClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterLibraryPypi:JobTaskForEachTaskTaskNewClusterLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterLibraryCran:JobTaskForEachTaskTaskNewClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterLibraryMaven:JobTaskForEachTaskTaskNewClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterLibraryPypi:JobTaskForEachTaskTaskNewClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadType:JobTaskForEachTaskTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadTypeClients:JobTaskForEachTaskTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadTypeClients:JobTaskForEachTaskTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNotebookTask:JobTaskForEachTaskTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNotificationSettings:JobTaskForEachTaskTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "description": "(Bool) do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n\nThe following parameter is only available on task level.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskPipelineTask:JobTaskForEachTaskTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskPythonWheelTask:JobTaskForEachTaskTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskRunJobTask:JobTaskForEachTaskTaskRunJobTask": {
            "properties": {
                "dbtCommands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "jarParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Job parameters for the task\n"
                },
                "notebookParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "pipelineParams": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskRunJobTaskPipelineParams:JobTaskForEachTaskTaskRunJobTaskPipelineParams"
                },
                "pythonNamedParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "pythonParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "sparkSubmitParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "sqlParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskRunJobTaskPipelineParams:JobTaskForEachTaskTaskRunJobTaskPipelineParams": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e The following configuration blocks are only supported inside a `task` block\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSparkJarTask:JobTaskForEachTaskTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                },
                "runAsRepl": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSparkPythonTask:JobTaskForEachTaskTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSparkSubmitTask:JobTaskForEachTaskTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTask:JobTaskForEachTaskTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskAlert:JobTaskForEachTaskTaskSqlTaskAlert",
                    "description": "block consisting of following fields:\n"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskDashboard:JobTaskForEachTaskTaskSqlTaskDashboard",
                    "description": "block consisting of following fields:\n"
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskFile:JobTaskForEachTaskTaskSqlTaskFile",
                    "description": "block consisting of single string fields:\n"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.\n"
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskQuery:JobTaskForEachTaskTaskSqlTaskQuery",
                    "description": "block consisting of single string field: `query_id` - identifier of the Databricks Query (databricks_query).\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless \u0026 Pro warehouses are supported right now.\n"
                }
            },
            "type": "object",
            "required": [
                "warehouseId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskAlert:JobTaskForEachTaskTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks Alert (databricks_alert).\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskAlertSubscription:JobTaskForEachTaskTaskSqlTaskAlertSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskAlertSubscription:JobTaskForEachTaskTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskDashboard:JobTaskForEachTaskTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string",
                    "description": "string specifying a custom subject of email sent.\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskDashboardSubscription:JobTaskForEachTaskTaskSqlTaskDashboardSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskDashboardSubscription:JobTaskForEachTaskTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskFile:JobTaskForEachTaskTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string",
                    "description": "If `source` is `GIT`: Relative path to the file in the repository specified in the `git_source` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.\n\nExample\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlAggregationJob = new databricks.Job(\"sql_aggregation_job\", {\n    name: \"Example SQL Job\",\n    tasks: [\n        {\n            taskKey: \"run_agg_query\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                query: {\n                    queryId: aggQuery.id,\n                },\n            },\n        },\n        {\n            taskKey: \"run_dashboard\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                dashboard: {\n                    dashboardId: dash.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            taskKey: \"run_alert\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                alert: {\n                    alertId: alert.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsql_aggregation_job = databricks.Job(\"sql_aggregation_job\",\n    name=\"Example SQL Job\",\n    tasks=[\n        {\n            \"task_key\": \"run_agg_query\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"query\": {\n                    \"query_id\": agg_query[\"id\"],\n                },\n            },\n        },\n        {\n            \"task_key\": \"run_dashboard\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"dashboard\": {\n                    \"dashboard_id\": dash[\"id\"],\n                    \"subscriptions\": [{\n                        \"user_name\": \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            \"task_key\": \"run_alert\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"alert\": {\n                    \"alert_id\": alert[\"id\"],\n                    \"subscriptions\": [{\n                        \"user_name\": \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlAggregationJob = new Databricks.Job(\"sql_aggregation_job\", new()\n    {\n        Name = \"Example SQL Job\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_agg_query\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Query = new Databricks.Inputs.JobTaskSqlTaskQueryArgs\n                    {\n                        QueryId = aggQuery.Id,\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_dashboard\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Dashboard = new Databricks.Inputs.JobTaskSqlTaskDashboardArgs\n                    {\n                        DashboardId = dash.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskDashboardSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_alert\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Alert = new Databricks.Inputs.JobTaskSqlTaskAlertArgs\n                    {\n                        AlertId = alert.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskAlertSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"sql_aggregation_job\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.String(\"Example SQL Job\"),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_agg_query\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tQuery: \u0026databricks.JobTaskSqlTaskQueryArgs{\n\t\t\t\t\t\t\tQueryId: pulumi.Any(aggQuery.Id),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_dashboard\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tDashboard: \u0026databricks.JobTaskSqlTaskDashboardArgs{\n\t\t\t\t\t\t\tDashboardId: pulumi.Any(dash.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskDashboardSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskDashboardSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_alert\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tAlert: \u0026databricks.JobTaskSqlTaskAlertArgs{\n\t\t\t\t\t\t\tAlertId: pulumi.Any(alert.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskAlertSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskAlertSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskQueryArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskDashboardArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskAlertArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sqlAggregationJob = new Job(\"sqlAggregationJob\", JobArgs.builder()\n            .name(\"Example SQL Job\")\n            .tasks(            \n                JobTaskArgs.builder()\n                    .taskKey(\"run_agg_query\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .query(JobTaskSqlTaskQueryArgs.builder()\n                            .queryId(aggQuery.id())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_dashboard\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .dashboard(JobTaskSqlTaskDashboardArgs.builder()\n                            .dashboardId(dash.id())\n                            .subscriptions(JobTaskSqlTaskDashboardSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_alert\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .alert(JobTaskSqlTaskAlertArgs.builder()\n                            .alertId(alert.id())\n                            .subscriptions(JobTaskSqlTaskAlertSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sqlAggregationJob:\n    type: databricks:Job\n    name: sql_aggregation_job\n    properties:\n      name: Example SQL Job\n      tasks:\n        - taskKey: run_agg_query\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            query:\n              queryId: ${aggQuery.id}\n        - taskKey: run_dashboard\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            dashboard:\n              dashboardId: ${dash.id}\n              subscriptions:\n                - userName: user@domain.com\n        - taskKey: run_alert\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            alert:\n              alertId: ${alert.id}\n              subscriptions:\n                - userName: user@domain.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.\n"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskQuery:JobTaskForEachTaskTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotifications:JobTaskForEachTaskTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnFailure:JobTaskForEachTaskTaskWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStart:JobTaskForEachTaskTaskWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnSuccess:JobTaskForEachTaskTaskWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnFailure:JobTaskForEachTaskTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStart:JobTaskForEachTaskTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnSuccess:JobTaskForEachTaskTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskGenAiComputeTask:JobTaskGenAiComputeTask": {
            "properties": {
                "command": {
                    "type": "string"
                },
                "compute": {
                    "$ref": "#/types/databricks:index/JobTaskGenAiComputeTaskCompute:JobTaskGenAiComputeTaskCompute"
                },
                "dlRuntimeImage": {
                    "type": "string"
                },
                "mlflowExperimentName": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "trainingScriptPath": {
                    "type": "string"
                },
                "yamlParameters": {
                    "type": "string"
                },
                "yamlParametersFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "dlRuntimeImage"
            ]
        },
        "databricks:index/JobTaskGenAiComputeTaskCompute:JobTaskGenAiComputeTaskCompute": {
            "properties": {
                "gpuNodePoolId": {
                    "type": "string"
                },
                "gpuType": {
                    "type": "string"
                },
                "numGpus": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "gpuNodePoolId",
                "numGpus"
            ]
        },
        "databricks:index/JobTaskHealth:JobTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskHealthRule:JobTaskHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobTaskHealthRule:JobTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/JobTaskLibrary:JobTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryCran:JobTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibraryCran:JobTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewCluster:JobTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "kind": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterLibrary:JobTaskNewClusterLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType",
                    "description": "isn't supported\n"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAzureAttributesLogAnalyticsInfo:JobTaskNewClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAzureAttributesLogAnalyticsInfo:JobTaskNewClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfVolumes:JobTaskNewClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterLogConfVolumes:JobTaskNewClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile",
                    "description": "block consisting of single string fields:\n"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptVolumes:JobTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptWorkspace:JobTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptVolumes:JobTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptWorkspace:JobTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterLibrary:JobTaskNewClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterLibraryCran:JobTaskNewClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterLibraryMaven:JobTaskNewClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterLibraryPypi:JobTaskNewClusterLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterLibraryCran:JobTaskNewClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewClusterLibraryMaven:JobTaskNewClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskNewClusterLibraryPypi:JobTaskNewClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNotebookTask:JobTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskNotificationSettings:JobTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "description": "(Bool) do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n\nThe following parameter is only available on task level.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskPipelineTask:JobTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskRunJobTask:JobTaskRunJobTask": {
            "properties": {
                "dbtCommands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "jarParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Job parameters for the task\n"
                },
                "notebookParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "pipelineParams": {
                    "$ref": "#/types/databricks:index/JobTaskRunJobTaskPipelineParams:JobTaskRunJobTaskPipelineParams"
                },
                "pythonNamedParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "pythonParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "sparkSubmitParams": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "sqlParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobTaskRunJobTaskPipelineParams:JobTaskRunJobTaskPipelineParams": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e The following configuration blocks are only supported inside a `task` block\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                },
                "runAsRepl": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "runAsRepl"
                    ]
                }
            }
        },
        "databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTask:JobTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert",
                    "description": "block consisting of following fields:\n"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard",
                    "description": "block consisting of following fields:\n"
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskFile:JobTaskSqlTaskFile",
                    "description": "block consisting of single string fields:\n"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.\n"
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery",
                    "description": "block consisting of single string field: `query_id` - identifier of the Databricks Query (databricks_query).\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless \u0026 Pro warehouses are supported right now.\n"
                }
            },
            "type": "object",
            "required": [
                "warehouseId"
            ]
        },
        "databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks Alert (databricks_alert).\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskSqlTaskAlertSubscription:JobTaskSqlTaskAlertSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/JobTaskSqlTaskAlertSubscription:JobTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string",
                    "description": "string specifying a custom subject of email sent.\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboardSubscription:JobTaskSqlTaskDashboardSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskSqlTaskDashboardSubscription:JobTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskFile:JobTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string",
                    "description": "If `source` is `GIT`: Relative path to the file in the repository specified in the `git_source` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.\n\nExample\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlAggregationJob = new databricks.Job(\"sql_aggregation_job\", {\n    name: \"Example SQL Job\",\n    tasks: [\n        {\n            taskKey: \"run_agg_query\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                query: {\n                    queryId: aggQuery.id,\n                },\n            },\n        },\n        {\n            taskKey: \"run_dashboard\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                dashboard: {\n                    dashboardId: dash.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            taskKey: \"run_alert\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                alert: {\n                    alertId: alert.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsql_aggregation_job = databricks.Job(\"sql_aggregation_job\",\n    name=\"Example SQL Job\",\n    tasks=[\n        {\n            \"task_key\": \"run_agg_query\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"query\": {\n                    \"query_id\": agg_query[\"id\"],\n                },\n            },\n        },\n        {\n            \"task_key\": \"run_dashboard\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"dashboard\": {\n                    \"dashboard_id\": dash[\"id\"],\n                    \"subscriptions\": [{\n                        \"user_name\": \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            \"task_key\": \"run_alert\",\n            \"sql_task\": {\n                \"warehouse_id\": sql_job_warehouse[\"id\"],\n                \"alert\": {\n                    \"alert_id\": alert[\"id\"],\n                    \"subscriptions\": [{\n                        \"user_name\": \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlAggregationJob = new Databricks.Job(\"sql_aggregation_job\", new()\n    {\n        Name = \"Example SQL Job\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_agg_query\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Query = new Databricks.Inputs.JobTaskSqlTaskQueryArgs\n                    {\n                        QueryId = aggQuery.Id,\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_dashboard\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Dashboard = new Databricks.Inputs.JobTaskSqlTaskDashboardArgs\n                    {\n                        DashboardId = dash.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskDashboardSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_alert\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Alert = new Databricks.Inputs.JobTaskSqlTaskAlertArgs\n                    {\n                        AlertId = alert.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskAlertSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"sql_aggregation_job\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.String(\"Example SQL Job\"),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_agg_query\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tQuery: \u0026databricks.JobTaskSqlTaskQueryArgs{\n\t\t\t\t\t\t\tQueryId: pulumi.Any(aggQuery.Id),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_dashboard\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tDashboard: \u0026databricks.JobTaskSqlTaskDashboardArgs{\n\t\t\t\t\t\t\tDashboardId: pulumi.Any(dash.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskDashboardSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskDashboardSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_alert\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tAlert: \u0026databricks.JobTaskSqlTaskAlertArgs{\n\t\t\t\t\t\t\tAlertId: pulumi.Any(alert.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskAlertSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskAlertSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskQueryArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskDashboardArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskAlertArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sqlAggregationJob = new Job(\"sqlAggregationJob\", JobArgs.builder()\n            .name(\"Example SQL Job\")\n            .tasks(            \n                JobTaskArgs.builder()\n                    .taskKey(\"run_agg_query\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .query(JobTaskSqlTaskQueryArgs.builder()\n                            .queryId(aggQuery.id())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_dashboard\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .dashboard(JobTaskSqlTaskDashboardArgs.builder()\n                            .dashboardId(dash.id())\n                            .subscriptions(JobTaskSqlTaskDashboardSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_alert\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .alert(JobTaskSqlTaskAlertArgs.builder()\n                            .alertId(alert.id())\n                            .subscriptions(JobTaskSqlTaskAlertSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sqlAggregationJob:\n    type: databricks:Job\n    name: sql_aggregation_job\n    properties:\n      name: Example SQL Job\n      tasks:\n        - taskKey: run_agg_query\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            query:\n              queryId: ${aggQuery.id}\n        - taskKey: run_dashboard\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            dashboard:\n              dashboardId: ${dash.id}\n              subscriptions:\n                - userName: user@domain.com\n        - taskKey: run_alert\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            alert:\n              alertId: ${alert.id}\n              subscriptions:\n                - userName: user@domain.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.\n"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/JobTaskWebhookNotifications:JobTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnFailure:JobTaskWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnStart:JobTaskWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnStreamingBacklogExceeded:JobTaskWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnSuccess:JobTaskWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnFailure:JobTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnStart:JobTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnStreamingBacklogExceeded:JobTaskWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnSuccess:JobTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTrigger:JobTrigger": {
            "properties": {
                "fileArrival": {
                    "$ref": "#/types/databricks:index/JobTriggerFileArrival:JobTriggerFileArrival",
                    "description": "configuration block to define a trigger for [File Arrival events](https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/file-arrival-triggers) consisting of following attributes:\n"
                },
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this trigger is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                },
                "periodic": {
                    "$ref": "#/types/databricks:index/JobTriggerPeriodic:JobTriggerPeriodic",
                    "description": "configuration block to define a trigger for Periodic Triggers consisting of the following attributes:\n"
                },
                "table": {
                    "$ref": "#/types/databricks:index/JobTriggerTable:JobTriggerTable"
                },
                "tableUpdate": {
                    "$ref": "#/types/databricks:index/JobTriggerTableUpdate:JobTriggerTableUpdate"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTriggerFileArrival:JobTriggerFileArrival": {
            "properties": {
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL to be monitored for file arrivals. The path must point to the root or a subpath of the external location. Please note that the URL must have a trailing slash character (`/`).\n"
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTriggerPeriodic:JobTriggerPeriodic": {
            "properties": {
                "interval": {
                    "type": "integer",
                    "description": "Specifies the interval at which the job should run. This value is required.\n"
                },
                "unit": {
                    "type": "string",
                    "description": "Options are {\"DAYS\", \"HOURS\", \"WEEKS\"}.\n"
                }
            },
            "type": "object",
            "required": [
                "interval",
                "unit"
            ]
        },
        "databricks:index/JobTriggerTable:JobTriggerTable": {
            "properties": {
                "condition": {
                    "type": "string"
                },
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.\n"
                },
                "tableNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTriggerTableUpdate:JobTriggerTableUpdate": {
            "properties": {
                "condition": {
                    "type": "string"
                },
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.\n"
                },
                "tableNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.\n"
                }
            },
            "type": "object",
            "required": [
                "tableNames"
            ]
        },
        "databricks:index/JobWebhookNotifications:JobWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnDurationWarningThresholdExceeded:JobWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnStreamingBacklogExceeded:JobWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobWebhookNotificationsOnDurationWarningThresholdExceeded:JobWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnStreamingBacklogExceeded:JobWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric": {
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "[create metric definition](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html#create-definition)\n"
                },
                "inputColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Columns on the monitored table to apply the custom metrics to.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the custom metric.\n"
                },
                "outputDataType": {
                    "type": "string",
                    "description": "The output type of the custom metric.\n"
                },
                "type": {
                    "type": "string",
                    "description": "The type of the custom metric.\n"
                }
            },
            "type": "object",
            "required": [
                "definition",
                "inputColumns",
                "name",
                "outputDataType",
                "type"
            ]
        },
        "databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "labelCol": {
                    "type": "string",
                    "description": "Column of the model label\n"
                },
                "modelIdCol": {
                    "type": "string",
                    "description": "Column of the model id or version\n"
                },
                "predictionCol": {
                    "type": "string",
                    "description": "Column of the model prediction\n"
                },
                "predictionProbaCol": {
                    "type": "string",
                    "description": "Column of the model prediction probabilities\n"
                },
                "problemType": {
                    "type": "string",
                    "description": "Problem type the model aims to solve. Either `PROBLEM_TYPE_CLASSIFICATION` or `PROBLEM_TYPE_REGRESSION`\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "modelIdCol",
                "predictionCol",
                "problemType",
                "timestampCol"
            ]
        },
        "databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications": {
            "properties": {
                "onFailure": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotificationsOnFailure:LakehouseMonitorNotificationsOnFailure",
                    "description": "who to send notifications to on monitor failure.\n"
                },
                "onNewClassificationTagDetected": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotificationsOnNewClassificationTagDetected:LakehouseMonitorNotificationsOnNewClassificationTagDetected",
                    "description": "Who to send notifications to when new data classification tags are detected.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorNotificationsOnFailure:LakehouseMonitorNotificationsOnFailure": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorNotificationsOnNewClassificationTagDetected:LakehouseMonitorNotificationsOnNewClassificationTagDetected": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "optional string field that indicates whether a schedule is paused (`PAUSED`) or not (`UNPAUSED`).\n"
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "string expression that determines when to run the monitor. See [Quartz documentation](https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for examples.\n"
                },
                "timezoneId": {
                    "type": "string",
                    "description": "string with timezone id (e.g., `PST`) in which to evaluate the Quartz expression.\n"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pauseStatus",
                        "quartzCronExpression",
                        "timezoneId"
                    ]
                }
            }
        },
        "databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot": {
            "type": "object"
        },
        "databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "timestampCol"
            ]
        },
        "databricks:index/LibraryCran:LibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/LibraryMaven:LibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/LibraryPypi:LibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "unityCatalogIamArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "externalId",
                        "roleArn",
                        "unityCatalogIamArn"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "accessConnectorId",
                        "credentialId"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecret": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/MetastoreDataAccessCloudflareApiToken:MetastoreDataAccessCloudflareApiToken": {
            "properties": {
                "accessKeyId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "accountId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "secretAccessKey": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "accessKeyId",
                "accountId",
                "secretAccessKey"
            ]
        },
        "databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "credentialId",
                        "email"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "privateKey": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "privateKeyId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/MlflowExperimentTag:MlflowExperimentTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/MlflowModelTag:MlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec": {
            "properties": {
                "authorization": {
                    "type": "string",
                    "description": "Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `\u003cauth type\u003e \u003ccredentials\u003e`, e.g. `Bearer \u003caccess_token\u003e`. If set to an empty string, no authorization header will be included in the request.\n"
                },
                "enableSslVerification": {
                    "type": "boolean",
                    "description": "Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.\n"
                },
                "secret": {
                    "type": "string",
                    "description": "Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as `X-Databricks-Signature: encoded_payload`.\n",
                    "secret": true
                },
                "url": {
                    "type": "string",
                    "description": "External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec": {
            "properties": {
                "accessToken": {
                    "type": "string",
                    "description": "The personal access token used to authorize webhook's job runs.\n",
                    "secret": true
                },
                "jobId": {
                    "type": "string",
                    "description": "ID of the Databricks job that the webhook runs.\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.\n"
                }
            },
            "type": "object",
            "required": [
                "accessToken",
                "jobId"
            ]
        },
        "databricks:index/ModelServingAiGateway:ModelServingAiGateway": {
            "properties": {
                "fallbackConfig": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayFallbackConfig:ModelServingAiGatewayFallbackConfig"
                },
                "guardrails": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayGuardrails:ModelServingAiGatewayGuardrails",
                    "description": "Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:\n"
                },
                "inferenceTableConfig": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayInferenceTableConfig:ModelServingAiGatewayInferenceTableConfig",
                    "description": "Block describing the configuration of usage tracking. Consists of the following attributes:\n"
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingAiGatewayRateLimit:ModelServingAiGatewayRateLimit"
                    },
                    "description": "Block describing rate limits for AI gateway. For details see the description of `rate_limits` block above.\n"
                },
                "usageTrackingConfig": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayUsageTrackingConfig:ModelServingAiGatewayUsageTrackingConfig",
                    "description": "Block with configuration for payload logging using inference tables. For details see the description of `auto_capture_config` block above.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayFallbackConfig:ModelServingAiGatewayFallbackConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/ModelServingAiGatewayGuardrails:ModelServingAiGatewayGuardrails": {
            "properties": {
                "input": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayGuardrailsInput:ModelServingAiGatewayGuardrailsInput",
                    "description": "A block with configuration for input guardrail filters:\n"
                },
                "output": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayGuardrailsOutput:ModelServingAiGatewayGuardrailsOutput",
                    "description": "A block with configuration for output guardrail filters.  Has the same structure as `input` block.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayGuardrailsInput:ModelServingAiGatewayGuardrailsInput": {
            "properties": {
                "invalidKeywords": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.\n"
                },
                "pii": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayGuardrailsInputPii:ModelServingAiGatewayGuardrailsInputPii",
                    "description": "Block with configuration for guardrail PII filter:\n"
                },
                "safety": {
                    "type": "boolean",
                    "description": "the boolean flag that indicates whether the safety filter is enabled.\n"
                },
                "validTopics": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayGuardrailsInputPii:ModelServingAiGatewayGuardrailsInputPii": {
            "properties": {
                "behavior": {
                    "type": "string",
                    "description": "a string that describes the behavior for PII filter. Currently only `BLOCK` value is supported.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayGuardrailsOutput:ModelServingAiGatewayGuardrailsOutput": {
            "properties": {
                "invalidKeywords": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.\n"
                },
                "pii": {
                    "$ref": "#/types/databricks:index/ModelServingAiGatewayGuardrailsOutputPii:ModelServingAiGatewayGuardrailsOutputPii",
                    "description": "Block with configuration for guardrail PII filter:\n"
                },
                "safety": {
                    "type": "boolean",
                    "description": "the boolean flag that indicates whether the safety filter is enabled.\n"
                },
                "validTopics": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayGuardrailsOutputPii:ModelServingAiGatewayGuardrailsOutputPii": {
            "properties": {
                "behavior": {
                    "type": "string",
                    "description": "a string that describes the behavior for PII filter. Currently only `BLOCK` value is supported.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayInferenceTableConfig:ModelServingAiGatewayInferenceTableConfig": {
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.\n"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "boolean flag specifying if usage tracking is enabled.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.\n"
                },
                "tableNamePrefix": {
                    "type": "string",
                    "description": "The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingAiGatewayRateLimit:ModelServingAiGatewayRateLimit": {
            "properties": {
                "calls": {
                    "type": "integer",
                    "description": "Used to specify how many calls are allowed for a key within the renewal_period.\n"
                },
                "key": {
                    "type": "string",
                    "description": "Key field for a serving endpoint rate limit. Currently, only `user` and `endpoint` are supported, with `endpoint` being the default if not specified.\n"
                },
                "renewalPeriod": {
                    "type": "string",
                    "description": "Renewal period field for a serving endpoint rate limit. Currently, only `minute` is supported.\n"
                }
            },
            "type": "object",
            "required": [
                "calls",
                "renewalPeriod"
            ]
        },
        "databricks:index/ModelServingAiGatewayUsageTrackingConfig:ModelServingAiGatewayUsageTrackingConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfig:ModelServingConfig": {
            "properties": {
                "autoCaptureConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigAutoCaptureConfig:ModelServingConfigAutoCaptureConfig",
                    "description": "Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.\n"
                },
                "servedEntities": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigServedEntity:ModelServingConfigServedEntity"
                    },
                    "description": "A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.\n"
                },
                "servedModels": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigServedModel:ModelServingConfigServedModel"
                    },
                    "description": "Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.\n",
                    "deprecationMessage": "Please use 'config.served_entities' instead of 'config.served_models'."
                },
                "trafficConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigTrafficConfig:ModelServingConfigTrafficConfig",
                    "description": "A single block represents the traffic split configuration amongst the served models.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "trafficConfig"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigAutoCaptureConfig:ModelServingConfigAutoCaptureConfig": {
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.\n",
                    "willReplaceOnChanges": true
                },
                "enabled": {
                    "type": "boolean",
                    "description": "If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable it again.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.\n",
                    "willReplaceOnChanges": true
                },
                "tableNamePrefix": {
                    "type": "string",
                    "description": "The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "enabled",
                        "tableNamePrefix"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigServedEntity:ModelServingConfigServedEntity": {
            "properties": {
                "entityName": {
                    "type": "string",
                    "description": "The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type `FEATURE_SPEC` in the UC. If it is a UC object, the full name of the object should be given in the form of `catalog_name.schema_name.model_name`.\n"
                },
                "entityVersion": {
                    "type": "string",
                    "description": "The version of the model in Databricks Model Registry to be served or empty if the entity is a `FEATURE_SPEC`.\n"
                },
                "environmentVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and is subject to change. Example entity environment variables that refer to Databricks secrets: ```{\"OPENAI_API_KEY\": \"{{secrets/my_scope/my_key}}\", \"DATABRICKS_TOKEN\": \"{{secrets/my_scope2/my_key2}}\"}```\n"
                },
                "externalModel": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModel:ModelServingConfigServedEntityExternalModel",
                    "description": "The external model to be served. NOTE: Only one of `external_model` and (`entity_name`, `entity_version`, `workload_size`, `workload_type`, and `scale_to_zero_enabled`) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an `external_model` is present, the served entities list can only have one `served_entity` object. An existing endpoint with `external_model` can not be updated to an endpoint without `external_model`. If the endpoint is created without `external_model`, users cannot update it to add `external_model` later.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "ARN of the instance profile that the served entity uses to access AWS resources.\n"
                },
                "maxProvisionedThroughput": {
                    "type": "integer",
                    "description": "The maximum tokens per second that the endpoint can scale up to.\n"
                },
                "minProvisionedThroughput": {
                    "type": "integer",
                    "description": "The minimum tokens per second that the endpoint can scale down to.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to `external_model.name`, with '.' and ':' replaced with '-', and if not specified for other entities, it defaults to -.\n"
                },
                "scaleToZeroEnabled": {
                    "type": "boolean",
                    "description": "Whether the compute resources for the served entity should scale down to zero.\n"
                },
                "workloadSize": {
                    "type": "string",
                    "description": "The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency). If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size is 0.\n"
                },
                "workloadType": {
                    "type": "string",
                    "description": "The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is `CPU`. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See the available [GPU types](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#gpu-workload-types).\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "name",
                        "workloadSize",
                        "workloadType"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigServedEntityExternalModel:ModelServingConfigServedEntityExternalModel": {
            "properties": {
                "ai21labsConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAi21labsConfig:ModelServingConfigServedEntityExternalModelAi21labsConfig",
                    "description": "AI21Labs Config\n"
                },
                "amazonBedrockConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAmazonBedrockConfig:ModelServingConfigServedEntityExternalModelAmazonBedrockConfig",
                    "description": "Amazon Bedrock Config\n"
                },
                "anthropicConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAnthropicConfig:ModelServingConfigServedEntityExternalModelAnthropicConfig",
                    "description": "Anthropic Config\n"
                },
                "cohereConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelCohereConfig:ModelServingConfigServedEntityExternalModelCohereConfig",
                    "description": "Cohere Config\n"
                },
                "customProviderConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfig:ModelServingConfigServedEntityExternalModelCustomProviderConfig"
                },
                "databricksModelServingConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig:ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig",
                    "description": "Databricks Model Serving Config\n"
                },
                "googleCloudVertexAiConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig:ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig",
                    "description": "Google Cloud Vertex AI Config.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the external model.\n"
                },
                "openaiConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelOpenaiConfig:ModelServingConfigServedEntityExternalModelOpenaiConfig",
                    "description": "OpenAI Config\n"
                },
                "palmConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelPalmConfig:ModelServingConfigServedEntityExternalModelPalmConfig",
                    "description": "PaLM Config\n"
                },
                "provider": {
                    "type": "string",
                    "description": "The name of the provider for the external model. Currently, the supported providers are `ai21labs`, `anthropic`, `amazon-bedrock`, `cohere`, `databricks-model-serving`, `google-cloud-vertex-ai`, `openai`, and `palm`.\n"
                },
                "task": {
                    "type": "string",
                    "description": "The task type of the external model.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "provider",
                "task"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAi21labsConfig:ModelServingConfigServedEntityExternalModelAi21labsConfig": {
            "properties": {
                "ai21labsApiKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for an AI21Labs API key.\n"
                },
                "ai21labsApiKeyPlaintext": {
                    "type": "string",
                    "description": "An AI21 Labs API key provided as a plaintext string.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAmazonBedrockConfig:ModelServingConfigServedEntityExternalModelAmazonBedrockConfig": {
            "properties": {
                "awsAccessKeyId": {
                    "type": "string",
                    "description": "The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.\n"
                },
                "awsAccessKeyIdPlaintext": {
                    "type": "string",
                    "description": "An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.\n"
                },
                "awsRegion": {
                    "type": "string",
                    "description": "The AWS region to use. Bedrock has to be enabled there.\n"
                },
                "awsSecretAccessKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.\n"
                },
                "awsSecretAccessKeyPlaintext": {
                    "type": "string",
                    "description": "An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.\n"
                },
                "bedrockProvider": {
                    "type": "string",
                    "description": "The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: `Anthropic`, `Cohere`, `AI21Labs`, `Amazon`.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "awsRegion",
                "bedrockProvider"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAnthropicConfig:ModelServingConfigServedEntityExternalModelAnthropicConfig": {
            "properties": {
                "anthropicApiKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for an Anthropic API key.\n"
                },
                "anthropicApiKeyPlaintext": {
                    "type": "string",
                    "description": "The Anthropic API key provided as a plaintext string.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelCohereConfig:ModelServingConfigServedEntityExternalModelCohereConfig": {
            "properties": {
                "cohereApiBase": {
                    "type": "string"
                },
                "cohereApiKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for a Cohere API key.\n"
                },
                "cohereApiKeyPlaintext": {
                    "type": "string",
                    "description": "The Cohere API key provided as a plaintext string.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfig:ModelServingConfigServedEntityExternalModelCustomProviderConfig": {
            "properties": {
                "apiKeyAuth": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth:ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth"
                },
                "bearerTokenAuth": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth:ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth"
                },
                "customProviderUrl": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "customProviderUrl"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth:ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string",
                    "description": "The value field for a tag.\n"
                },
                "valuePlaintext": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth:ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth": {
            "properties": {
                "token": {
                    "type": "string"
                },
                "tokenPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig:ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig": {
            "properties": {
                "databricksApiToken": {
                    "type": "string",
                    "description": "The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.\n"
                },
                "databricksApiTokenPlaintext": {
                    "type": "string",
                    "description": "The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.\n"
                },
                "databricksWorkspaceUrl": {
                    "type": "string",
                    "description": "The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.\n"
                }
            },
            "type": "object",
            "required": [
                "databricksWorkspaceUrl"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig:ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig": {
            "properties": {
                "privateKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.\n"
                },
                "privateKeyPlaintext": {
                    "type": "string",
                    "description": "The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.\n"
                },
                "projectId": {
                    "type": "string",
                    "description": "This is the Google Cloud project id that the service account is associated with.\n"
                },
                "region": {
                    "type": "string",
                    "description": "This is the region for the Google Cloud Vertex AI Service.\n"
                }
            },
            "type": "object",
            "required": [
                "projectId",
                "region"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelOpenaiConfig:ModelServingConfigServedEntityExternalModelOpenaiConfig": {
            "properties": {
                "microsoftEntraClientId": {
                    "type": "string",
                    "description": "This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.\n"
                },
                "microsoftEntraClientSecret": {
                    "type": "string",
                    "description": "The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.\n"
                },
                "microsoftEntraClientSecretPlaintext": {
                    "type": "string",
                    "description": "The client secret used for Microsoft Entra ID authentication provided as a plaintext string.\n"
                },
                "microsoftEntraTenantId": {
                    "type": "string",
                    "description": "This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.\n"
                },
                "openaiApiBase": {
                    "type": "string",
                    "description": "This is the base URL for the OpenAI API (default: \"https://api.openai.com/v1\"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.\n"
                },
                "openaiApiKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for an OpenAI or Azure OpenAI API key.\n"
                },
                "openaiApiKeyPlaintext": {
                    "type": "string",
                    "description": "The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.\n"
                },
                "openaiApiType": {
                    "type": "string",
                    "description": "This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use `azure`. For authentication using Azure Active Directory (Azure AD) use, `azuread`.\n"
                },
                "openaiApiVersion": {
                    "type": "string",
                    "description": "This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.\n"
                },
                "openaiDeploymentName": {
                    "type": "string",
                    "description": "This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.\n"
                },
                "openaiOrganization": {
                    "type": "string",
                    "description": "This is an optional field to specify the organization in OpenAI or Azure OpenAI.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelPalmConfig:ModelServingConfigServedEntityExternalModelPalmConfig": {
            "properties": {
                "palmApiKey": {
                    "type": "string",
                    "description": "The Databricks secret key reference for a PaLM API key.\n"
                },
                "palmApiKeyPlaintext": {
                    "type": "string",
                    "description": "The PaLM API key provided as a plaintext string.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedModel:ModelServingConfigServedModel": {
            "properties": {
                "environmentVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: `{{secrets/secret_scope/secret_key}}`.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "ARN of the instance profile that the served model will use to access AWS resources.\n"
                },
                "maxProvisionedThroughput": {
                    "type": "integer",
                    "description": "The maximum tokens per second that the endpoint can scale up to.\n"
                },
                "minProvisionedThroughput": {
                    "type": "integer",
                    "description": "The minimum tokens per second that the endpoint can scale down to.\n"
                },
                "modelName": {
                    "type": "string",
                    "description": "The name of the model in Databricks Model Registry to be served.\n"
                },
                "modelVersion": {
                    "type": "string",
                    "description": "The version of the model in Databricks Model Registry to be served.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of a served model. It must be unique across an endpoint. If not specified, this field will default to `modelname-modelversion`. A served model name can consist of alphanumeric characters, dashes, and underscores.\n"
                },
                "scaleToZeroEnabled": {
                    "type": "boolean",
                    "description": "Whether the compute resources for the served model should scale down to zero. If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is `true`.\n"
                },
                "workloadSize": {
                    "type": "string",
                    "description": "The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency).\n"
                },
                "workloadType": {
                    "type": "string",
                    "description": "The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See the documentation for all options. The default value is `CPU`.\n"
                }
            },
            "type": "object",
            "required": [
                "modelName",
                "modelVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "modelName",
                        "modelVersion",
                        "name",
                        "workloadType"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigTrafficConfig:ModelServingConfigTrafficConfig": {
            "properties": {
                "routes": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigTrafficConfigRoute:ModelServingConfigTrafficConfigRoute"
                    },
                    "description": "Each block represents a route that defines traffic to each served entity. Each `served_entity` block needs to have a corresponding `routes` block.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigTrafficConfigRoute:ModelServingConfigTrafficConfigRoute": {
            "properties": {
                "servedModelName": {
                    "type": "string"
                },
                "trafficPercentage": {
                    "type": "integer",
                    "description": "The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.\n"
                }
            },
            "type": "object",
            "required": [
                "servedModelName",
                "trafficPercentage"
            ]
        },
        "databricks:index/ModelServingRateLimit:ModelServingRateLimit": {
            "properties": {
                "calls": {
                    "type": "integer",
                    "description": "Used to specify how many calls are allowed for a key within the renewal_period.\n"
                },
                "key": {
                    "type": "string",
                    "description": "Key field for a serving endpoint rate limit. Currently, only `user` and `endpoint` are supported, with `endpoint` being the default if not specified.\n"
                },
                "renewalPeriod": {
                    "type": "string",
                    "description": "Renewal period field for a serving endpoint rate limit. Currently, only `minute` is supported.\n"
                }
            },
            "type": "object",
            "required": [
                "calls",
                "renewalPeriod"
            ]
        },
        "databricks:index/ModelServingTag:ModelServingTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "The key field for a tag.\n"
                },
                "value": {
                    "type": "string",
                    "description": "The value field for a tag.\n"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/MountAbfs:MountAbfs": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initializeFileSystem": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "initializeFileSystem"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "containerName",
                        "initializeFileSystem",
                        "storageAccountName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountAdl:MountAdl": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sparkConfPrefix": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageResourceName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "storageResourceName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountGs:MountGs": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "serviceAccount": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountS3:MountS3": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instanceProfile": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountWasb:MountWasb": {
            "properties": {
                "authType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tokenSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tokenSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "authType",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "authType",
                        "containerName",
                        "storageAccountName",
                        "tokenSecretKey",
                        "tokenSecretScope"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo": {
            "properties": {
                "keyAlias": {
                    "type": "string",
                    "description": "The AWS KMS key alias.\n",
                    "willReplaceOnChanges": true
                },
                "keyArn": {
                    "type": "string",
                    "description": "The AWS KMS key's Amazon Resource Name (ARN).\n",
                    "willReplaceOnChanges": true
                },
                "keyRegion": {
                    "type": "string",
                    "description": "(Computed) The AWS region in which KMS key is deployed to. This is not required.\n"
                }
            },
            "type": "object",
            "required": [
                "keyArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "keyArn",
                        "keyRegion"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo": {
            "properties": {
                "kmsKeyId": {
                    "type": "string",
                    "description": "The GCP KMS key's resource name.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "kmsKeyId"
            ]
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig": {
            "properties": {
                "defaultRules": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRules:MwsNetworkConnectivityConfigEgressConfigDefaultRules",
                    "description": "block describing network connectivity rules that are applied by default without resource specific configurations.  Consists of the following fields:\n"
                },
                "targetRules": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRules:MwsNetworkConnectivityConfigEgressConfigTargetRules",
                    "description": "block describing network connectivity rules that configured for each destinations. These rules override default rules.  Consists of the following fields:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRules:MwsNetworkConnectivityConfigEgressConfigDefaultRules": {
            "properties": {
                "awsStableIpRule": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule",
                    "description": "(AWS only) - block with information about stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.  Consists of the following fields:\n"
                },
                "azureServiceEndpointRule": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule",
                    "description": "(Azure only) - block with information about stable Azure service endpoints. You can configure the firewall of your Azure resources to allow traffic from your Databricks serverless compute resources.  Consists of the following fields:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule": {
            "properties": {
                "cidrBlocks": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "list of IP CIDR blocks.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule": {
            "properties": {
                "subnets": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "list of subnets from which Databricks network traffic originates when accessing your Azure resources.\n"
                },
                "targetRegion": {
                    "type": "string",
                    "description": "the Azure region in which this service endpoint rule applies.\n"
                },
                "targetServices": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "the Azure services to which this service endpoint rule applies to.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRules:MwsNetworkConnectivityConfigEgressConfigTargetRules": {
            "properties": {
                "azurePrivateEndpointRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule"
                    },
                    "description": "(Azure only) - list containing information about configure Azure Private Endpoints.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule": {
            "properties": {
                "connectionState": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "deactivated": {
                    "type": "boolean"
                },
                "deactivatedAt": {
                    "type": "integer"
                },
                "endpointName": {
                    "type": "string"
                },
                "groupId": {
                    "type": "string"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "resourceId": {
                    "type": "string"
                },
                "ruleId": {
                    "type": "string"
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage": {
            "properties": {
                "errorMessage": {
                    "type": "string"
                },
                "errorType": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo": {
            "properties": {
                "networkProjectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID of the VPC network.\n",
                    "willReplaceOnChanges": true
                },
                "podIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.\n",
                    "deprecationMessage": "gcp_network_info.pod_ip_range_name is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-vpc"
                },
                "serviceIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.\n",
                    "deprecationMessage": "gcp_network_info.service_ip_range_name is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-vpc"
                },
                "subnetId": {
                    "type": "string",
                    "description": "The ID of the subnet associated with this network.\n",
                    "willReplaceOnChanges": true
                },
                "subnetRegion": {
                    "type": "string",
                    "description": "The Google Cloud region of the workspace data plane. For example, `us-east4`.\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "networkProjectId",
                "subnetId",
                "subnetRegion",
                "vpcId"
            ]
        },
        "databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints": {
            "properties": {
                "dataplaneRelays": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "restApis": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "dataplaneRelays",
                "restApis"
            ]
        },
        "databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo": {
            "properties": {
                "endpointRegion": {
                    "type": "string",
                    "description": "Region of the PSC endpoint.\n",
                    "willReplaceOnChanges": true
                },
                "projectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID of the VPC network where the PSC connection resides.\n",
                    "willReplaceOnChanges": true
                },
                "pscConnectionId": {
                    "type": "string",
                    "description": "The unique ID of this PSC connection.\n"
                },
                "pscEndpointName": {
                    "type": "string",
                    "description": "The name of the PSC endpoint in the Google Cloud project.\n",
                    "willReplaceOnChanges": true
                },
                "serviceAttachmentId": {
                    "type": "string",
                    "description": "The service attachment this PSC connection connects to.\n"
                }
            },
            "type": "object",
            "required": [
                "endpointRegion",
                "projectId",
                "pscEndpointName"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "endpointRegion",
                        "projectId",
                        "pscConnectionId",
                        "pscEndpointName",
                        "serviceAttachmentId"
                    ]
                }
            }
        },
        "databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer": {
            "properties": {
                "gcp": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp",
                    "description": "A block that consists of the following field:\n"
                }
            },
            "type": "object",
            "required": [
                "gcp"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp": {
            "properties": {
                "projectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.\n"
                }
            },
            "type": "object",
            "required": [
                "projectId"
            ]
        },
        "databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo": {
            "properties": {
                "authoritativeUserEmail": {
                    "type": "string"
                },
                "authoritativeUserFullName": {
                    "type": "string"
                },
                "customerName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "authoritativeUserEmail",
                "authoritativeUserFullName",
                "customerName"
            ]
        },
        "databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig": {
            "properties": {
                "gkeClusterPodIpRange": {
                    "type": "string",
                    "deprecationMessage": "gcp_managed_network_config.gke_cluster_pod_ip_range is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-databricks-workspace"
                },
                "gkeClusterServiceIpRange": {
                    "type": "string",
                    "deprecationMessage": "gcp_managed_network_config.gke_cluster_service_ip_range is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-databricks-workspace"
                },
                "subnetCidr": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "subnetCidr"
            ]
        },
        "databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig": {
            "properties": {
                "connectivityType": {
                    "type": "string",
                    "description": "Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: `PRIVATE_NODE_PUBLIC_MASTER`, `PUBLIC_NODE_PUBLIC_MASTER`.\n"
                },
                "masterIpRange": {
                    "type": "string",
                    "description": "The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as `/28`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsWorkspacesToken:MwsWorkspacesToken": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "Comment, that will appear in \"User Settings / Access Tokens\" page on Workspace UI. By default it's \"Pulumi PAT\".\n"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "Token expiry lifetime. By default its 2592000 (30 days).\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "secret": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "tokenId",
                        "tokenValue"
                    ]
                }
            }
        },
        "databricks:index/NotificationDestinationConfig:NotificationDestinationConfig": {
            "properties": {
                "email": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfigEmail:NotificationDestinationConfigEmail",
                    "description": "The email configuration of the Notification Destination. It must contain the following:\n",
                    "willReplaceOnChanges": true
                },
                "genericWebhook": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfigGenericWebhook:NotificationDestinationConfigGenericWebhook",
                    "description": "The Generic Webhook configuration of the Notification Destination. It must contain the following:\n",
                    "willReplaceOnChanges": true
                },
                "microsoftTeams": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfigMicrosoftTeams:NotificationDestinationConfigMicrosoftTeams",
                    "description": "The Microsoft Teams configuration of the Notification Destination. It must contain the following:\n",
                    "willReplaceOnChanges": true
                },
                "pagerduty": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfigPagerduty:NotificationDestinationConfigPagerduty",
                    "description": "The PagerDuty configuration of the Notification Destination. It must contain the following:\n",
                    "willReplaceOnChanges": true
                },
                "slack": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfigSlack:NotificationDestinationConfigSlack",
                    "description": "The Slack configuration of the Notification Destination. It must contain the following:\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/NotificationDestinationConfigEmail:NotificationDestinationConfigEmail": {
            "properties": {
                "addresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of email addresses to send notifications to.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/NotificationDestinationConfigGenericWebhook:NotificationDestinationConfigGenericWebhook": {
            "properties": {
                "password": {
                    "type": "string",
                    "description": "The password for basic authentication.\n\n\u003e **NOTE** If the type of notification destination is changed, the existing notification destination will be deleted and a new notification destination will be created with the new type.\n",
                    "secret": true
                },
                "passwordSet": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string",
                    "description": "The Generic Webhook URL.\n",
                    "secret": true
                },
                "urlSet": {
                    "type": "boolean"
                },
                "username": {
                    "type": "string",
                    "description": "The username for basic authentication.\n",
                    "secret": true
                },
                "usernameSet": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "passwordSet",
                        "urlSet",
                        "usernameSet"
                    ]
                }
            }
        },
        "databricks:index/NotificationDestinationConfigMicrosoftTeams:NotificationDestinationConfigMicrosoftTeams": {
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The Microsoft Teams webhook URL.\n",
                    "secret": true
                },
                "urlSet": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "urlSet"
                    ]
                }
            }
        },
        "databricks:index/NotificationDestinationConfigPagerduty:NotificationDestinationConfigPagerduty": {
            "properties": {
                "integrationKey": {
                    "type": "string",
                    "description": "The PagerDuty integration key.\n",
                    "secret": true
                },
                "integrationKeySet": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "integrationKeySet"
                    ]
                }
            }
        },
        "databricks:index/NotificationDestinationConfigSlack:NotificationDestinationConfigSlack": {
            "properties": {
                "url": {
                    "type": "string",
                    "description": "The Slack webhook URL.\n",
                    "secret": true
                },
                "urlSet": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "urlSet"
                    ]
                }
            }
        },
        "databricks:index/OnlineTableSpec:OnlineTableSpec": {
            "properties": {
                "performFullCopy": {
                    "type": "boolean",
                    "description": "Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports \"triggered\" scheduling policy.\n",
                    "willReplaceOnChanges": true
                },
                "pipelineId": {
                    "type": "string",
                    "description": "ID of the associated Delta Live Table pipeline.\n"
                },
                "primaryKeyColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "list of the columns comprising the primary key.\n",
                    "willReplaceOnChanges": true
                },
                "runContinuously": {
                    "$ref": "#/types/databricks:index/OnlineTableSpecRunContinuously:OnlineTableSpecRunContinuously",
                    "description": "empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with `run_triggered`.\n",
                    "willReplaceOnChanges": true
                },
                "runTriggered": {
                    "$ref": "#/types/databricks:index/OnlineTableSpecRunTriggered:OnlineTableSpecRunTriggered",
                    "description": "empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers).\n",
                    "willReplaceOnChanges": true
                },
                "sourceTableFullName": {
                    "type": "string",
                    "description": "full name of the source table.\n",
                    "willReplaceOnChanges": true
                },
                "timeseriesKey": {
                    "type": "string",
                    "description": "Time series key to deduplicate (tie-break) rows with the same primary key.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pipelineId"
                    ]
                }
            }
        },
        "databricks:index/OnlineTableSpecRunContinuously:OnlineTableSpecRunContinuously": {
            "type": "object"
        },
        "databricks:index/OnlineTableSpecRunTriggered:OnlineTableSpecRunTriggered": {
            "type": "object"
        },
        "databricks:index/OnlineTableStatus:OnlineTableStatus": {
            "properties": {
                "continuousUpdateStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusContinuousUpdateStatus:OnlineTableStatusContinuousUpdateStatus"
                },
                "detailedState": {
                    "type": "string",
                    "description": "The state of the online table.\n"
                },
                "failedStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusFailedStatus:OnlineTableStatusFailedStatus"
                },
                "message": {
                    "type": "string",
                    "description": "A text description of the current state of the online table.\n"
                },
                "provisioningStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusProvisioningStatus:OnlineTableStatusProvisioningStatus"
                },
                "triggeredUpdateStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusTriggeredUpdateStatus:OnlineTableStatusTriggeredUpdateStatus"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusContinuousUpdateStatus:OnlineTableStatusContinuousUpdateStatus": {
            "properties": {
                "initialPipelineSyncProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress:OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress"
                },
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress:OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusFailedStatus:OnlineTableStatusFailedStatus": {
            "properties": {
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusProvisioningStatus:OnlineTableStatusProvisioningStatus": {
            "properties": {
                "initialPipelineSyncProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress:OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress:OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusTriggeredUpdateStatus:OnlineTableStatusTriggeredUpdateStatus": {
            "properties": {
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                },
                "triggeredUpdateProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress:OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress:OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/PermissionsAccessControl:PermissionsAccessControl": {
            "properties": {
                "groupName": {
                    "type": "string",
                    "description": "name of the group. We recommend setting permissions on groups.\n"
                },
                "permissionLevel": {
                    "type": "string",
                    "description": "permission level according to specific resource. See examples above for the reference.\n\nExactly one of the below arguments is required:\n"
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "Application ID of the service_principal.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "name of the user.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineCluster:PipelineCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineClusterInitScript:PipelineClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "label": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverNodeTypeId",
                        "enableLocalDiskEncryption",
                        "nodeTypeId"
                    ]
                }
            }
        },
        "databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                },
                "mode": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "maxWorkers",
                "minWorkers"
            ]
        },
        "databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/PipelineClusterAzureAttributesLogAnalyticsInfo:PipelineClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAzureAttributesLogAnalyticsInfo:PipelineClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfVolumes:PipelineClusterClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterClusterLogConfVolumes:PipelineClusterClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScript:PipelineClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptVolumes:PipelineClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptWorkspace:PipelineClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptVolumes:PipelineClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptWorkspace:PipelineClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineDeployment:PipelineDeployment": {
            "properties": {
                "kind": {
                    "type": "string",
                    "description": "The deployment method that manages the pipeline.\n"
                },
                "metadataFilePath": {
                    "type": "string",
                    "description": "The path to the file containing metadata about the deployment.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineEventLog:PipelineEventLog": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The UC catalog the event log is published under.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The table name the event log is published to in UC.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The UC schema the event log is published under.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "catalog",
                        "name",
                        "schema"
                    ]
                }
            }
        },
        "databricks:index/PipelineFilters:PipelineFilters": {
            "properties": {
                "excludes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Paths to exclude.\n"
                },
                "includes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Paths to include.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineGatewayDefinition:PipelineGatewayDefinition": {
            "properties": {
                "connectionId": {
                    "type": "string",
                    "description": "Immutable. The Unity Catalog connection this gateway pipeline uses to communicate with the source.\n",
                    "willReplaceOnChanges": true
                },
                "connectionName": {
                    "type": "string"
                },
                "gatewayStorageCatalog": {
                    "type": "string",
                    "description": "Required, Immutable. The name of the catalog for the gateway pipeline's storage location.\n",
                    "willReplaceOnChanges": true
                },
                "gatewayStorageName": {
                    "type": "string",
                    "description": "Required. The Unity Catalog-compatible naming for the gateway storage location. This is the destination to use for the data that is extracted by the gateway. Delta Live Tables system will automatically create the storage location under the catalog and schema.\n"
                },
                "gatewayStorageSchema": {
                    "type": "string",
                    "description": "Required, Immutable. The name of the schema for the gateway pipelines's storage location.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinition:PipelineIngestionDefinition": {
            "properties": {
                "connectionName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "ingestionGatewayId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObject:PipelineIngestionDefinitionObject"
                    }
                },
                "tableConfiguration": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionTableConfiguration:PipelineIngestionDefinitionTableConfiguration"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObject:PipelineIngestionDefinitionObject": {
            "properties": {
                "report": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectReport:PipelineIngestionDefinitionObjectReport"
                },
                "schema": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectSchema:PipelineIngestionDefinitionObjectSchema",
                    "description": "The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.\n"
                },
                "table": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectTable:PipelineIngestionDefinitionObjectTable"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectReport:PipelineIngestionDefinitionObjectReport": {
            "properties": {
                "destinationCatalog": {
                    "type": "string"
                },
                "destinationSchema": {
                    "type": "string"
                },
                "destinationTable": {
                    "type": "string"
                },
                "sourceUrl": {
                    "type": "string"
                },
                "tableConfiguration": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectReportTableConfiguration:PipelineIngestionDefinitionObjectReportTableConfiguration"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectReportTableConfiguration:PipelineIngestionDefinitionObjectReportTableConfiguration": {
            "properties": {
                "primaryKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "salesforceIncludeFormulaFields": {
                    "type": "boolean"
                },
                "scdType": {
                    "type": "string"
                },
                "sequenceBies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectSchema:PipelineIngestionDefinitionObjectSchema": {
            "properties": {
                "destinationCatalog": {
                    "type": "string"
                },
                "destinationSchema": {
                    "type": "string"
                },
                "sourceCatalog": {
                    "type": "string"
                },
                "sourceSchema": {
                    "type": "string"
                },
                "tableConfiguration": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectSchemaTableConfiguration:PipelineIngestionDefinitionObjectSchemaTableConfiguration"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectSchemaTableConfiguration:PipelineIngestionDefinitionObjectSchemaTableConfiguration": {
            "properties": {
                "primaryKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "salesforceIncludeFormulaFields": {
                    "type": "boolean"
                },
                "scdType": {
                    "type": "string"
                },
                "sequenceBies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectTable:PipelineIngestionDefinitionObjectTable": {
            "properties": {
                "destinationCatalog": {
                    "type": "string"
                },
                "destinationSchema": {
                    "type": "string"
                },
                "destinationTable": {
                    "type": "string"
                },
                "sourceCatalog": {
                    "type": "string"
                },
                "sourceSchema": {
                    "type": "string"
                },
                "sourceTable": {
                    "type": "string"
                },
                "tableConfiguration": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinitionObjectTableTableConfiguration:PipelineIngestionDefinitionObjectTableTableConfiguration"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionObjectTableTableConfiguration:PipelineIngestionDefinitionObjectTableTableConfiguration": {
            "properties": {
                "primaryKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "salesforceIncludeFormulaFields": {
                    "type": "boolean"
                },
                "scdType": {
                    "type": "string"
                },
                "sequenceBies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineIngestionDefinitionTableConfiguration:PipelineIngestionDefinitionTableConfiguration": {
            "properties": {
                "primaryKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "salesforceIncludeFormulaFields": {
                    "type": "boolean"
                },
                "scdType": {
                    "type": "string"
                },
                "sequenceBies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLatestUpdate:PipelineLatestUpdate": {
            "properties": {
                "creationTime": {
                    "type": "string"
                },
                "state": {
                    "type": "string"
                },
                "updateId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibrary:PipelineLibrary": {
            "properties": {
                "file": {
                    "$ref": "#/types/databricks:index/PipelineLibraryFile:PipelineLibraryFile"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/PipelineLibraryMaven:PipelineLibraryMaven"
                },
                "notebook": {
                    "$ref": "#/types/databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook"
                },
                "whl": {
                    "type": "string",
                    "deprecationMessage": "The 'whl' field is deprecated"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibraryFile:PipelineLibraryFile": {
            "properties": {
                "path": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibraryMaven:PipelineLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook": {
            "properties": {
                "path": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineNotification:PipelineNotification": {
            "properties": {
                "alerts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list\n* `on-update-success` - a pipeline update completes successfully.\n* `on-update-failure` - a pipeline update fails with a retryable error.\n* `on-update-fatal-failure` - a pipeline update fails with a non-retryable (fatal) error.\n* `on-flow-failure` - a single data flow fails.\n"
                },
                "emailRecipients": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "non-empty list of emails to notify.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineRestartWindow:PipelineRestartWindow": {
            "properties": {
                "daysOfWeeks": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "startHour": {
                    "type": "integer"
                },
                "timeZoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "startHour"
            ]
        },
        "databricks:index/PipelineRunAs:PipelineRunAs": {
            "properties": {
                "servicePrincipalName": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineTrigger:PipelineTrigger": {
            "properties": {
                "cron": {
                    "$ref": "#/types/databricks:index/PipelineTriggerCron:PipelineTriggerCron"
                },
                "manual": {
                    "$ref": "#/types/databricks:index/PipelineTriggerManual:PipelineTriggerManual"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineTriggerCron:PipelineTriggerCron": {
            "properties": {
                "quartzCronSchedule": {
                    "type": "string"
                },
                "timezoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineTriggerManual:PipelineTriggerManual": {
            "type": "object"
        },
        "databricks:index/QualityMonitorCustomMetric:QualityMonitorCustomMetric": {
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "[create metric definition](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html#create-definition)\n"
                },
                "inputColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Columns on the monitored table to apply the custom metrics to.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the custom metric.\n"
                },
                "outputDataType": {
                    "type": "string",
                    "description": "The output type of the custom metric.\n"
                },
                "type": {
                    "type": "string",
                    "description": "The type of the custom metric.\n"
                }
            },
            "type": "object",
            "required": [
                "definition",
                "inputColumns",
                "name",
                "outputDataType",
                "type"
            ]
        },
        "databricks:index/QualityMonitorDataClassificationConfig:QualityMonitorDataClassificationConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/QualityMonitorInferenceLog:QualityMonitorInferenceLog": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "labelCol": {
                    "type": "string",
                    "description": "Column of the model label\n"
                },
                "modelIdCol": {
                    "type": "string",
                    "description": "Column of the model id or version\n"
                },
                "predictionCol": {
                    "type": "string",
                    "description": "Column of the model prediction\n"
                },
                "predictionProbaCol": {
                    "type": "string",
                    "description": "Column of the model prediction probabilities\n"
                },
                "problemType": {
                    "type": "string",
                    "description": "Problem type the model aims to solve. Either `PROBLEM_TYPE_CLASSIFICATION` or `PROBLEM_TYPE_REGRESSION`\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "modelIdCol",
                "predictionCol",
                "problemType",
                "timestampCol"
            ]
        },
        "databricks:index/QualityMonitorNotifications:QualityMonitorNotifications": {
            "properties": {
                "onFailure": {
                    "$ref": "#/types/databricks:index/QualityMonitorNotificationsOnFailure:QualityMonitorNotificationsOnFailure",
                    "description": "who to send notifications to on monitor failure.\n"
                },
                "onNewClassificationTagDetected": {
                    "$ref": "#/types/databricks:index/QualityMonitorNotificationsOnNewClassificationTagDetected:QualityMonitorNotificationsOnNewClassificationTagDetected",
                    "description": "Who to send notifications to when new data classification tags are detected.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QualityMonitorNotificationsOnFailure:QualityMonitorNotificationsOnFailure": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/QualityMonitorNotificationsOnNewClassificationTagDetected:QualityMonitorNotificationsOnNewClassificationTagDetected": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/QualityMonitorSchedule:QualityMonitorSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string"
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "string expression that determines when to run the monitor. See [Quartz documentation](https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for examples.\n"
                },
                "timezoneId": {
                    "type": "string",
                    "description": "string with timezone id (e.g., `PST`) in which to evaluate the Quartz expression.\n"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pauseStatus",
                        "quartzCronExpression",
                        "timezoneId"
                    ]
                }
            }
        },
        "databricks:index/QualityMonitorSnapshot:QualityMonitorSnapshot": {
            "type": "object"
        },
        "databricks:index/QualityMonitorTimeSeries:QualityMonitorTimeSeries": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "timestampCol"
            ]
        },
        "databricks:index/QueryParameter:QueryParameter": {
            "properties": {
                "dateRangeValue": {
                    "$ref": "#/types/databricks:index/QueryParameterDateRangeValue:QueryParameterDateRangeValue",
                    "description": "Date-range query parameter value. Consists of following attributes (Can only specify one of `dynamic_date_range_value` or `date_range_value`):\n"
                },
                "dateValue": {
                    "$ref": "#/types/databricks:index/QueryParameterDateValue:QueryParameterDateValue",
                    "description": "Date query parameter value. Consists of following attributes (Can only specify one of `dynamic_date_value` or `date_value`):\n"
                },
                "enumValue": {
                    "$ref": "#/types/databricks:index/QueryParameterEnumValue:QueryParameterEnumValue",
                    "description": "Dropdown parameter value. Consists of following attributes:\n"
                },
                "name": {
                    "type": "string",
                    "description": "Literal parameter marker that appears between double curly braces in the query text.\n"
                },
                "numericValue": {
                    "$ref": "#/types/databricks:index/QueryParameterNumericValue:QueryParameterNumericValue",
                    "description": "Numeric parameter value. Consists of following attributes:\n"
                },
                "queryBackedValue": {
                    "$ref": "#/types/databricks:index/QueryParameterQueryBackedValue:QueryParameterQueryBackedValue",
                    "description": "Query-based dropdown parameter value. Consists of following attributes:\n"
                },
                "textValue": {
                    "$ref": "#/types/databricks:index/QueryParameterTextValue:QueryParameterTextValue",
                    "description": "Text parameter value. Consists of following attributes:\n"
                },
                "title": {
                    "type": "string",
                    "description": "Text displayed in the user-facing parameter widget in the UI.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/QueryParameterDateRangeValue:QueryParameterDateRangeValue": {
            "properties": {
                "dateRangeValue": {
                    "$ref": "#/types/databricks:index/QueryParameterDateRangeValueDateRangeValue:QueryParameterDateRangeValueDateRangeValue",
                    "description": "Manually specified date-time range value.  Consists of the following attributes:\n"
                },
                "dynamicDateRangeValue": {
                    "type": "string",
                    "description": "Dynamic date-time range value based on current date-time.  Possible values are `TODAY`, `YESTERDAY`, `THIS_WEEK`, `THIS_MONTH`, `THIS_YEAR`, `LAST_WEEK`, `LAST_MONTH`, `LAST_YEAR`, `LAST_HOUR`, `LAST_8_HOURS`, `LAST_24_HOURS`, `LAST_7_DAYS`, `LAST_14_DAYS`, `LAST_30_DAYS`, `LAST_60_DAYS`, `LAST_90_DAYS`, `LAST_12_MONTHS`.\n"
                },
                "precision": {
                    "type": "string",
                    "description": "Date-time precision to format the value into when the query is run.  Possible values are `DAY_PRECISION`, `MINUTE_PRECISION`, `SECOND_PRECISION`.  Defaults to `DAY_PRECISION` (`YYYY-MM-DD`).\n"
                },
                "startDayOfWeek": {
                    "type": "integer",
                    "description": "Specify what day that starts the week.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QueryParameterDateRangeValueDateRangeValue:QueryParameterDateRangeValueDateRangeValue": {
            "properties": {
                "end": {
                    "type": "string",
                    "description": "end of the date range.\n"
                },
                "start": {
                    "type": "string",
                    "description": "begin of the date range.\n"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/QueryParameterDateValue:QueryParameterDateValue": {
            "properties": {
                "dateValue": {
                    "type": "string",
                    "description": "Manually specified date-time value\n"
                },
                "dynamicDateValue": {
                    "type": "string",
                    "description": "Dynamic date-time value based on current date-time.  Possible values are `NOW`, `YESTERDAY`.\n"
                },
                "precision": {
                    "type": "string",
                    "description": "Date-time precision to format the value into when the query is run.  Possible values are `DAY_PRECISION`, `MINUTE_PRECISION`, `SECOND_PRECISION`.  Defaults to `DAY_PRECISION` (`YYYY-MM-DD`).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QueryParameterEnumValue:QueryParameterEnumValue": {
            "properties": {
                "enumOptions": {
                    "type": "string",
                    "description": "List of valid query parameter values, newline delimited.\n"
                },
                "multiValuesOptions": {
                    "$ref": "#/types/databricks:index/QueryParameterEnumValueMultiValuesOptions:QueryParameterEnumValueMultiValuesOptions",
                    "description": "If specified, allows multiple values to be selected for this parameter. Consists of following attributes:\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of selected query parameter values.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QueryParameterEnumValueMultiValuesOptions:QueryParameterEnumValueMultiValuesOptions": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "description": "Character that prefixes each selected parameter value.\n"
                },
                "separator": {
                    "type": "string",
                    "description": "Character that separates each selected parameter value. Defaults to a comma.\n"
                },
                "suffix": {
                    "type": "string",
                    "description": "Character that suffixes each selected parameter value.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QueryParameterNumericValue:QueryParameterNumericValue": {
            "properties": {
                "value": {
                    "type": "number",
                    "description": "actual numeric value.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/QueryParameterQueryBackedValue:QueryParameterQueryBackedValue": {
            "properties": {
                "multiValuesOptions": {
                    "$ref": "#/types/databricks:index/QueryParameterQueryBackedValueMultiValuesOptions:QueryParameterQueryBackedValueMultiValuesOptions",
                    "description": "If specified, allows multiple values to be selected for this parameter. Consists of following attributes:\n"
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query that provides the parameter values.\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of selected query parameter values.\n"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/QueryParameterQueryBackedValueMultiValuesOptions:QueryParameterQueryBackedValueMultiValuesOptions": {
            "properties": {
                "prefix": {
                    "type": "string",
                    "description": "Character that prefixes each selected parameter value.\n"
                },
                "separator": {
                    "type": "string",
                    "description": "Character that separates each selected parameter value. Defaults to a comma.\n"
                },
                "suffix": {
                    "type": "string",
                    "description": "Character that suffixes each selected parameter value.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/QueryParameterTextValue:QueryParameterTextValue": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "actual text value.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/RecipientIpAccessList:RecipientIpAccessList": {
            "properties": {
                "allowedIpAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Allowed IP Addresses in CIDR notation. Limit of 100.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs": {
            "properties": {
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "a map of string key-value pairs with recipient's properties.  Properties with name starting with `databricks.` are reserved.\n"
                }
            },
            "type": "object",
            "required": [
                "properties"
            ]
        },
        "databricks:index/RecipientToken:RecipientToken": {
            "properties": {
                "activationUrl": {
                    "type": "string",
                    "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of recipient creator.\n"
                },
                "expirationTime": {
                    "type": "integer",
                    "description": "Expiration timestamp of the token in epoch milliseconds.\n"
                },
                "id": {
                    "type": "string",
                    "description": "Unique ID of the recipient token.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of recipient Token updater.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "activationUrl",
                        "createdAt",
                        "createdBy",
                        "expirationTime",
                        "id",
                        "updatedAt",
                        "updatedBy"
                    ]
                }
            }
        },
        "databricks:index/RepoSparseCheckout:RepoSparseCheckout": {
            "properties": {
                "patterns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.\n\nAddition or removal of the `sparse_checkout` configuration block will lead to recreation of the Git folder.\n"
                }
            },
            "type": "object",
            "required": [
                "patterns"
            ]
        },
        "databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins": {
            "properties": {
                "status": {
                    "type": "string",
                    "description": "The restrict workspace admins status for the workspace.\n"
                }
            },
            "type": "object",
            "required": [
                "status"
            ]
        },
        "databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata": {
            "properties": {
                "dnsName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "dnsName",
                "resourceId"
            ]
        },
        "databricks:index/ShareObject:ShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer"
                },
                "addedBy": {
                    "type": "string"
                },
                "cdfEnabled": {
                    "type": "boolean",
                    "description": "Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field `history_data_sharing_status` can not be set.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n"
                },
                "content": {
                    "type": "string"
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the data object, currently `TABLE`, `VIEW`, `SCHEMA`, `VOLUME`, and `MODEL` are supported.\n"
                },
                "historyDataSharingStatus": {
                    "type": "string",
                    "description": "Whether to enable history sharing, one of: `ENABLED`, `DISABLED`. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. *NOTE*: The start_version should be less than or equal the current version of the object. When this field is set, field `cdf_enabled` can not be set.\n\nTo share only part of a table when you add the table to a share, you can provide partition specifications. This is specified by a number of `partition` blocks. Each entry in `partition` block takes a list of `value` blocks. The field is documented below.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the object, e.g. `catalog.schema.name` for a tables, views, volumes and models, or `catalog.schema` for schemas.\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObjectPartition:ShareObjectPartition"
                    }
                },
                "sharedAs": {
                    "type": "string",
                    "description": "A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the `shared_as` name. The `shared_as` name must be unique within a Share. Change forces creation of a new resource.\n"
                },
                "startVersion": {
                    "type": "integer",
                    "description": "The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of the object, one of: `ACTIVE`, `PERMISSION_DENIED`.\n"
                },
                "stringSharedAs": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "dataObjectType",
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "addedAt",
                        "addedBy",
                        "dataObjectType",
                        "name",
                        "status"
                    ]
                }
            }
        },
        "databricks:index/ShareObjectPartition:ShareObjectPartition": {
            "properties": {
                "values": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObjectPartitionValue:ShareObjectPartitionValue"
                    },
                    "description": "The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipient_property_key` can not be set.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ShareObjectPartitionValue:ShareObjectPartitionValue": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the partition column.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The operator to apply for the value, one of: `EQUAL`, `LIKE`\n"
                },
                "recipientPropertyKey": {
                    "type": "string",
                    "description": "The key of a Delta Sharing recipient's property. For example `databricks-account-id`. When this field is set, field `value` can not be set.\n"
                },
                "value": {
                    "type": "string",
                    "description": "The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipient_property_key` can not be set.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "op"
            ]
        },
        "databricks:index/SqlAlertOptions:SqlAlertOptions": {
            "properties": {
                "column": {
                    "type": "string",
                    "description": "Name of column in the query result to compare in alert evaluation.\n"
                },
                "customBody": {
                    "type": "string",
                    "description": "Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "customSubject": {
                    "type": "string",
                    "description": "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "emptyResultState": {
                    "type": "string",
                    "description": "State that alert evaluates to when query result is empty.  Currently supported values are `unknown`, `triggered`, `ok` - check [API documentation](https://docs.databricks.com/api/workspace/alerts/create) for full list of supported values.\n"
                },
                "muted": {
                    "type": "boolean",
                    "description": "Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.\n"
                },
                "op": {
                    "type": "string",
                    "description": "Operator used to compare in alert evaluation. (Enum: `\u003e`, `\u003e=`, `\u003c`, `\u003c=`, `==`, `!=`)\n"
                },
                "value": {
                    "type": "string",
                    "description": "Value used to compare in alert evaluation.\n"
                }
            },
            "type": "object",
            "required": [
                "column",
                "op",
                "value"
            ]
        },
        "databricks:index/SqlEndpointChannel:SqlEndpointChannel": {
            "properties": {
                "dbsqlVersion": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointHealth:SqlEndpointHealth": {
            "properties": {
                "details": {
                    "type": "string"
                },
                "failureReason": {
                    "$ref": "#/types/databricks:index/SqlEndpointHealthFailureReason:SqlEndpointHealthFailureReason"
                },
                "message": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "summary": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointHealthFailureReason:SqlEndpointHealthFailureReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams": {
            "properties": {
                "hostname": {
                    "type": "string"
                },
                "path": {
                    "type": "string"
                },
                "port": {
                    "type": "integer"
                },
                "protocol": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointTags:SqlEndpointTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment": {
            "properties": {
                "principal": {
                    "type": "string",
                    "description": "`display_name` for a databricks.Group or databricks_user, `application_id` for a databricks_service_principal.\n"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/SqlQueryParameter:SqlQueryParameter": {
            "properties": {
                "date": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDate:SqlQueryParameterDate"
                },
                "dateRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange"
                },
                "datetime": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime"
                },
                "datetimeRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange"
                },
                "datetimesec": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec"
                },
                "datetimesecRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange"
                },
                "enum": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum"
                },
                "name": {
                    "type": "string",
                    "description": "The literal parameter marker that appears between double curly braces in the query text.\nParameters can have several different types. Type is specified using one of the following configuration blocks: `text`, `number`, `enum`, `query`, `date`, `datetime`, `datetimesec`, `date_range`, `datetime_range`, `datetimesec_range`.\n\nFor `text`, `number`, `date`, `datetime`, `datetimesec` block\n"
                },
                "number": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber"
                },
                "query": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery",
                    "description": "The text of the query to be run.\n"
                },
                "text": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterText:SqlQueryParameterText"
                },
                "title": {
                    "type": "string",
                    "description": "The text displayed in a parameter picking widget.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/SqlQueryParameterDate:SqlQueryParameterDate": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange"
                },
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange"
                },
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange"
                },
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple"
                },
                "options": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "options"
            ]
        },
        "databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple": {
            "properties": {
                "prefix": {
                    "type": "string"
                },
                "separator": {
                    "type": "string"
                },
                "suffix": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "separator"
            ]
        },
        "databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber": {
            "properties": {
                "value": {
                    "type": "number",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple"
                },
                "queryId": {
                    "type": "string"
                },
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple": {
            "properties": {
                "prefix": {
                    "type": "string"
                },
                "separator": {
                    "type": "string"
                },
                "suffix": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "separator"
            ]
        },
        "databricks:index/SqlQueryParameterText:SqlQueryParameterText": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The default value for this parameter.\n"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQuerySchedule:SqlQuerySchedule": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous"
                },
                "daily": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily"
                },
                "weekly": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous": {
            "properties": {
                "intervalSeconds": {
                    "type": "integer"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "intervalSeconds"
            ]
        },
        "databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily": {
            "properties": {
                "intervalDays": {
                    "type": "integer"
                },
                "timeOfDay": {
                    "type": "string"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "intervalDays",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly": {
            "properties": {
                "dayOfWeek": {
                    "type": "string"
                },
                "intervalWeeks": {
                    "type": "integer"
                },
                "timeOfDay": {
                    "type": "string"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "dayOfWeek",
                "intervalWeeks",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlTableColumn:SqlTableColumn": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "identity": {
                    "type": "string",
                    "description": "Whether the field is an identity column. Can be `default`, `always`, or unset. It is unset by default.\n"
                },
                "name": {
                    "type": "string",
                    "description": "User-visible name of column\n"
                },
                "nullable": {
                    "type": "boolean",
                    "description": "Whether field is nullable (Default: `true`)\n"
                },
                "type": {
                    "type": "string",
                    "description": "Column type spec (with metadata) as SQL text. Not supported for `VIEW` table_type.\n"
                },
                "typeJson": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "name",
                        "type",
                        "typeJson"
                    ]
                }
            }
        },
        "databricks:index/SqlWidgetParameter:SqlWidgetParameter": {
            "properties": {
                "mapTo": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "type"
            ]
        },
        "databricks:index/SqlWidgetPosition:SqlWidgetPosition": {
            "properties": {
                "autoHeight": {
                    "type": "boolean"
                },
                "posX": {
                    "type": "integer"
                },
                "posY": {
                    "type": "integer"
                },
                "sizeX": {
                    "type": "integer"
                },
                "sizeY": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "sizeX",
                "sizeY"
            ]
        },
        "databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n\n`azure_managed_identity` optional configuration block for using managed identity as credential details for Azure (recommended over service principal):\n"
                },
                "unityCatalogIamArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "externalId",
                        "roleArn",
                        "unityCatalogIamArn"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.\n"
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.\n\n`databricks_gcp_service_account` optional configuration block for creating a Databricks-managed GCP Service Account:\n"
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "accessConnectorId",
                        "credentialId"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n"
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n",
                    "secret": true
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n"
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/StorageCredentialCloudflareApiToken:StorageCredentialCloudflareApiToken": {
            "properties": {
                "accessKeyId": {
                    "type": "string",
                    "description": "R2 API token access key ID\n"
                },
                "accountId": {
                    "type": "string",
                    "description": "R2 account ID\n"
                },
                "secretAccessKey": {
                    "type": "string",
                    "description": "R2 API token secret access key\n\n`azure_service_principal` optional configuration block to use service principal as credential details for Azure (Legacy):\n",
                    "secret": true
                }
            },
            "type": "object",
            "required": [
                "accessKeyId",
                "accountId",
                "secretAccessKey"
            ]
        },
        "databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n\n`cloudflare_api_token` optional configuration block for using a Cloudflare API Token as credential details. This requires account admin access:\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "credentialId",
                        "email"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n\n`cloudflare_api_token` optional configuration block for using a Cloudflare API Token as credential details. This requires account admin access:\n"
                },
                "privateKey": {
                    "type": "string",
                    "secret": true
                },
                "privateKeyId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/TableColumn:TableColumn": {
            "properties": {
                "comment": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "nullable": {
                    "type": "boolean"
                },
                "partitionIndex": {
                    "type": "integer"
                },
                "position": {
                    "type": "integer"
                },
                "typeIntervalType": {
                    "type": "string"
                },
                "typeJson": {
                    "type": "string"
                },
                "typeName": {
                    "type": "string"
                },
                "typePrecision": {
                    "type": "integer"
                },
                "typeScale": {
                    "type": "integer"
                },
                "typeText": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Additional status message.\n"
                },
                "state": {
                    "type": "string",
                    "description": "Current state of the endpoint. Currently following values are supported: `PROVISIONING`, `ONLINE`, and `OFFLINE`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec": {
            "properties": {
                "embeddingSourceColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding source.  Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "embeddingVectorColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding vectors. Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "embeddingWritebackTable": {
                    "type": "string",
                    "description": "Automatically sync the vector index contents and computed embeddings to the specified Delta table. The only supported table name is the index name with the suffix `_writeback_table`.\n",
                    "willReplaceOnChanges": true
                },
                "pipelineId": {
                    "type": "string",
                    "description": "ID of the associated Delta Live Table pipeline.\n"
                },
                "pipelineType": {
                    "type": "string",
                    "description": "Pipeline execution mode. Possible values are:\n* `TRIGGERED`: If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started.\n* `CONTINUOUS`: If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep the vector index fresh.\n",
                    "willReplaceOnChanges": true
                },
                "sourceTable": {
                    "type": "string",
                    "description": "The name of the source table.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pipelineId"
                    ]
                }
            }
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn": {
            "properties": {
                "embeddingModelEndpointName": {
                    "type": "string",
                    "description": "The name of the embedding model endpoint\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn": {
            "properties": {
                "embeddingDimension": {
                    "type": "integer",
                    "description": "Dimension of the embedding vector.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec": {
            "properties": {
                "embeddingSourceColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding source.  Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "embeddingVectorColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding vectors. Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "schemaJson": {
                    "type": "string",
                    "description": "The schema of the index in JSON format.  Check the [API documentation](https://docs.databricks.com/api/workspace/vectorsearchindexes/createindex#direct_access_index_spec-schema_json) for a list of supported data types.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn": {
            "properties": {
                "embeddingModelEndpointName": {
                    "type": "string",
                    "description": "The name of the embedding model endpoint\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn": {
            "properties": {
                "embeddingDimension": {
                    "type": "integer",
                    "description": "Dimension of the embedding vector.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus": {
            "properties": {
                "indexUrl": {
                    "type": "string",
                    "description": "Index API Url to be used to perform operations on the index\n"
                },
                "indexedRowCount": {
                    "type": "integer",
                    "description": "Number of rows indexed\n"
                },
                "message": {
                    "type": "string",
                    "description": "Message associated with the index status\n"
                },
                "ready": {
                    "type": "boolean",
                    "description": "Whether the index is ready for search\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getAppApp:getAppApp": {
            "properties": {
                "activeDeployment": {
                    "$ref": "#/types/databricks:index/getAppAppActiveDeployment:getAppAppActiveDeployment"
                },
                "appStatus": {
                    "$ref": "#/types/databricks:index/getAppAppAppStatus:getAppAppAppStatus",
                    "description": "attribute\n"
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The Budget Policy ID set for this resource.\n"
                },
                "computeStatus": {
                    "$ref": "#/types/databricks:index/getAppAppComputeStatus:getAppAppComputeStatus",
                    "description": "attribute\n"
                },
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "defaultSourceCodePath": {
                    "type": "string",
                    "description": "The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.\n"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the resource.\n"
                },
                "effectiveBudgetPolicyId": {
                    "type": "string",
                    "description": "The effective budget policy ID.\n"
                },
                "effectiveUserApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the app.\n"
                },
                "oauth2AppClientId": {
                    "type": "string"
                },
                "oauth2AppIntegrationId": {
                    "type": "string"
                },
                "pendingDeployment": {
                    "$ref": "#/types/databricks:index/getAppAppPendingDeployment:getAppAppPendingDeployment"
                },
                "resources": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getAppAppResource:getAppAppResource"
                    },
                    "description": "A list of resources that the app have access to.\n"
                },
                "servicePrincipalClientId": {
                    "type": "string"
                },
                "servicePrincipalId": {
                    "type": "integer",
                    "description": "id of the app service principal\n"
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "name of the app service principal\n"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                },
                "updater": {
                    "type": "string",
                    "description": "The email of the user that last updated the app.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the app once it is deployed.\n"
                },
                "userApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "activeDeployment",
                "appStatus",
                "computeStatus",
                "createTime",
                "creator",
                "defaultSourceCodePath",
                "effectiveBudgetPolicyId",
                "effectiveUserApiScopes",
                "id",
                "name",
                "oauth2AppClientId",
                "oauth2AppIntegrationId",
                "pendingDeployment",
                "servicePrincipalClientId",
                "servicePrincipalId",
                "servicePrincipalName",
                "updateTime",
                "updater",
                "url"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppActiveDeployment:getAppAppActiveDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/getAppAppActiveDeploymentDeploymentArtifacts:getAppAppActiveDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/getAppAppActiveDeploymentStatus:getAppAppActiveDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "required": [
                "createTime",
                "creator",
                "deploymentArtifacts",
                "status",
                "updateTime"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppActiveDeploymentDeploymentArtifacts:getAppAppActiveDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getAppAppActiveDeploymentStatus:getAppAppActiveDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppAppStatus:getAppAppAppStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppComputeStatus:getAppAppComputeStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppPendingDeployment:getAppAppPendingDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/getAppAppPendingDeploymentDeploymentArtifacts:getAppAppPendingDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/getAppAppPendingDeploymentStatus:getAppAppPendingDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "required": [
                "createTime",
                "creator",
                "deploymentArtifacts",
                "status",
                "updateTime"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppPendingDeploymentDeploymentArtifacts:getAppAppPendingDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getAppAppPendingDeploymentStatus:getAppAppPendingDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppResource:getAppAppResource": {
            "properties": {
                "description": {
                    "type": "string",
                    "description": "The description of the resource.\n"
                },
                "job": {
                    "$ref": "#/types/databricks:index/getAppAppResourceJob:getAppAppResourceJob",
                    "description": "attribute\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the app.\n"
                },
                "secret": {
                    "$ref": "#/types/databricks:index/getAppAppResourceSecret:getAppAppResourceSecret",
                    "description": "attribute\n"
                },
                "servingEndpoint": {
                    "$ref": "#/types/databricks:index/getAppAppResourceServingEndpoint:getAppAppResourceServingEndpoint",
                    "description": "attribute\n"
                },
                "sqlWarehouse": {
                    "$ref": "#/types/databricks:index/getAppAppResourceSqlWarehouse:getAppAppResourceSqlWarehouse",
                    "description": "attribute\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppResourceJob:getAppAppResourceJob": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppResourceSecret:getAppAppResourceSecret": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "Key of the secret to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                },
                "scope": {
                    "type": "string",
                    "description": "Scope of the secret to grant permission on.\n"
                }
            },
            "type": "object",
            "required": [
                "key",
                "permission",
                "scope"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppResourceServingEndpoint:getAppAppResourceServingEndpoint": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the app.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppAppResourceSqlWarehouse:getAppAppResourceSqlWarehouse": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsApp:getAppsApp": {
            "properties": {
                "activeDeployment": {
                    "$ref": "#/types/databricks:index/getAppsAppActiveDeployment:getAppsAppActiveDeployment"
                },
                "appStatus": {
                    "$ref": "#/types/databricks:index/getAppsAppAppStatus:getAppsAppAppStatus",
                    "description": "attribute\n"
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The Budget Policy ID set for this resource.\n"
                },
                "computeStatus": {
                    "$ref": "#/types/databricks:index/getAppsAppComputeStatus:getAppsAppComputeStatus",
                    "description": "attribute\n"
                },
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "defaultSourceCodePath": {
                    "type": "string",
                    "description": "The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.\n"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the resource.\n"
                },
                "effectiveBudgetPolicyId": {
                    "type": "string",
                    "description": "The effective budget policy ID.\n"
                },
                "effectiveUserApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the serving endpoint to grant permission on.\n"
                },
                "oauth2AppClientId": {
                    "type": "string"
                },
                "oauth2AppIntegrationId": {
                    "type": "string"
                },
                "pendingDeployment": {
                    "$ref": "#/types/databricks:index/getAppsAppPendingDeployment:getAppsAppPendingDeployment"
                },
                "resources": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getAppsAppResource:getAppsAppResource"
                    },
                    "description": "A list of resources that the app have access to.\n"
                },
                "servicePrincipalClientId": {
                    "type": "string"
                },
                "servicePrincipalId": {
                    "type": "integer",
                    "description": "id of the app service principal\n"
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "name of the app service principal\n"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                },
                "updater": {
                    "type": "string",
                    "description": "The email of the user that last updated the app.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the app once it is deployed.\n"
                },
                "userApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "activeDeployment",
                "appStatus",
                "computeStatus",
                "createTime",
                "creator",
                "defaultSourceCodePath",
                "effectiveBudgetPolicyId",
                "effectiveUserApiScopes",
                "id",
                "name",
                "oauth2AppClientId",
                "oauth2AppIntegrationId",
                "pendingDeployment",
                "servicePrincipalClientId",
                "servicePrincipalId",
                "servicePrincipalName",
                "updateTime",
                "updater",
                "url"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppActiveDeployment:getAppsAppActiveDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/getAppsAppActiveDeploymentDeploymentArtifacts:getAppsAppActiveDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/getAppsAppActiveDeploymentStatus:getAppsAppActiveDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "required": [
                "createTime",
                "creator",
                "deploymentArtifacts",
                "status",
                "updateTime"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppActiveDeploymentDeploymentArtifacts:getAppsAppActiveDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getAppsAppActiveDeploymentStatus:getAppsAppActiveDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppAppStatus:getAppsAppAppStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppComputeStatus:getAppsAppComputeStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppPendingDeployment:getAppsAppPendingDeployment": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "deploymentArtifacts": {
                    "$ref": "#/types/databricks:index/getAppsAppPendingDeploymentDeploymentArtifacts:getAppsAppPendingDeploymentDeploymentArtifacts"
                },
                "deploymentId": {
                    "type": "string"
                },
                "mode": {
                    "type": "string"
                },
                "sourceCodePath": {
                    "type": "string"
                },
                "status": {
                    "$ref": "#/types/databricks:index/getAppsAppPendingDeploymentStatus:getAppsAppPendingDeploymentStatus"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                }
            },
            "type": "object",
            "required": [
                "createTime",
                "creator",
                "deploymentArtifacts",
                "status",
                "updateTime"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppPendingDeploymentDeploymentArtifacts:getAppsAppPendingDeploymentDeploymentArtifacts": {
            "properties": {
                "sourceCodePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getAppsAppPendingDeploymentStatus:getAppsAppPendingDeploymentStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Application status message\n"
                },
                "state": {
                    "type": "string",
                    "description": "State of the application.\n"
                }
            },
            "type": "object",
            "required": [
                "message",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppResource:getAppsAppResource": {
            "properties": {
                "description": {
                    "type": "string",
                    "description": "The description of the resource.\n"
                },
                "job": {
                    "$ref": "#/types/databricks:index/getAppsAppResourceJob:getAppsAppResourceJob",
                    "description": "attribute\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the serving endpoint to grant permission on.\n"
                },
                "secret": {
                    "$ref": "#/types/databricks:index/getAppsAppResourceSecret:getAppsAppResourceSecret",
                    "description": "attribute\n"
                },
                "servingEndpoint": {
                    "$ref": "#/types/databricks:index/getAppsAppResourceServingEndpoint:getAppsAppResourceServingEndpoint",
                    "description": "attribute\n"
                },
                "sqlWarehouse": {
                    "$ref": "#/types/databricks:index/getAppsAppResourceSqlWarehouse:getAppsAppResourceSqlWarehouse",
                    "description": "attribute\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppResourceJob:getAppsAppResourceJob": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppResourceSecret:getAppsAppResourceSecret": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "Key of the secret to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                },
                "scope": {
                    "type": "string",
                    "description": "Scope of the secret to grant permission on.\n"
                }
            },
            "type": "object",
            "required": [
                "key",
                "permission",
                "scope"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppResourceServingEndpoint:getAppsAppResourceServingEndpoint": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Name of the serving endpoint to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getAppsAppResourceSqlWarehouse:getAppsAppResourceSqlWarehouse": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "Id of the job to grant permission on.\n"
                },
                "permission": {
                    "type": "string",
                    "description": "Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.\n"
                }
            },
            "type": "object",
            "required": [
                "id",
                "permission"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getBudgetPoliciesBudgetPolicy:getBudgetPoliciesBudgetPolicy": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getBudgetPoliciesBudgetPolicyCustomTag:getBudgetPoliciesBudgetPolicyCustomTag"
                    }
                },
                "policyId": {
                    "type": "string"
                },
                "policyName": {
                    "type": "string",
                    "description": "The partial name of policies to be filtered on. If unspecified, all policies will be returned.\n"
                }
            },
            "type": "object",
            "required": [
                "policyId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getBudgetPoliciesBudgetPolicyCustomTag:getBudgetPoliciesBudgetPolicyCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getBudgetPolicyCustomTag:getBudgetPolicyCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/getCatalogCatalogInfo:getCatalogCatalogInfo": {
            "properties": {
                "browseOnly": {
                    "type": "boolean"
                },
                "catalogType": {
                    "type": "string",
                    "description": "Type of the catalog, e.g. `MANAGED_CATALOG`, `DELTASHARING_CATALOG`, `SYSTEM_CATALOG`,\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text description\n"
                },
                "connectionName": {
                    "type": "string",
                    "description": "The name of the connection to an external data source.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of catalog creator.\n"
                },
                "effectivePredictiveOptimizationFlag": {
                    "$ref": "#/types/databricks:index/getCatalogCatalogInfoEffectivePredictiveOptimizationFlag:getCatalogCatalogInfoEffectivePredictiveOptimizationFlag",
                    "description": "object describing applied predictive optimization flag.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it.\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "The full name of the catalog. Corresponds with the name field.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the current securable is accessible from all workspaces or a  specific set of workspaces.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "name of the catalog\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "A map of key-value properties attached to the securable.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Current owner of the catalog\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "A map of key-value properties attached to the securable.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "The name of delta sharing provider.\n"
                },
                "provisioningInfo": {
                    "$ref": "#/types/databricks:index/getCatalogCatalogInfoProvisioningInfo:getCatalogCatalogInfoProvisioningInfo"
                },
                "securableType": {
                    "type": "string",
                    "description": "Securable type.\n"
                },
                "shareName": {
                    "type": "string",
                    "description": "The name of the share under the share provider.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "Storage Location URL (full path) for managed tables within catalog.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Storage root URL for managed tables within catalog.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was last modified, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified catalog.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getCatalogCatalogInfoEffectivePredictiveOptimizationFlag:getCatalogCatalogInfoEffectivePredictiveOptimizationFlag": {
            "properties": {
                "inheritedFromName": {
                    "type": "string"
                },
                "inheritedFromType": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/getCatalogCatalogInfoProvisioningInfo:getCatalogCatalogInfoProvisioningInfo": {
            "properties": {
                "state": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfo:getClusterClusterInfo": {
            "properties": {
                "autoscale": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes"
                },
                "clusterCores": {
                    "type": "number"
                },
                "clusterId": {
                    "type": "string",
                    "description": "The id of the cluster.\n"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf"
                },
                "clusterLogStatus": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus"
                },
                "clusterMemoryMb": {
                    "type": "integer"
                },
                "clusterName": {
                    "type": "string",
                    "description": "The exact name of the cluster to search. Can only be specified if there is exactly one cluster with the provided name.\n"
                },
                "clusterSource": {
                    "type": "string"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Additional tags for cluster resources.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage"
                },
                "driver": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "Use autoscaling local storage.\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Enable local disk encryption.\n"
                },
                "executors": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor"
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "The pool of idle instances the cluster is attached to.\n"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "jdbcPort": {
                    "type": "integer"
                },
                "kind": {
                    "type": "string"
                },
                "lastRestartedTime": {
                    "type": "integer"
                },
                "lastStateLossTime": {
                    "type": "integer"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id.\n"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime of the cluster\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters.\n"
                },
                "sparkContextId": {
                    "type": "integer"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.\n"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpec:getClusterClusterInfoSpec"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster.\n"
                },
                "startTime": {
                    "type": "integer"
                },
                "state": {
                    "type": "string"
                },
                "stateMessage": {
                    "type": "string"
                },
                "terminatedTime": {
                    "type": "integer"
                },
                "terminationReason": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason"
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoWorkloadType:getClusterClusterInfoWorkloadType"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAzureAttributesLogAnalyticsInfo:getClusterClusterInfoAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAzureAttributesLogAnalyticsInfo:getClusterClusterInfoAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfVolumes:getClusterClusterInfoClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogConfVolumes:getClusterClusterInfoClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus": {
            "properties": {
                "lastAttempted": {
                    "type": "integer"
                },
                "lastException": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string"
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string"
                },
                "instanceId": {
                    "type": "string"
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes"
                },
                "nodeId": {
                    "type": "string"
                },
                "privateIp": {
                    "type": "string"
                },
                "publicDns": {
                    "type": "string"
                },
                "startTimestamp": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string"
                },
                "instanceId": {
                    "type": "string"
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes"
                },
                "nodeId": {
                    "type": "string"
                },
                "privateIp": {
                    "type": "string"
                },
                "publicDns": {
                    "type": "string"
                },
                "startTimestamp": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptAbfss:getClusterClusterInfoInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptFile:getClusterClusterInfoInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptGcs:getClusterClusterInfoInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptVolumes:getClusterClusterInfoInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptWorkspace:getClusterClusterInfoInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScriptAbfss:getClusterClusterInfoInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptFile:getClusterClusterInfoInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptGcs:getClusterClusterInfoInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptVolumes:getClusterClusterInfoInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptWorkspace:getClusterClusterInfoInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpec:getClusterClusterInfoSpec": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecAutoscale:getClusterClusterInfoSpecAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecAwsAttributes:getClusterClusterInfoSpecAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecAzureAttributes:getClusterClusterInfoSpecAzureAttributes"
                },
                "clusterId": {
                    "type": "string",
                    "description": "The id of the cluster.\n"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterLogConf:getClusterClusterInfoSpecClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterMountInfo:getClusterClusterInfoSpecClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "The exact name of the cluster to search. Can only be specified if there is exactly one cluster with the provided name.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Additional tags for cluster resources.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecDockerImage:getClusterClusterInfoSpecDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "Use autoscaling local storage.\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Enable local disk encryption.\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecGcpAttributes:getClusterClusterInfoSpecGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests.\n"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScript:getClusterClusterInfoSpecInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "The pool of idle instances the cluster is attached to.\n"
                },
                "isSingleNode": {
                    "type": "boolean"
                },
                "kind": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoSpecLibrary:getClusterClusterInfoSpecLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id.\n"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime of the cluster\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters.\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster.\n"
                },
                "useMlRuntime": {
                    "type": "boolean"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecWorkloadType:getClusterClusterInfoSpecWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "clusterId",
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getClusterClusterInfoSpecAutoscale:getClusterClusterInfoSpecAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecAwsAttributes:getClusterClusterInfoSpecAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecAzureAttributes:getClusterClusterInfoSpecAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo:getClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo:getClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecClusterLogConf:getClusterClusterInfoSpecClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterLogConfDbfs:getClusterClusterInfoSpecClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterLogConfS3:getClusterClusterInfoSpecClusterLogConfS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterLogConfVolumes:getClusterClusterInfoSpecClusterLogConfVolumes"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecClusterLogConfDbfs:getClusterClusterInfoSpecClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecClusterLogConfS3:getClusterClusterInfoSpecClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecClusterLogConfVolumes:getClusterClusterInfoSpecClusterLogConfVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecClusterMountInfo:getClusterClusterInfoSpecClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo:getClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo:getClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecDockerImage:getClusterClusterInfoSpecDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecDockerImageBasicAuth:getClusterClusterInfoSpecDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecDockerImageBasicAuth:getClusterClusterInfoSpecDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecGcpAttributes:getClusterClusterInfoSpecGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecInitScript:getClusterClusterInfoSpecInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptAbfss:getClusterClusterInfoSpecInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptDbfs:getClusterClusterInfoSpecInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptFile:getClusterClusterInfoSpecInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptGcs:getClusterClusterInfoSpecInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptS3:getClusterClusterInfoSpecInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptVolumes:getClusterClusterInfoSpecInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecInitScriptWorkspace:getClusterClusterInfoSpecInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptAbfss:getClusterClusterInfoSpecInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptDbfs:getClusterClusterInfoSpecInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptFile:getClusterClusterInfoSpecInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptGcs:getClusterClusterInfoSpecInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptS3:getClusterClusterInfoSpecInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptVolumes:getClusterClusterInfoSpecInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecInitScriptWorkspace:getClusterClusterInfoSpecInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecLibrary:getClusterClusterInfoSpecLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecLibraryCran:getClusterClusterInfoSpecLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecLibraryMaven:getClusterClusterInfoSpecLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecLibraryPypi:getClusterClusterInfoSpecLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoSpecLibraryCran:getClusterClusterInfoSpecLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecLibraryMaven:getClusterClusterInfoSpecLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecLibraryPypi:getClusterClusterInfoSpecLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecWorkloadType:getClusterClusterInfoSpecWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoSpecWorkloadTypeClients:getClusterClusterInfoSpecWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getClusterClusterInfoSpecWorkloadTypeClients:getClusterClusterInfoSpecWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoWorkloadType:getClusterClusterInfoWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoWorkloadTypeClients:getClusterClusterInfoWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getClusterClusterInfoWorkloadTypeClients:getClusterClusterInfoWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClustersFilterBy:getClustersFilterBy": {
            "properties": {
                "clusterSources": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of cluster sources to filter by. Possible values are `API`, `JOB`, `MODELS`, `PIPELINE`, `PIPELINE_MAINTENANCE`, `SQL`, and `UI`.\n"
                },
                "clusterStates": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of cluster states to filter by. Possible values are `RUNNING`, `PENDING`, `RESIZING`, `RESTARTING`, `TERMINATING`, `TERMINATED`, `ERROR`, and `UNKNOWN`.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "Whether to filter by pinned clusters.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Filter by databricks.ClusterPolicy id.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo": {
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Timestamp (in milliseconds) when the current metastore was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "the ID of the identity that created the current metastore.\n"
                },
                "defaultDataAccessConfigId": {
                    "type": "string",
                    "description": "the ID of the default data access configuration.\n"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "the expiration duration in seconds on recipient data access tokens.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "externalAccessEnabled": {
                    "type": "boolean"
                },
                "globalMetastoreId": {
                    "type": "string",
                    "description": "Identifier in form of `\u003ccloud\u003e:\u003cregion\u003e:\u003cmetastore_id\u003e` for use in Databricks to Databricks Delta Sharing.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Metastore ID.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/group name/sp application_id of the metastore owner.\n"
                },
                "privilegeModelVersion": {
                    "type": "string",
                    "description": "the version of the privilege model used by the metastore.\n"
                },
                "region": {
                    "type": "string",
                    "description": "(Mandatory for account-level) The region of the metastore.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored.\n"
                },
                "storageRootCredentialId": {
                    "type": "string",
                    "description": "ID of a storage credential used for the `storage_root`.\n"
                },
                "storageRootCredentialName": {
                    "type": "string",
                    "description": "Name of a storage credential used for the `storage_root`.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Timestamp (in milliseconds) when the current metastore was updated.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the ID of the identity that updated the current metastore.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getDashboardsDashboard:getDashboardsDashboard": {
            "properties": {
                "createTime": {
                    "type": "string",
                    "description": "The timestamp of when the dashboard was created.\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "The unique ID of the dashboard.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the dashboard.\n"
                },
                "etag": {
                    "type": "string"
                },
                "lifecycleState": {
                    "type": "string"
                },
                "parentPath": {
                    "type": "string"
                },
                "path": {
                    "type": "string"
                },
                "serializedDashboard": {
                    "type": "string"
                },
                "updateTime": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "createTime",
                "dashboardId",
                "etag",
                "lifecycleState",
                "parentPath",
                "path",
                "updateTime"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList": {
            "properties": {
                "fileSize": {
                    "type": "integer"
                },
                "path": {
                    "type": "string",
                    "description": "Path on DBFS for the file to perform listing\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo": {
            "properties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied comment.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of catalog creator.\n"
                },
                "credentialId": {
                    "type": "string",
                    "description": "Unique ID of storage credential.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfoEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "fallback": {
                    "type": "boolean"
                },
                "isolationMode": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the external location\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was last modified, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified catalog.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfoEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunction:getFunctionsFunction": {
            "properties": {
                "browseOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the principal is limited to retrieving metadata for the associated object through the `BROWSE` privilege when `include_browse` is enabled in the request.\n"
                },
                "catalogName": {
                    "type": "string",
                    "description": "Name of databricks_catalog.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-provided free-form text description.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this function was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of function creator.\n"
                },
                "dataType": {
                    "type": "string",
                    "description": "Scalar function return data type.\n"
                },
                "externalLanguage": {
                    "type": "string",
                    "description": "External function language.\n"
                },
                "externalName": {
                    "type": "string",
                    "description": "External function name.\n"
                },
                "fullDataType": {
                    "type": "string",
                    "description": "Pretty printed function data type.\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "Full name of function, in form of catalog_name.schema_name.function__name\n"
                },
                "functionId": {
                    "type": "string",
                    "description": "Id of Function, relative to parent schema.\n"
                },
                "inputParams": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionInputParam:getFunctionsFunctionInputParam"
                    },
                    "description": "object describing input parameters. Consists of the single attribute:\n"
                },
                "isDeterministic": {
                    "type": "boolean",
                    "description": "Boolean flag specifying whether the function is deterministic.\n"
                },
                "isNullCall": {
                    "type": "boolean",
                    "description": "Boolean flag whether function null call.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of parameter.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username of current owner of function.\n"
                },
                "parameterStyle": {
                    "type": "string",
                    "description": "Function parameter style. `S` is the value for SQL.\n"
                },
                "properties": {
                    "type": "string",
                    "description": "JSON-serialized key-value pair map, encoded (escaped) as a string.\n"
                },
                "returnParams": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionReturnParam:getFunctionsFunctionReturnParam"
                    },
                    "description": "Table function return parameters.  See `input_params` for description.\n"
                },
                "routineBody": {
                    "type": "string",
                    "description": "Function language (`SQL` or `EXTERNAL`). When `EXTERNAL` is used, the language of the routine function should be specified in the `external_language` field, and the `return_params` of the function cannot be used (as `TABLE` return type is not supported), and the `sql_data_access` field must be `NO_SQL`.\n"
                },
                "routineDefinition": {
                    "type": "string",
                    "description": "Function body.\n"
                },
                "routineDependencies": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionRoutineDependency:getFunctionsFunctionRoutineDependency"
                    },
                    "description": "Function dependencies.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of databricks_schema.\n"
                },
                "securityType": {
                    "type": "string",
                    "description": "Function security type. (Enum: `DEFINER`).\n"
                },
                "specificName": {
                    "type": "string",
                    "description": "Specific name of the function; Reserved for future use.\n"
                },
                "sqlDataAccess": {
                    "type": "string",
                    "description": "Function SQL data access (`CONTAINS_SQL`, `READS_SQL_DATA`, `NO_SQL`).\n"
                },
                "sqlPath": {
                    "type": "string",
                    "description": "List of schemes whose objects can be referenced without qualification.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this function was created, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified function.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunctionInputParam:getFunctionsFunctionInputParam": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionInputParamParameter:getFunctionsFunctionInputParamParameter"
                    },
                    "description": "The array of definitions of the function's parameters:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunctionInputParamParameter:getFunctionsFunctionInputParamParameter": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-provided free-form text description.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of parameter.\n"
                },
                "parameterDefault": {
                    "type": "string",
                    "description": "Default value of the parameter.\n"
                },
                "parameterMode": {
                    "type": "string",
                    "description": "The mode of the function parameter.\n"
                },
                "parameterType": {
                    "type": "string",
                    "description": "The type of function parameter (`PARAM` or `COLUMN`).\n"
                },
                "position": {
                    "type": "integer",
                    "description": "Ordinal position of column (starting at position 0).\n"
                },
                "typeIntervalType": {
                    "type": "string",
                    "description": "Format of IntervalType.\n"
                },
                "typeJson": {
                    "type": "string",
                    "description": "Full data type spec, JSON-serialized.\n"
                },
                "typeName": {
                    "type": "string",
                    "description": "Name of type (INT, STRUCT, MAP, etc.).\n"
                },
                "typePrecision": {
                    "type": "integer",
                    "description": "Digits of precision; required on Create for DecimalTypes.\n"
                },
                "typeScale": {
                    "type": "integer",
                    "description": "Digits to right of decimal; Required on Create for DecimalTypes.\n"
                },
                "typeText": {
                    "type": "string",
                    "description": "Full data type spec, SQL/catalogString text.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/getFunctionsFunctionReturnParam:getFunctionsFunctionReturnParam": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionReturnParamParameter:getFunctionsFunctionReturnParamParameter"
                    },
                    "description": "The array of definitions of the function's parameters:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunctionReturnParamParameter:getFunctionsFunctionReturnParamParameter": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-provided free-form text description.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of parameter.\n"
                },
                "parameterDefault": {
                    "type": "string",
                    "description": "Default value of the parameter.\n"
                },
                "parameterMode": {
                    "type": "string",
                    "description": "The mode of the function parameter.\n"
                },
                "parameterType": {
                    "type": "string",
                    "description": "The type of function parameter (`PARAM` or `COLUMN`).\n"
                },
                "position": {
                    "type": "integer",
                    "description": "Ordinal position of column (starting at position 0).\n"
                },
                "typeIntervalType": {
                    "type": "string",
                    "description": "Format of IntervalType.\n"
                },
                "typeJson": {
                    "type": "string",
                    "description": "Full data type spec, JSON-serialized.\n"
                },
                "typeName": {
                    "type": "string",
                    "description": "Name of type (INT, STRUCT, MAP, etc.).\n"
                },
                "typePrecision": {
                    "type": "integer",
                    "description": "Digits of precision; required on Create for DecimalTypes.\n"
                },
                "typeScale": {
                    "type": "integer",
                    "description": "Digits to right of decimal; Required on Create for DecimalTypes.\n"
                },
                "typeText": {
                    "type": "string",
                    "description": "Full data type spec, SQL/catalogString text.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/getFunctionsFunctionRoutineDependency:getFunctionsFunctionRoutineDependency": {
            "properties": {
                "dependencies": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionRoutineDependencyDependency:getFunctionsFunctionRoutineDependencyDependency"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunctionRoutineDependencyDependency:getFunctionsFunctionRoutineDependencyDependency": {
            "properties": {
                "functions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionRoutineDependencyDependencyFunction:getFunctionsFunctionRoutineDependencyDependencyFunction"
                    }
                },
                "tables": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getFunctionsFunctionRoutineDependencyDependencyTable:getFunctionsFunctionRoutineDependencyDependencyTable"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getFunctionsFunctionRoutineDependencyDependencyFunction:getFunctionsFunctionRoutineDependencyDependencyFunction": {
            "properties": {
                "functionFullName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "functionFullName"
            ]
        },
        "databricks:index/getFunctionsFunctionRoutineDependencyDependencyTable:getFunctionsFunctionRoutineDependencyDependencyTable": {
            "properties": {
                "tableFullName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "tableFullName"
            ]
        },
        "databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo": {
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer"
                },
                "instancePoolFleetAttributes": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string"
                },
                "maxCapacity": {
                    "type": "integer"
                },
                "minIdleInstances": {
                    "type": "integer"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "state": {
                    "type": "string"
                },
                "stats": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats"
                }
            },
            "type": "object",
            "required": [
                "defaultTags",
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "idleInstanceAutoterminationMinutes",
                        "instancePoolName"
                    ]
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "zoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer"
                },
                "diskSize": {
                    "type": "integer"
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string"
                },
                "ebsVolumeType": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localSsdCount",
                "zoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption"
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption"
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride"
                    }
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string"
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string"
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string"
                },
                "instanceType": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats": {
            "properties": {
                "idleCount": {
                    "type": "integer"
                },
                "pendingIdleCount": {
                    "type": "integer"
                },
                "pendingUsedCount": {
                    "type": "integer"
                },
                "usedCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile": {
            "properties": {
                "arn": {
                    "type": "string",
                    "description": "ARN of the instance profile.\n"
                },
                "isMeta": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile or not.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the instance profile.\n"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of the role attached to the instance profile.\n"
                }
            },
            "type": "object",
            "required": [
                "arn",
                "isMeta",
                "name",
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettings:getJobJobSettings": {
            "properties": {
                "createdTime": {
                    "type": "integer"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "jobId": {
                    "type": "integer"
                },
                "runAsUserName": {
                    "type": "string"
                },
                "settings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings"
                }
            },
            "type": "object",
            "required": [
                "runAsUserName"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsContinuous:getJobJobSettingsSettingsContinuous"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsDeployment:getJobJobSettingsSettingsDeployment"
                },
                "description": {
                    "type": "string"
                },
                "editMode": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEnvironment:getJobJobSettingsSettingsEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsHealth:getJobJobSettingsSettingsHealth"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary"
                    }
                },
                "maxConcurrentRuns": {
                    "type": "integer"
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "the job name of databricks.Job if the resource was matched by id.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNotificationSettings:getJobJobSettingsSettingsNotificationSettings"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsParameter:getJobJobSettingsSettingsParameter"
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsQueue:getJobJobSettingsSettingsQueue"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsRunAs:getJobJobSettingsSettingsRunAs"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsRunJobTask:getJobJobSettingsSettingsRunJobTask"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTrigger:getJobJobSettingsSettingsTrigger"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "format",
                "runAs"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsContinuous:getJobJobSettingsSettingsContinuous": {
            "properties": {
                "pauseStatus": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsDeployment:getJobJobSettingsSettingsDeployment": {
            "properties": {
                "kind": {
                    "type": "string"
                },
                "metadataFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "kind"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsEnvironment:getJobJobSettingsSettingsEnvironment": {
            "properties": {
                "environmentKey": {
                    "type": "string"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEnvironmentSpec:getJobJobSettingsSettingsEnvironmentSpec"
                }
            },
            "type": "object",
            "required": [
                "environmentKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsEnvironmentSpec:getJobJobSettingsSettingsEnvironmentSpec": {
            "properties": {
                "client": {
                    "type": "string"
                },
                "dependencies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "client"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource": {
            "properties": {
                "branch": {
                    "type": "string"
                },
                "commit": {
                    "type": "string"
                },
                "jobSource": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsGitSourceJobSource:getJobJobSettingsSettingsGitSourceJobSource"
                },
                "provider": {
                    "type": "string"
                },
                "tag": {
                    "type": "string"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsGitSourceJobSource:getJobJobSettingsSettingsGitSourceJobSource": {
            "properties": {
                "dirtyState": {
                    "type": "string"
                },
                "importFromGitBranch": {
                    "type": "string"
                },
                "jobConfigPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "importFromGitBranch",
                "jobConfigPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsHealth:getJobJobSettingsSettingsHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsHealthRule:getJobJobSettingsSettingsHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsHealthRule:getJobJobSettingsSettingsHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster"
                }
            },
            "type": "object",
            "required": [
                "jobClusterKey",
                "newCluster"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes:getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace:getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes:getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace:getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptVolumes:getJobJobSettingsSettingsNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptWorkspace:getJobJobSettingsSettingsNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptVolumes:getJobJobSettingsSettingsNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptWorkspace:getJobJobSettingsSettingsNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNotificationSettings:getJobJobSettingsSettingsNotificationSettings": {
            "properties": {
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsParameter:getJobJobSettingsSettingsParameter": {
            "properties": {
                "default": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the job name of databricks.Job if the resource was matched by id.\n"
                }
            },
            "type": "object",
            "required": [
                "default",
                "name"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsQueue:getJobJobSettingsSettingsQueue": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsRunAs:getJobJobSettingsSettingsRunAs": {
            "properties": {
                "servicePrincipalName": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsRunJobTask:getJobJobSettingsSettingsRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string"
                },
                "quartzCronExpression": {
                    "type": "string"
                },
                "timezoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskConditionTask:getJobJobSettingsSettingsTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn"
                    }
                },
                "description": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "forEachTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTask:getJobJobSettingsSettingsTaskForEachTask"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskHealth:getJobJobSettingsSettingsTaskHealth"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary"
                    }
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNotificationSettings:getJobJobSettingsSettingsTaskNotificationSettings"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runIf": {
                    "type": "string"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskRunJobTask:getJobJobSettingsSettingsTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask"
                },
                "taskKey": {
                    "type": "string"
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotifications:getJobJobSettingsSettingsTaskWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "retryOnTimeout",
                "taskKey"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "taskKey"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskConditionTask:getJobJobSettingsSettingsTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "right": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string"
                },
                "taskKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTask:getJobJobSettingsSettingsTaskForEachTask": {
            "properties": {
                "concurrency": {
                    "type": "integer"
                },
                "inputs": {
                    "type": "string"
                },
                "task": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTask:getJobJobSettingsSettingsTaskForEachTaskTask"
                }
            },
            "type": "object",
            "required": [
                "inputs",
                "task"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTask:getJobJobSettingsSettingsTaskForEachTaskTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask:getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask:getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn:getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn"
                    }
                },
                "description": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealth:getJobJobSettingsSettingsTaskForEachTaskTaskHealth"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibrary:getJobJobSettingsSettingsTaskForEachTaskTaskLibrary"
                    }
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster:getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask:getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings:getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask:getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask:getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runIf": {
                    "type": "string"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask:getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask"
                },
                "taskKey": {
                    "type": "string"
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "retryOnTimeout",
                "taskKey"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "taskKey"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask:getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "right": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask:getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn:getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string"
                },
                "taskKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealth:getJobJobSettingsSettingsTaskForEachTaskTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule:getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule:getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibrary:getJobJobSettingsSettingsTaskForEachTaskTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster:getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask:getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings:getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask:getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask:getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask:getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "warehouseId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string"
                },
                "dashboardId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskHealth:getJobJobSettingsSettingsTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskHealthRule:getJobJobSettingsSettingsTaskHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskHealthRule:getJobJobSettingsSettingsTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "metric",
                "op",
                "value"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNotificationSettings:getJobJobSettingsSettingsTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskRunJobTask:getJobJobSettingsSettingsTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskFile:getJobJobSettingsSettingsTaskSqlTaskFile"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "warehouseId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskSqlTaskAlertSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string"
                },
                "dashboardId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskFile:getJobJobSettingsSettingsTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotifications:getJobJobSettingsSettingsTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskWebhookNotificationsOnStart"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTrigger:getJobJobSettingsSettingsTrigger": {
            "properties": {
                "fileArrival": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTriggerFileArrival:getJobJobSettingsSettingsTriggerFileArrival"
                },
                "pauseStatus": {
                    "type": "string"
                },
                "periodic": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTriggerPeriodic:getJobJobSettingsSettingsTriggerPeriodic"
                },
                "tableUpdate": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTriggerTableUpdate:getJobJobSettingsSettingsTriggerTableUpdate"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTriggerFileArrival:getJobJobSettingsSettingsTriggerFileArrival": {
            "properties": {
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer"
                },
                "url": {
                    "type": "string"
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTriggerPeriodic:getJobJobSettingsSettingsTriggerPeriodic": {
            "properties": {
                "interval": {
                    "type": "integer"
                },
                "unit": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "interval",
                "unit"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTriggerTableUpdate:getJobJobSettingsSettingsTriggerTableUpdate": {
            "properties": {
                "condition": {
                    "type": "string"
                },
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer"
                },
                "tableNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "tableNames"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart"
                    }
                },
                "onStreamingBacklogExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded:getJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo": {
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Used to set expiration duration in seconds on recipient data access tokens.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "externalAccessEnabled": {
                    "type": "boolean"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the metastore\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "privilegeModelVersion": {
                    "type": "string"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored.\n"
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "storageRootCredentialName": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowExperimentTag:getMlflowExperimentTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion": {
            "properties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "currentStage": {
                    "type": "string"
                },
                "description": {
                    "type": "string",
                    "description": "User-specified description for the object.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the registered model.\n"
                },
                "runId": {
                    "type": "string"
                },
                "runLink": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "statusMessage": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getMlflowModelLatestVersionTag:getMlflowModelLatestVersionTag"
                    },
                    "description": "Array of tags associated with the model.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "The username of the user that created the object.\n"
                },
                "version": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelLatestVersionTag:getMlflowModelLatestVersionTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelTag:getMlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfig:getMwsNetworkConnectivityConfigEgressConfig": {
            "properties": {
                "defaultRules": {
                    "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRules:getMwsNetworkConnectivityConfigEgressConfigDefaultRules",
                    "description": "Array of default rules.\n"
                },
                "targetRules": {
                    "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfigTargetRules:getMwsNetworkConnectivityConfigEgressConfigTargetRules",
                    "description": "Array of target rules.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRules:getMwsNetworkConnectivityConfigEgressConfigDefaultRules": {
            "properties": {
                "awsStableIpRule": {
                    "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule",
                    "description": "The stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.\n"
                },
                "azureServiceEndpointRule": {
                    "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule",
                    "description": "Array of Azure service endpoint rules.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule": {
            "properties": {
                "cidrBlocks": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of stable IP CIDR blocks from which Databricks network traffic originates when accessing your resources.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:getMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule": {
            "properties": {
                "subnets": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Array of strings representing the subnet IDs.\n"
                },
                "targetRegion": {
                    "type": "string",
                    "description": "The target region for the service endpoint.\n"
                },
                "targetServices": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Array of target services.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfigTargetRules:getMwsNetworkConnectivityConfigEgressConfigTargetRules": {
            "properties": {
                "azurePrivateEndpointRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:getMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule"
                    },
                    "description": "Array of private endpoint rule objects.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:getMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule": {
            "properties": {
                "connectionState": {
                    "type": "string",
                    "description": "The current status of this private endpoint.\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was created.\n"
                },
                "deactivated": {
                    "type": "boolean",
                    "description": "Whether this private endpoint is deactivated.\n"
                },
                "deactivatedAt": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was deactivated.\n"
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Azure private endpoint resource.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "The sub-resource type (group ID) of the target resource.\n"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "The Databricks network connectivity configuration ID.\n"
                },
                "resourceId": {
                    "type": "string",
                    "description": "The Azure resource ID of the target resource.\n"
                },
                "ruleId": {
                    "type": "string",
                    "description": "The ID of a private endpoint rule.\n"
                },
                "updatedTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when the network was updated.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList": {
            "properties": {
                "language": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "Path to workspace directory\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getNotificationDestinationsNotificationDestination:getNotificationDestinationsNotificationDestination": {
            "properties": {
                "destinationType": {
                    "type": "string",
                    "description": "The type of the notification destination. Possible values are `EMAIL`, `MICROSOFT_TEAMS`, `PAGERDUTY`, `SLACK`, or `WEBHOOK`.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the Notification Destination.\n"
                },
                "id": {
                    "type": "string",
                    "description": "The unique ID of the Notification Destination.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelModelInfo:getRegisteredModelModelInfo": {
            "properties": {
                "aliases": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelModelInfoAlias:getRegisteredModelModelInfoAlias"
                    },
                    "description": "the list of aliases associated with this model. Each item is object consisting of following attributes:\n"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "the Unix timestamp at the model's creation\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "the identifier of the user who created the model\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "The fully-qualified name of the registered model (`catalog_name.schema_name.name`).\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "the unique identifier of the metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the registered model.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the registered model owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "the timestamp of the last time changes were made to the model\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the identifier of the user who updated the model last time\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelModelInfoAlias:getRegisteredModelModelInfoAlias": {
            "properties": {
                "aliasName": {
                    "type": "string",
                    "description": "string with the name of alias\n"
                },
                "versionNum": {
                    "type": "integer",
                    "description": "associated model version\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelVersionsModelVersion:getRegisteredModelVersionsModelVersion": {
            "properties": {
                "aliases": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersionAlias:getRegisteredModelVersionsModelVersionAlias"
                    },
                    "description": "the list of aliases associated with this model. Each item is object consisting of following attributes:\n"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "the Unix timestamp at the model's creation\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "the identifier of the user who created the model\n"
                },
                "id": {
                    "type": "string",
                    "description": "The unique identifier of the model version\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "the unique identifier of the metastore\n"
                },
                "modelName": {
                    "type": "string"
                },
                "modelVersionDependencies": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependency:getRegisteredModelVersionsModelVersionModelVersionDependency"
                    },
                    "description": "block describing model version dependencies, for feature-store packaged models. Consists of following attributes:\n"
                },
                "runId": {
                    "type": "string",
                    "description": "MLflow run ID used when creating the model version, if `source` was generated by an experiment run stored in an MLflow tracking server\n"
                },
                "runWorkspaceId": {
                    "type": "integer",
                    "description": "ID of the Databricks workspace containing the MLflow run that generated this model version, if applicable\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides.\n"
                },
                "source": {
                    "type": "string",
                    "description": "URI indicating the location of the source artifacts (files) for the model version.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Current status of the model version.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "the timestamp of the last time changes were made to the model\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the identifier of the user who updated the model last time\n"
                },
                "version": {
                    "type": "integer",
                    "description": "Integer model version number, used to reference the model version in API requests.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelVersionsModelVersionAlias:getRegisteredModelVersionsModelVersionAlias": {
            "properties": {
                "aliasName": {
                    "type": "string",
                    "description": "string with the name of alias\n"
                },
                "versionNum": {
                    "type": "integer",
                    "description": "associated model version\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependency:getRegisteredModelVersionsModelVersionModelVersionDependency": {
            "properties": {
                "dependencies": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependency:getRegisteredModelVersionsModelVersionModelVersionDependencyDependency"
                    },
                    "description": "list of dependencies consisting of following attributes:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependency:getRegisteredModelVersionsModelVersionModelVersionDependencyDependency": {
            "properties": {
                "functions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction:getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction"
                    },
                    "description": "A function that is dependent on a SQL object:\n"
                },
                "tables": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable:getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable"
                    },
                    "description": "A table that is dependent on a SQL object\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction:getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction": {
            "properties": {
                "functionFullName": {
                    "type": "string",
                    "description": "Full name of the dependent function\n"
                }
            },
            "type": "object",
            "required": [
                "functionFullName"
            ]
        },
        "databricks:index/getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable:getRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable": {
            "properties": {
                "tableFullName": {
                    "type": "string",
                    "description": "Full name of the dependent table\n"
                }
            },
            "type": "object",
            "required": [
                "tableFullName"
            ]
        },
        "databricks:index/getSchemaSchemaInfo:getSchemaSchemaInfo": {
            "properties": {
                "browseOnly": {
                    "type": "boolean",
                    "description": "indicates whether the principal is limited to retrieving metadata for the schema through the BROWSE privilege.\n"
                },
                "catalogName": {
                    "type": "string",
                    "description": "the name of the catalog where the schema is.\n"
                },
                "catalogType": {
                    "type": "string",
                    "description": "the type of the parent catalog.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "the comment attached to the volume\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "time at which this schema was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "username of schema creator.\n"
                },
                "effectivePredictiveOptimizationFlag": {
                    "$ref": "#/types/databricks:index/getSchemaSchemaInfoEffectivePredictiveOptimizationFlag:getSchemaSchemaInfoEffectivePredictiveOptimizationFlag",
                    "description": "information about actual state of predictive optimization.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "whether predictive optimization should be enabled for this object and objects under it.\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "the two-level (fully qualified) name of the schema\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "the unique identifier of the metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "a fully qualified name of databricks_schema: *`catalog`.`schema`*\n"
                },
                "owner": {
                    "type": "string",
                    "description": "the identifier of the user who owns the schema\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "map of properties set on the schema\n"
                },
                "schemaId": {
                    "type": "string",
                    "description": "the unique identifier of the volume\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "the storage location on the cloud.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "storage root URL for managed tables within schema.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "the timestamp of the last time changes were made to the schema\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the identifier of the user who updated the schema last time\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getSchemaSchemaInfoEffectivePredictiveOptimizationFlag:getSchemaSchemaInfoEffectivePredictiveOptimizationFlag": {
            "properties": {
                "inheritedFromName": {
                    "type": "string"
                },
                "inheritedFromType": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/getServingEndpointsEndpoint:getServingEndpointsEndpoint": {
            "properties": {
                "aiGateways": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGateway:getServingEndpointsEndpointAiGateway"
                    },
                    "description": "A block with AI Gateway configuration for the serving endpoint.\n"
                },
                "budgetPolicyId": {
                    "type": "string"
                },
                "configs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfig:getServingEndpointsEndpointConfig"
                    },
                    "description": "The model serving endpoint configuration.\n"
                },
                "creationTimestamp": {
                    "type": "integer"
                },
                "creator": {
                    "type": "string"
                },
                "id": {
                    "type": "string"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint.\n"
                },
                "states": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointState:getServingEndpointsEndpointState"
                    }
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointTag:getServingEndpointsEndpointTag"
                    },
                    "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                },
                "task": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGateway:getServingEndpointsEndpointAiGateway": {
            "properties": {
                "fallbackConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayFallbackConfig:getServingEndpointsEndpointAiGatewayFallbackConfig"
                    }
                },
                "guardrails": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayGuardrail:getServingEndpointsEndpointAiGatewayGuardrail"
                    }
                },
                "inferenceTableConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayInferenceTableConfig:getServingEndpointsEndpointAiGatewayInferenceTableConfig"
                    }
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayRateLimit:getServingEndpointsEndpointAiGatewayRateLimit"
                    },
                    "description": "A list of rate limit blocks to be applied to the serving endpoint.\n"
                },
                "usageTrackingConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayUsageTrackingConfig:getServingEndpointsEndpointAiGatewayUsageTrackingConfig"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayFallbackConfig:getServingEndpointsEndpointAiGatewayFallbackConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayGuardrail:getServingEndpointsEndpointAiGatewayGuardrail": {
            "properties": {
                "inputProperties": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayGuardrailInputProperty:getServingEndpointsEndpointAiGatewayGuardrailInputProperty"
                    }
                },
                "outputs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayGuardrailOutput:getServingEndpointsEndpointAiGatewayGuardrailOutput"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayGuardrailInputProperty:getServingEndpointsEndpointAiGatewayGuardrailInputProperty": {
            "properties": {
                "invalidKeywords": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "piis": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii:getServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii"
                    }
                },
                "safety": {
                    "type": "boolean"
                },
                "validTopics": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii:getServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii": {
            "properties": {
                "behavior": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayGuardrailOutput:getServingEndpointsEndpointAiGatewayGuardrailOutput": {
            "properties": {
                "invalidKeywords": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "piis": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointAiGatewayGuardrailOutputPii:getServingEndpointsEndpointAiGatewayGuardrailOutputPii"
                    }
                },
                "safety": {
                    "type": "boolean"
                },
                "validTopics": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayGuardrailOutputPii:getServingEndpointsEndpointAiGatewayGuardrailOutputPii": {
            "properties": {
                "behavior": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayInferenceTableConfig:getServingEndpointsEndpointAiGatewayInferenceTableConfig": {
            "properties": {
                "catalogName": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean"
                },
                "schemaName": {
                    "type": "string"
                },
                "tableNamePrefix": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayRateLimit:getServingEndpointsEndpointAiGatewayRateLimit": {
            "properties": {
                "calls": {
                    "type": "integer"
                },
                "key": {
                    "type": "string"
                },
                "renewalPeriod": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "calls",
                "renewalPeriod"
            ]
        },
        "databricks:index/getServingEndpointsEndpointAiGatewayUsageTrackingConfig:getServingEndpointsEndpointAiGatewayUsageTrackingConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfig:getServingEndpointsEndpointConfig": {
            "properties": {
                "servedEntities": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntity:getServingEndpointsEndpointConfigServedEntity"
                    }
                },
                "servedModels": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedModel:getServingEndpointsEndpointConfigServedModel"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntity:getServingEndpointsEndpointConfigServedEntity": {
            "properties": {
                "entityName": {
                    "type": "string"
                },
                "entityVersion": {
                    "type": "string"
                },
                "externalModels": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModel:getServingEndpointsEndpointConfigServedEntityExternalModel"
                    }
                },
                "foundationModels": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityFoundationModel:getServingEndpointsEndpointConfigServedEntityFoundationModel"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModel:getServingEndpointsEndpointConfigServedEntityExternalModel": {
            "properties": {
                "ai21labsConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig"
                    }
                },
                "amazonBedrockConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig"
                    }
                },
                "anthropicConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig"
                    }
                },
                "cohereConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig:getServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig"
                    }
                },
                "customProviderConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig"
                    }
                },
                "databricksModelServingConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig:getServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig"
                    }
                },
                "googleCloudVertexAiConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig:getServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint.\n"
                },
                "openaiConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig:getServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig"
                    }
                },
                "palmConfigs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig:getServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig"
                    }
                },
                "provider": {
                    "type": "string"
                },
                "task": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name",
                "provider",
                "task"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig": {
            "properties": {
                "ai21labsApiKey": {
                    "type": "string"
                },
                "ai21labsApiKeyPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig": {
            "properties": {
                "awsAccessKeyId": {
                    "type": "string"
                },
                "awsAccessKeyIdPlaintext": {
                    "type": "string"
                },
                "awsRegion": {
                    "type": "string"
                },
                "awsSecretAccessKey": {
                    "type": "string"
                },
                "awsSecretAccessKeyPlaintext": {
                    "type": "string"
                },
                "bedrockProvider": {
                    "type": "string"
                },
                "instanceProfileArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "awsRegion",
                "bedrockProvider"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig:getServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig": {
            "properties": {
                "anthropicApiKey": {
                    "type": "string"
                },
                "anthropicApiKeyPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig:getServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig": {
            "properties": {
                "cohereApiBase": {
                    "type": "string"
                },
                "cohereApiKey": {
                    "type": "string"
                },
                "cohereApiKeyPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig": {
            "properties": {
                "apiKeyAuths": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth"
                    }
                },
                "bearerTokenAuths": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth"
                    }
                },
                "customProviderUrl": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "customProviderUrl"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                },
                "valuePlaintext": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth:getServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth": {
            "properties": {
                "token": {
                    "type": "string"
                },
                "tokenPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig:getServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig": {
            "properties": {
                "databricksApiToken": {
                    "type": "string"
                },
                "databricksApiTokenPlaintext": {
                    "type": "string"
                },
                "databricksWorkspaceUrl": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "databricksWorkspaceUrl"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig:getServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig": {
            "properties": {
                "privateKey": {
                    "type": "string"
                },
                "privateKeyPlaintext": {
                    "type": "string"
                },
                "projectId": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "projectId",
                "region"
            ]
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig:getServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig": {
            "properties": {
                "microsoftEntraClientId": {
                    "type": "string"
                },
                "microsoftEntraClientSecret": {
                    "type": "string"
                },
                "microsoftEntraClientSecretPlaintext": {
                    "type": "string"
                },
                "microsoftEntraTenantId": {
                    "type": "string"
                },
                "openaiApiBase": {
                    "type": "string"
                },
                "openaiApiKey": {
                    "type": "string"
                },
                "openaiApiKeyPlaintext": {
                    "type": "string"
                },
                "openaiApiType": {
                    "type": "string"
                },
                "openaiApiVersion": {
                    "type": "string"
                },
                "openaiDeploymentName": {
                    "type": "string"
                },
                "openaiOrganization": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig:getServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig": {
            "properties": {
                "palmApiKey": {
                    "type": "string"
                },
                "palmApiKeyPlaintext": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedEntityFoundationModel:getServingEndpointsEndpointConfigServedEntityFoundationModel": {
            "properties": {
                "description": {
                    "type": "string"
                },
                "displayName": {
                    "type": "string"
                },
                "docs": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointConfigServedModel:getServingEndpointsEndpointConfigServedModel": {
            "properties": {
                "modelName": {
                    "type": "string"
                },
                "modelVersion": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointState:getServingEndpointsEndpointState": {
            "properties": {
                "configUpdate": {
                    "type": "string"
                },
                "ready": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getServingEndpointsEndpointTag:getServingEndpointsEndpointTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/getShareObject:getShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer"
                },
                "addedBy": {
                    "type": "string"
                },
                "cdfEnabled": {
                    "type": "boolean"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n"
                },
                "content": {
                    "type": "string"
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the object.\n"
                },
                "historyDataSharingStatus": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the share\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getShareObjectPartition:getShareObjectPartition"
                    }
                },
                "sharedAs": {
                    "type": "string"
                },
                "startVersion": {
                    "type": "integer"
                },
                "status": {
                    "type": "string"
                },
                "stringSharedAs": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "addedAt",
                "addedBy",
                "dataObjectType",
                "name",
                "status"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "dataObjectType",
                        "name"
                    ]
                }
            }
        },
        "databricks:index/getShareObjectPartition:getShareObjectPartition": {
            "properties": {
                "values": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getShareObjectPartitionValue:getShareObjectPartitionValue"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getShareObjectPartitionValue:getShareObjectPartitionValue": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the share\n"
                },
                "op": {
                    "type": "string"
                },
                "recipientPropertyKey": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name",
                "op"
            ]
        },
        "databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel": {
            "properties": {
                "dbsqlVersion": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse to search (case-sensitive).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth": {
            "properties": {
                "details": {
                    "type": "string"
                },
                "failureReason": {
                    "$ref": "#/types/databricks:index/getSqlWarehouseHealthFailureReason:getSqlWarehouseHealthFailureReason"
                },
                "message": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "summary": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseHealthFailureReason:getSqlWarehouseHealthFailureReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams": {
            "properties": {
                "hostname": {
                    "type": "string"
                },
                "path": {
                    "type": "string"
                },
                "port": {
                    "type": "integer"
                },
                "protocol": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseTags:getSqlWarehouseTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo": {
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAwsIamRole:getStorageCredentialStorageCredentialInfoAwsIamRole",
                    "description": "credential details for AWS:\n"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAzureManagedIdentity:getStorageCredentialStorageCredentialInfoAzureManagedIdentity",
                    "description": "managed identity credential details for Azure\n"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAzureServicePrincipal:getStorageCredentialStorageCredentialInfoAzureServicePrincipal",
                    "description": "service principal credential details for Azure:\n"
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoCloudflareApiToken:getStorageCredentialStorageCredentialInfoCloudflareApiToken"
                },
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of catalog creator.\n"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount:getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount",
                    "description": "credential details for GCP:\n"
                },
                "fullName": {
                    "type": "string"
                },
                "id": {
                    "type": "string",
                    "description": "Unique ID of storage credential.\n"
                },
                "isolationMode": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the storage credential\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this catalog was last modified, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified catalog.\n"
                },
                "usedForManagedStorage": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAwsIamRole:getStorageCredentialStorageCredentialInfoAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string",
                    "description": "(output only) - The external ID used in role assumption to prevent confused deputy problem.\n"
                },
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n"
                },
                "unityCatalogIamArn": {
                    "type": "string",
                    "description": "(output only) - The Amazon Resource Name (ARN) of the AWS IAM user managed by Databricks. This is the identity that is going to assume the AWS IAM role.\n"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAzureManagedIdentity:getStorageCredentialStorageCredentialInfoAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.\n"
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.\n"
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAzureServicePrincipal:getStorageCredentialStorageCredentialInfoAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n"
                },
                "clientSecret": {
                    "type": "string"
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n"
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoCloudflareApiToken:getStorageCredentialStorageCredentialInfoCloudflareApiToken": {
            "properties": {
                "accessKeyId": {
                    "type": "string"
                },
                "accountId": {
                    "type": "string"
                },
                "secretAccessKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "accessKeyId",
                "accountId",
                "secretAccessKey"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount:getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfo:getTableTableInfo": {
            "properties": {
                "accessPoint": {
                    "type": "string"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog.\n"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getTableTableInfoColumn:getTableTableInfoColumn"
                    },
                    "description": "Array of ColumnInfo objects of the table's columns\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text description\n"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "dataAccessConfigurationId": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "Table format, e.g. DELTA, CSV, JSON\n"
                },
                "deletedAt": {
                    "type": "integer"
                },
                "deltaRuntimePropertiesKvpairs": {
                    "$ref": "#/types/databricks:index/getTableTableInfoDeltaRuntimePropertiesKvpairs:getTableTableInfoDeltaRuntimePropertiesKvpairs"
                },
                "effectivePredictiveOptimizationFlag": {
                    "$ref": "#/types/databricks:index/getTableTableInfoEffectivePredictiveOptimizationFlag:getTableTableInfoEffectivePredictiveOptimizationFlag"
                },
                "enablePredictiveOptimization": {
                    "type": "string"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/getTableTableInfoEncryptionDetails:getTableTableInfoEncryptionDetails"
                },
                "fullName": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Current owner of the table\n"
                },
                "pipelineId": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "rowFilter": {
                    "$ref": "#/types/databricks:index/getTableTableInfoRowFilter:getTableTableInfoRowFilter"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent schema relative to its parent catalog.\n"
                },
                "sqlPath": {
                    "type": "string"
                },
                "storageCredentialName": {
                    "type": "string"
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableConstraints": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getTableTableInfoTableConstraint:getTableTableInfoTableConstraint"
                    }
                },
                "tableId": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string",
                    "description": "Table type, e.g. MANAGED, EXTERNAL, VIEW\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "View definition SQL (when `table_type` is VIEW, MATERIALIZED_VIEW, or STREAMING_TABLE)\n"
                },
                "viewDependencies": {
                    "$ref": "#/types/databricks:index/getTableTableInfoViewDependencies:getTableTableInfoViewDependencies",
                    "description": "View dependencies (when `table_type` is VIEW or MATERIALIZED_VIEW, STREAMING_TABLE)\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoColumn:getTableTableInfoColumn": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "Free-form text description\n"
                },
                "mask": {
                    "$ref": "#/types/databricks:index/getTableTableInfoColumnMask:getTableTableInfoColumnMask"
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n"
                },
                "nullable": {
                    "type": "boolean"
                },
                "partitionIndex": {
                    "type": "integer"
                },
                "position": {
                    "type": "integer"
                },
                "typeIntervalType": {
                    "type": "string"
                },
                "typeJson": {
                    "type": "string"
                },
                "typeName": {
                    "type": "string"
                },
                "typePrecision": {
                    "type": "integer"
                },
                "typeScale": {
                    "type": "integer"
                },
                "typeText": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoColumnMask:getTableTableInfoColumnMask": {
            "properties": {
                "functionName": {
                    "type": "string"
                },
                "usingColumnNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoDeltaRuntimePropertiesKvpairs:getTableTableInfoDeltaRuntimePropertiesKvpairs": {
            "properties": {
                "deltaRuntimeProperties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "deltaRuntimeProperties"
            ]
        },
        "databricks:index/getTableTableInfoEffectivePredictiveOptimizationFlag:getTableTableInfoEffectivePredictiveOptimizationFlag": {
            "properties": {
                "inheritedFromName": {
                    "type": "string"
                },
                "inheritedFromType": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/getTableTableInfoEncryptionDetails:getTableTableInfoEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/getTableTableInfoEncryptionDetailsSseEncryptionDetails:getTableTableInfoEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoEncryptionDetailsSseEncryptionDetails:getTableTableInfoEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoRowFilter:getTableTableInfoRowFilter": {
            "properties": {
                "functionName": {
                    "type": "string"
                },
                "inputColumnNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "functionName",
                "inputColumnNames"
            ]
        },
        "databricks:index/getTableTableInfoTableConstraint:getTableTableInfoTableConstraint": {
            "properties": {
                "foreignKeyConstraint": {
                    "$ref": "#/types/databricks:index/getTableTableInfoTableConstraintForeignKeyConstraint:getTableTableInfoTableConstraintForeignKeyConstraint"
                },
                "namedTableConstraint": {
                    "$ref": "#/types/databricks:index/getTableTableInfoTableConstraintNamedTableConstraint:getTableTableInfoTableConstraintNamedTableConstraint"
                },
                "primaryKeyConstraint": {
                    "$ref": "#/types/databricks:index/getTableTableInfoTableConstraintPrimaryKeyConstraint:getTableTableInfoTableConstraintPrimaryKeyConstraint"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoTableConstraintForeignKeyConstraint:getTableTableInfoTableConstraintForeignKeyConstraint": {
            "properties": {
                "childColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n"
                },
                "parentColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "parentTable": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "childColumns",
                "name",
                "parentColumns",
                "parentTable"
            ]
        },
        "databricks:index/getTableTableInfoTableConstraintNamedTableConstraint:getTableTableInfoTableConstraintNamedTableConstraint": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/getTableTableInfoTableConstraintPrimaryKeyConstraint:getTableTableInfoTableConstraintPrimaryKeyConstraint": {
            "properties": {
                "childColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n"
                }
            },
            "type": "object",
            "required": [
                "childColumns",
                "name"
            ]
        },
        "databricks:index/getTableTableInfoViewDependencies:getTableTableInfoViewDependencies": {
            "properties": {
                "dependencies": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getTableTableInfoViewDependenciesDependency:getTableTableInfoViewDependenciesDependency"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoViewDependenciesDependency:getTableTableInfoViewDependenciesDependency": {
            "properties": {
                "function": {
                    "$ref": "#/types/databricks:index/getTableTableInfoViewDependenciesDependencyFunction:getTableTableInfoViewDependenciesDependencyFunction"
                },
                "table": {
                    "$ref": "#/types/databricks:index/getTableTableInfoViewDependenciesDependencyTable:getTableTableInfoViewDependenciesDependencyTable"
                }
            },
            "type": "object"
        },
        "databricks:index/getTableTableInfoViewDependenciesDependencyFunction:getTableTableInfoViewDependenciesDependencyFunction": {
            "properties": {
                "functionFullName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "functionFullName"
            ]
        },
        "databricks:index/getTableTableInfoViewDependenciesDependencyTable:getTableTableInfoViewDependenciesDependencyTable": {
            "properties": {
                "tableFullName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "tableFullName"
            ]
        },
        "databricks:index/getVolumeVolumeInfo:getVolumeVolumeInfo": {
            "properties": {
                "accessPoint": {
                    "type": "string",
                    "description": "the AWS access point to use when accessing s3 bucket for this volume's external location\n"
                },
                "browseOnly": {
                    "type": "boolean",
                    "description": "indicates whether the principal is limited to retrieving metadata for the volume through the BROWSE privilege when include_browse is enabled in the request.\n"
                },
                "catalogName": {
                    "type": "string",
                    "description": "the name of the catalog where the schema and the volume are\n"
                },
                "comment": {
                    "type": "string",
                    "description": "the comment attached to the volume\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "the Unix timestamp at the volume's creation\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "the identifier of the user who created the volume\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/getVolumeVolumeInfoEncryptionDetails:getVolumeVolumeInfoEncryptionDetails",
                    "description": "encryption options that apply to clients connecting to cloud storage\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "the three-level (fully qualified) name of the volume\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "the unique identifier of the metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "a fully qualified name of databricks_volume: *`catalog`.`schema`.`volume`*\n"
                },
                "owner": {
                    "type": "string",
                    "description": "the identifier of the user who owns the volume\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "the name of the schema where the volume is\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "the storage location on the cloud\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "the timestamp of the last time changes were made to the volume\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the identifier of the user who updated the volume last time\n"
                },
                "volumeId": {
                    "type": "string",
                    "description": "the unique identifier of the volume\n"
                },
                "volumeType": {
                    "type": "string",
                    "description": "whether the volume is `MANAGED` or `EXTERNAL`\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getVolumeVolumeInfoEncryptionDetails:getVolumeVolumeInfoEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/getVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails:getVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/getVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails:getVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        }
    },
    "provider": {
        "description": "The provider type for the databricks package. By default, resources use package-wide configuration\nsettings, however an explicit `Provider` instance may be created and passed during resource\nconstruction to achieve fine-grained programmatic control over provider settings. See the\n[documentation](https://www.pulumi.com/docs/reference/programming-model/#providers) for more information.\n",
        "properties": {
            "accountId": {
                "type": "string"
            },
            "actionsIdTokenRequestToken": {
                "type": "string"
            },
            "actionsIdTokenRequestUrl": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "serverlessComputeId": {
                "type": "string"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        },
        "inputProperties": {
            "accountId": {
                "type": "string"
            },
            "actionsIdTokenRequestToken": {
                "type": "string"
            },
            "actionsIdTokenRequestUrl": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "serverlessComputeId": {
                "type": "string"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        }
    },
    "resources": {
        "databricks:index/accessControlRuleSet:AccessControlRuleSet": {
            "description": "\u003e This resource can be used with an account or workspace-level provider.\n\nThis resource allows you to manage access rules on Databricks account level resources. For convenience we allow accessing this resource through the Databricks account and workspace.\n\n\u003e Currently, we only support managing access rules on specific object resources (service principal, group, budget policies and account) through `databricks.AccessControlRuleSet`.\n!\u003e `databricks.AccessControlRuleSet` cannot be used to manage access rules for resources supported by databricks_permissions. Refer to its documentation for more information.\n\n\u003e This resource is _authoritative_ for permissions on objects. Configuring this resource for an object will **OVERWRITE** any existing permissions of the same type unless imported, and changes made outside of Pulumi will be reset.\n\n## Service principal rule set usage\n\nThrough a Databricks workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.then(ds =\u003e ds.aclPrincipalId)],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[{\n        \"principals\": [ds.acl_principal_id],\n        \"role\": \"roles/servicePrincipal.user\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\tds, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(ds.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()\n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(automationSp.applicationId().applyValue(_applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,_applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: Data Science\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[{\n        \"principals\": [ds.acl_principal_id],\n        \"role\": \"roles/servicePrincipal.user\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()\n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(automationSp.applicationId().applyValue(_applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,_applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {\n    applicationId: \"00000000-0000-0000-0000-000000000000\",\n    displayName: \"SP_FOR_AUTOMATION\",\n});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\",\n    application_id=\"00000000-0000-0000-0000-000000000000\",\n    display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[{\n        \"principals\": [ds.acl_principal_id],\n        \"role\": \"roles/servicePrincipal.user\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tDisplayName:   pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()\n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(automationSp.applicationId().applyValue(_applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,_applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough GCP Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[{\n        \"principals\": [ds.acl_principal_id],\n        \"role\": \"roles/servicePrincipal.user\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()\n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(automationSp.applicationId().applyValue(_applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,_applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Group rule set usage\n\nRefer to the appropriate provider configuration as shown in the examples for service principal rule set.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\nconst john = databricks.getUser({\n    userName: \"john.doe@example.com\",\n});\nconst dsGroupRuleSet = new databricks.AccessControlRuleSet(\"ds_group_rule_set\", {\n    name: `accounts/${accountId}/groups/${dsDatabricksGroup.id}/ruleSets/default`,\n    grantRules: [{\n        principals: [john.then(john =\u003e john.aclPrincipalId)],\n        role: \"roles/group.manager\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\njohn = databricks.get_user(user_name=\"john.doe@example.com\")\nds_group_rule_set = databricks.AccessControlRuleSet(\"ds_group_rule_set\",\n    name=f\"accounts/{account_id}/groups/{ds_databricks_group['id']}/ruleSets/default\",\n    grant_rules=[{\n        \"principals\": [john.acl_principal_id],\n        \"role\": \"roles/group.manager\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var john = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"john.doe@example.com\",\n    });\n\n    var dsGroupRuleSet = new Databricks.AccessControlRuleSet(\"ds_group_rule_set\", new()\n    {\n        Name = $\"accounts/{accountId}/groups/{dsDatabricksGroup.Id}/ruleSets/default\",\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    john.Apply(getUserResult =\u003e getUserResult.AclPrincipalId),\n                },\n                Role = \"roles/group.manager\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\t_, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjohn, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"john.doe@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"ds_group_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: pulumi.Sprintf(\"accounts/%v/groups/%v/ruleSets/default\", accountId, dsDatabricksGroup.Id),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(john.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/group.manager\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        final var john = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"john.doe@example.com\")\n            .build());\n\n        var dsGroupRuleSet = new AccessControlRuleSet(\"dsGroupRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(String.format(\"accounts/%s/groups/%s/ruleSets/default\", accountId,dsDatabricksGroup.id()))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(john.aclPrincipalId())\n                .role(\"roles/group.manager\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dsGroupRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: ds_group_rule_set\n    properties:\n      name: accounts/${accountId}/groups/${dsDatabricksGroup.id}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${john.aclPrincipalId}\n          role: roles/group.manager\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: Data Science\n  john:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: john.doe@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Account rule set usage\n\nRefer to the appropriate provider configuration as shown in the examples for service principal rule set.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\n// account level group\nconst marketplaceAdmins = databricks.getGroup({\n    displayName: \"Marketplace Admins\",\n});\nconst john = databricks.getUser({\n    userName: \"john.doe@example.com\",\n});\nconst accountRuleSet = new databricks.AccessControlRuleSet(\"account_rule_set\", {\n    name: `accounts/${accountId}/ruleSets/default`,\n    grantRules: [\n        {\n            principals: [john.then(john =\u003e john.aclPrincipalId)],\n            role: \"roles/group.manager\",\n        },\n        {\n            principals: [ds.then(ds =\u003e ds.aclPrincipalId)],\n            role: \"roles/servicePrincipal.manager\",\n        },\n        {\n            principals: [marketplaceAdmins.then(marketplaceAdmins =\u003e marketplaceAdmins.aclPrincipalId)],\n            role: \"roles/marketplace.admin\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\n# account level group\nmarketplace_admins = databricks.get_group(display_name=\"Marketplace Admins\")\njohn = databricks.get_user(user_name=\"john.doe@example.com\")\naccount_rule_set = databricks.AccessControlRuleSet(\"account_rule_set\",\n    name=f\"accounts/{account_id}/ruleSets/default\",\n    grant_rules=[\n        {\n            \"principals\": [john.acl_principal_id],\n            \"role\": \"roles/group.manager\",\n        },\n        {\n            \"principals\": [ds.acl_principal_id],\n            \"role\": \"roles/servicePrincipal.manager\",\n        },\n        {\n            \"principals\": [marketplace_admins.acl_principal_id],\n            \"role\": \"roles/marketplace.admin\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    // account level group\n    var marketplaceAdmins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Marketplace Admins\",\n    });\n\n    var john = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"john.doe@example.com\",\n    });\n\n    var accountRuleSet = new Databricks.AccessControlRuleSet(\"account_rule_set\", new()\n    {\n        Name = $\"accounts/{accountId}/ruleSets/default\",\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    john.Apply(getUserResult =\u003e getUserResult.AclPrincipalId),\n                },\n                Role = \"roles/group.manager\",\n            },\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/servicePrincipal.manager\",\n            },\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    marketplaceAdmins.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/marketplace.admin\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\tds, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// account level group\n\t\tmarketplaceAdmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Marketplace Admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjohn, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"john.doe@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"account_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: pulumi.Sprintf(\"accounts/%v/ruleSets/default\", accountId),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(john.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/group.manager\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(ds.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.manager\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(marketplaceAdmins.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/marketplace.admin\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        // account level group\n        final var marketplaceAdmins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Marketplace Admins\")\n            .build());\n\n        final var john = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"john.doe@example.com\")\n            .build());\n\n        var accountRuleSet = new AccessControlRuleSet(\"accountRuleSet\", AccessControlRuleSetArgs.builder()\n            .name(String.format(\"accounts/%s/ruleSets/default\", accountId))\n            .grantRules(            \n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(john.aclPrincipalId())\n                    .role(\"roles/group.manager\")\n                    .build(),\n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(ds.aclPrincipalId())\n                    .role(\"roles/servicePrincipal.manager\")\n                    .build(),\n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(marketplaceAdmins.aclPrincipalId())\n                    .role(\"roles/marketplace.admin\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: account_rule_set\n    properties:\n      name: accounts/${accountId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${john.aclPrincipalId}\n          role: roles/group.manager\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.manager\n        - principals:\n            - ${marketplaceAdmins.aclPrincipalId}\n          role: roles/marketplace.admin\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: Data Science\n  # account level group\n  marketplaceAdmins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: Marketplace Admins\n  john:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: john.doe@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Budget policy usage\n\nAccess to budget policies could be controlled with this resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\nconst john = databricks.getUser({\n    userName: \"john.doe@example.com\",\n});\nconst _this = new databricks.BudgetPolicy(\"this\", {\n    policyName: \"data-science-budget-policy\",\n    customTags: [{\n        key: \"mykey\",\n        value: \"myvalue\",\n    }],\n});\nconst budgetPolicyUsage = new databricks.AccessControlRuleSet(\"budget_policy_usage\", {\n    name: pulumi.interpolate`accounts/${accountId}/budgetPolicies/${_this.policyId}/ruleSets/default`,\n    grantRules: [\n        {\n            principals: [john.then(john =\u003e john.aclPrincipalId)],\n            role: \"roles/budgetPolicy.manager\",\n        },\n        {\n            principals: [ds.then(ds =\u003e ds.aclPrincipalId)],\n            role: \"roles/budgetPolicy.user\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\njohn = databricks.get_user(user_name=\"john.doe@example.com\")\nthis = databricks.BudgetPolicy(\"this\",\n    policy_name=\"data-science-budget-policy\",\n    custom_tags=[{\n        \"key\": \"mykey\",\n        \"value\": \"myvalue\",\n    }])\nbudget_policy_usage = databricks.AccessControlRuleSet(\"budget_policy_usage\",\n    name=this.policy_id.apply(lambda policy_id: f\"accounts/{account_id}/budgetPolicies/{policy_id}/ruleSets/default\"),\n    grant_rules=[\n        {\n            \"principals\": [john.acl_principal_id],\n            \"role\": \"roles/budgetPolicy.manager\",\n        },\n        {\n            \"principals\": [ds.acl_principal_id],\n            \"role\": \"roles/budgetPolicy.user\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var john = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"john.doe@example.com\",\n    });\n\n    var @this = new Databricks.BudgetPolicy(\"this\", new()\n    {\n        PolicyName = \"data-science-budget-policy\",\n        CustomTags = new[]\n        {\n            new Databricks.Inputs.BudgetPolicyCustomTagArgs\n            {\n                Key = \"mykey\",\n                Value = \"myvalue\",\n            },\n        },\n    });\n\n    var budgetPolicyUsage = new Databricks.AccessControlRuleSet(\"budget_policy_usage\", new()\n    {\n        Name = @this.PolicyId.Apply(policyId =\u003e $\"accounts/{accountId}/budgetPolicies/{policyId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    john.Apply(getUserResult =\u003e getUserResult.AclPrincipalId),\n                },\n                Role = \"roles/budgetPolicy.manager\",\n            },\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/budgetPolicy.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\tds, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjohn, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"john.doe@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewBudgetPolicy(ctx, \"this\", \u0026databricks.BudgetPolicyArgs{\n\t\t\tPolicyName: pulumi.String(\"data-science-budget-policy\"),\n\t\t\tCustomTags: databricks.BudgetPolicyCustomTagArray{\n\t\t\t\t\u0026databricks.BudgetPolicyCustomTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"mykey\"),\n\t\t\t\t\tValue: pulumi.String(\"myvalue\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"budget_policy_usage\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: this.PolicyId.ApplyT(func(policyId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/budgetPolicies/%v/ruleSets/default\", accountId, policyId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(john.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/budgetPolicy.manager\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(ds.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/budgetPolicy.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.BudgetPolicy;\nimport com.pulumi.databricks.BudgetPolicyArgs;\nimport com.pulumi.databricks.inputs.BudgetPolicyCustomTagArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        final var john = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"john.doe@example.com\")\n            .build());\n\n        var this_ = new BudgetPolicy(\"this\", BudgetPolicyArgs.builder()\n            .policyName(\"data-science-budget-policy\")\n            .customTags(BudgetPolicyCustomTagArgs.builder()\n                .key(\"mykey\")\n                .value(\"myvalue\")\n                .build())\n            .build());\n\n        var budgetPolicyUsage = new AccessControlRuleSet(\"budgetPolicyUsage\", AccessControlRuleSetArgs.builder()\n            .name(this_.policyId().applyValue(_policyId -\u003e String.format(\"accounts/%s/budgetPolicies/%s/ruleSets/default\", accountId,_policyId)))\n            .grantRules(            \n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(john.aclPrincipalId())\n                    .role(\"roles/budgetPolicy.manager\")\n                    .build(),\n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(ds.aclPrincipalId())\n                    .role(\"roles/budgetPolicy.user\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:BudgetPolicy\n    properties:\n      policyName: data-science-budget-policy\n      customTags:\n        - key: mykey\n          value: myvalue\n  budgetPolicyUsage:\n    type: databricks:AccessControlRuleSet\n    name: budget_policy_usage\n    properties:\n      name: accounts/${accountId}/budgetPolicies/${this.policyId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${john.aclPrincipalId}\n          role: roles/budgetPolicy.manager\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/budgetPolicy.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: Data Science\n  john:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: john.doe@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Group\n* databricks.User\n* databricks.ServicePrincipal\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "grantRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                    },
                    "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. **Changing the name recreates the resource!**. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/ruleSets/default` - account-level access control.\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default` - access control for a specific service principal.\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default` - access control for a specific group.\n* `accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default` - access control for a specific budget policy.\n"
                }
            },
            "required": [
                "etag",
                "name"
            ],
            "inputProperties": {
                "grantRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                    },
                    "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. **Changing the name recreates the resource!**. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/ruleSets/default` - account-level access control.\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default` - access control for a specific service principal.\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default` - access control for a specific group.\n* `accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default` - access control for a specific budget policy.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AccessControlRuleSet resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "grantRules": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                        },
                        "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. **Changing the name recreates the resource!**. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/ruleSets/default` - account-level access control.\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default` - access control for a specific service principal.\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default` - access control for a specific group.\n* `accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default` - access control for a specific budget policy.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/aibiDashboardEmbeddingAccessPolicySetting:AibiDashboardEmbeddingAccessPolicySetting": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThe `databricks.AibiDashboardEmbeddingAccessPolicySetting` resource allows you to control [embedding of AI/BI Dashboards](https://learn.microsoft.com/en-us/azure/databricks/dashboards/admin/#manage-dashboard-embedding) into other sites.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", {aibiDashboardEmbeddingAccessPolicy: {\n    accessPolicyType: \"ALLOW_APPROVED_DOMAINS\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", aibi_dashboard_embedding_access_policy={\n    \"access_policy_type\": \"ALLOW_APPROVED_DOMAINS\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", new()\n    {\n        AibiDashboardEmbeddingAccessPolicy = new Databricks.Inputs.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs\n        {\n            AccessPolicyType = \"ALLOW_APPROVED_DOMAINS\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewAibiDashboardEmbeddingAccessPolicySetting(ctx, \"this\", \u0026databricks.AibiDashboardEmbeddingAccessPolicySettingArgs{\n\t\t\tAibiDashboardEmbeddingAccessPolicy: \u0026databricks.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs{\n\t\t\t\tAccessPolicyType: pulumi.String(\"ALLOW_APPROVED_DOMAINS\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.AibiDashboardEmbeddingAccessPolicySetting;\nimport com.pulumi.databricks.AibiDashboardEmbeddingAccessPolicySettingArgs;\nimport com.pulumi.databricks.inputs.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new AibiDashboardEmbeddingAccessPolicySetting(\"this\", AibiDashboardEmbeddingAccessPolicySettingArgs.builder()\n            .aibiDashboardEmbeddingAccessPolicy(AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs.builder()\n                .accessPolicyType(\"ALLOW_APPROVED_DOMAINS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:AibiDashboardEmbeddingAccessPolicySetting\n    properties:\n      aibiDashboardEmbeddingAccessPolicy:\n        accessPolicyType: ALLOW_APPROVED_DOMAINS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n- databricks.AibiDashboardEmbeddingApprovedDomainsSetting is used to control approved domains.\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/aibiDashboardEmbeddingAccessPolicySetting:AibiDashboardEmbeddingAccessPolicySetting this global\n```\n\n",
            "properties": {
                "aibiDashboardEmbeddingAccessPolicy": {
                    "$ref": "#/types/databricks:index/AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy:AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy",
                    "description": "block with following attributes:\n"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "aibiDashboardEmbeddingAccessPolicy",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "aibiDashboardEmbeddingAccessPolicy": {
                    "$ref": "#/types/databricks:index/AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy:AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy",
                    "description": "block with following attributes:\n"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "aibiDashboardEmbeddingAccessPolicy"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AibiDashboardEmbeddingAccessPolicySetting resources.\n",
                "properties": {
                    "aibiDashboardEmbeddingAccessPolicy": {
                        "$ref": "#/types/databricks:index/AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy:AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy",
                        "description": "block with following attributes:\n"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/aibiDashboardEmbeddingApprovedDomainsSetting:AibiDashboardEmbeddingApprovedDomainsSetting": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThe `databricks.AibiDashboardEmbeddingApprovedDomainsSetting` resource allows you to specify the list of domains allowed for  [embedding of AI/BI Dashboards](https://learn.microsoft.com/en-us/azure/databricks/dashboards/admin/#manage-dashboard-embedding) into other sites.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", {aibiDashboardEmbeddingAccessPolicy: {\n    accessPolicyType: \"ALLOW_APPROVED_DOMAINS\",\n}});\nconst thisAibiDashboardEmbeddingApprovedDomainsSetting = new databricks.AibiDashboardEmbeddingApprovedDomainsSetting(\"this\", {aibiDashboardEmbeddingApprovedDomains: {\n    approvedDomains: [\"test.com\"],\n}}, {\n    dependsOn: [_this],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", aibi_dashboard_embedding_access_policy={\n    \"access_policy_type\": \"ALLOW_APPROVED_DOMAINS\",\n})\nthis_aibi_dashboard_embedding_approved_domains_setting = databricks.AibiDashboardEmbeddingApprovedDomainsSetting(\"this\", aibi_dashboard_embedding_approved_domains={\n    \"approved_domains\": [\"test.com\"],\n},\nopts = pulumi.ResourceOptions(depends_on=[this]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.AibiDashboardEmbeddingAccessPolicySetting(\"this\", new()\n    {\n        AibiDashboardEmbeddingAccessPolicy = new Databricks.Inputs.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs\n        {\n            AccessPolicyType = \"ALLOW_APPROVED_DOMAINS\",\n        },\n    });\n\n    var thisAibiDashboardEmbeddingApprovedDomainsSetting = new Databricks.AibiDashboardEmbeddingApprovedDomainsSetting(\"this\", new()\n    {\n        AibiDashboardEmbeddingApprovedDomains = new Databricks.Inputs.AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomainsArgs\n        {\n            ApprovedDomains = new[]\n            {\n                \"test.com\",\n            },\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            @this,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewAibiDashboardEmbeddingAccessPolicySetting(ctx, \"this\", \u0026databricks.AibiDashboardEmbeddingAccessPolicySettingArgs{\n\t\t\tAibiDashboardEmbeddingAccessPolicy: \u0026databricks.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs{\n\t\t\t\tAccessPolicyType: pulumi.String(\"ALLOW_APPROVED_DOMAINS\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAibiDashboardEmbeddingApprovedDomainsSetting(ctx, \"this\", \u0026databricks.AibiDashboardEmbeddingApprovedDomainsSettingArgs{\n\t\t\tAibiDashboardEmbeddingApprovedDomains: \u0026databricks.AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomainsArgs{\n\t\t\t\tApprovedDomains: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"test.com\"),\n\t\t\t\t},\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthis,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.AibiDashboardEmbeddingAccessPolicySetting;\nimport com.pulumi.databricks.AibiDashboardEmbeddingAccessPolicySettingArgs;\nimport com.pulumi.databricks.inputs.AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs;\nimport com.pulumi.databricks.AibiDashboardEmbeddingApprovedDomainsSetting;\nimport com.pulumi.databricks.AibiDashboardEmbeddingApprovedDomainsSettingArgs;\nimport com.pulumi.databricks.inputs.AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomainsArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new AibiDashboardEmbeddingAccessPolicySetting(\"this\", AibiDashboardEmbeddingAccessPolicySettingArgs.builder()\n            .aibiDashboardEmbeddingAccessPolicy(AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicyArgs.builder()\n                .accessPolicyType(\"ALLOW_APPROVED_DOMAINS\")\n                .build())\n            .build());\n\n        var thisAibiDashboardEmbeddingApprovedDomainsSetting = new AibiDashboardEmbeddingApprovedDomainsSetting(\"thisAibiDashboardEmbeddingApprovedDomainsSetting\", AibiDashboardEmbeddingApprovedDomainsSettingArgs.builder()\n            .aibiDashboardEmbeddingApprovedDomains(AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomainsArgs.builder()\n                .approvedDomains(\"test.com\")\n                .build())\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(this_)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:AibiDashboardEmbeddingAccessPolicySetting\n    properties:\n      aibiDashboardEmbeddingAccessPolicy:\n        accessPolicyType: ALLOW_APPROVED_DOMAINS\n  thisAibiDashboardEmbeddingApprovedDomainsSetting:\n    type: databricks:AibiDashboardEmbeddingApprovedDomainsSetting\n    name: this\n    properties:\n      aibiDashboardEmbeddingApprovedDomains:\n        approvedDomains:\n          - test.com\n    options:\n      dependsOn:\n        - ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n- databricks.AibiDashboardEmbeddingAccessPolicySetting is used to control embedding policy.\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/aibiDashboardEmbeddingApprovedDomainsSetting:AibiDashboardEmbeddingApprovedDomainsSetting this global\n```\n\n",
            "properties": {
                "aibiDashboardEmbeddingApprovedDomains": {
                    "$ref": "#/types/databricks:index/AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains:AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains",
                    "description": "block with following attributes:\n"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "aibiDashboardEmbeddingApprovedDomains",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "aibiDashboardEmbeddingApprovedDomains": {
                    "$ref": "#/types/databricks:index/AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains:AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains",
                    "description": "block with following attributes:\n"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "aibiDashboardEmbeddingApprovedDomains"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AibiDashboardEmbeddingApprovedDomainsSetting resources.\n",
                "properties": {
                    "aibiDashboardEmbeddingApprovedDomains": {
                        "$ref": "#/types/databricks:index/AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains:AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains",
                        "description": "block with following attributes:\n"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/alert:Alert": {
            "description": "\n\n## Import\n\nThis resource can be imported using alert ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/alert:Alert this \u003calert-id\u003e\n```\n\n",
            "properties": {
                "condition": {
                    "$ref": "#/types/databricks:index/AlertCondition:AlertCondition",
                    "description": "Trigger conditions of the alert. Block consists of the following attributes:\n"
                },
                "createTime": {
                    "type": "string",
                    "description": "The timestamp string indicating when the alert was created.\n"
                },
                "customBody": {
                    "type": "string",
                    "description": "Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "customSubject": {
                    "type": "string",
                    "description": "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "lifecycleState": {
                    "type": "string",
                    "description": "The workspace state of the alert. Used for tracking trashed status. (Possible values are `ACTIVE` or `TRASHED`).\n"
                },
                "notifyOnOk": {
                    "type": "boolean",
                    "description": "Whether to notify alert subscribers when alert returns back to normal.\n"
                },
                "ownerUserName": {
                    "type": "string",
                    "description": "Alert owner's username.\n"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The path to a workspace folder containing the alert. The default is the user's home folder.  If changed, the alert will be recreated.\n"
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "secondsToRetrigger": {
                    "type": "integer",
                    "description": "Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again.\n"
                },
                "state": {
                    "type": "string",
                    "description": "Current state of the alert's trigger status (`UNKNOWN`, `OK`, `TRIGGERED`). This field is set to `UNKNOWN` if the alert has not yet been evaluated or ran into an error during the last evaluation.\n"
                },
                "triggerTime": {
                    "type": "string",
                    "description": "The timestamp string when the alert was last triggered if the alert has been triggered before.\n"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The timestamp string indicating when the alert was updated.\n"
                }
            },
            "required": [
                "condition",
                "createTime",
                "displayName",
                "lifecycleState",
                "queryId",
                "state",
                "triggerTime",
                "updateTime"
            ],
            "inputProperties": {
                "condition": {
                    "$ref": "#/types/databricks:index/AlertCondition:AlertCondition",
                    "description": "Trigger conditions of the alert. Block consists of the following attributes:\n"
                },
                "customBody": {
                    "type": "string",
                    "description": "Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "customSubject": {
                    "type": "string",
                    "description": "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "notifyOnOk": {
                    "type": "boolean",
                    "description": "Whether to notify alert subscribers when alert returns back to normal.\n"
                },
                "ownerUserName": {
                    "type": "string",
                    "description": "Alert owner's username.\n"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The path to a workspace folder containing the alert. The default is the user's home folder.  If changed, the alert will be recreated.\n",
                    "willReplaceOnChanges": true
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "secondsToRetrigger": {
                    "type": "integer",
                    "description": "Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again.\n"
                }
            },
            "requiredInputs": [
                "condition",
                "displayName",
                "queryId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Alert resources.\n",
                "properties": {
                    "condition": {
                        "$ref": "#/types/databricks:index/AlertCondition:AlertCondition",
                        "description": "Trigger conditions of the alert. Block consists of the following attributes:\n"
                    },
                    "createTime": {
                        "type": "string",
                        "description": "The timestamp string indicating when the alert was created.\n"
                    },
                    "customBody": {
                        "type": "string",
                        "description": "Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                    },
                    "customSubject": {
                        "type": "string",
                        "description": "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/en/sql/user/alerts/index.html) for custom templating instructions.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Name of the alert.\n"
                    },
                    "lifecycleState": {
                        "type": "string",
                        "description": "The workspace state of the alert. Used for tracking trashed status. (Possible values are `ACTIVE` or `TRASHED`).\n"
                    },
                    "notifyOnOk": {
                        "type": "boolean",
                        "description": "Whether to notify alert subscribers when alert returns back to normal.\n"
                    },
                    "ownerUserName": {
                        "type": "string",
                        "description": "Alert owner's username.\n"
                    },
                    "parentPath": {
                        "type": "string",
                        "description": "The path to a workspace folder containing the alert. The default is the user's home folder.  If changed, the alert will be recreated.\n",
                        "willReplaceOnChanges": true
                    },
                    "queryId": {
                        "type": "string",
                        "description": "ID of the query evaluated by the alert.\n"
                    },
                    "secondsToRetrigger": {
                        "type": "integer",
                        "description": "Number of seconds an alert must wait after being triggered to rearm itself. After rearming, it can be triggered again. If 0 or not specified, the alert will not be triggered again.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "Current state of the alert's trigger status (`UNKNOWN`, `OK`, `TRIGGERED`). This field is set to `UNKNOWN` if the alert has not yet been evaluated or ran into an error during the last evaluation.\n"
                    },
                    "triggerTime": {
                        "type": "string",
                        "description": "The timestamp string when the alert was last triggered if the alert has been triggered before.\n"
                    },
                    "updateTime": {
                        "type": "string",
                        "description": "The timestamp string indicating when the alert was updated.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/app:App": {
            "description": "\u003e This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n[Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```yaml\nresources:\n  this:\n    type: databricks:App\n    properties:\n      name: my-custom-app\n      description: My app\n      resources:\n        - name: sql-warehouse\n          sql_warehouse:\n            id: e9ca293f79a74b5c\n            permission: CAN_MANAGE\n        - name: serving-endpoint\n          serving_endpoint:\n            name: databricks-meta-llama-3-1-70b-instruct\n            permission: CAN_MANAGE\n        - name: job\n          job:\n            id: '1234'\n            permission: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.\n\n## Import\n\nThis resource can be imported by name:\n\nhcl\n\nimport {\n\n  to = databricks_app.this\n\n  id = \"\u003capp_name\u003e\"\n\n}\n\nor using the `terraform` CLI:\n\nbash\n\n```sh\n$ pulumi import databricks:index/app:App this \u003capp_name\u003e\n```\n\n",
            "properties": {
                "activeDeployment": {
                    "$ref": "#/types/databricks:index/AppActiveDeployment:AppActiveDeployment"
                },
                "appStatus": {
                    "$ref": "#/types/databricks:index/AppAppStatus:AppAppStatus",
                    "description": "attribute\n"
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The optional Budget Policy ID set for this resource.\n"
                },
                "computeStatus": {
                    "$ref": "#/types/databricks:index/AppComputeStatus:AppComputeStatus",
                    "description": "attribute\n"
                },
                "createTime": {
                    "type": "string",
                    "description": "The creation time of the app.\n"
                },
                "creator": {
                    "type": "string",
                    "description": "The email of the user that created the app.\n"
                },
                "defaultSourceCodePath": {
                    "type": "string",
                    "description": "The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.\n"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the app.\n"
                },
                "effectiveBudgetPolicyId": {
                    "type": "string",
                    "description": "The effective budget policy ID.\n"
                },
                "effectiveUserApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "The name of the app. The name must contain only lowercase alphanumeric characters and hyphens. It must be unique within the workspace.\n"
                },
                "noCompute": {
                    "type": "boolean"
                },
                "oauth2AppClientId": {
                    "type": "string"
                },
                "oauth2AppIntegrationId": {
                    "type": "string"
                },
                "pendingDeployment": {
                    "$ref": "#/types/databricks:index/AppPendingDeployment:AppPendingDeployment"
                },
                "resources": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AppResource:AppResource"
                    },
                    "description": "A list of resources that the app have access to.\n"
                },
                "servicePrincipalClientId": {
                    "type": "string"
                },
                "servicePrincipalId": {
                    "type": "integer",
                    "description": "id of the app service principal\n"
                },
                "servicePrincipalName": {
                    "type": "string",
                    "description": "name of the app service principal\n"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The update time of the app.\n"
                },
                "updater": {
                    "type": "string",
                    "description": "The email of the user that last updated the app.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the app once it is deployed.\n"
                },
                "userApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "activeDeployment",
                "appStatus",
                "computeStatus",
                "createTime",
                "creator",
                "defaultSourceCodePath",
                "effectiveBudgetPolicyId",
                "effectiveUserApiScopes",
                "name",
                "oauth2AppClientId",
                "oauth2AppIntegrationId",
                "pendingDeployment",
                "servicePrincipalClientId",
                "servicePrincipalId",
                "servicePrincipalName",
                "updateTime",
                "updater",
                "url"
            ],
            "inputProperties": {
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The optional Budget Policy ID set for this resource.\n"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the app.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the app. The name must contain only lowercase alphanumeric characters and hyphens. It must be unique within the workspace.\n"
                },
                "noCompute": {
                    "type": "boolean"
                },
                "resources": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AppResource:AppResource"
                    },
                    "description": "A list of resources that the app have access to.\n"
                },
                "userApiScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering App resources.\n",
                "properties": {
                    "activeDeployment": {
                        "$ref": "#/types/databricks:index/AppActiveDeployment:AppActiveDeployment"
                    },
                    "appStatus": {
                        "$ref": "#/types/databricks:index/AppAppStatus:AppAppStatus",
                        "description": "attribute\n"
                    },
                    "budgetPolicyId": {
                        "type": "string",
                        "description": "The optional Budget Policy ID set for this resource.\n"
                    },
                    "computeStatus": {
                        "$ref": "#/types/databricks:index/AppComputeStatus:AppComputeStatus",
                        "description": "attribute\n"
                    },
                    "createTime": {
                        "type": "string",
                        "description": "The creation time of the app.\n"
                    },
                    "creator": {
                        "type": "string",
                        "description": "The email of the user that created the app.\n"
                    },
                    "defaultSourceCodePath": {
                        "type": "string",
                        "description": "The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the app.\n"
                    },
                    "effectiveBudgetPolicyId": {
                        "type": "string",
                        "description": "The effective budget policy ID.\n"
                    },
                    "effectiveUserApiScopes": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the app. The name must contain only lowercase alphanumeric characters and hyphens. It must be unique within the workspace.\n"
                    },
                    "noCompute": {
                        "type": "boolean"
                    },
                    "oauth2AppClientId": {
                        "type": "string"
                    },
                    "oauth2AppIntegrationId": {
                        "type": "string"
                    },
                    "pendingDeployment": {
                        "$ref": "#/types/databricks:index/AppPendingDeployment:AppPendingDeployment"
                    },
                    "resources": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/AppResource:AppResource"
                        },
                        "description": "A list of resources that the app have access to.\n"
                    },
                    "servicePrincipalClientId": {
                        "type": "string"
                    },
                    "servicePrincipalId": {
                        "type": "integer",
                        "description": "id of the app service principal\n"
                    },
                    "servicePrincipalName": {
                        "type": "string",
                        "description": "name of the app service principal\n"
                    },
                    "updateTime": {
                        "type": "string",
                        "description": "The update time of the app.\n"
                    },
                    "updater": {
                        "type": "string",
                        "description": "The email of the user that last updated the app.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "The URL of the app once it is deployed.\n"
                    },
                    "userApiScopes": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/artifactAllowlist:ArtifactAllowlist": {
            "description": "\u003e It is required to define all allowlist for an artifact type in a single resource, otherwise Pulumi cannot guarantee config drift prevention.\n\n\u003e This resource can only be used with a workspace-level provider!\n\nIn Databricks Runtime 13.3 and above, you can add libraries and init scripts to the allowlist in UC so that users can leverage these artifacts on compute configured with shared access mode.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst initScripts = new databricks.ArtifactAllowlist(\"init_scripts\", {\n    artifactType: \"INIT_SCRIPT\",\n    artifactMatchers: [{\n        artifact: \"/Volumes/inits\",\n        matchType: \"PREFIX_MATCH\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninit_scripts = databricks.ArtifactAllowlist(\"init_scripts\",\n    artifact_type=\"INIT_SCRIPT\",\n    artifact_matchers=[{\n        \"artifact\": \"/Volumes/inits\",\n        \"match_type\": \"PREFIX_MATCH\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var initScripts = new Databricks.ArtifactAllowlist(\"init_scripts\", new()\n    {\n        ArtifactType = \"INIT_SCRIPT\",\n        ArtifactMatchers = new[]\n        {\n            new Databricks.Inputs.ArtifactAllowlistArtifactMatcherArgs\n            {\n                Artifact = \"/Volumes/inits\",\n                MatchType = \"PREFIX_MATCH\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewArtifactAllowlist(ctx, \"init_scripts\", \u0026databricks.ArtifactAllowlistArgs{\n\t\t\tArtifactType: pulumi.String(\"INIT_SCRIPT\"),\n\t\t\tArtifactMatchers: databricks.ArtifactAllowlistArtifactMatcherArray{\n\t\t\t\t\u0026databricks.ArtifactAllowlistArtifactMatcherArgs{\n\t\t\t\t\tArtifact:  pulumi.String(\"/Volumes/inits\"),\n\t\t\t\t\tMatchType: pulumi.String(\"PREFIX_MATCH\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ArtifactAllowlist;\nimport com.pulumi.databricks.ArtifactAllowlistArgs;\nimport com.pulumi.databricks.inputs.ArtifactAllowlistArtifactMatcherArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var initScripts = new ArtifactAllowlist(\"initScripts\", ArtifactAllowlistArgs.builder()\n            .artifactType(\"INIT_SCRIPT\")\n            .artifactMatchers(ArtifactAllowlistArtifactMatcherArgs.builder()\n                .artifact(\"/Volumes/inits\")\n                .matchType(\"PREFIX_MATCH\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  initScripts:\n    type: databricks:ArtifactAllowlist\n    name: init_scripts\n    properties:\n      artifactType: INIT_SCRIPT\n      artifactMatchers:\n        - artifact: /Volumes/inits\n          matchType: PREFIX_MATCH\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/artifactAllowlist:ArtifactAllowlist this '\u003cmetastore_id\u003e|\u003cartifact_type\u003e'\n```\n\n",
            "properties": {
                "artifactMatchers": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                    }
                },
                "artifactType": {
                    "type": "string",
                    "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this artifact allowlist was set.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Identity that set the artifact allowlist.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                }
            },
            "required": [
                "artifactMatchers",
                "artifactType",
                "createdAt",
                "createdBy",
                "metastoreId"
            ],
            "inputProperties": {
                "artifactMatchers": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                    }
                },
                "artifactType": {
                    "type": "string",
                    "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this artifact allowlist was set.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Identity that set the artifact allowlist.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                }
            },
            "requiredInputs": [
                "artifactMatchers",
                "artifactType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ArtifactAllowlist resources.\n",
                "properties": {
                    "artifactMatchers": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                        }
                    },
                    "artifactType": {
                        "type": "string",
                        "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this artifact allowlist was set.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Identity that set the artifact allowlist.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "ID of the parent metastore.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/automaticClusterUpdateWorkspaceSetting:AutomaticClusterUpdateWorkspaceSetting": {
            "properties": {
                "automaticClusterUpdateWorkspace": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "automaticClusterUpdateWorkspace",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "automaticClusterUpdateWorkspace": {
                    "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "automaticClusterUpdateWorkspace"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AutomaticClusterUpdateWorkspaceSetting resources.\n",
                "properties": {
                    "automaticClusterUpdateWorkspace": {
                        "$ref": "#/types/databricks:index/AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace:AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/budget:Budget": {
            "description": "\u003e Initialize provider with `alias = \"account\"`, and `host` pointing to the account URL, like, `host = \"https://accounts.cloud.databricks.com\"`. Use `provider = databricks.account` for all account-level resources.\n\n\u003e This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\nThis resource allows you to manage [Databricks Budgets](https://docs.databricks.com/en/admin/account-settings/budgets.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Budget(\"this\", {\n    displayName: \"databricks-workspace-budget\",\n    alertConfigurations: [{\n        timePeriod: \"MONTH\",\n        triggerType: \"CUMULATIVE_SPENDING_EXCEEDED\",\n        quantityType: \"LIST_PRICE_DOLLARS_USD\",\n        quantityThreshold: \"840\",\n        actionConfigurations: [{\n            actionType: \"EMAIL_NOTIFICATION\",\n            target: \"abc@gmail.com\",\n        }],\n    }],\n    filter: {\n        workspaceId: {\n            operator: \"IN\",\n            values: [1234567890098765],\n        },\n        tags: [\n            {\n                key: \"Team\",\n                value: {\n                    operator: \"IN\",\n                    values: [\"Data Science\"],\n                },\n            },\n            {\n                key: \"Environment\",\n                value: {\n                    operator: \"IN\",\n                    values: [\"Development\"],\n                },\n            },\n        ],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Budget(\"this\",\n    display_name=\"databricks-workspace-budget\",\n    alert_configurations=[{\n        \"time_period\": \"MONTH\",\n        \"trigger_type\": \"CUMULATIVE_SPENDING_EXCEEDED\",\n        \"quantity_type\": \"LIST_PRICE_DOLLARS_USD\",\n        \"quantity_threshold\": \"840\",\n        \"action_configurations\": [{\n            \"action_type\": \"EMAIL_NOTIFICATION\",\n            \"target\": \"abc@gmail.com\",\n        }],\n    }],\n    filter={\n        \"workspace_id\": {\n            \"operator\": \"IN\",\n            \"values\": [1234567890098765],\n        },\n        \"tags\": [\n            {\n                \"key\": \"Team\",\n                \"value\": {\n                    \"operator\": \"IN\",\n                    \"values\": [\"Data Science\"],\n                },\n            },\n            {\n                \"key\": \"Environment\",\n                \"value\": {\n                    \"operator\": \"IN\",\n                    \"values\": [\"Development\"],\n                },\n            },\n        ],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Budget(\"this\", new()\n    {\n        DisplayName = \"databricks-workspace-budget\",\n        AlertConfigurations = new[]\n        {\n            new Databricks.Inputs.BudgetAlertConfigurationArgs\n            {\n                TimePeriod = \"MONTH\",\n                TriggerType = \"CUMULATIVE_SPENDING_EXCEEDED\",\n                QuantityType = \"LIST_PRICE_DOLLARS_USD\",\n                QuantityThreshold = \"840\",\n                ActionConfigurations = new[]\n                {\n                    new Databricks.Inputs.BudgetAlertConfigurationActionConfigurationArgs\n                    {\n                        ActionType = \"EMAIL_NOTIFICATION\",\n                        Target = \"abc@gmail.com\",\n                    },\n                },\n            },\n        },\n        Filter = new Databricks.Inputs.BudgetFilterArgs\n        {\n            WorkspaceId = new Databricks.Inputs.BudgetFilterWorkspaceIdArgs\n            {\n                Operator = \"IN\",\n                Values = new[]\n                {\n                    1234567890098765,\n                },\n            },\n            Tags = new[]\n            {\n                new Databricks.Inputs.BudgetFilterTagArgs\n                {\n                    Key = \"Team\",\n                    Value = new Databricks.Inputs.BudgetFilterTagValueArgs\n                    {\n                        Operator = \"IN\",\n                        Values = new[]\n                        {\n                            \"Data Science\",\n                        },\n                    },\n                },\n                new Databricks.Inputs.BudgetFilterTagArgs\n                {\n                    Key = \"Environment\",\n                    Value = new Databricks.Inputs.BudgetFilterTagValueArgs\n                    {\n                        Operator = \"IN\",\n                        Values = new[]\n                        {\n                            \"Development\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewBudget(ctx, \"this\", \u0026databricks.BudgetArgs{\n\t\t\tDisplayName: pulumi.String(\"databricks-workspace-budget\"),\n\t\t\tAlertConfigurations: databricks.BudgetAlertConfigurationArray{\n\t\t\t\t\u0026databricks.BudgetAlertConfigurationArgs{\n\t\t\t\t\tTimePeriod:        pulumi.String(\"MONTH\"),\n\t\t\t\t\tTriggerType:       pulumi.String(\"CUMULATIVE_SPENDING_EXCEEDED\"),\n\t\t\t\t\tQuantityType:      pulumi.String(\"LIST_PRICE_DOLLARS_USD\"),\n\t\t\t\t\tQuantityThreshold: pulumi.String(\"840\"),\n\t\t\t\t\tActionConfigurations: databricks.BudgetAlertConfigurationActionConfigurationArray{\n\t\t\t\t\t\t\u0026databricks.BudgetAlertConfigurationActionConfigurationArgs{\n\t\t\t\t\t\t\tActionType: pulumi.String(\"EMAIL_NOTIFICATION\"),\n\t\t\t\t\t\t\tTarget:     pulumi.String(\"abc@gmail.com\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tFilter: \u0026databricks.BudgetFilterArgs{\n\t\t\t\tWorkspaceId: \u0026databricks.BudgetFilterWorkspaceIdArgs{\n\t\t\t\t\tOperator: pulumi.String(\"IN\"),\n\t\t\t\t\tValues: pulumi.IntArray{\n\t\t\t\t\t\tpulumi.Int(1234567890098765),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tTags: databricks.BudgetFilterTagArray{\n\t\t\t\t\t\u0026databricks.BudgetFilterTagArgs{\n\t\t\t\t\t\tKey: pulumi.String(\"Team\"),\n\t\t\t\t\t\tValue: \u0026databricks.BudgetFilterTagValueArgs{\n\t\t\t\t\t\t\tOperator: pulumi.String(\"IN\"),\n\t\t\t\t\t\t\tValues: pulumi.StringArray{\n\t\t\t\t\t\t\t\tpulumi.String(\"Data Science\"),\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\t\u0026databricks.BudgetFilterTagArgs{\n\t\t\t\t\t\tKey: pulumi.String(\"Environment\"),\n\t\t\t\t\t\tValue: \u0026databricks.BudgetFilterTagValueArgs{\n\t\t\t\t\t\t\tOperator: pulumi.String(\"IN\"),\n\t\t\t\t\t\t\tValues: pulumi.StringArray{\n\t\t\t\t\t\t\t\tpulumi.String(\"Development\"),\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Budget;\nimport com.pulumi.databricks.BudgetArgs;\nimport com.pulumi.databricks.inputs.BudgetAlertConfigurationArgs;\nimport com.pulumi.databricks.inputs.BudgetFilterArgs;\nimport com.pulumi.databricks.inputs.BudgetFilterWorkspaceIdArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Budget(\"this\", BudgetArgs.builder()\n            .displayName(\"databricks-workspace-budget\")\n            .alertConfigurations(BudgetAlertConfigurationArgs.builder()\n                .timePeriod(\"MONTH\")\n                .triggerType(\"CUMULATIVE_SPENDING_EXCEEDED\")\n                .quantityType(\"LIST_PRICE_DOLLARS_USD\")\n                .quantityThreshold(\"840\")\n                .actionConfigurations(BudgetAlertConfigurationActionConfigurationArgs.builder()\n                    .actionType(\"EMAIL_NOTIFICATION\")\n                    .target(\"abc@gmail.com\")\n                    .build())\n                .build())\n            .filter(BudgetFilterArgs.builder()\n                .workspaceId(BudgetFilterWorkspaceIdArgs.builder()\n                    .operator(\"IN\")\n                    .values(1234567890098765)\n                    .build())\n                .tags(                \n                    BudgetFilterTagArgs.builder()\n                        .key(\"Team\")\n                        .value(BudgetFilterTagValueArgs.builder()\n                            .operator(\"IN\")\n                            .values(\"Data Science\")\n                            .build())\n                        .build(),\n                    BudgetFilterTagArgs.builder()\n                        .key(\"Environment\")\n                        .value(BudgetFilterTagValueArgs.builder()\n                            .operator(\"IN\")\n                            .values(\"Development\")\n                            .build())\n                        .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Budget\n    properties:\n      displayName: databricks-workspace-budget\n      alertConfigurations:\n        - timePeriod: MONTH\n          triggerType: CUMULATIVE_SPENDING_EXCEEDED\n          quantityType: LIST_PRICE_DOLLARS_USD\n          quantityThreshold: '840'\n          actionConfigurations:\n            - actionType: EMAIL_NOTIFICATION\n              target: abc@gmail.com\n      filter:\n        workspaceId:\n          operator: IN\n          values:\n            - 1.234567890098765e+15\n        tags:\n          - key: Team\n            value:\n              operator: IN\n              values:\n                - Data Science\n          - key: Environment\n            value:\n              operator: IN\n              values:\n                - Development\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n\n## Import\n\nThis resource can be imported by Databricks account ID and Budget.\n\n```sh\n$ pulumi import databricks:index/budget:Budget this '\u003caccount_id\u003e|\u003cbudget_configuration_id\u003e'\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "The ID of the Databricks Account.\n"
                },
                "alertConfigurations": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetAlertConfiguration:BudgetAlertConfiguration"
                    }
                },
                "budgetConfigurationId": {
                    "type": "string",
                    "description": "The ID of the budget configuration.\n"
                },
                "createTime": {
                    "type": "integer"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the budget in Databricks Account.\n"
                },
                "filter": {
                    "$ref": "#/types/databricks:index/BudgetFilter:BudgetFilter"
                },
                "updateTime": {
                    "type": "integer"
                }
            },
            "required": [
                "accountId",
                "budgetConfigurationId",
                "createTime",
                "updateTime"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "The ID of the Databricks Account.\n"
                },
                "alertConfigurations": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetAlertConfiguration:BudgetAlertConfiguration"
                    }
                },
                "budgetConfigurationId": {
                    "type": "string",
                    "description": "The ID of the budget configuration.\n"
                },
                "createTime": {
                    "type": "integer"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the budget in Databricks Account.\n"
                },
                "filter": {
                    "$ref": "#/types/databricks:index/BudgetFilter:BudgetFilter"
                },
                "updateTime": {
                    "type": "integer"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Budget resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "The ID of the Databricks Account.\n"
                    },
                    "alertConfigurations": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/BudgetAlertConfiguration:BudgetAlertConfiguration"
                        }
                    },
                    "budgetConfigurationId": {
                        "type": "string",
                        "description": "The ID of the budget configuration.\n"
                    },
                    "createTime": {
                        "type": "integer"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Name of the budget in Databricks Account.\n"
                    },
                    "filter": {
                        "$ref": "#/types/databricks:index/BudgetFilter:BudgetFilter"
                    },
                    "updateTime": {
                        "type": "integer"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/budgetPolicy:BudgetPolicy": {
            "description": "Administrators can use budget policies to ensure that the correct tags appear automatically on serverless resources without depending on users to attach tags manually, allowing for customized cost reporting and chargebacks. Budget policies consist of tags that are applied to any serverless compute activity incurred by a user assigned to the policy. The tags are logged in your billing records, allowing you to attribute serverless usage to specific budgets.\n\n\u003e This resource can only be used with an account-level provider!\n\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.BudgetPolicy(\"this\", {\n    policyName: \"my-budget-policy\",\n    customTags: [{\n        key: \"mykey\",\n        value: \"myvalue\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.BudgetPolicy(\"this\",\n    policy_name=\"my-budget-policy\",\n    custom_tags=[{\n        \"key\": \"mykey\",\n        \"value\": \"myvalue\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.BudgetPolicy(\"this\", new()\n    {\n        PolicyName = \"my-budget-policy\",\n        CustomTags = new[]\n        {\n            new Databricks.Inputs.BudgetPolicyCustomTagArgs\n            {\n                Key = \"mykey\",\n                Value = \"myvalue\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewBudgetPolicy(ctx, \"this\", \u0026databricks.BudgetPolicyArgs{\n\t\t\tPolicyName: pulumi.String(\"my-budget-policy\"),\n\t\t\tCustomTags: databricks.BudgetPolicyCustomTagArray{\n\t\t\t\t\u0026databricks.BudgetPolicyCustomTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"mykey\"),\n\t\t\t\t\tValue: pulumi.String(\"myvalue\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.BudgetPolicy;\nimport com.pulumi.databricks.BudgetPolicyArgs;\nimport com.pulumi.databricks.inputs.BudgetPolicyCustomTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new BudgetPolicy(\"this\", BudgetPolicyArgs.builder()\n            .policyName(\"my-budget-policy\")\n            .customTags(BudgetPolicyCustomTagArgs.builder()\n                .key(\"mykey\")\n                .value(\"myvalue\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:BudgetPolicy\n    properties:\n      policyName: my-budget-policy\n      customTags:\n        - key: mykey\n          value: myvalue\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.AccessControlRuleSet can control which groups or individual users can manage or use the given budget policy.\n\n## Import\n\nThis resource can be imported by ID.\n\n```sh\n$ pulumi import databricks:index/budgetPolicy:BudgetPolicy this policy_id\n```\n\n",
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetPolicyCustomTag:BudgetPolicyCustomTag"
                    },
                    "description": "A list of tags defined by the customer. At most 20 entries are allowed per policy.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "ID of the budget policy\n"
                },
                "policyName": {
                    "type": "string",
                    "description": "The name of the policy. Must be unique among active policies. Can contain only characters from the ISO 8859-1 (latin1) set.\n"
                }
            },
            "required": [
                "policyId"
            ],
            "inputProperties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/BudgetPolicyCustomTag:BudgetPolicyCustomTag"
                    },
                    "description": "A list of tags defined by the customer. At most 20 entries are allowed per policy.\n"
                },
                "policyName": {
                    "type": "string",
                    "description": "The name of the policy. Must be unique among active policies. Can contain only characters from the ISO 8859-1 (latin1) set.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering BudgetPolicy resources.\n",
                "properties": {
                    "customTags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/BudgetPolicyCustomTag:BudgetPolicyCustomTag"
                        },
                        "description": "A list of tags defined by the customer. At most 20 entries are allowed per policy.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "ID of the budget policy\n"
                    },
                    "policyName": {
                        "type": "string",
                        "description": "The name of the policy. Must be unique among active policies. Can contain only characters from the ISO 8859-1 (latin1) set.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/catalog:Catalog": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nWithin a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, Databases (also called Schemas), and Tables / Views.\n\nA `databricks.Catalog` is contained within databricks.Metastore and can contain databricks_schema. By default, Databricks creates `default` schema for every new catalog, but Pulumi plugin is removing this auto-created schema, so that resource destruction could be done in a clean way.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/catalog:Catalog this \u003cname\u003e\n```\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "connectionName": {
                    "type": "string",
                    "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n"
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "enablePredictiveOptimization",
                "isolationMode",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "connectionName": {
                    "type": "string",
                    "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Catalog resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "connectionName": {
                        "type": "string",
                        "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "enablePredictiveOptimization": {
                        "type": "string",
                        "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete catalog regardless of its contents.\n"
                    },
                    "isolationMode": {
                        "type": "string",
                        "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "ID of the parent metastore.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Catalog relative to parent metastore.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the catalog owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Extensible Catalog properties.\n"
                    },
                    "providerName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "shareName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/catalogWorkspaceBinding:CatalogWorkspaceBinding": {
            "description": "\u003e This resource has been deprecated and will be removed soon. Please use the databricks.WorkspaceBinding resource instead.\n\nIf you use workspaces to isolate user data access, you may want to limit catalog access to specific workspaces in your account, also known as workspace-catalog binding\n\nBy default, Databricks assigns the catalog to all workspaces attached to the current metastore. By using `databricks.CatalogWorkspaceBinding`, the catalog will be unassigned from all workspaces and only assigned explicitly using this resource.\n\n\u003e To use this resource the catalog must have its isolation mode set to `ISOLATED` in the `databricks.Catalog` resource. Alternatively, the isolation mode can be set using the UI or API by following [this guide](https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html#configuration).\n\n\u003e If the catalog's isolation mode was set to `ISOLATED` using Pulumi then the catalog will have been automatically bound to the workspace it was created from.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    isolationMode: \"ISOLATED\",\n});\nconst sandboxCatalogWorkspaceBinding = new databricks.CatalogWorkspaceBinding(\"sandbox\", {\n    securableName: sandbox.name,\n    workspaceId: other.workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    isolation_mode=\"ISOLATED\")\nsandbox_catalog_workspace_binding = databricks.CatalogWorkspaceBinding(\"sandbox\",\n    securable_name=sandbox.name,\n    workspace_id=other[\"workspaceId\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        IsolationMode = \"ISOLATED\",\n    });\n\n    var sandboxCatalogWorkspaceBinding = new Databricks.CatalogWorkspaceBinding(\"sandbox\", new()\n    {\n        SecurableName = sandbox.Name,\n        WorkspaceId = other.WorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:          pulumi.String(\"sandbox\"),\n\t\t\tIsolationMode: pulumi.String(\"ISOLATED\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCatalogWorkspaceBinding(ctx, \"sandbox\", \u0026databricks.CatalogWorkspaceBindingArgs{\n\t\t\tSecurableName: sandbox.Name,\n\t\t\tWorkspaceId:   pulumi.Any(other.WorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.CatalogWorkspaceBinding;\nimport com.pulumi.databricks.CatalogWorkspaceBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .isolationMode(\"ISOLATED\")\n            .build());\n\n        var sandboxCatalogWorkspaceBinding = new CatalogWorkspaceBinding(\"sandboxCatalogWorkspaceBinding\", CatalogWorkspaceBindingArgs.builder()\n            .securableName(sandbox.name())\n            .workspaceId(other.workspaceId())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      isolationMode: ISOLATED\n  sandboxCatalogWorkspaceBinding:\n    type: databricks:CatalogWorkspaceBinding\n    name: sandbox\n    properties:\n      securableName: ${sandbox.name}\n      workspaceId: ${other.workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by using combination of workspace ID, securable type and name:\n\n```sh\n$ pulumi import databricks:index/catalogWorkspaceBinding:CatalogWorkspaceBinding this \"\u003cworkspace_id\u003e|\u003csecurable_type\u003e|\u003csecurable_name\u003e\"\n```\n\n",
            "properties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n"
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead."
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n"
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "securableName"
            ],
            "inputProperties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n",
                    "willReplaceOnChanges": true
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                    "willReplaceOnChanges": true
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n"
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering CatalogWorkspaceBinding resources.\n",
                "properties": {
                    "bindingType": {
                        "type": "string",
                        "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n",
                        "willReplaceOnChanges": true
                    },
                    "catalogName": {
                        "type": "string",
                        "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                        "willReplaceOnChanges": true
                    },
                    "securableName": {
                        "type": "string",
                        "description": "Name of securable. Change forces creation of a new resource.\n"
                    },
                    "securableType": {
                        "type": "string",
                        "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "ID of the workspace. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/cluster:Cluster": {
            "description": "\n\n## Import\n\nThe resource cluster can be imported using cluster id.\n\nbash\n\n```sh\n$ pulumi import databricks:index/cluster:Cluster this \u003ccluster-id\u003e\n```\n\n",
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.String(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.String(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\"ResourceClass\": pulumi.String(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()\n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:\n* `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.\n* `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.\n* `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n"
                },
                "isSingleNode": {
                    "type": "boolean",
                    "description": "When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.\n"
                },
                "kind": {
                    "type": "string",
                    "description": "The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "noWait": {
                    "type": "boolean",
                    "description": "If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    },\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", \"true\" },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.String(\"true\"),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.String(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.String(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()\n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", \"true\"),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "state": {
                    "type": "string",
                    "description": "(string) State of the cluster.\n"
                },
                "url": {
                    "type": "string"
                },
                "useMlRuntime": {
                    "type": "boolean",
                    "description": "Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.\n"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "required": [
                "clusterId",
                "defaultTags",
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "sparkVersion",
                "state",
                "url"
            ],
            "inputProperties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.String(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.String(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\"ResourceClass\": pulumi.String(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()\n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:\n* `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.\n* `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.\n* `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n"
                },
                "isSingleNode": {
                    "type": "boolean",
                    "description": "When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.\n"
                },
                "kind": {
                    "type": "string",
                    "description": "The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "noWait": {
                    "type": "boolean",
                    "description": "If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    },\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", \"true\" },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.String(\"true\"),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.String(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.String(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()\n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", \"true\"),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "useMlRuntime": {
                    "type": "boolean",
                    "description": "Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.\n"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "requiredInputs": [
                "sparkVersion"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Cluster resources.\n",
                "properties": {
                    "applyPolicyDefaultValues": {
                        "type": "boolean",
                        "description": "Whether to use policy default values for missing cluster attributes.\n"
                    },
                    "autoscale": {
                        "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                    },
                    "autoterminationMinutes": {
                        "type": "integer",
                        "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                    },
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterLogConf": {
                        "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                    },
                    "clusterMountInfos": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                        }
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.String(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.String(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\"ResourceClass\": pulumi.String(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()\n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                    },
                    "dataSecurityMode": {
                        "type": "string",
                        "description": "Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:\n* `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.\n* `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.\n* `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.\n"
                    },
                    "defaultTags": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                    },
                    "dockerImage": {
                        "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                    },
                    "driverInstancePoolId": {
                        "type": "string",
                        "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                    },
                    "driverNodeTypeId": {
                        "type": "string",
                        "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                    },
                    "enableLocalDiskEncryption": {
                        "type": "boolean",
                        "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                    },
                    "idempotencyToken": {
                        "type": "string",
                        "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "initScripts": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                        }
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                    },
                    "isPinned": {
                        "type": "boolean",
                        "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n"
                    },
                    "isSingleNode": {
                        "type": "boolean",
                        "description": "When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.\n"
                    },
                    "kind": {
                        "type": "string",
                        "description": "The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                        }
                    },
                    "noWait": {
                        "type": "boolean",
                        "description": "If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    },\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": \"true\",\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", \"true\" },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.String(\"true\"),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.String(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.String(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()\n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", \"true\"),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                    },
                    "numWorkers": {
                        "type": "integer",
                        "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.\n"
                    },
                    "runtimeEngine": {
                        "type": "string",
                        "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                    },
                    "singleUserName": {
                        "type": "string",
                        "description": "The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                    },
                    "sparkConf": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                    },
                    "sparkEnvVars": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                    },
                    "sshPublicKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "(string) State of the cluster.\n"
                    },
                    "url": {
                        "type": "string"
                    },
                    "useMlRuntime": {
                        "type": "boolean",
                        "description": "Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.\n"
                    },
                    "workloadType": {
                        "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/clusterPolicy:ClusterPolicy": {
            "description": "This resource creates a cluster policy, which limits the ability to create clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. cluster policies have ACLs that limit their use to specific users and groups. Only admin users can create, edit, and delete policies. Admin users also have access to all policies.\n\nCluster policies let you:\n\n* Limit users to create clusters with prescribed settings.\n* Simplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).\n* Control cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).\n\nCluster policy permissions limit which policies a user can select in the Policy drop-down when the user creates a cluster:\n\n* If no policies have been created in the workspace, the Policy drop-down does not display.\n* A user who has cluster create permission can select the `Free form` policy and create fully-configurable clusters.\n* A user who has both cluster create permission and access to cluster policies can select the Free form policy and policies they have access to.\n* A user that has access to only cluster policies, can select the policies they have access to.\n\n### Overriding the built-in cluster policies\n\nYou can override built-in cluster policies by creating a `databricks.ClusterPolicy` resource with following attributes:\n\n* `name` - the name of the built-in cluster policy.\n* `policy_family_id` - the ID of the cluster policy family used for built-in cluster policy.\n* `policy_family_definition_overrides` - settings to override in the built-in cluster policy.\n\nYou can obtain the list of defined cluster policies families using the `databricks policy-families list` command of the new [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html), or via [list policy families](https://docs.databricks.com/api/workspace/policyfamilies/list) REST API.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst personalVmOverride = {\n    autotermination_minutes: {\n        type: \"fixed\",\n        value: 220,\n        hidden: true,\n    },\n    \"custom_tags.Team\": {\n        type: \"fixed\",\n        value: team,\n    },\n};\nconst personalVm = new databricks.ClusterPolicy(\"personal_vm\", {\n    policyFamilyId: \"personal-vm\",\n    policyFamilyDefinitionOverrides: JSON.stringify(personalVmOverride),\n    name: \"Personal Compute\",\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\npersonal_vm_override = {\n    \"autotermination_minutes\": {\n        \"type\": \"fixed\",\n        \"value\": 220,\n        \"hidden\": True,\n    },\n    \"custom_tags.Team\": {\n        \"type\": \"fixed\",\n        \"value\": team,\n    },\n}\npersonal_vm = databricks.ClusterPolicy(\"personal_vm\",\n    policy_family_id=\"personal-vm\",\n    policy_family_definition_overrides=json.dumps(personal_vm_override),\n    name=\"Personal Compute\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var personalVmOverride = \n    {\n        { \"autotermination_minutes\", \n        {\n            { \"type\", \"fixed\" },\n            { \"value\", 220 },\n            { \"hidden\", true },\n        } },\n        { \"custom_tags.Team\", \n        {\n            { \"type\", \"fixed\" },\n            { \"value\", team },\n        } },\n    };\n\n    var personalVm = new Databricks.ClusterPolicy(\"personal_vm\", new()\n    {\n        PolicyFamilyId = \"personal-vm\",\n        PolicyFamilyDefinitionOverrides = JsonSerializer.Serialize(personalVmOverride),\n        Name = \"Personal Compute\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpersonalVmOverride := map[string]interface{}{\n\t\t\t\"autotermination_minutes\": map[string]interface{}{\n\t\t\t\t\"type\":   \"fixed\",\n\t\t\t\t\"value\":  220,\n\t\t\t\t\"hidden\": true,\n\t\t\t},\n\t\t\t\"custom_tags.Team\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": team,\n\t\t\t},\n\t\t}\n\t\ttmpJSON0, err := json.Marshal(personalVmOverride)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"personal_vm\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tPolicyFamilyId:                  pulumi.String(\"personal-vm\"),\n\t\t\tPolicyFamilyDefinitionOverrides: pulumi.String(json0),\n\t\t\tName:                            pulumi.String(\"Personal Compute\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var personalVmOverride = Map.ofEntries(\n            Map.entry(\"autotermination_minutes\", Map.ofEntries(\n                Map.entry(\"type\", \"fixed\"),\n                Map.entry(\"value\", 220),\n                Map.entry(\"hidden\", true)\n            )),\n            Map.entry(\"custom_tags.Team\", Map.ofEntries(\n                Map.entry(\"type\", \"fixed\"),\n                Map.entry(\"value\", team)\n            ))\n        );\n\n        var personalVm = new ClusterPolicy(\"personalVm\", ClusterPolicyArgs.builder()\n            .policyFamilyId(\"personal-vm\")\n            .policyFamilyDefinitionOverrides(serializeJson(\n                personalVmOverride))\n            .name(\"Personal Compute\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  personalVm:\n    type: databricks:ClusterPolicy\n    name: personal_vm\n    properties:\n      policyFamilyId: personal-vm\n      policyFamilyDefinitionOverrides:\n        fn::toJSON: ${personalVmOverride}\n      name: Personal Compute\nvariables:\n  personalVmOverride:\n    autotermination_minutes:\n      type: fixed\n      value: 220\n      hidden: true\n    custom_tags.Team:\n      type: fixed\n      value: ${team}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* Dynamic Passthrough Clusters for a Group guide.\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.getNodeType data to get the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n## Import\n\nThe resource cluster policy can be imported using the policy id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/clusterPolicy:ClusterPolicy this \u003ccluster-policy-id\u003e\n```\n\n",
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                },
                "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                    }
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyFamilyDefinitionOverrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                },
                "policyFamilyId": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the cluster policy.\n"
                }
            },
            "required": [
                "definition",
                "name",
                "policyId"
            ],
            "inputProperties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                },
                "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                    }
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyFamilyDefinitionOverrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                },
                "policyFamilyId": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ClusterPolicy resources.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "Additional human-readable description of the cluster policy.\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                        }
                    },
                    "maxClustersPerUser": {
                        "type": "integer",
                        "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "type": "string",
                        "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                    },
                    "policyFamilyId": {
                        "type": "string",
                        "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the cluster policy.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/complianceSecurityProfileWorkspaceSetting:ComplianceSecurityProfileWorkspaceSetting": {
            "properties": {
                "complianceSecurityProfileWorkspace": {
                    "$ref": "#/types/databricks:index/ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace:ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "complianceSecurityProfileWorkspace",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "complianceSecurityProfileWorkspace": {
                    "$ref": "#/types/databricks:index/ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace:ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "complianceSecurityProfileWorkspace"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ComplianceSecurityProfileWorkspaceSetting resources.\n",
                "properties": {
                    "complianceSecurityProfileWorkspace": {
                        "$ref": "#/types/databricks:index/ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace:ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/connection:Connection": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nLakehouse Federation is the query federation platform for Databricks. Databricks uses Unity Catalog to manage query federation. To make a dataset available for read-only querying using Lakehouse Federation, you create the following:\n\n- A connection, a securable object in Unity Catalog that specifies a path and credentials for accessing an external database system.\n- A foreign catalog\n\nThis resource manages connections in Unity Catalog\n\n## Example Usage\n\nCreate a connection to a MySQL database\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst mysql = new databricks.Connection(\"mysql\", {\n    name: \"mysql_connection\",\n    connectionType: \"MYSQL\",\n    comment: \"this is a connection to mysql db\",\n    options: {\n        host: \"test.mysql.database.azure.com\",\n        port: \"3306\",\n        user: \"user\",\n        password: \"password\",\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmysql = databricks.Connection(\"mysql\",\n    name=\"mysql_connection\",\n    connection_type=\"MYSQL\",\n    comment=\"this is a connection to mysql db\",\n    options={\n        \"host\": \"test.mysql.database.azure.com\",\n        \"port\": \"3306\",\n        \"user\": \"user\",\n        \"password\": \"password\",\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var mysql = new Databricks.Connection(\"mysql\", new()\n    {\n        Name = \"mysql_connection\",\n        ConnectionType = \"MYSQL\",\n        Comment = \"this is a connection to mysql db\",\n        Options = \n        {\n            { \"host\", \"test.mysql.database.azure.com\" },\n            { \"port\", \"3306\" },\n            { \"user\", \"user\" },\n            { \"password\", \"password\" },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewConnection(ctx, \"mysql\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"mysql_connection\"),\n\t\t\tConnectionType: pulumi.String(\"MYSQL\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to mysql db\"),\n\t\t\tOptions: pulumi.StringMap{\n\t\t\t\t\"host\":     pulumi.String(\"test.mysql.database.azure.com\"),\n\t\t\t\t\"port\":     pulumi.String(\"3306\"),\n\t\t\t\t\"user\":     pulumi.String(\"user\"),\n\t\t\t\t\"password\": pulumi.String(\"password\"),\n\t\t\t},\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mysql = new Connection(\"mysql\", ConnectionArgs.builder()\n            .name(\"mysql_connection\")\n            .connectionType(\"MYSQL\")\n            .comment(\"this is a connection to mysql db\")\n            .options(Map.ofEntries(\n                Map.entry(\"host\", \"test.mysql.database.azure.com\"),\n                Map.entry(\"port\", \"3306\"),\n                Map.entry(\"user\", \"user\"),\n                Map.entry(\"password\", \"password\")\n            ))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  mysql:\n    type: databricks:Connection\n    properties:\n      name: mysql_connection\n      connectionType: MYSQL\n      comment: this is a connection to mysql db\n      options:\n        host: test.mysql.database.azure.com\n        port: '3306'\n        user: user\n        password: password\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreate a connection to a BigQuery database\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst bigquery = new databricks.Connection(\"bigquery\", {\n    name: \"bq_connection\",\n    connectionType: \"BIGQUERY\",\n    comment: \"this is a connection to BQ\",\n    options: {\n        GoogleServiceAccountKeyJson: JSON.stringify({\n            type: \"service_account\",\n            project_id: \"PROJECT_ID\",\n            private_key_id: \"KEY_ID\",\n            private_key: `-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n`,\n            client_email: \"SERVICE_ACCOUNT_EMAIL\",\n            client_id: \"CLIENT_ID\",\n            auth_uri: \"https://accounts.google.com/o/oauth2/auth\",\n            token_uri: \"https://accounts.google.com/o/oauth2/token\",\n            auth_provider_x509_cert_url: \"https://www.googleapis.com/oauth2/v1/certs\",\n            client_x509_cert_url: \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n            universe_domain: \"googleapis.com\",\n        }),\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nbigquery = databricks.Connection(\"bigquery\",\n    name=\"bq_connection\",\n    connection_type=\"BIGQUERY\",\n    comment=\"this is a connection to BQ\",\n    options={\n        \"GoogleServiceAccountKeyJson\": json.dumps({\n            \"type\": \"service_account\",\n            \"project_id\": \"PROJECT_ID\",\n            \"private_key_id\": \"KEY_ID\",\n            \"private_key\": \"\"\"-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n\"\"\",\n            \"client_email\": \"SERVICE_ACCOUNT_EMAIL\",\n            \"client_id\": \"CLIENT_ID\",\n            \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n            \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n            \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n            \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n            \"universe_domain\": \"googleapis.com\",\n        }),\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var bigquery = new Databricks.Connection(\"bigquery\", new()\n    {\n        Name = \"bq_connection\",\n        ConnectionType = \"BIGQUERY\",\n        Comment = \"this is a connection to BQ\",\n        Options = \n        {\n            { \"GoogleServiceAccountKeyJson\", JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"service_account\",\n                [\"project_id\"] = \"PROJECT_ID\",\n                [\"private_key_id\"] = \"KEY_ID\",\n                [\"private_key\"] = @\"-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n\",\n                [\"client_email\"] = \"SERVICE_ACCOUNT_EMAIL\",\n                [\"client_id\"] = \"CLIENT_ID\",\n                [\"auth_uri\"] = \"https://accounts.google.com/o/oauth2/auth\",\n                [\"token_uri\"] = \"https://accounts.google.com/o/oauth2/token\",\n                [\"auth_provider_x509_cert_url\"] = \"https://www.googleapis.com/oauth2/v1/certs\",\n                [\"client_x509_cert_url\"] = \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n                [\"universe_domain\"] = \"googleapis.com\",\n            }) },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"type\":                        \"service_account\",\n\t\t\t\"project_id\":                  \"PROJECT_ID\",\n\t\t\t\"private_key_id\":              \"KEY_ID\",\n\t\t\t\"private_key\":                 \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n\t\t\t\"client_email\":                \"SERVICE_ACCOUNT_EMAIL\",\n\t\t\t\"client_id\":                   \"CLIENT_ID\",\n\t\t\t\"auth_uri\":                    \"https://accounts.google.com/o/oauth2/auth\",\n\t\t\t\"token_uri\":                   \"https://accounts.google.com/o/oauth2/token\",\n\t\t\t\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\t\t\t\"client_x509_cert_url\":        \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n\t\t\t\"universe_domain\":             \"googleapis.com\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewConnection(ctx, \"bigquery\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"bq_connection\"),\n\t\t\tConnectionType: pulumi.String(\"BIGQUERY\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to BQ\"),\n\t\t\tOptions: pulumi.StringMap{\n\t\t\t\t\"GoogleServiceAccountKeyJson\": pulumi.String(json0),\n\t\t\t},\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var bigquery = new Connection(\"bigquery\", ConnectionArgs.builder()\n            .name(\"bq_connection\")\n            .connectionType(\"BIGQUERY\")\n            .comment(\"this is a connection to BQ\")\n            .options(Map.of(\"GoogleServiceAccountKeyJson\", serializeJson(\n                jsonObject(\n                    jsonProperty(\"type\", \"service_account\"),\n                    jsonProperty(\"project_id\", \"PROJECT_ID\"),\n                    jsonProperty(\"private_key_id\", \"KEY_ID\"),\n                    jsonProperty(\"private_key\", \"\"\"\n-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n                    \"\"\"),\n                    jsonProperty(\"client_email\", \"SERVICE_ACCOUNT_EMAIL\"),\n                    jsonProperty(\"client_id\", \"CLIENT_ID\"),\n                    jsonProperty(\"auth_uri\", \"https://accounts.google.com/o/oauth2/auth\"),\n                    jsonProperty(\"token_uri\", \"https://accounts.google.com/o/oauth2/token\"),\n                    jsonProperty(\"auth_provider_x509_cert_url\", \"https://www.googleapis.com/oauth2/v1/certs\"),\n                    jsonProperty(\"client_x509_cert_url\", \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\"),\n                    jsonProperty(\"universe_domain\", \"googleapis.com\")\n                ))))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  bigquery:\n    type: databricks:Connection\n    properties:\n      name: bq_connection\n      connectionType: BIGQUERY\n      comment: this is a connection to BQ\n      options:\n        GoogleServiceAccountKeyJson:\n          fn::toJSON:\n            type: service_account\n            project_id: PROJECT_ID\n            private_key_id: KEY_ID\n            private_key: |\n              -----BEGIN PRIVATE KEY-----\n              PRIVATE_KEY\n              -----END PRIVATE KEY-----\n            client_email: SERVICE_ACCOUNT_EMAIL\n            client_id: CLIENT_ID\n            auth_uri: https://accounts.google.com/o/oauth2/auth\n            token_uri: https://accounts.google.com/o/oauth2/token\n            auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n            client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\n            universe_domain: googleapis.com\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreate a connection to builtin Hive Metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Connection(\"this\", {\n    name: \"hms-builtin\",\n    connectionType: \"HIVE_METASTORE\",\n    comment: \"This is a connection to builtin HMS\",\n    options: {\n        builtin: \"true\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Connection(\"this\",\n    name=\"hms-builtin\",\n    connection_type=\"HIVE_METASTORE\",\n    comment=\"This is a connection to builtin HMS\",\n    options={\n        \"builtin\": \"true\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Connection(\"this\", new()\n    {\n        Name = \"hms-builtin\",\n        ConnectionType = \"HIVE_METASTORE\",\n        Comment = \"This is a connection to builtin HMS\",\n        Options = \n        {\n            { \"builtin\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewConnection(ctx, \"this\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"hms-builtin\"),\n\t\t\tConnectionType: pulumi.String(\"HIVE_METASTORE\"),\n\t\t\tComment:        pulumi.String(\"This is a connection to builtin HMS\"),\n\t\t\tOptions: pulumi.StringMap{\n\t\t\t\t\"builtin\": pulumi.String(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Connection(\"this\", ConnectionArgs.builder()\n            .name(\"hms-builtin\")\n            .connectionType(\"HIVE_METASTORE\")\n            .comment(\"This is a connection to builtin HMS\")\n            .options(Map.of(\"builtin\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Connection\n    properties:\n      name: hms-builtin\n      connectionType: HIVE_METASTORE\n      comment: This is a connection to builtin HMS\n      options:\n        builtin: 'true'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by `id`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/connection:Connection this '\u003cmetastore_id\u003e|\u003cname\u003e'\n```\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "connectionId": {
                    "type": "string",
                    "description": "Unique ID of the connection.\n"
                },
                "connectionType": {
                    "type": "string",
                    "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER`, `SALESFORCE`, `HIVE_METASTORE`, `GLUE`, `TERADATA`, `ORACLE` or `DATABRICKS` are supported. Up-to-date list of connection type supported is in the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this connection was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of connection creator.\n"
                },
                "credentialType": {
                    "type": "string",
                    "description": "The type of credential for this connection.\n"
                },
                "fullName": {
                    "type": "string",
                    "description": "Full name of connection.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique ID of the UC metastore for this connection.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Connection.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                    "secret": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the connection owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Free-form connection properties.\n"
                },
                "provisioningInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ConnectionProvisioningInfo:ConnectionProvisioningInfo"
                    },
                    "description": "Object with the status of an asynchronously provisioned resource.\n"
                },
                "readOnly": {
                    "type": "boolean"
                },
                "securableType": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which connection this was last modified, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified the connection.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the remote data source, extracted from options.\n"
                }
            },
            "required": [
                "connectionId",
                "createdAt",
                "createdBy",
                "credentialType",
                "fullName",
                "metastoreId",
                "name",
                "owner",
                "provisioningInfos",
                "readOnly",
                "securableType",
                "updatedAt",
                "updatedBy",
                "url"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n",
                    "willReplaceOnChanges": true
                },
                "connectionType": {
                    "type": "string",
                    "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER`, `SALESFORCE`, `HIVE_METASTORE`, `GLUE`, `TERADATA`, `ORACLE` or `DATABRICKS` are supported. Up-to-date list of connection type supported is in the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Connection.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                    "secret": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the connection owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Free-form connection properties.\n",
                    "willReplaceOnChanges": true
                },
                "readOnly": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Connection resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "Free-form text.\n",
                        "willReplaceOnChanges": true
                    },
                    "connectionId": {
                        "type": "string",
                        "description": "Unique ID of the connection.\n"
                    },
                    "connectionType": {
                        "type": "string",
                        "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER`, `SALESFORCE`, `HIVE_METASTORE`, `GLUE`, `TERADATA`, `ORACLE` or `DATABRICKS` are supported. Up-to-date list of connection type supported is in the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n",
                        "willReplaceOnChanges": true
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this connection was created, in epoch milliseconds.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Username of connection creator.\n"
                    },
                    "credentialType": {
                        "type": "string",
                        "description": "The type of credential for this connection.\n"
                    },
                    "fullName": {
                        "type": "string",
                        "description": "Full name of connection.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique ID of the UC metastore for this connection.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Connection.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                        "secret": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the connection owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Free-form connection properties.\n",
                        "willReplaceOnChanges": true
                    },
                    "provisioningInfos": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ConnectionProvisioningInfo:ConnectionProvisioningInfo"
                        },
                        "description": "Object with the status of an asynchronously provisioned resource.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "securableType": {
                        "type": "string"
                    },
                    "updatedAt": {
                        "type": "integer",
                        "description": "Time at which connection this was last modified, in epoch milliseconds.\n"
                    },
                    "updatedBy": {
                        "type": "string",
                        "description": "Username of user who last modified the connection.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the remote data source, extracted from options.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/credential:Credential": {
            "description": "\u003e This resource can only be used with a workspace-level provider.\n\nA credential represents an authentication and authorization mechanism for accessing services on your cloud tenant. Each credential is subject to Unity Catalog access-control policies that control which users and groups can access the credential.\n\nThe type of credential to be created is determined by the `purpose` field, which should be either `SERVICE` or `STORAGE`.\nThe caller must be a metastore admin or have the metastore privilege `CREATE_STORAGE_CREDENTIAL` for storage credentials, or `CREATE_SERVICE_CREDENTIAL` for service credentials. The user who creates the credential can delegate ownership to another user or group to manage permissions on it\n\nOn AWS, the IAM role for a credential requires a trust policy. See [documentation](https://docs.databricks.com/en/connect/unity-catalog/cloud-services/service-credentials.html#step-1-create-an-iam-role) for more details. The data source databricks.getAwsUnityCatalogAssumeRolePolicy can be used to create the necessary AWS Unity Catalog assume role policy.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.Credential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    purpose: \"SERVICE\",\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    credential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"ACCESS\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.Credential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    purpose=\"SERVICE\",\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    credential=external.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"ACCESS\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.Credential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.CredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Purpose = \"SERVICE\",\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        Credential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"ACCESS\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewCredential(ctx, \"external\", \u0026databricks.CredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.CredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tPurpose: pulumi.String(\"SERVICE\"),\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"ACCESS\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Credential;\nimport com.pulumi.databricks.CredentialArgs;\nimport com.pulumi.databricks.inputs.CredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new Credential(\"external\", CredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(CredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .purpose(\"SERVICE\")\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .credential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"ACCESS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:Credential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      purpose: SERVICE\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      credential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - ACCESS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst externalMi = new databricks.Credential(\"external_mi\", {\n    name: \"mi_credential\",\n    azureManagedIdentity: {\n        accessConnectorId: example.id,\n    },\n    purpose: \"SERVICE\",\n    comment: \"Managed identity credential managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    credential: externalMi.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"ACCESS\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal_mi = databricks.Credential(\"external_mi\",\n    name=\"mi_credential\",\n    azure_managed_identity={\n        \"access_connector_id\": example[\"id\"],\n    },\n    purpose=\"SERVICE\",\n    comment=\"Managed identity credential managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    credential=external_mi.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"ACCESS\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var externalMi = new Databricks.Credential(\"external_mi\", new()\n    {\n        Name = \"mi_credential\",\n        AzureManagedIdentity = new Databricks.Inputs.CredentialAzureManagedIdentityArgs\n        {\n            AccessConnectorId = example.Id,\n        },\n        Purpose = \"SERVICE\",\n        Comment = \"Managed identity credential managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        Credential = externalMi.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"ACCESS\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternalMi, err := databricks.NewCredential(ctx, \"external_mi\", \u0026databricks.CredentialArgs{\n\t\t\tName: pulumi.String(\"mi_credential\"),\n\t\t\tAzureManagedIdentity: \u0026databricks.CredentialAzureManagedIdentityArgs{\n\t\t\t\tAccessConnectorId: pulumi.Any(example.Id),\n\t\t\t},\n\t\t\tPurpose: pulumi.String(\"SERVICE\"),\n\t\t\tComment: pulumi.String(\"Managed identity credential managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tCredential: externalMi.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"ACCESS\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Credential;\nimport com.pulumi.databricks.CredentialArgs;\nimport com.pulumi.databricks.inputs.CredentialAzureManagedIdentityArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var externalMi = new Credential(\"externalMi\", CredentialArgs.builder()\n            .name(\"mi_credential\")\n            .azureManagedIdentity(CredentialAzureManagedIdentityArgs.builder()\n                .accessConnectorId(example.id())\n                .build())\n            .purpose(\"SERVICE\")\n            .comment(\"Managed identity credential managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .credential(externalMi.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"ACCESS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  externalMi:\n    type: databricks:Credential\n    name: external_mi\n    properties:\n      name: mi_credential\n      azureManagedIdentity:\n        accessConnectorId: ${example.id}\n      purpose: SERVICE\n      comment: Managed identity credential managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      credential: ${externalMi.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - ACCESS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor GCP \n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst externalGcpSa = new databricks.Credential(\"external_gcp_sa\", {\n    name: \"gcp_sa_credential\",\n    databricksGcpServiceAccount: {},\n    purpose: \"SERVICE\",\n    comment: \"GCP SA credential managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    credential: externalGcpSa.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"ACCESS\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal_gcp_sa = databricks.Credential(\"external_gcp_sa\",\n    name=\"gcp_sa_credential\",\n    databricks_gcp_service_account={},\n    purpose=\"SERVICE\",\n    comment=\"GCP SA credential managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    credential=external_gcp_sa.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"ACCESS\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var externalGcpSa = new Databricks.Credential(\"external_gcp_sa\", new()\n    {\n        Name = \"gcp_sa_credential\",\n        DatabricksGcpServiceAccount = null,\n        Purpose = \"SERVICE\",\n        Comment = \"GCP SA credential managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        Credential = externalGcpSa.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"ACCESS\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternalGcpSa, err := databricks.NewCredential(ctx, \"external_gcp_sa\", \u0026databricks.CredentialArgs{\n\t\t\tName:                        pulumi.String(\"gcp_sa_credential\"),\n\t\t\tDatabricksGcpServiceAccount: \u0026databricks.CredentialDatabricksGcpServiceAccountArgs{},\n\t\t\tPurpose:                     pulumi.String(\"SERVICE\"),\n\t\t\tComment:                     pulumi.String(\"GCP SA credential managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tCredential: externalGcpSa.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"ACCESS\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Credential;\nimport com.pulumi.databricks.CredentialArgs;\nimport com.pulumi.databricks.inputs.CredentialDatabricksGcpServiceAccountArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var externalGcpSa = new Credential(\"externalGcpSa\", CredentialArgs.builder()\n            .name(\"gcp_sa_credential\")\n            .databricksGcpServiceAccount(CredentialDatabricksGcpServiceAccountArgs.builder()\n                .build())\n            .purpose(\"SERVICE\")\n            .comment(\"GCP SA credential managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .credential(externalGcpSa.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"ACCESS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  externalGcpSa:\n    type: databricks:Credential\n    name: external_gcp_sa\n    properties:\n      name: gcp_sa_credential\n      databricksGcpServiceAccount: {}\n      purpose: SERVICE\n      comment: GCP SA credential managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      credential: ${externalGcpSa.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - ACCESS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/credential:Credential this \u003cname\u003e\n```\n\n",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/CredentialAwsIamRole:CredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/CredentialAzureManagedIdentity:CredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/CredentialAzureServicePrincipal:CredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "credentialId": {
                    "type": "string",
                    "description": "Unique ID of the credential.\n"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/CredentialDatabricksGcpServiceAccount:CredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update credential regardless of its dependents.\n"
                },
                "fullName": {
                    "type": "string"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically restrict access to only from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the credential owner.\n"
                },
                "purpose": {
                    "type": "string",
                    "description": "Indicates the purpose of the credential. Can be `SERVICE` or `STORAGE`.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the credential is only usable for read operations. Only applicable when purpose is `STORAGE`.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the credential.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                },
                "usedForManagedStorage": {
                    "type": "boolean"
                }
            },
            "required": [
                "createdAt",
                "createdBy",
                "credentialId",
                "databricksGcpServiceAccount",
                "fullName",
                "isolationMode",
                "metastoreId",
                "name",
                "owner",
                "purpose",
                "updatedAt",
                "updatedBy",
                "usedForManagedStorage"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/CredentialAwsIamRole:CredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/CredentialAzureManagedIdentity:CredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/CredentialAzureServicePrincipal:CredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/CredentialDatabricksGcpServiceAccount:CredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update credential regardless of its dependents.\n"
                },
                "fullName": {
                    "type": "string"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically restrict access to only from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the credential owner.\n"
                },
                "purpose": {
                    "type": "string",
                    "description": "Indicates the purpose of the credential. Can be `SERVICE` or `STORAGE`.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the credential is only usable for read operations. Only applicable when purpose is `STORAGE`.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the credential.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                },
                "usedForManagedStorage": {
                    "type": "boolean"
                }
            },
            "requiredInputs": [
                "purpose"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Credential resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/CredentialAwsIamRole:CredentialAwsIamRole"
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/CredentialAzureManagedIdentity:CredentialAzureManagedIdentity"
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/CredentialAzureServicePrincipal:CredentialAzureServicePrincipal"
                    },
                    "comment": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer"
                    },
                    "createdBy": {
                        "type": "string"
                    },
                    "credentialId": {
                        "type": "string",
                        "description": "Unique ID of the credential.\n"
                    },
                    "databricksGcpServiceAccount": {
                        "$ref": "#/types/databricks:index/CredentialDatabricksGcpServiceAccount:CredentialDatabricksGcpServiceAccount"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete credential regardless of its dependencies.\n"
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "description": "Update credential regardless of its dependents.\n"
                    },
                    "fullName": {
                        "type": "string"
                    },
                    "isolationMode": {
                        "type": "string",
                        "description": "Whether the credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically restrict access to only from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the credential owner.\n"
                    },
                    "purpose": {
                        "type": "string",
                        "description": "Indicates the purpose of the credential. Can be `SERVICE` or `STORAGE`.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "description": "Indicates whether the credential is only usable for read operations. Only applicable when purpose is `STORAGE`.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the credential.\n"
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    },
                    "usedForManagedStorage": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/customAppIntegration:CustomAppIntegration": {
            "description": "\u003e Initialize provider with `alias = \"account\"`, and `host` pointing to the account URL, like, `host = \"https://accounts.cloud.databricks.com\"`. Use `provider = databricks.account` for all account-level resources.\n\nThis resource allows you to enable [custom OAuth applications](https://docs.databricks.com/en/integrations/enable-disable-oauth.html#enable-custom-oauth-applications-using-the-databricks-ui).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.CustomAppIntegration(\"this\", {\n    name: \"custom_integration_name\",\n    redirectUrls: [\"https://example.com\"],\n    scopes: [\"all-apis\"],\n    tokenAccessPolicy: {\n        accessTokenTtlInMinutes: 15,\n        refreshTokenTtlInMinutes: 30,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.CustomAppIntegration(\"this\",\n    name=\"custom_integration_name\",\n    redirect_urls=[\"https://example.com\"],\n    scopes=[\"all-apis\"],\n    token_access_policy={\n        \"access_token_ttl_in_minutes\": 15,\n        \"refresh_token_ttl_in_minutes\": 30,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.CustomAppIntegration(\"this\", new()\n    {\n        Name = \"custom_integration_name\",\n        RedirectUrls = new[]\n        {\n            \"https://example.com\",\n        },\n        Scopes = new[]\n        {\n            \"all-apis\",\n        },\n        TokenAccessPolicy = new Databricks.Inputs.CustomAppIntegrationTokenAccessPolicyArgs\n        {\n            AccessTokenTtlInMinutes = 15,\n            RefreshTokenTtlInMinutes = 30,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCustomAppIntegration(ctx, \"this\", \u0026databricks.CustomAppIntegrationArgs{\n\t\t\tName: pulumi.String(\"custom_integration_name\"),\n\t\t\tRedirectUrls: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"https://example.com\"),\n\t\t\t},\n\t\t\tScopes: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"all-apis\"),\n\t\t\t},\n\t\t\tTokenAccessPolicy: \u0026databricks.CustomAppIntegrationTokenAccessPolicyArgs{\n\t\t\t\tAccessTokenTtlInMinutes:  pulumi.Int(15),\n\t\t\t\tRefreshTokenTtlInMinutes: pulumi.Int(30),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.CustomAppIntegration;\nimport com.pulumi.databricks.CustomAppIntegrationArgs;\nimport com.pulumi.databricks.inputs.CustomAppIntegrationTokenAccessPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new CustomAppIntegration(\"this\", CustomAppIntegrationArgs.builder()\n            .name(\"custom_integration_name\")\n            .redirectUrls(\"https://example.com\")\n            .scopes(\"all-apis\")\n            .tokenAccessPolicy(CustomAppIntegrationTokenAccessPolicyArgs.builder()\n                .accessTokenTtlInMinutes(15)\n                .refreshTokenTtlInMinutes(30)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:CustomAppIntegration\n    properties:\n      name: custom_integration_name\n      redirectUrls:\n        - https://example.com\n      scopes:\n        - all-apis\n      tokenAccessPolicy:\n        accessTokenTtlInMinutes: 15\n        refreshTokenTtlInMinutes: 30\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n\n## Import\n\nThis resource can be imported by its integration ID.\n\n```sh\n$ pulumi import databricks:index/customAppIntegration:CustomAppIntegration this '\u003cintegration_id\u003e'\n```\n\n",
            "properties": {
                "clientId": {
                    "type": "string",
                    "description": "OAuth client-id generated by Databricks\n"
                },
                "clientSecret": {
                    "type": "string",
                    "description": "OAuth client-secret generated by the Databricks if this is a confidential OAuth app.\n",
                    "secret": true
                },
                "confidential": {
                    "type": "boolean",
                    "description": "Indicates whether an OAuth client secret is required to authenticate this client. Default to `false`. Change requires a new resource.\n"
                },
                "createTime": {
                    "type": "string"
                },
                "createdBy": {
                    "type": "integer"
                },
                "creatorUsername": {
                    "type": "string"
                },
                "integrationId": {
                    "type": "string",
                    "description": "Unique integration id for the custom OAuth app.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the custom OAuth app. Change requires a new resource.\n"
                },
                "redirectUrls": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of OAuth redirect urls.\n"
                },
                "scopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "OAuth scopes granted to the application. Supported scopes: `all-apis`, `sql`, `offline_access`, `openid`, `profile`, `email`.\n"
                },
                "tokenAccessPolicy": {
                    "$ref": "#/types/databricks:index/CustomAppIntegrationTokenAccessPolicy:CustomAppIntegrationTokenAccessPolicy"
                },
                "userAuthorizedScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "required": [
                "clientId",
                "clientSecret",
                "createTime",
                "createdBy",
                "creatorUsername",
                "integrationId",
                "name"
            ],
            "inputProperties": {
                "clientId": {
                    "type": "string",
                    "description": "OAuth client-id generated by Databricks\n"
                },
                "clientSecret": {
                    "type": "string",
                    "description": "OAuth client-secret generated by the Databricks if this is a confidential OAuth app.\n",
                    "secret": true
                },
                "confidential": {
                    "type": "boolean",
                    "description": "Indicates whether an OAuth client secret is required to authenticate this client. Default to `false`. Change requires a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "createTime": {
                    "type": "string"
                },
                "createdBy": {
                    "type": "integer"
                },
                "creatorUsername": {
                    "type": "string"
                },
                "integrationId": {
                    "type": "string",
                    "description": "Unique integration id for the custom OAuth app.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the custom OAuth app. Change requires a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "redirectUrls": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of OAuth redirect urls.\n"
                },
                "scopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "OAuth scopes granted to the application. Supported scopes: `all-apis`, `sql`, `offline_access`, `openid`, `profile`, `email`.\n"
                },
                "tokenAccessPolicy": {
                    "$ref": "#/types/databricks:index/CustomAppIntegrationTokenAccessPolicy:CustomAppIntegrationTokenAccessPolicy"
                },
                "userAuthorizedScopes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering CustomAppIntegration resources.\n",
                "properties": {
                    "clientId": {
                        "type": "string",
                        "description": "OAuth client-id generated by Databricks\n"
                    },
                    "clientSecret": {
                        "type": "string",
                        "description": "OAuth client-secret generated by the Databricks if this is a confidential OAuth app.\n",
                        "secret": true
                    },
                    "confidential": {
                        "type": "boolean",
                        "description": "Indicates whether an OAuth client secret is required to authenticate this client. Default to `false`. Change requires a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "createTime": {
                        "type": "string"
                    },
                    "createdBy": {
                        "type": "integer"
                    },
                    "creatorUsername": {
                        "type": "string"
                    },
                    "integrationId": {
                        "type": "string",
                        "description": "Unique integration id for the custom OAuth app.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the custom OAuth app. Change requires a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "redirectUrls": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of OAuth redirect urls.\n"
                    },
                    "scopes": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "OAuth scopes granted to the application. Supported scopes: `all-apis`, `sql`, `offline_access`, `openid`, `profile`, `email`.\n"
                    },
                    "tokenAccessPolicy": {
                        "$ref": "#/types/databricks:index/CustomAppIntegrationTokenAccessPolicy:CustomAppIntegrationTokenAccessPolicy"
                    },
                    "userAuthorizedScopes": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/dashboard:Dashboard": {
            "description": "This resource allows you to manage Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html). To manage [Dashboards](https://docs.databricks.com/en/dashboards/index.html) you must have a warehouse access on your databricks workspace.\n\n## Example Usage\n\nDashboard using `serialized_dashboard` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst starter = databricks.getSqlWarehouse({\n    name: \"Starter Warehouse\",\n});\nconst dashboard = new databricks.Dashboard(\"dashboard\", {\n    displayName: \"New Dashboard\",\n    warehouseId: starter.then(starter =\u003e starter.id),\n    serializedDashboard: \"{\\\"pages\\\":[{\\\"name\\\":\\\"new_name\\\",\\\"displayName\\\":\\\"New Page\\\"}]}\",\n    embedCredentials: false,\n    parentPath: \"/Shared/provider-test\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nstarter = databricks.get_sql_warehouse(name=\"Starter Warehouse\")\ndashboard = databricks.Dashboard(\"dashboard\",\n    display_name=\"New Dashboard\",\n    warehouse_id=starter.id,\n    serialized_dashboard=\"{\\\"pages\\\":[{\\\"name\\\":\\\"new_name\\\",\\\"displayName\\\":\\\"New Page\\\"}]}\",\n    embed_credentials=False,\n    parent_path=\"/Shared/provider-test\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var starter = Databricks.GetSqlWarehouse.Invoke(new()\n    {\n        Name = \"Starter Warehouse\",\n    });\n\n    var dashboard = new Databricks.Dashboard(\"dashboard\", new()\n    {\n        DisplayName = \"New Dashboard\",\n        WarehouseId = starter.Apply(getSqlWarehouseResult =\u003e getSqlWarehouseResult.Id),\n        SerializedDashboard = \"{\\\"pages\\\":[{\\\"name\\\":\\\"new_name\\\",\\\"displayName\\\":\\\"New Page\\\"}]}\",\n        EmbedCredentials = false,\n        ParentPath = \"/Shared/provider-test\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tstarter, err := databricks.GetSqlWarehouse(ctx, \u0026databricks.GetSqlWarehouseArgs{\n\t\t\tName: pulumi.StringRef(\"Starter Warehouse\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewDashboard(ctx, \"dashboard\", \u0026databricks.DashboardArgs{\n\t\t\tDisplayName:         pulumi.String(\"New Dashboard\"),\n\t\t\tWarehouseId:         pulumi.String(starter.Id),\n\t\t\tSerializedDashboard: pulumi.String(\"{\\\"pages\\\":[{\\\"name\\\":\\\"new_name\\\",\\\"displayName\\\":\\\"New Page\\\"}]}\"),\n\t\t\tEmbedCredentials:    pulumi.Bool(false),\n\t\t\tParentPath:          pulumi.String(\"/Shared/provider-test\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehouseArgs;\nimport com.pulumi.databricks.Dashboard;\nimport com.pulumi.databricks.DashboardArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var starter = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()\n            .name(\"Starter Warehouse\")\n            .build());\n\n        var dashboard = new Dashboard(\"dashboard\", DashboardArgs.builder()\n            .displayName(\"New Dashboard\")\n            .warehouseId(starter.id())\n            .serializedDashboard(\"{\\\"pages\\\":[{\\\"name\\\":\\\"new_name\\\",\\\"displayName\\\":\\\"New Page\\\"}]}\")\n            .embedCredentials(false)\n            .parentPath(\"/Shared/provider-test\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dashboard:\n    type: databricks:Dashboard\n    properties:\n      displayName: New Dashboard\n      warehouseId: ${starter.id}\n      serializedDashboard: '{\"pages\":[{\"name\":\"new_name\",\"displayName\":\"New Page\"}]}'\n      embedCredentials: false # Optional\n      parentPath: /Shared/provider-test\nvariables:\n  starter:\n    fn::invoke:\n      function: databricks:getSqlWarehouse\n      arguments:\n        name: Starter Warehouse\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nDashboard using `file_path` attribute:\n\n## Import\n\nYou can import a `databricks_dashboard` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/dashboard:Dashboard this \u003cdashboard-id\u003e\n```\n\n",
            "properties": {
                "createTime": {
                    "type": "string"
                },
                "dashboardChangeDetected": {
                    "type": "boolean"
                },
                "dashboardId": {
                    "type": "string"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the dashboard.\n"
                },
                "embedCredentials": {
                    "type": "boolean",
                    "description": "Whether to embed credentials in the dashboard. Default is `true`.\n"
                },
                "etag": {
                    "type": "string"
                },
                "filePath": {
                    "type": "string",
                    "description": "The path to the dashboard JSON file. Conflicts with `serialized_dashboard`.\n"
                },
                "lifecycleState": {
                    "type": "string"
                },
                "md5": {
                    "type": "string"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn't exist, it will be created.\n"
                },
                "path": {
                    "type": "string"
                },
                "serializedDashboard": {
                    "type": "string",
                    "description": "The contents of the dashboard in serialized string form. Conflicts with `file_path`.\n"
                },
                "updateTime": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The warehouse ID used to run the dashboard.\n"
                }
            },
            "required": [
                "createTime",
                "dashboardId",
                "displayName",
                "etag",
                "lifecycleState",
                "md5",
                "parentPath",
                "path",
                "updateTime",
                "warehouseId"
            ],
            "inputProperties": {
                "createTime": {
                    "type": "string"
                },
                "dashboardChangeDetected": {
                    "type": "boolean"
                },
                "dashboardId": {
                    "type": "string"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the dashboard.\n"
                },
                "embedCredentials": {
                    "type": "boolean",
                    "description": "Whether to embed credentials in the dashboard. Default is `true`.\n"
                },
                "etag": {
                    "type": "string"
                },
                "filePath": {
                    "type": "string",
                    "description": "The path to the dashboard JSON file. Conflicts with `serialized_dashboard`.\n"
                },
                "lifecycleState": {
                    "type": "string"
                },
                "md5": {
                    "type": "string"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn't exist, it will be created.\n",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string"
                },
                "serializedDashboard": {
                    "type": "string",
                    "description": "The contents of the dashboard in serialized string form. Conflicts with `file_path`.\n"
                },
                "updateTime": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The warehouse ID used to run the dashboard.\n"
                }
            },
            "requiredInputs": [
                "displayName",
                "parentPath",
                "warehouseId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Dashboard resources.\n",
                "properties": {
                    "createTime": {
                        "type": "string"
                    },
                    "dashboardChangeDetected": {
                        "type": "boolean"
                    },
                    "dashboardId": {
                        "type": "string"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "The display name of the dashboard.\n"
                    },
                    "embedCredentials": {
                        "type": "boolean",
                        "description": "Whether to embed credentials in the dashboard. Default is `true`.\n"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "filePath": {
                        "type": "string",
                        "description": "The path to the dashboard JSON file. Conflicts with `serialized_dashboard`.\n"
                    },
                    "lifecycleState": {
                        "type": "string"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "parentPath": {
                        "type": "string",
                        "description": "The workspace path of the folder containing the dashboard. Includes leading slash and no trailing slash.  If folder doesn't exist, it will be created.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string"
                    },
                    "serializedDashboard": {
                        "type": "string",
                        "description": "The contents of the dashboard in serialized string form. Conflicts with `file_path`.\n"
                    },
                    "updateTime": {
                        "type": "string"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "The warehouse ID used to run the dashboard.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/dbfsFile:DbfsFile": {
            "description": "This is a resource that lets you manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html). The best use cases are libraries for databricks.Cluster or databricks_job. You can also use databricks.DbfsFile and databricks.getDbfsFilePaths data sources.\n\n## Import\n\nThe resource dbfs file can be imported using the path of the file:\n\nbash\n\n```sh\n$ pulumi import databricks:index/dbfsFile:DbfsFile this \u003cpath\u003e\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Encoded file contents. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.\n"
                },
                "dbfsPath": {
                    "type": "string",
                    "description": "Path, but with `dbfs:` prefix.\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "dbfsPath",
                "fileSize",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Encoded file contents. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.\n",
                    "willReplaceOnChanges": true
                },
                "md5": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DbfsFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "Encoded file contents. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a data pipeline configuration file.\n",
                        "willReplaceOnChanges": true
                    },
                    "dbfsPath": {
                        "type": "string",
                        "description": "Path, but with `dbfs:` prefix.\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save.\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/defaultNamespaceSetting:DefaultNamespaceSetting": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThe `databricks.DefaultNamespaceSetting` resource allows you to operate the setting configuration for the default namespace in the Databricks workspace.\nSetting the default catalog for the workspace determines the catalog that is used when queries do not reference\na fully qualified 3 level name. For example, if the default catalog is set to 'retail_prod' then a query\n'SELECT * FROM myTable' would reference the object 'retail_prod.default.myTable'\n(the schema 'default' is always assumed).\nThis setting requires a restart of clusters and SQL warehouses to take effect. Additionally, the default namespace only applies when using Unity Catalog-enabled compute.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.DefaultNamespaceSetting(\"this\", {namespace: {\n    value: \"namespace_value\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.DefaultNamespaceSetting(\"this\", namespace={\n    \"value\": \"namespace_value\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.DefaultNamespaceSetting(\"this\", new()\n    {\n        Namespace = new Databricks.Inputs.DefaultNamespaceSettingNamespaceArgs\n        {\n            Value = \"namespace_value\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewDefaultNamespaceSetting(ctx, \"this\", \u0026databricks.DefaultNamespaceSettingArgs{\n\t\t\tNamespace: \u0026databricks.DefaultNamespaceSettingNamespaceArgs{\n\t\t\t\tValue: pulumi.String(\"namespace_value\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DefaultNamespaceSetting;\nimport com.pulumi.databricks.DefaultNamespaceSettingArgs;\nimport com.pulumi.databricks.inputs.DefaultNamespaceSettingNamespaceArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new DefaultNamespaceSetting(\"this\", DefaultNamespaceSettingArgs.builder()\n            .namespace(DefaultNamespaceSettingNamespaceArgs.builder()\n                .value(\"namespace_value\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:DefaultNamespaceSetting\n    properties:\n      namespace:\n        value: namespace_value\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/defaultNamespaceSetting:DefaultNamespaceSetting this global\n```\n\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "namespace": {
                    "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "etag",
                "namespace",
                "settingName"
            ],
            "inputProperties": {
                "etag": {
                    "type": "string"
                },
                "namespace": {
                    "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "namespace"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DefaultNamespaceSetting resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "namespace": {
                        "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                        "description": "The configuration details.\n"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/directory:Directory": {
            "description": "This resource allows you to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n\n## Example Usage\n\nYou can declare a Pulumi-managed directory by specifying the `path` attribute of the corresponding directory.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myCustomDirectory = new databricks.Directory(\"my_custom_directory\", {path: \"/my_custom_directory\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_custom_directory = databricks.Directory(\"my_custom_directory\", path=\"/my_custom_directory\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myCustomDirectory = new Databricks.Directory(\"my_custom_directory\", new()\n    {\n        Path = \"/my_custom_directory\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewDirectory(ctx, \"my_custom_directory\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/my_custom_directory\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myCustomDirectory = new Directory(\"myCustomDirectory\", DirectoryArgs.builder()\n            .path(\"/my_custom_directory\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCustomDirectory:\n    type: databricks:Directory\n    name: my_custom_directory\n    properties:\n      path: /my_custom_directory\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n- databricks.Permissions can control which groups or individual users can access folders.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n- End to end workspace management guide.\n- databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n- databricks.Notebook data to export a notebook from Databricks Workspace.\n- databricks.getNotebookPaths data to list notebooks in Databricks Workspace.\n- databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n- databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n- databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n## Import\n\nThe resource directory can be imported using directory path:\n\nbash\n\n```sh\n$ pulumi import databricks:index/directory:Directory this /path/to/directory\n```\n\n",
            "properties": {
                "deleteRecursive": {
                    "type": "boolean",
                    "description": "Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Pulumi. Defaults to `false`\n"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "objectId",
                "path",
                "workspacePath"
            ],
            "inputProperties": {
                "deleteRecursive": {
                    "type": "boolean",
                    "description": "Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Pulumi. Defaults to `false`\n"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Directory resources.\n",
                "properties": {
                    "deleteRecursive": {
                        "type": "boolean",
                        "description": "Whether or not to trigger a recursive delete of this directory and its resources when deleting this on Pulumi. Defaults to `false`\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a DIRECTORY\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/disableLegacyAccessSetting:DisableLegacyAccessSetting": {
            "properties": {
                "disableLegacyAccess": {
                    "$ref": "#/types/databricks:index/DisableLegacyAccessSettingDisableLegacyAccess:DisableLegacyAccessSettingDisableLegacyAccess"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "disableLegacyAccess",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "disableLegacyAccess": {
                    "$ref": "#/types/databricks:index/DisableLegacyAccessSettingDisableLegacyAccess:DisableLegacyAccessSettingDisableLegacyAccess"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "disableLegacyAccess"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DisableLegacyAccessSetting resources.\n",
                "properties": {
                    "disableLegacyAccess": {
                        "$ref": "#/types/databricks:index/DisableLegacyAccessSettingDisableLegacyAccess:DisableLegacyAccessSettingDisableLegacyAccess"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/enhancedSecurityMonitoringWorkspaceSetting:EnhancedSecurityMonitoringWorkspaceSetting": {
            "properties": {
                "enhancedSecurityMonitoringWorkspace": {
                    "$ref": "#/types/databricks:index/EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace:EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "enhancedSecurityMonitoringWorkspace",
                "etag",
                "settingName"
            ],
            "inputProperties": {
                "enhancedSecurityMonitoringWorkspace": {
                    "$ref": "#/types/databricks:index/EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace:EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace"
                },
                "etag": {
                    "type": "string"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "enhancedSecurityMonitoringWorkspace"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering EnhancedSecurityMonitoringWorkspaceSetting resources.\n",
                "properties": {
                    "enhancedSecurityMonitoringWorkspace": {
                        "$ref": "#/types/databricks:index/EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace:EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace"
                    },
                    "etag": {
                        "type": "string"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/entitlements:Entitlements": {
            "description": "This resource allows you to set entitlements to existing databricks_users, databricks.Group or databricks_service_principal.\n\n\u003e You must define entitlements of a principal using either `databricks.Entitlements` or directly within one of databricks_users, databricks.Group or databricks_service_principal. Having entitlements defined in both resources will result in non-deterministic behaviour.\n\n## Example Usage\n\nSetting entitlements for a regular user:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst meEntitlements = new databricks.Entitlements(\"me\", {\n    userId: me.then(me =\u003e me.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_user(user_name=\"me@example.com\")\nme_entitlements = databricks.Entitlements(\"me\",\n    user_id=me.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var meEntitlements = new Databricks.Entitlements(\"me\", new()\n    {\n        UserId = me.Apply(getUserResult =\u003e getUserResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"me\", \u0026databricks.EntitlementsArgs{\n\t\t\tUserId:                  pulumi.String(me.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var meEntitlements = new Entitlements(\"meEntitlements\", EntitlementsArgs.builder()\n            .userId(me.id())\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  meEntitlements:\n    type: databricks:Entitlements\n    name: me\n    properties:\n      userId: ${me.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nSetting entitlements for a service principal:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst thisEntitlements = new databricks.Entitlements(\"this\", {\n    servicePrincipalId: _this.then(_this =\u003e _this.spId),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nthis_entitlements = databricks.Entitlements(\"this\",\n    service_principal_id=this.sp_id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var thisEntitlements = new Databricks.Entitlements(\"this\", new()\n    {\n        ServicePrincipalId = @this.Apply(@this =\u003e @this.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.SpId)),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"this\", \u0026databricks.EntitlementsArgs{\n\t\t\tServicePrincipalId:      pulumi.String(this.SpId),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var thisEntitlements = new Entitlements(\"thisEntitlements\", EntitlementsArgs.builder()\n            .servicePrincipalId(this_.spId())\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisEntitlements:\n    type: databricks:Entitlements\n    name: this\n    properties:\n      servicePrincipalId: ${this.spId}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getServicePrincipal\n      arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nSetting entitlements to all users in a workspace - referencing special `users` databricks.Group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst workspace_users = new databricks.Entitlements(\"workspace-users\", {\n    groupId: users.then(users =\u003e users.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusers = databricks.get_group(display_name=\"users\")\nworkspace_users = databricks.Entitlements(\"workspace-users\",\n    group_id=users.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var workspace_users = new Databricks.Entitlements(\"workspace-users\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"workspace-users\", \u0026databricks.EntitlementsArgs{\n\t\t\tGroupId:                 pulumi.String(users.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var workspace_users = new Entitlements(\"workspace-users\", EntitlementsArgs.builder()\n            .groupId(users.id())\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  workspace-users:\n    type: databricks:Entitlements\n    properties:\n      groupId: ${users.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  users:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: users\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are:\n\n* `user/user_id` - user `user_id`.\n\n* `group/group_id` - group `group_id`.\n\n* `spn/spn_id` - service principal `spn_id`.\n\nbash\n\n```sh\n$ pulumi import databricks:index/entitlements:Entitlements me user/\u003cuser-id\u003e\n```\n\n",
            "properties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "inputProperties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Entitlements resources.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "groupId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the group.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the user.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/externalLocation:ExternalLocation": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- databricks.StorageCredential represent authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- `databricks.ExternalLocation` are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.id,\n    comment: \"Managed by TF\",\n});\nconst someGrants = new databricks.Grants(\"some\", {\n    externalLocation: some.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\n            \"CREATE_EXTERNAL_TABLE\",\n            \"READ_FILES\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    comment=\"Managed by TF\")\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external.id,\n    comment=\"Managed by TF\")\nsome_grants = databricks.Grants(\"some\",\n    external_location=some.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\n            \"CREATE_EXTERNAL_TABLE\",\n            \"READ_FILES\",\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Id,\n        Comment = \"Managed by TF\",\n    });\n\n    var someGrants = new Databricks.Grants(\"some\", new()\n    {\n        ExternalLocation = some.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external\"),\n\t\t\tUrl:            pulumi.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id),\n\t\t\tCredentialName: external.ID(),\n\t\t\tComment:        pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"some\", \u0026databricks.GrantsArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()\n            .name(\"external\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.id())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var someGrants = new Grants(\"someGrants\", GrantsArgs.builder()\n            .externalLocation(some.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(                \n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.id}\n      comment: Managed by TF\n  someGrants:\n    type: databricks:Grants\n    name: some\n    properties:\n      externalLocation: ${some.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n## Import\n\nThis resource can be imported by `name`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/externalLocation:ExternalLocation this \u003cname\u003e\n```\n\n",
            "properties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this external location was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of external location creator.\n"
                },
                "credentialId": {
                    "type": "string",
                    "description": "Unique ID of the location's storage credential.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "fallback": {
                    "type": "boolean",
                    "description": "Indicates whether fallback mode is enabled for this external location. When fallback mode is enabled (disabled by default), the access to the location falls back to cluster credentials if UC credentials are not sufficient.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy external location regardless of its dependents.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update external location regardless of its dependents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the external location to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which external location this was last modified, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of user who last modified the external location.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "required": [
                "browseOnly",
                "createdAt",
                "createdBy",
                "credentialId",
                "credentialName",
                "isolationMode",
                "metastoreId",
                "name",
                "owner",
                "updatedAt",
                "updatedBy",
                "url"
            ],
            "inputProperties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "fallback": {
                    "type": "boolean",
                    "description": "Indicates whether fallback mode is enabled for this external location. When fallback mode is enabled (disabled by default), the access to the location falls back to cluster credentials if UC credentials are not sufficient.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy external location regardless of its dependents.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update external location regardless of its dependents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the external location to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "requiredInputs": [
                "credentialName",
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ExternalLocation resources.\n",
                "properties": {
                    "accessPoint": {
                        "type": "string",
                        "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                    },
                    "browseOnly": {
                        "type": "boolean"
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this external location was created, in epoch milliseconds.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Username of external location creator.\n"
                    },
                    "credentialId": {
                        "type": "string",
                        "description": "Unique ID of the location's storage credential.\n"
                    },
                    "credentialName": {
                        "type": "string",
                        "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                    },
                    "encryptionDetails": {
                        "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                        "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                    },
                    "fallback": {
                        "type": "boolean",
                        "description": "Indicates whether fallback mode is enabled for this external location. When fallback mode is enabled (disabled by default), the access to the location falls back to cluster credentials if UC credentials are not sufficient.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy external location regardless of its dependents.\n"
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "description": "Update external location regardless of its dependents.\n"
                    },
                    "isolationMode": {
                        "type": "string",
                        "description": "Whether the external location is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the external location to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the external location owner.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "description": "Indicates whether the external location is read-only.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the external location\n"
                    },
                    "updatedAt": {
                        "type": "integer",
                        "description": "Time at which external location this was last modified, in epoch milliseconds.\n"
                    },
                    "updatedBy": {
                        "type": "string",
                        "description": "Username of user who last modified the external location.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/file:File": {
            "description": "This resource allows uploading and downloading files in databricks_volume.\n\nNotes:\n\n* Currently the limit is 5GiB in octet-stream.\n* Currently, only UC volumes are supported. The list of destinations may change.\n\n## Example Usage\n\nIn order to manage a file on Unity Catalog Volumes with Pulumi, you must specify the `source` attribute containing the full path to the file on the local filesystem.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    metastoreId: thisDatabricksMetastore.id,\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.name,\n    name: \"things\",\n    comment: \"this schema is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst _this = new databricks.Volume(\"this\", {\n    name: \"quickstart_volume\",\n    catalogName: sandbox.name,\n    schemaName: things.name,\n    volumeType: \"MANAGED\",\n    comment: \"this volume is managed by terraform\",\n});\nconst thisFile = new databricks.File(\"this\", {\n    source: \"/full/path/on/local/system\",\n    path: pulumi.interpolate`${_this.volumePath}/fileName`,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    metastore_id=this_databricks_metastore[\"id\"],\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.name,\n    name=\"things\",\n    comment=\"this schema is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nthis = databricks.Volume(\"this\",\n    name=\"quickstart_volume\",\n    catalog_name=sandbox.name,\n    schema_name=things.name,\n    volume_type=\"MANAGED\",\n    comment=\"this volume is managed by terraform\")\nthis_file = databricks.File(\"this\",\n    source=\"/full/path/on/local/system\",\n    path=this.volume_path.apply(lambda volume_path: f\"{volume_path}/fileName\"))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        MetastoreId = thisDatabricksMetastore.Id,\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Name,\n        Name = \"things\",\n        Comment = \"this schema is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var @this = new Databricks.Volume(\"this\", new()\n    {\n        Name = \"quickstart_volume\",\n        CatalogName = sandbox.Name,\n        SchemaName = things.Name,\n        VolumeType = \"MANAGED\",\n        Comment = \"this volume is managed by terraform\",\n    });\n\n    var thisFile = new Databricks.File(\"this\", new()\n    {\n        Source = \"/full/path/on/local/system\",\n        Path = @this.VolumePath.Apply(volumePath =\u003e $\"{volumePath}/fileName\"),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tMetastoreId: pulumi.Any(thisDatabricksMetastore.Id),\n\t\t\tName:        pulumi.String(\"sandbox\"),\n\t\t\tComment:     pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.Name,\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this schema is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewVolume(ctx, \"this\", \u0026databricks.VolumeArgs{\n\t\t\tName:        pulumi.String(\"quickstart_volume\"),\n\t\t\tCatalogName: sandbox.Name,\n\t\t\tSchemaName:  things.Name,\n\t\t\tVolumeType:  pulumi.String(\"MANAGED\"),\n\t\t\tComment:     pulumi.String(\"this volume is managed by terraform\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewFile(ctx, \"this\", \u0026databricks.FileArgs{\n\t\t\tSource: pulumi.String(\"/full/path/on/local/system\"),\n\t\t\tPath: this.VolumePath.ApplyT(func(volumePath string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"%v/fileName\", volumePath), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.Volume;\nimport com.pulumi.databricks.VolumeArgs;\nimport com.pulumi.databricks.File;\nimport com.pulumi.databricks.FileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .metastoreId(thisDatabricksMetastore.id())\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.name())\n            .name(\"things\")\n            .comment(\"this schema is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var this_ = new Volume(\"this\", VolumeArgs.builder()\n            .name(\"quickstart_volume\")\n            .catalogName(sandbox.name())\n            .schemaName(things.name())\n            .volumeType(\"MANAGED\")\n            .comment(\"this volume is managed by terraform\")\n            .build());\n\n        var thisFile = new File(\"thisFile\", FileArgs.builder()\n            .source(\"/full/path/on/local/system\")\n            .path(this_.volumePath().applyValue(_volumePath -\u003e String.format(\"%s/fileName\", _volumePath)))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      metastoreId: ${thisDatabricksMetastore.id}\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.name}\n      name: things\n      comment: this schema is managed by terraform\n      properties:\n        kind: various\n  this:\n    type: databricks:Volume\n    properties:\n      name: quickstart_volume\n      catalogName: ${sandbox.name}\n      schemaName: ${things.name}\n      volumeType: MANAGED\n      comment: this volume is managed by terraform\n  thisFile:\n    type: databricks:File\n    name: this\n    properties:\n      source: /full/path/on/local/system\n      path: ${this.volumePath}/fileName\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nYou can also inline sources through `content_base64`  attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst initScript = new databricks.File(\"init_script\", {\n    contentBase64: std.base64encode({\n        input: `#!/bin/bash\necho \"Hello World\"\n`,\n    }).then(invoke =\u003e invoke.result),\n    path: `${_this.volumePath}/fileName`,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\ninit_script = databricks.File(\"init_script\",\n    content_base64=std.base64encode(input=\"\"\"#!/bin/bash\necho \"Hello World\"\n\"\"\").result,\n    path=f\"{this['volumePath']}/fileName\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var initScript = new Databricks.File(\"init_script\", new()\n    {\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = @\"#!/bin/bash\necho \"\"Hello World\"\"\n\",\n        }).Apply(invoke =\u003e invoke.Result),\n        Path = $\"{@this.VolumePath}/fileName\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: \"#!/bin/bash\\necho \\\"Hello World\\\"\\n\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewFile(ctx, \"init_script\", \u0026databricks.FileArgs{\n\t\t\tContentBase64: pulumi.String(invokeBase64encode.Result),\n\t\t\tPath:          pulumi.Sprintf(\"%v/fileName\", this.VolumePath),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.File;\nimport com.pulumi.databricks.FileArgs;\nimport com.pulumi.std.StdFunctions;\nimport com.pulumi.std.inputs.Base64encodeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var initScript = new File(\"initScript\", FileArgs.builder()\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"\"\"\n#!/bin/bash\necho \"Hello World\"\n                \"\"\")\n                .build()).result())\n            .path(String.format(\"%s/fileName\", this_.volumePath()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  initScript:\n    type: databricks:File\n    name: init_script\n    properties:\n      contentBase64:\n        fn::invoke:\n          function: std:base64encode\n          arguments:\n            input: |\n              #!/bin/bash\n              echo \"Hello World\"\n          return: result\n      path: ${this.volumePath}/fileName\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.WorkspaceFile\n* End to end workspace management guide.\n* databricks.Volume to manage [volumes within Unity Catalog](https://docs.databricks.com/en/connect/unity-catalog/volumes.html).\n\n## Import\n\nThe resource `databricks_file` can be imported using the path of the file:\n\nbash\n\n```sh\n$ pulumi import databricks:index/file:File this \u003cpath\u003e\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Contents in base 64 format. Conflicts with `source`.\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "modificationTime": {
                    "type": "string",
                    "description": "The last time stamp when the file was modified\n"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n"
                },
                "remoteFileModified": {
                    "type": "boolean"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "fileSize",
                "modificationTime",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Contents in base 64 format. Conflicts with `source`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n",
                    "willReplaceOnChanges": true
                },
                "remoteFileModified": {
                    "type": "boolean"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering File resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "Contents in base 64 format. Conflicts with `source`.\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "modificationTime": {
                        "type": "string",
                        "description": "The last time stamp when the file was modified\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n",
                        "willReplaceOnChanges": true
                    },
                    "remoteFileModified": {
                        "type": "boolean"
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/gitCredential:GitCredential": {
            "description": "This resource allows you to manage credentials for [Databricks Repos](https://docs.databricks.com/repos.html) using [Git Credentials API](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html).\n\n## Example Usage\n\nYou can declare Pulumi-managed Git credential using following code:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ado = new databricks.GitCredential(\"ado\", {\n    gitUsername: \"myuser\",\n    gitProvider: \"azureDevOpsServices\",\n    personalAccessToken: \"sometoken\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nado = databricks.GitCredential(\"ado\",\n    git_username=\"myuser\",\n    git_provider=\"azureDevOpsServices\",\n    personal_access_token=\"sometoken\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ado = new Databricks.GitCredential(\"ado\", new()\n    {\n        GitUsername = \"myuser\",\n        GitProvider = \"azureDevOpsServices\",\n        PersonalAccessToken = \"sometoken\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGitCredential(ctx, \"ado\", \u0026databricks.GitCredentialArgs{\n\t\t\tGitUsername:         pulumi.String(\"myuser\"),\n\t\t\tGitProvider:         pulumi.String(\"azureDevOpsServices\"),\n\t\t\tPersonalAccessToken: pulumi.String(\"sometoken\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.GitCredential;\nimport com.pulumi.databricks.GitCredentialArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ado = new GitCredential(\"ado\", GitCredentialArgs.builder()\n            .gitUsername(\"myuser\")\n            .gitProvider(\"azureDevOpsServices\")\n            .personalAccessToken(\"sometoken\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ado:\n    type: databricks:GitCredential\n    properties:\n      gitUsername: myuser\n      gitProvider: azureDevOpsServices\n      personalAccessToken: sometoken\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Repo to manage Databricks Repos.\n\n## Import\n\nThe resource cluster can be imported using ID of Git credential that could be obtained via REST API:\n\nbash\n\n```sh\n$ pulumi import databricks:index/gitCredential:GitCredential this \u003cgit-credential-id\u003e\n```\n\n",
            "properties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, `GITLAB_TOKEN`, or `AZDO_PERSONAL_ACCESS_TOKEN`, that has a non-empty value.\n"
                }
            },
            "required": [
                "gitProvider"
            ],
            "inputProperties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, `GITLAB_TOKEN`, or `AZDO_PERSONAL_ACCESS_TOKEN`, that has a non-empty value.\n"
                }
            },
            "requiredInputs": [
                "gitProvider"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GitCredential resources.\n",
                "properties": {
                    "force": {
                        "type": "boolean",
                        "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                    },
                    "gitUsername": {
                        "type": "string",
                        "description": "user name at Git provider.\n"
                    },
                    "personalAccessToken": {
                        "type": "string",
                        "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, `GITLAB_TOKEN`, or `AZDO_PERSONAL_ACCESS_TOKEN`, that has a non-empty value.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/globalInitScript:GlobalInitScript": {
            "description": "This resource allows you to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n\n## Import\n\nThe resource global init script can be imported using script ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/globalInitScript:GlobalInitScript this script_id\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded source code global init script. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances\n"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "required": [
                "name",
                "position"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded source code global init script. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances\n"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GlobalInitScript resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "The base64-encoded source code global init script. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances\n"
                    },
                    "enabled": {
                        "type": "boolean",
                        "description": "specifies if the script is enabled for execution, or not\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "the name of the script.  It should be unique\n"
                    },
                    "position": {
                        "type": "integer",
                        "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grant:Grant": {
            "description": "\u003e This article refers to the privileges and inheritance model in Privilege Model version 1.0. If you created your metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [Upgrade to privilege inheritance](https://docs.databricks.com/data-governance/unity-catalog/hive-metastore.html)\n\n\u003e Most of Unity Catalog APIs are only accessible via **workspace-level APIs**. This design may change in the future. Account-level principal grants can be assigned with any valid workspace as the Unity Catalog is decoupled from specific workspaces. More information in [the official documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html).\n\nIn Unity Catalog all users initially have no access to data. Only Metastore Admins can create objects and can grant/revoke access on individual objects to users and groups. Every securable object in Unity Catalog has an owner. The owner can be any account-level user or group, called principals in general. The principal that creates an object becomes its owner. Owners receive `ALL_PRIVILEGES` on the securable object (e.g., `SELECT` and `MODIFY` on a table), as well as the permission to grant privileges to other principals.\n\nSecurable objects are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a metastore are not inherited.\n\nEvery `databricks.Grant` resource must have exactly one securable identifier and the following arguments:\n\n- `principal` - User name, group name or service principal application ID.\n- `privileges` - One or more privileges that are specific to a securable type.\n\nFor the latest list of privilege types that apply to each securable object in Unity Catalog, please refer to the [official documentation](https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/privileges.html#privilege-types-by-securable-object-in-unity-catalog)\n\nPulumi will handle any configuration drift for the specified principal on every `pulumi up` run, even when grants are changed outside of Pulumi state.\n\nSee databricks.Grants for the list of privilege types that apply to each securable object.\n\n## Metastore grants\n\nSee databricks.Grants Metastore grants for the list of privileges that apply to Metastores.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandboxDataEngineers = new databricks.Grant(\"sandbox_data_engineers\", {\n    metastore: \"metastore_id\",\n    principal: \"Data Engineers\",\n    privileges: [\n        \"CREATE_CATALOG\",\n        \"CREATE_EXTERNAL_LOCATION\",\n    ],\n});\nconst sandboxDataSharer = new databricks.Grant(\"sandbox_data_sharer\", {\n    metastore: \"metastore_id\",\n    principal: \"Data Sharer\",\n    privileges: [\n        \"CREATE_RECIPIENT\",\n        \"CREATE_SHARE\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox_data_engineers = databricks.Grant(\"sandbox_data_engineers\",\n    metastore=\"metastore_id\",\n    principal=\"Data Engineers\",\n    privileges=[\n        \"CREATE_CATALOG\",\n        \"CREATE_EXTERNAL_LOCATION\",\n    ])\nsandbox_data_sharer = databricks.Grant(\"sandbox_data_sharer\",\n    metastore=\"metastore_id\",\n    principal=\"Data Sharer\",\n    privileges=[\n        \"CREATE_RECIPIENT\",\n        \"CREATE_SHARE\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandboxDataEngineers = new Databricks.Grant(\"sandbox_data_engineers\", new()\n    {\n        Metastore = \"metastore_id\",\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"CREATE_CATALOG\",\n            \"CREATE_EXTERNAL_LOCATION\",\n        },\n    });\n\n    var sandboxDataSharer = new Databricks.Grant(\"sandbox_data_sharer\", new()\n    {\n        Metastore = \"metastore_id\",\n        Principal = \"Data Sharer\",\n        Privileges = new[]\n        {\n            \"CREATE_RECIPIENT\",\n            \"CREATE_SHARE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrant(ctx, \"sandbox_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tMetastore: pulumi.String(\"metastore_id\"),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"CREATE_CATALOG\"),\n\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_LOCATION\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"sandbox_data_sharer\", \u0026databricks.GrantArgs{\n\t\t\tMetastore: pulumi.String(\"metastore_id\"),\n\t\t\tPrincipal: pulumi.String(\"Data Sharer\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"CREATE_RECIPIENT\"),\n\t\t\t\tpulumi.String(\"CREATE_SHARE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandboxDataEngineers = new Grant(\"sandboxDataEngineers\", GrantArgs.builder()\n            .metastore(\"metastore_id\")\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"CREATE_CATALOG\",\n                \"CREATE_EXTERNAL_LOCATION\")\n            .build());\n\n        var sandboxDataSharer = new Grant(\"sandboxDataSharer\", GrantArgs.builder()\n            .metastore(\"metastore_id\")\n            .principal(\"Data Sharer\")\n            .privileges(            \n                \"CREATE_RECIPIENT\",\n                \"CREATE_SHARE\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandboxDataEngineers:\n    type: databricks:Grant\n    name: sandbox_data_engineers\n    properties:\n      metastore: metastore_id\n      principal: Data Engineers\n      privileges:\n        - CREATE_CATALOG\n        - CREATE_EXTERNAL_LOCATION\n  sandboxDataSharer:\n    type: databricks:Grant\n    name: sandbox_data_sharer\n    properties:\n      metastore: metastore_id\n      principal: Data Sharer\n      privileges:\n        - CREATE_RECIPIENT\n        - CREATE_SHARE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Catalog grants\n\nSee databricks.Grants Catalog grants for the list of privileges that apply to Catalogs.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst sandboxDataScientists = new databricks.Grant(\"sandbox_data_scientists\", {\n    catalog: sandbox.name,\n    principal: \"Data Scientists\",\n    privileges: [\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"CREATE_TABLE\",\n        \"SELECT\",\n    ],\n});\nconst sandboxDataEngineers = new databricks.Grant(\"sandbox_data_engineers\", {\n    catalog: sandbox.name,\n    principal: \"Data Engineers\",\n    privileges: [\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"CREATE_SCHEMA\",\n        \"CREATE_TABLE\",\n        \"MODIFY\",\n    ],\n});\nconst sandboxDataAnalyst = new databricks.Grant(\"sandbox_data_analyst\", {\n    catalog: sandbox.name,\n    principal: \"Data Analyst\",\n    privileges: [\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"SELECT\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nsandbox_data_scientists = databricks.Grant(\"sandbox_data_scientists\",\n    catalog=sandbox.name,\n    principal=\"Data Scientists\",\n    privileges=[\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"CREATE_TABLE\",\n        \"SELECT\",\n    ])\nsandbox_data_engineers = databricks.Grant(\"sandbox_data_engineers\",\n    catalog=sandbox.name,\n    principal=\"Data Engineers\",\n    privileges=[\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"CREATE_SCHEMA\",\n        \"CREATE_TABLE\",\n        \"MODIFY\",\n    ])\nsandbox_data_analyst = databricks.Grant(\"sandbox_data_analyst\",\n    catalog=sandbox.name,\n    principal=\"Data Analyst\",\n    privileges=[\n        \"USE_CATALOG\",\n        \"USE_SCHEMA\",\n        \"SELECT\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var sandboxDataScientists = new Databricks.Grant(\"sandbox_data_scientists\", new()\n    {\n        Catalog = sandbox.Name,\n        Principal = \"Data Scientists\",\n        Privileges = new[]\n        {\n            \"USE_CATALOG\",\n            \"USE_SCHEMA\",\n            \"CREATE_TABLE\",\n            \"SELECT\",\n        },\n    });\n\n    var sandboxDataEngineers = new Databricks.Grant(\"sandbox_data_engineers\", new()\n    {\n        Catalog = sandbox.Name,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"USE_CATALOG\",\n            \"USE_SCHEMA\",\n            \"CREATE_SCHEMA\",\n            \"CREATE_TABLE\",\n            \"MODIFY\",\n        },\n    });\n\n    var sandboxDataAnalyst = new Databricks.Grant(\"sandbox_data_analyst\", new()\n    {\n        Catalog = sandbox.Name,\n        Principal = \"Data Analyst\",\n        Privileges = new[]\n        {\n            \"USE_CATALOG\",\n            \"USE_SCHEMA\",\n            \"SELECT\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"sandbox_data_scientists\", \u0026databricks.GrantArgs{\n\t\t\tCatalog:   sandbox.Name,\n\t\t\tPrincipal: pulumi.String(\"Data Scientists\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"sandbox_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tCatalog:   sandbox.Name,\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"CREATE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"sandbox_data_analyst\", \u0026databricks.GrantArgs{\n\t\t\tCatalog:   sandbox.Name,\n\t\t\tPrincipal: pulumi.String(\"Data Analyst\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var sandboxDataScientists = new Grant(\"sandboxDataScientists\", GrantArgs.builder()\n            .catalog(sandbox.name())\n            .principal(\"Data Scientists\")\n            .privileges(            \n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"SELECT\")\n            .build());\n\n        var sandboxDataEngineers = new Grant(\"sandboxDataEngineers\", GrantArgs.builder()\n            .catalog(sandbox.name())\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"MODIFY\")\n            .build());\n\n        var sandboxDataAnalyst = new Grant(\"sandboxDataAnalyst\", GrantArgs.builder()\n            .catalog(sandbox.name())\n            .principal(\"Data Analyst\")\n            .privileges(            \n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"SELECT\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  sandboxDataScientists:\n    type: databricks:Grant\n    name: sandbox_data_scientists\n    properties:\n      catalog: ${sandbox.name}\n      principal: Data Scientists\n      privileges:\n        - USE_CATALOG\n        - USE_SCHEMA\n        - CREATE_TABLE\n        - SELECT\n  sandboxDataEngineers:\n    type: databricks:Grant\n    name: sandbox_data_engineers\n    properties:\n      catalog: ${sandbox.name}\n      principal: Data Engineers\n      privileges:\n        - USE_CATALOG\n        - USE_SCHEMA\n        - CREATE_SCHEMA\n        - CREATE_TABLE\n        - MODIFY\n  sandboxDataAnalyst:\n    type: databricks:Grant\n    name: sandbox_data_analyst\n    properties:\n      catalog: ${sandbox.name}\n      principal: Data Analyst\n      privileges:\n        - USE_CATALOG\n        - USE_SCHEMA\n        - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Schema grants\n\nSee databricks.Grants Schema grants for the list of privileges that apply to Schemas.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this schema is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst thingsGrant = new databricks.Grant(\"things\", {\n    schema: things.id,\n    principal: \"Data Engineers\",\n    privileges: [\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox[\"id\"],\n    name=\"things\",\n    comment=\"this schema is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nthings_grant = databricks.Grant(\"things\",\n    schema=things.id,\n    principal=\"Data Engineers\",\n    privileges=[\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this schema is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var thingsGrant = new Databricks.Grant(\"things\", new()\n    {\n        Schema = things.Id,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: pulumi.Any(sandbox.Id),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this schema is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"things\", \u0026databricks.GrantArgs{\n\t\t\tSchema:    things.ID(),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this schema is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var thingsGrant = new Grant(\"thingsGrant\", GrantArgs.builder()\n            .schema(things.id())\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"USE_SCHEMA\",\n                \"MODIFY\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this schema is managed by terraform\n      properties:\n        kind: various\n  thingsGrant:\n    type: databricks:Grant\n    name: things\n    properties:\n      schema: ${things.id}\n      principal: Data Engineers\n      privileges:\n        - USE_SCHEMA\n        - MODIFY\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Table grants\n\nSee databricks.Grants Table grants for the list of privileges that apply to Tables.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customersDataEngineers = new databricks.Grant(\"customers_data_engineers\", {\n    table: \"main.reporting.customers\",\n    principal: \"Data Engineers\",\n    privileges: [\n        \"MODIFY\",\n        \"SELECT\",\n    ],\n});\nconst customersDataAnalysts = new databricks.Grant(\"customers_data_analysts\", {\n    table: \"main.reporting.customers\",\n    principal: \"Data Analysts\",\n    privileges: [\"SELECT\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers_data_engineers = databricks.Grant(\"customers_data_engineers\",\n    table=\"main.reporting.customers\",\n    principal=\"Data Engineers\",\n    privileges=[\n        \"MODIFY\",\n        \"SELECT\",\n    ])\ncustomers_data_analysts = databricks.Grant(\"customers_data_analysts\",\n    table=\"main.reporting.customers\",\n    principal=\"Data Analysts\",\n    privileges=[\"SELECT\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customersDataEngineers = new Databricks.Grant(\"customers_data_engineers\", new()\n    {\n        Table = \"main.reporting.customers\",\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"MODIFY\",\n            \"SELECT\",\n        },\n    });\n\n    var customersDataAnalysts = new Databricks.Grant(\"customers_data_analysts\", new()\n    {\n        Table = \"main.reporting.customers\",\n        Principal = \"Data Analysts\",\n        Privileges = new[]\n        {\n            \"SELECT\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrant(ctx, \"customers_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tTable:     pulumi.String(\"main.reporting.customers\"),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"customers_data_analysts\", \u0026databricks.GrantArgs{\n\t\t\tTable:     pulumi.String(\"main.reporting.customers\"),\n\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customersDataEngineers = new Grant(\"customersDataEngineers\", GrantArgs.builder()\n            .table(\"main.reporting.customers\")\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"MODIFY\",\n                \"SELECT\")\n            .build());\n\n        var customersDataAnalysts = new Grant(\"customersDataAnalysts\", GrantArgs.builder()\n            .table(\"main.reporting.customers\")\n            .principal(\"Data Analysts\")\n            .privileges(\"SELECT\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customersDataEngineers:\n    type: databricks:Grant\n    name: customers_data_engineers\n    properties:\n      table: main.reporting.customers\n      principal: Data Engineers\n      privileges:\n        - MODIFY\n        - SELECT\n  customersDataAnalysts:\n    type: databricks:Grant\n    name: customers_data_analysts\n    properties:\n      table: main.reporting.customers\n      principal: Data Analysts\n      privileges:\n        - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nYou can also apply grants dynamically with databricks.getTables data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const things = await databricks.getTables({\n        catalogName: \"sandbox\",\n        schemaName: \"things\",\n    });\n    const thingsGrant: databricks.Grant[] = [];\n    for (const range of things.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        thingsGrant.push(new databricks.Grant(`things-${range.key}`, {\n            table: range.value,\n            principal: \"sensitive\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_tables(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthings_grant = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(things.ids)]:\n    things_grant.append(databricks.Grant(f\"things-{range['key']}\",\n        table=range[\"value\"],\n        principal=\"sensitive\",\n        privileges=[\n            \"SELECT\",\n            \"MODIFY\",\n        ]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var things = await Databricks.GetTables.InvokeAsync(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var thingsGrant = new List\u003cDatabricks.Grant\u003e();\n    foreach (var range in )\n    {\n        thingsGrant.Add(new Databricks.Grant($\"things-{range.Key}\", new()\n        {\n            Table = range.Value,\n            Principal = \"sensitive\",\n            Privileges = new[]\n            {\n                \"SELECT\",\n                \"MODIFY\",\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.GetTables(ctx, \u0026databricks.GetTablesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar thingsGrant []*databricks.Grant\n\t\tfor key0, val0 := range things.Ids {\n\t\t\t__res, err := databricks.NewGrant(ctx, fmt.Sprintf(\"things-%v\", key0), \u0026databricks.GrantArgs{\n\t\t\t\tTable:     pulumi.String(val0),\n\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tthingsGrant = append(thingsGrant, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTablesArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrant = things.applyValue(getTablesResult -\u003e {\n            final var resources = new ArrayList\u003cGrant\u003e();\n            for (var range : KeyedValue.of(getTablesResult.ids())) {\n                var resource = new Grant(\"thingsGrant-\" + range.key(), GrantArgs.builder()\n                    .table(range.value())\n                    .principal(\"sensitive\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrant:\n    type: databricks:Grant\n    name: things\n    properties:\n      table: ${range.value}\n      principal: sensitive\n      privileges:\n        - SELECT\n        - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      function: databricks:getTables\n      arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## View grants\n\nSee databricks.Grants View grants for the list of privileges that apply to Views.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customer360 = new databricks.Grant(\"customer360\", {\n    table: \"main.reporting.customer360\",\n    principal: \"Data Analysts\",\n    privileges: [\"SELECT\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomer360 = databricks.Grant(\"customer360\",\n    table=\"main.reporting.customer360\",\n    principal=\"Data Analysts\",\n    privileges=[\"SELECT\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customer360 = new Databricks.Grant(\"customer360\", new()\n    {\n        Table = \"main.reporting.customer360\",\n        Principal = \"Data Analysts\",\n        Privileges = new[]\n        {\n            \"SELECT\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrant(ctx, \"customer360\", \u0026databricks.GrantArgs{\n\t\t\tTable:     pulumi.String(\"main.reporting.customer360\"),\n\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customer360 = new Grant(\"customer360\", GrantArgs.builder()\n            .table(\"main.reporting.customer360\")\n            .principal(\"Data Analysts\")\n            .privileges(\"SELECT\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customer360:\n    type: databricks:Grant\n    properties:\n      table: main.reporting.customer360\n      principal: Data Analysts\n      privileges:\n        - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nYou can also apply grants dynamically with databricks.getViews data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const customers = await databricks.getViews({\n        catalogName: \"main\",\n        schemaName: \"customers\",\n    });\n    const customersGrant: databricks.Grant[] = [];\n    for (const range of customers.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        customersGrant.push(new databricks.Grant(`customers-${range.key}`, {\n            table: range.value,\n            principal: \"sensitive\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers = databricks.get_views(catalog_name=\"main\",\n    schema_name=\"customers\")\ncustomers_grant = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(customers.ids)]:\n    customers_grant.append(databricks.Grant(f\"customers-{range['key']}\",\n        table=range[\"value\"],\n        principal=\"sensitive\",\n        privileges=[\n            \"SELECT\",\n            \"MODIFY\",\n        ]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var customers = await Databricks.GetViews.InvokeAsync(new()\n    {\n        CatalogName = \"main\",\n        SchemaName = \"customers\",\n    });\n\n    var customersGrant = new List\u003cDatabricks.Grant\u003e();\n    foreach (var range in )\n    {\n        customersGrant.Add(new Databricks.Grant($\"customers-{range.Key}\", new()\n        {\n            Table = range.Value,\n            Principal = \"sensitive\",\n            Privileges = new[]\n            {\n                \"SELECT\",\n                \"MODIFY\",\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcustomers, err := databricks.GetViews(ctx, \u0026databricks.GetViewsArgs{\n\t\t\tCatalogName: \"main\",\n\t\t\tSchemaName:  \"customers\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar customersGrant []*databricks.Grant\n\t\tfor key0, val0 := range customers.Ids {\n\t\t\t__res, err := databricks.NewGrant(ctx, fmt.Sprintf(\"customers-%v\", key0), \u0026databricks.GrantArgs{\n\t\t\t\tTable:     pulumi.String(val0),\n\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcustomersGrant = append(customersGrant, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetViewsArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var customers = DatabricksFunctions.getViews(GetViewsArgs.builder()\n            .catalogName(\"main\")\n            .schemaName(\"customers\")\n            .build());\n\n        final var customersGrant = customers.applyValue(getViewsResult -\u003e {\n            final var resources = new ArrayList\u003cGrant\u003e();\n            for (var range : KeyedValue.of(getViewsResult.ids())) {\n                var resource = new Grant(\"customersGrant-\" + range.key(), GrantArgs.builder()\n                    .table(range.value())\n                    .principal(\"sensitive\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  customersGrant:\n    type: databricks:Grant\n    name: customers\n    properties:\n      table: ${range.value}\n      principal: sensitive\n      privileges:\n        - SELECT\n        - MODIFY\n    options: {}\nvariables:\n  customers:\n    fn::invoke:\n      function: databricks:getViews\n      arguments:\n        catalogName: main\n        schemaName: customers\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Volume grants\n\nSee databricks.Grants Volume grants for the list of privileges that apply to Volumes.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Volume(\"this\", {\n    name: \"quickstart_volume\",\n    catalogName: sandbox.name,\n    schemaName: things.name,\n    volumeType: \"EXTERNAL\",\n    storageLocation: some.url,\n    comment: \"this volume is managed by terraform\",\n});\nconst volume = new databricks.Grant(\"volume\", {\n    volume: _this.id,\n    principal: \"Data Engineers\",\n    privileges: [\"WRITE_VOLUME\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Volume(\"this\",\n    name=\"quickstart_volume\",\n    catalog_name=sandbox[\"name\"],\n    schema_name=things[\"name\"],\n    volume_type=\"EXTERNAL\",\n    storage_location=some[\"url\"],\n    comment=\"this volume is managed by terraform\")\nvolume = databricks.Grant(\"volume\",\n    volume=this.id,\n    principal=\"Data Engineers\",\n    privileges=[\"WRITE_VOLUME\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Volume(\"this\", new()\n    {\n        Name = \"quickstart_volume\",\n        CatalogName = sandbox.Name,\n        SchemaName = things.Name,\n        VolumeType = \"EXTERNAL\",\n        StorageLocation = some.Url,\n        Comment = \"this volume is managed by terraform\",\n    });\n\n    var volume = new Databricks.Grant(\"volume\", new()\n    {\n        Volume = @this.Id,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"WRITE_VOLUME\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewVolume(ctx, \"this\", \u0026databricks.VolumeArgs{\n\t\t\tName:            pulumi.String(\"quickstart_volume\"),\n\t\t\tCatalogName:     pulumi.Any(sandbox.Name),\n\t\t\tSchemaName:      pulumi.Any(things.Name),\n\t\t\tVolumeType:      pulumi.String(\"EXTERNAL\"),\n\t\t\tStorageLocation: pulumi.Any(some.Url),\n\t\t\tComment:         pulumi.String(\"this volume is managed by terraform\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"volume\", \u0026databricks.GrantArgs{\n\t\t\tVolume:    this.ID(),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"WRITE_VOLUME\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Volume;\nimport com.pulumi.databricks.VolumeArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Volume(\"this\", VolumeArgs.builder()\n            .name(\"quickstart_volume\")\n            .catalogName(sandbox.name())\n            .schemaName(things.name())\n            .volumeType(\"EXTERNAL\")\n            .storageLocation(some.url())\n            .comment(\"this volume is managed by terraform\")\n            .build());\n\n        var volume = new Grant(\"volume\", GrantArgs.builder()\n            .volume(this_.id())\n            .principal(\"Data Engineers\")\n            .privileges(\"WRITE_VOLUME\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Volume\n    properties:\n      name: quickstart_volume\n      catalogName: ${sandbox.name}\n      schemaName: ${things.name}\n      volumeType: EXTERNAL\n      storageLocation: ${some.url}\n      comment: this volume is managed by terraform\n  volume:\n    type: databricks:Grant\n    properties:\n      volume: ${this.id}\n      principal: Data Engineers\n      privileges:\n        - WRITE_VOLUME\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Registered model grants\n\nSee databricks.Grants Registered model grants for the list of privileges that apply to Registered models.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customersDataEngineers = new databricks.Grant(\"customers_data_engineers\", {\n    model: \"main.reporting.customer_model\",\n    principal: \"Data Engineers\",\n    privileges: [\n        \"APPLY_TAG\",\n        \"EXECUTE\",\n    ],\n});\nconst customersDataAnalysts = new databricks.Grant(\"customers_data_analysts\", {\n    model: \"main.reporting.customer_model\",\n    principal: \"Data Analysts\",\n    privileges: [\"EXECUTE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers_data_engineers = databricks.Grant(\"customers_data_engineers\",\n    model=\"main.reporting.customer_model\",\n    principal=\"Data Engineers\",\n    privileges=[\n        \"APPLY_TAG\",\n        \"EXECUTE\",\n    ])\ncustomers_data_analysts = databricks.Grant(\"customers_data_analysts\",\n    model=\"main.reporting.customer_model\",\n    principal=\"Data Analysts\",\n    privileges=[\"EXECUTE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customersDataEngineers = new Databricks.Grant(\"customers_data_engineers\", new()\n    {\n        Model = \"main.reporting.customer_model\",\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"APPLY_TAG\",\n            \"EXECUTE\",\n        },\n    });\n\n    var customersDataAnalysts = new Databricks.Grant(\"customers_data_analysts\", new()\n    {\n        Model = \"main.reporting.customer_model\",\n        Principal = \"Data Analysts\",\n        Privileges = new[]\n        {\n            \"EXECUTE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrant(ctx, \"customers_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tModel:     pulumi.String(\"main.reporting.customer_model\"),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"APPLY_TAG\"),\n\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"customers_data_analysts\", \u0026databricks.GrantArgs{\n\t\t\tModel:     pulumi.String(\"main.reporting.customer_model\"),\n\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customersDataEngineers = new Grant(\"customersDataEngineers\", GrantArgs.builder()\n            .model(\"main.reporting.customer_model\")\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"APPLY_TAG\",\n                \"EXECUTE\")\n            .build());\n\n        var customersDataAnalysts = new Grant(\"customersDataAnalysts\", GrantArgs.builder()\n            .model(\"main.reporting.customer_model\")\n            .principal(\"Data Analysts\")\n            .privileges(\"EXECUTE\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customersDataEngineers:\n    type: databricks:Grant\n    name: customers_data_engineers\n    properties:\n      model: main.reporting.customer_model\n      principal: Data Engineers\n      privileges:\n        - APPLY_TAG\n        - EXECUTE\n  customersDataAnalysts:\n    type: databricks:Grant\n    name: customers_data_analysts\n    properties:\n      model: main.reporting.customer_model\n      principal: Data Analysts\n      privileges:\n        - EXECUTE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Function grants\n\nSee databricks.Grants Function grants for the list of privileges that apply to Registered models.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst udfDataEngineers = new databricks.Grant(\"udf_data_engineers\", {\n    \"function\": \"main.reporting.udf\",\n    principal: \"Data Engineers\",\n    privileges: [\"EXECUTE\"],\n});\nconst udfDataAnalysts = new databricks.Grant(\"udf_data_analysts\", {\n    \"function\": \"main.reporting.udf\",\n    principal: \"Data Analysts\",\n    privileges: [\"EXECUTE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nudf_data_engineers = databricks.Grant(\"udf_data_engineers\",\n    function=\"main.reporting.udf\",\n    principal=\"Data Engineers\",\n    privileges=[\"EXECUTE\"])\nudf_data_analysts = databricks.Grant(\"udf_data_analysts\",\n    function=\"main.reporting.udf\",\n    principal=\"Data Analysts\",\n    privileges=[\"EXECUTE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var udfDataEngineers = new Databricks.Grant(\"udf_data_engineers\", new()\n    {\n        Function = \"main.reporting.udf\",\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"EXECUTE\",\n        },\n    });\n\n    var udfDataAnalysts = new Databricks.Grant(\"udf_data_analysts\", new()\n    {\n        Function = \"main.reporting.udf\",\n        Principal = \"Data Analysts\",\n        Privileges = new[]\n        {\n            \"EXECUTE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrant(ctx, \"udf_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tFunction:  pulumi.String(\"main.reporting.udf\"),\n\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"udf_data_analysts\", \u0026databricks.GrantArgs{\n\t\t\tFunction:  pulumi.String(\"main.reporting.udf\"),\n\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var udfDataEngineers = new Grant(\"udfDataEngineers\", GrantArgs.builder()\n            .function(\"main.reporting.udf\")\n            .principal(\"Data Engineers\")\n            .privileges(\"EXECUTE\")\n            .build());\n\n        var udfDataAnalysts = new Grant(\"udfDataAnalysts\", GrantArgs.builder()\n            .function(\"main.reporting.udf\")\n            .principal(\"Data Analysts\")\n            .privileges(\"EXECUTE\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  udfDataEngineers:\n    type: databricks:Grant\n    name: udf_data_engineers\n    properties:\n      function: main.reporting.udf\n      principal: Data Engineers\n      privileges:\n        - EXECUTE\n  udfDataAnalysts:\n    type: databricks:Grant\n    name: udf_data_analysts\n    properties:\n      function: main.reporting.udf\n      principal: Data Analysts\n      privileges:\n        - EXECUTE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Service credential grants\n\nSee databricks.Grants Service credential grants for the list of privileges that apply to Service credentials.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.Credential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    purpose: \"SERVICE\",\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grant(\"external_creds\", {\n    credential: external.id,\n    principal: \"Data Engineers\",\n    privileges: [\"ACCESS\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.Credential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    purpose=\"SERVICE\",\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grant(\"external_creds\",\n    credential=external.id,\n    principal=\"Data Engineers\",\n    privileges=[\"ACCESS\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.Credential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.CredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Purpose = \"SERVICE\",\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grant(\"external_creds\", new()\n    {\n        Credential = external.Id,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"ACCESS\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewCredential(ctx, \"external\", \u0026databricks.CredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.CredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tPurpose: pulumi.String(\"SERVICE\"),\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"external_creds\", \u0026databricks.GrantArgs{\n\t\t\tCredential: external.ID(),\n\t\t\tPrincipal:  pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ACCESS\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Credential;\nimport com.pulumi.databricks.CredentialArgs;\nimport com.pulumi.databricks.inputs.CredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new Credential(\"external\", CredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(CredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .purpose(\"SERVICE\")\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grant(\"externalCreds\", GrantArgs.builder()\n            .credential(external.id())\n            .principal(\"Data Engineers\")\n            .privileges(\"ACCESS\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:Credential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      purpose: SERVICE\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grant\n    name: external_creds\n    properties:\n      credential: ${external.id}\n      principal: Data Engineers\n      privileges:\n        - ACCESS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Storage credential grants\n\nSee databricks.Grants Storage credential grants for the list of privileges that apply to Storage credentials.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grant(\"external_creds\", {\n    storageCredential: external.id,\n    principal: \"Data Engineers\",\n    privileges: [\"CREATE_EXTERNAL_TABLE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grant(\"external_creds\",\n    storage_credential=external.id,\n    principal=\"Data Engineers\",\n    privileges=[\"CREATE_EXTERNAL_TABLE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grant(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"CREATE_EXTERNAL_TABLE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"external_creds\", \u0026databricks.GrantArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tPrincipal:         pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grant(\"externalCreds\", GrantArgs.builder()\n            .storageCredential(external.id())\n            .principal(\"Data Engineers\")\n            .privileges(\"CREATE_EXTERNAL_TABLE\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grant\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      principal: Data Engineers\n      privileges:\n        - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## External location grants\n\nSee databricks.Grants External location grants for the list of privileges that apply to External locations.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.id,\n    comment: \"Managed by TF\",\n});\nconst someDataEngineers = new databricks.Grant(\"some_data_engineers\", {\n    externalLocation: some.id,\n    principal: \"Data Engineers\",\n    privileges: [\n        \"CREATE_EXTERNAL_TABLE\",\n        \"READ_FILES\",\n    ],\n});\nconst someServicePrincipal = new databricks.Grant(\"some_service_principal\", {\n    externalLocation: some.id,\n    principal: mySp.applicationId,\n    privileges: [\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ],\n});\nconst someGroup = new databricks.Grant(\"some_group\", {\n    externalLocation: some.id,\n    principal: myGroup.displayName,\n    privileges: [\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ],\n});\nconst someUser = new databricks.Grant(\"some_user\", {\n    externalLocation: some.id,\n    principal: myUser.userName,\n    privileges: [\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external[\"id\"],\n    comment=\"Managed by TF\")\nsome_data_engineers = databricks.Grant(\"some_data_engineers\",\n    external_location=some.id,\n    principal=\"Data Engineers\",\n    privileges=[\n        \"CREATE_EXTERNAL_TABLE\",\n        \"READ_FILES\",\n    ])\nsome_service_principal = databricks.Grant(\"some_service_principal\",\n    external_location=some.id,\n    principal=my_sp[\"applicationId\"],\n    privileges=[\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ])\nsome_group = databricks.Grant(\"some_group\",\n    external_location=some.id,\n    principal=my_group[\"displayName\"],\n    privileges=[\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ])\nsome_user = databricks.Grant(\"some_user\",\n    external_location=some.id,\n    principal=my_user[\"userName\"],\n    privileges=[\n        \"USE_SCHEMA\",\n        \"MODIFY\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Id,\n        Comment = \"Managed by TF\",\n    });\n\n    var someDataEngineers = new Databricks.Grant(\"some_data_engineers\", new()\n    {\n        ExternalLocation = some.Id,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"CREATE_EXTERNAL_TABLE\",\n            \"READ_FILES\",\n        },\n    });\n\n    var someServicePrincipal = new Databricks.Grant(\"some_service_principal\", new()\n    {\n        ExternalLocation = some.Id,\n        Principal = mySp.ApplicationId,\n        Privileges = new[]\n        {\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        },\n    });\n\n    var someGroup = new Databricks.Grant(\"some_group\", new()\n    {\n        ExternalLocation = some.Id,\n        Principal = myGroup.DisplayName,\n        Privileges = new[]\n        {\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        },\n    });\n\n    var someUser = new Databricks.Grant(\"some_user\", new()\n    {\n        ExternalLocation = some.Id,\n        Principal = myUser.UserName,\n        Privileges = new[]\n        {\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external\"),\n\t\t\tUrl:            pulumi.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id),\n\t\t\tCredentialName: pulumi.Any(external.Id),\n\t\t\tComment:        pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some_data_engineers\", \u0026databricks.GrantArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tPrincipal:        pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some_service_principal\", \u0026databricks.GrantArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tPrincipal:        pulumi.Any(mySp.ApplicationId),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some_group\", \u0026databricks.GrantArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tPrincipal:        pulumi.Any(myGroup.DisplayName),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some_user\", \u0026databricks.GrantArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tPrincipal:        pulumi.Any(myUser.UserName),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()\n            .name(\"external\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.id())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var someDataEngineers = new Grant(\"someDataEngineers\", GrantArgs.builder()\n            .externalLocation(some.id())\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\")\n            .build());\n\n        var someServicePrincipal = new Grant(\"someServicePrincipal\", GrantArgs.builder()\n            .externalLocation(some.id())\n            .principal(mySp.applicationId())\n            .privileges(            \n                \"USE_SCHEMA\",\n                \"MODIFY\")\n            .build());\n\n        var someGroup = new Grant(\"someGroup\", GrantArgs.builder()\n            .externalLocation(some.id())\n            .principal(myGroup.displayName())\n            .privileges(            \n                \"USE_SCHEMA\",\n                \"MODIFY\")\n            .build());\n\n        var someUser = new Grant(\"someUser\", GrantArgs.builder()\n            .externalLocation(some.id())\n            .principal(myUser.userName())\n            .privileges(            \n                \"USE_SCHEMA\",\n                \"MODIFY\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.id}\n      comment: Managed by TF\n  someDataEngineers:\n    type: databricks:Grant\n    name: some_data_engineers\n    properties:\n      externalLocation: ${some.id}\n      principal: Data Engineers\n      privileges:\n        - CREATE_EXTERNAL_TABLE\n        - READ_FILES\n  someServicePrincipal:\n    type: databricks:Grant\n    name: some_service_principal\n    properties:\n      externalLocation: ${some.id}\n      principal: ${mySp.applicationId}\n      privileges:\n        - USE_SCHEMA\n        - MODIFY\n  someGroup:\n    type: databricks:Grant\n    name: some_group\n    properties:\n      externalLocation: ${some.id}\n      principal: ${myGroup.displayName}\n      privileges:\n        - USE_SCHEMA\n        - MODIFY\n  someUser:\n    type: databricks:Grant\n    name: some_user\n    properties:\n      externalLocation: ${some.id}\n      principal: ${myUser.userName}\n      privileges:\n        - USE_SCHEMA\n        - MODIFY\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Connection grants\n\nSee databricks.Grants Connection grants for the list of privileges that apply to Connections.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst mysql = new databricks.Connection(\"mysql\", {\n    name: \"mysql_connection\",\n    connectionType: \"MYSQL\",\n    comment: \"this is a connection to mysql db\",\n    options: {\n        host: \"test.mysql.database.azure.com\",\n        port: \"3306\",\n        user: \"user\",\n        password: \"password\",\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst some = new databricks.Grant(\"some\", {\n    foreignConnection: mysql.name,\n    principal: \"Data Engineers\",\n    privileges: [\n        \"CREATE_FOREIGN_CATALOG\",\n        \"USE_CONNECTION\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmysql = databricks.Connection(\"mysql\",\n    name=\"mysql_connection\",\n    connection_type=\"MYSQL\",\n    comment=\"this is a connection to mysql db\",\n    options={\n        \"host\": \"test.mysql.database.azure.com\",\n        \"port\": \"3306\",\n        \"user\": \"user\",\n        \"password\": \"password\",\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\nsome = databricks.Grant(\"some\",\n    foreign_connection=mysql.name,\n    principal=\"Data Engineers\",\n    privileges=[\n        \"CREATE_FOREIGN_CATALOG\",\n        \"USE_CONNECTION\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var mysql = new Databricks.Connection(\"mysql\", new()\n    {\n        Name = \"mysql_connection\",\n        ConnectionType = \"MYSQL\",\n        Comment = \"this is a connection to mysql db\",\n        Options = \n        {\n            { \"host\", \"test.mysql.database.azure.com\" },\n            { \"port\", \"3306\" },\n            { \"user\", \"user\" },\n            { \"password\", \"password\" },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var some = new Databricks.Grant(\"some\", new()\n    {\n        ForeignConnection = mysql.Name,\n        Principal = \"Data Engineers\",\n        Privileges = new[]\n        {\n            \"CREATE_FOREIGN_CATALOG\",\n            \"USE_CONNECTION\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmysql, err := databricks.NewConnection(ctx, \"mysql\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"mysql_connection\"),\n\t\t\tConnectionType: pulumi.String(\"MYSQL\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to mysql db\"),\n\t\t\tOptions: pulumi.StringMap{\n\t\t\t\t\"host\":     pulumi.String(\"test.mysql.database.azure.com\"),\n\t\t\t\t\"port\":     pulumi.String(\"3306\"),\n\t\t\t\t\"user\":     pulumi.String(\"user\"),\n\t\t\t\t\"password\": pulumi.String(\"password\"),\n\t\t\t},\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some\", \u0026databricks.GrantArgs{\n\t\t\tForeignConnection: mysql.Name,\n\t\t\tPrincipal:         pulumi.String(\"Data Engineers\"),\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"CREATE_FOREIGN_CATALOG\"),\n\t\t\t\tpulumi.String(\"USE_CONNECTION\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mysql = new Connection(\"mysql\", ConnectionArgs.builder()\n            .name(\"mysql_connection\")\n            .connectionType(\"MYSQL\")\n            .comment(\"this is a connection to mysql db\")\n            .options(Map.ofEntries(\n                Map.entry(\"host\", \"test.mysql.database.azure.com\"),\n                Map.entry(\"port\", \"3306\"),\n                Map.entry(\"user\", \"user\"),\n                Map.entry(\"password\", \"password\")\n            ))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var some = new Grant(\"some\", GrantArgs.builder()\n            .foreignConnection(mysql.name())\n            .principal(\"Data Engineers\")\n            .privileges(            \n                \"CREATE_FOREIGN_CATALOG\",\n                \"USE_CONNECTION\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  mysql:\n    type: databricks:Connection\n    properties:\n      name: mysql_connection\n      connectionType: MYSQL\n      comment: this is a connection to mysql db\n      options:\n        host: test.mysql.database.azure.com\n        port: '3306'\n        user: user\n        password: password\n      properties:\n        purpose: testing\n  some:\n    type: databricks:Grant\n    properties:\n      foreignConnection: ${mysql.name}\n      principal: Data Engineers\n      privileges:\n        - CREATE_FOREIGN_CATALOG\n        - USE_CONNECTION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Delta Sharing share grants\n\nSee databricks.Grants Delta Sharing share grants for the list of privileges that apply to Delta Sharing shares.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst some = new databricks.Share(\"some\", {name: \"my_share\"});\nconst someRecipient = new databricks.Recipient(\"some\", {name: \"my_recipient\"});\nconst someGrant = new databricks.Grant(\"some\", {\n    share: some.name,\n    principal: someRecipient.name,\n    privileges: [\"SELECT\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsome = databricks.Share(\"some\", name=\"my_share\")\nsome_recipient = databricks.Recipient(\"some\", name=\"my_recipient\")\nsome_grant = databricks.Grant(\"some\",\n    share=some.name,\n    principal=some_recipient.name,\n    privileges=[\"SELECT\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var some = new Databricks.Share(\"some\", new()\n    {\n        Name = \"my_share\",\n    });\n\n    var someRecipient = new Databricks.Recipient(\"some\", new()\n    {\n        Name = \"my_recipient\",\n    });\n\n    var someGrant = new Databricks.Grant(\"some\", new()\n    {\n        Share = some.Name,\n        Principal = someRecipient.Name,\n        Privileges = new[]\n        {\n            \"SELECT\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsome, err := databricks.NewShare(ctx, \"some\", \u0026databricks.ShareArgs{\n\t\t\tName: pulumi.String(\"my_share\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsomeRecipient, err := databricks.NewRecipient(ctx, \"some\", \u0026databricks.RecipientArgs{\n\t\t\tName: pulumi.String(\"my_recipient\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrant(ctx, \"some\", \u0026databricks.GrantArgs{\n\t\t\tShare:     some.Name,\n\t\t\tPrincipal: someRecipient.Name,\n\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Share;\nimport com.pulumi.databricks.ShareArgs;\nimport com.pulumi.databricks.Recipient;\nimport com.pulumi.databricks.RecipientArgs;\nimport com.pulumi.databricks.Grant;\nimport com.pulumi.databricks.GrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var some = new Share(\"some\", ShareArgs.builder()\n            .name(\"my_share\")\n            .build());\n\n        var someRecipient = new Recipient(\"someRecipient\", RecipientArgs.builder()\n            .name(\"my_recipient\")\n            .build());\n\n        var someGrant = new Grant(\"someGrant\", GrantArgs.builder()\n            .share(some.name())\n            .principal(someRecipient.name())\n            .privileges(\"SELECT\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:Share\n    properties:\n      name: my_share\n  someRecipient:\n    type: databricks:Recipient\n    name: some\n    properties:\n      name: my_recipient\n  someGrant:\n    type: databricks:Grant\n    name: some\n    properties:\n      share: ${some.name}\n      principal: ${someRecipient.name}\n      privileges:\n        - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Other access control\n\nYou can control Databricks General Permissions through databricks.Permissions resource.\n\n## Import\n\nThe resource can be imported using combination of securable type (`table`, `catalog`, `foreign_connection`, ...), it's name and `principal`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/grant:Grant this catalog/abc/user_name\n```\n\n",
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "credential": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "foreignConnection": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "metastore": {
                    "type": "string"
                },
                "model": {
                    "type": "string"
                },
                "pipeline": {
                    "type": "string"
                },
                "principal": {
                    "type": "string"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "recipient": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "share": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "volume": {
                    "type": "string"
                }
            },
            "required": [
                "principal",
                "privileges"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "credential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "foreignConnection": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "model": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "pipeline": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "recipient": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "share": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "volume": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "principal",
                "privileges"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grant resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "credential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "foreignConnection": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "model": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "pipeline": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "privileges": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "recipient": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "share": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "volume": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grants:Grants": {
            "description": "\u003e This article refers to the privileges and inheritance model in Privilege Model version 1.0. If you created your metastore during the public preview (before August 25, 2022), you can upgrade to Privilege Model version 1.0 following [Upgrade to privilege inheritance](https://docs.databricks.com/data-governance/unity-catalog/hive-metastore.html)\n\n\u003e Most of Unity Catalog APIs are only accessible via **workspace-level APIs**. This design may change in the future. Account-level principal grants can be assigned with any valid workspace as the Unity Catalog is decoupled from specific workspaces. More information in [the official documentation](https://docs.databricks.com/data-governance/unity-catalog/index.html).\n\nTwo different resources help you manage your Unity Catalog grants for a securable. Each of these resources serves a different use case:\n\n- databricks_grants: Authoritative. Sets the grants of a securable and *replaces* any existing grants defined inside or outside of Pulumi.\n- databricks_grant: Authoritative for a given principal. Updates the grants of a securable to a single principal. Other principals within the grants for the securables are preserved.\n\nIn Unity Catalog all users initially have no access to data. Only Metastore Admins can create objects and can grant/revoke access on individual objects to users and groups. Every securable object in Unity Catalog has an owner. The owner can be any account-level user or group, called principals in general. The principal that creates an object becomes its owner. Owners receive `ALL_PRIVILEGES` on the securable object (e.g., `SELECT` and `MODIFY` on a table), as well as the permission to grant privileges to other principals.\n\nSecurable objects are hierarchical and privileges are inherited downward. The highest level object that privileges are inherited from is the catalog. This means that granting a privilege on a catalog or schema automatically grants the privilege to all current and future objects within the catalog or schema. Privileges that are granted on a metastore are not inherited.\n\nEvery `databricks.Grants` resource must have exactly one securable identifier and one or more `grant` blocks with the following arguments:\n\n- `principal` - User name, group name or service principal application ID.\n- `privileges` - One or more privileges that are specific to a securable type.\n\nFor the latest list of privilege types that apply to each securable object in Unity Catalog, please refer to the [official documentation](https://docs.databricks.com/en/data-governance/unity-catalog/manage-privileges/privileges.html#privilege-types-by-securable-object-in-unity-catalog)\n\nPulumi will handle any configuration drift on every `pulumi up` run, even when grants are changed outside of Pulumi state.\n\nWhen applying grants using an identity with [`MANAGE` permission](https://docs.databricks.com/aws/en/data-governance/unity-catalog/manage-privileges/ownership#ownership-versus-the-manage-privilege), their `MANAGE` permission must also be defined, otherwise Pulumi will remove their permissions, leading to errors.\n\nUnlike the [SQL specification](https://docs.databricks.com/sql/language-manual/sql-ref-privileges.html#privilege-types), all privileges to be written with underscore instead of space, e.g. `CREATE_TABLE` and not `CREATE TABLE`. Below summarizes which privilege types apply to each securable object in the catalog:\n\n## Metastore grants\n\nYou can grant `CREATE_CATALOG`, `CREATE_CLEAN_ROOM`, `CREATE_CONNECTION`, `CREATE_EXTERNAL_LOCATION`, `CREATE_PROVIDER`, `CREATE_RECIPIENT`, `CREATE_SHARE`, `CREATE_SERVICE_CREDENTIAL`, `CREATE_STORAGE_CREDENTIAL`, `SET_SHARE_PERMISSION`, `USE_MARKETPLACE_ASSETS`, `USE_PROVIDER`, `USE_RECIPIENT`, and `USE_SHARE` privileges to databricks.Metastore assigned to the workspace.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Grants(\"sandbox\", {\n    metastore: \"metastore_id\",\n    grants: [\n        {\n            principal: \"Data Engineers\",\n            privileges: [\n                \"CREATE_CATALOG\",\n                \"CREATE_EXTERNAL_LOCATION\",\n            ],\n        },\n        {\n            principal: \"Data Sharer\",\n            privileges: [\n                \"CREATE_RECIPIENT\",\n                \"CREATE_SHARE\",\n            ],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Grants(\"sandbox\",\n    metastore=\"metastore_id\",\n    grants=[\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\n                \"CREATE_CATALOG\",\n                \"CREATE_EXTERNAL_LOCATION\",\n            ],\n        },\n        {\n            \"principal\": \"Data Sharer\",\n            \"privileges\": [\n                \"CREATE_RECIPIENT\",\n                \"CREATE_SHARE\",\n            ],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Grants(\"sandbox\", new()\n    {\n        Metastore = \"metastore_id\",\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_CATALOG\",\n                    \"CREATE_EXTERNAL_LOCATION\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Sharer\",\n                Privileges = new[]\n                {\n                    \"CREATE_RECIPIENT\",\n                    \"CREATE_SHARE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrants(ctx, \"sandbox\", \u0026databricks.GrantsArgs{\n\t\t\tMetastore: pulumi.String(\"metastore_id\"),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_CATALOG\"),\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_LOCATION\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Sharer\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_RECIPIENT\"),\n\t\t\t\t\t\tpulumi.String(\"CREATE_SHARE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Grants(\"sandbox\", GrantsArgs.builder()\n            .metastore(\"metastore_id\")\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(                    \n                        \"CREATE_CATALOG\",\n                        \"CREATE_EXTERNAL_LOCATION\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Sharer\")\n                    .privileges(                    \n                        \"CREATE_RECIPIENT\",\n                        \"CREATE_SHARE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Grants\n    properties:\n      metastore: metastore_id\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_CATALOG\n            - CREATE_EXTERNAL_LOCATION\n        - principal: Data Sharer\n          privileges:\n            - CREATE_RECIPIENT\n            - CREATE_SHARE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Catalog grants\n\nYou can grant `ALL_PRIVILEGES`, `APPLY_TAG`, `CREATE_CONNECTION`, `CREATE_SCHEMA`, `MANAGE`, and `USE_CATALOG` privileges to databricks.Catalog specified in the `catalog` attribute. You can also grant `CREATE_FUNCTION`, `CREATE_TABLE`, `CREATE_VOLUME`, `EXECUTE`, `MODIFY`, `REFRESH`, `SELECT`, `READ_VOLUME`, `WRITE_VOLUME` and `USE_SCHEMA` at the catalog level to apply them to the pertinent current and future securable objects within the catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst sandboxGrants = new databricks.Grants(\"sandbox\", {\n    catalog: sandbox.name,\n    grants: [\n        {\n            principal: \"Data Scientists\",\n            privileges: [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"SELECT\",\n            ],\n        },\n        {\n            principal: \"Data Engineers\",\n            privileges: [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            principal: \"Data Analyst\",\n            privileges: [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"SELECT\",\n            ],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nsandbox_grants = databricks.Grants(\"sandbox\",\n    catalog=sandbox.name,\n    grants=[\n        {\n            \"principal\": \"Data Scientists\",\n            \"privileges\": [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"SELECT\",\n            ],\n        },\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"CREATE_SCHEMA\",\n                \"CREATE_TABLE\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            \"principal\": \"Data Analyst\",\n            \"privileges\": [\n                \"USE_CATALOG\",\n                \"USE_SCHEMA\",\n                \"SELECT\",\n            ],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var sandboxGrants = new Databricks.Grants(\"sandbox\", new()\n    {\n        Catalog = sandbox.Name,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Scientists\",\n                Privileges = new[]\n                {\n                    \"USE_CATALOG\",\n                    \"USE_SCHEMA\",\n                    \"CREATE_TABLE\",\n                    \"SELECT\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"USE_CATALOG\",\n                    \"USE_SCHEMA\",\n                    \"CREATE_SCHEMA\",\n                    \"CREATE_TABLE\",\n                    \"MODIFY\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Analyst\",\n                Privileges = new[]\n                {\n                    \"USE_CATALOG\",\n                    \"USE_SCHEMA\",\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"sandbox\", \u0026databricks.GrantsArgs{\n\t\t\tCatalog: sandbox.Name,\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Scientists\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\t\t\tpulumi.String(\"CREATE_SCHEMA\"),\n\t\t\t\t\t\tpulumi.String(\"CREATE_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Analyst\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var sandboxGrants = new Grants(\"sandboxGrants\", GrantsArgs.builder()\n            .catalog(sandbox.name())\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Scientists\")\n                    .privileges(                    \n                        \"USE_CATALOG\",\n                        \"USE_SCHEMA\",\n                        \"CREATE_TABLE\",\n                        \"SELECT\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(                    \n                        \"USE_CATALOG\",\n                        \"USE_SCHEMA\",\n                        \"CREATE_SCHEMA\",\n                        \"CREATE_TABLE\",\n                        \"MODIFY\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Analyst\")\n                    .privileges(                    \n                        \"USE_CATALOG\",\n                        \"USE_SCHEMA\",\n                        \"SELECT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  sandboxGrants:\n    type: databricks:Grants\n    name: sandbox\n    properties:\n      catalog: ${sandbox.name}\n      grants:\n        - principal: Data Scientists\n          privileges:\n            - USE_CATALOG\n            - USE_SCHEMA\n            - CREATE_TABLE\n            - SELECT\n        - principal: Data Engineers\n          privileges:\n            - USE_CATALOG\n            - USE_SCHEMA\n            - CREATE_SCHEMA\n            - CREATE_TABLE\n            - MODIFY\n        - principal: Data Analyst\n          privileges:\n            - USE_CATALOG\n            - USE_SCHEMA\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Schema grants\n\nYou can grant `ALL_PRIVILEGES`, `APPLY_TAG`, `CREATE_FUNCTION`, `CREATE_TABLE`, `CREATE_VOLUME`, `MANAGE` and `USE_SCHEMA` privileges to *`catalog.schema`* specified in the `schema` attribute. You can also grant `EXECUTE`, `MODIFY`, `REFRESH`, `SELECT`, `READ_VOLUME`, `WRITE_VOLUME` at the schema level to apply them to the pertinent current and future securable objects within the schema:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this schema is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst thingsGrants = new databricks.Grants(\"things\", {\n    schema: things.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox[\"id\"],\n    name=\"things\",\n    comment=\"this schema is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nthings_grants = databricks.Grants(\"things\",\n    schema=things.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\n            \"USE_SCHEMA\",\n            \"MODIFY\",\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this schema is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var thingsGrants = new Databricks.Grants(\"things\", new()\n    {\n        Schema = things.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"USE_SCHEMA\",\n                    \"MODIFY\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: pulumi.Any(sandbox.Id),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this schema is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"things\", \u0026databricks.GrantsArgs{\n\t\t\tSchema: things.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"USE_SCHEMA\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this schema is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var thingsGrants = new Grants(\"thingsGrants\", GrantsArgs.builder()\n            .schema(things.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(                \n                    \"USE_SCHEMA\",\n                    \"MODIFY\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this schema is managed by terraform\n      properties:\n        kind: various\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      schema: ${things.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - USE_SCHEMA\n            - MODIFY\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Table grants\n\nYou can grant `ALL_PRIVILEGES`, `APPLY_TAG`, `MANAGE`, `SELECT` and `MODIFY` privileges to *`catalog.schema.table`* specified in the `table` attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customers = new databricks.Grants(\"customers\", {\n    table: \"main.reporting.customers\",\n    grants: [\n        {\n            principal: \"Data Engineers\",\n            privileges: [\n                \"MODIFY\",\n                \"SELECT\",\n            ],\n        },\n        {\n            principal: \"Data Analysts\",\n            privileges: [\"SELECT\"],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers = databricks.Grants(\"customers\",\n    table=\"main.reporting.customers\",\n    grants=[\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\n                \"MODIFY\",\n                \"SELECT\",\n            ],\n        },\n        {\n            \"principal\": \"Data Analysts\",\n            \"privileges\": [\"SELECT\"],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customers = new Databricks.Grants(\"customers\", new()\n    {\n        Table = \"main.reporting.customers\",\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"MODIFY\",\n                    \"SELECT\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Analysts\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrants(ctx, \"customers\", \u0026databricks.GrantsArgs{\n\t\t\tTable: pulumi.String(\"main.reporting.customers\"),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customers = new Grants(\"customers\", GrantsArgs.builder()\n            .table(\"main.reporting.customers\")\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(                    \n                        \"MODIFY\",\n                        \"SELECT\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Analysts\")\n                    .privileges(\"SELECT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customers:\n    type: databricks:Grants\n    properties:\n      table: main.reporting.customers\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - MODIFY\n            - SELECT\n        - principal: Data Analysts\n          privileges:\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nYou can also apply grants dynamically with databricks.getTables data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const things = await databricks.getTables({\n        catalogName: \"sandbox\",\n        schemaName: \"things\",\n    });\n    const thingsGrants: databricks.Grants[] = [];\n    for (const range of things.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        thingsGrants.push(new databricks.Grants(`things-${range.key}`, {\n            table: range.value,\n            grants: [{\n                principal: \"sensitive\",\n                privileges: [\n                    \"SELECT\",\n                    \"MODIFY\",\n                ],\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_tables(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthings_grants = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(things.ids)]:\n    things_grants.append(databricks.Grants(f\"things-{range['key']}\",\n        table=range[\"value\"],\n        grants=[{\n            \"principal\": \"sensitive\",\n            \"privileges\": [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var things = await Databricks.GetTables.InvokeAsync(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var thingsGrants = new List\u003cDatabricks.Grants\u003e();\n    foreach (var range in )\n    {\n        thingsGrants.Add(new Databricks.Grants($\"things-{range.Key}\", new()\n        {\n            Table = range.Value,\n            GrantDetails = new[]\n            {\n                new Databricks.Inputs.GrantsGrantArgs\n                {\n                    Principal = \"sensitive\",\n                    Privileges = new[]\n                    {\n                        \"SELECT\",\n                        \"MODIFY\",\n                    },\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.GetTables(ctx, \u0026databricks.GetTablesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar thingsGrants []*databricks.Grants\n\t\tfor key0, val0 := range things.Ids {\n\t\t\t__res, err := databricks.NewGrants(ctx, fmt.Sprintf(\"things-%v\", key0), \u0026databricks.GrantsArgs{\n\t\t\t\tTable: pulumi.String(val0),\n\t\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tthingsGrants = append(thingsGrants, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTablesArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrants = things.applyValue(getTablesResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getTablesResult.ids())) {\n                var resource = new Grants(\"thingsGrants-\" + range.key(), GrantsArgs.builder()\n                    .table(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      table: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      function: databricks:getTables\n      arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## View grants\n\nYou can grant `ALL_PRIVILEGES`, `APPLY_TAG`, `MANAGE` and `SELECT` privileges to *`catalog.schema.view`* specified in `table` attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customer360 = new databricks.Grants(\"customer360\", {\n    table: \"main.reporting.customer360\",\n    grants: [{\n        principal: \"Data Analysts\",\n        privileges: [\"SELECT\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomer360 = databricks.Grants(\"customer360\",\n    table=\"main.reporting.customer360\",\n    grants=[{\n        \"principal\": \"Data Analysts\",\n        \"privileges\": [\"SELECT\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customer360 = new Databricks.Grants(\"customer360\", new()\n    {\n        Table = \"main.reporting.customer360\",\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Analysts\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrants(ctx, \"customer360\", \u0026databricks.GrantsArgs{\n\t\t\tTable: pulumi.String(\"main.reporting.customer360\"),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customer360 = new Grants(\"customer360\", GrantsArgs.builder()\n            .table(\"main.reporting.customer360\")\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Analysts\")\n                .privileges(\"SELECT\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customer360:\n    type: databricks:Grants\n    properties:\n      table: main.reporting.customer360\n      grants:\n        - principal: Data Analysts\n          privileges:\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nYou can also apply grants dynamically with databricks.getViews data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const customers = await databricks.getViews({\n        catalogName: \"main\",\n        schemaName: \"customers\",\n    });\n    const customersGrants: databricks.Grants[] = [];\n    for (const range of customers.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        customersGrants.push(new databricks.Grants(`customers-${range.key}`, {\n            table: range.value,\n            grants: [{\n                principal: \"sensitive\",\n                privileges: [\n                    \"SELECT\",\n                    \"MODIFY\",\n                ],\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers = databricks.get_views(catalog_name=\"main\",\n    schema_name=\"customers\")\ncustomers_grants = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(customers.ids)]:\n    customers_grants.append(databricks.Grants(f\"customers-{range['key']}\",\n        table=range[\"value\"],\n        grants=[{\n            \"principal\": \"sensitive\",\n            \"privileges\": [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var customers = await Databricks.GetViews.InvokeAsync(new()\n    {\n        CatalogName = \"main\",\n        SchemaName = \"customers\",\n    });\n\n    var customersGrants = new List\u003cDatabricks.Grants\u003e();\n    foreach (var range in )\n    {\n        customersGrants.Add(new Databricks.Grants($\"customers-{range.Key}\", new()\n        {\n            Table = range.Value,\n            GrantDetails = new[]\n            {\n                new Databricks.Inputs.GrantsGrantArgs\n                {\n                    Principal = \"sensitive\",\n                    Privileges = new[]\n                    {\n                        \"SELECT\",\n                        \"MODIFY\",\n                    },\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcustomers, err := databricks.GetViews(ctx, \u0026databricks.GetViewsArgs{\n\t\t\tCatalogName: \"main\",\n\t\t\tSchemaName:  \"customers\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar customersGrants []*databricks.Grants\n\t\tfor key0, val0 := range customers.Ids {\n\t\t\t__res, err := databricks.NewGrants(ctx, fmt.Sprintf(\"customers-%v\", key0), \u0026databricks.GrantsArgs{\n\t\t\t\tTable: pulumi.String(val0),\n\t\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcustomersGrants = append(customersGrants, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetViewsArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var customers = DatabricksFunctions.getViews(GetViewsArgs.builder()\n            .catalogName(\"main\")\n            .schemaName(\"customers\")\n            .build());\n\n        final var customersGrants = customers.applyValue(getViewsResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getViewsResult.ids())) {\n                var resource = new Grants(\"customersGrants-\" + range.key(), GrantsArgs.builder()\n                    .table(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  customersGrants:\n    type: databricks:Grants\n    name: customers\n    properties:\n      table: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  customers:\n    fn::invoke:\n      function: databricks:getViews\n      arguments:\n        catalogName: main\n        schemaName: customers\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Volume grants\n\nYou can grant `ALL_PRIVILEGES`, `MANAGE`, `READ_VOLUME` and `WRITE_VOLUME` privileges to *`catalog.schema.volume`* specified in the `volume` attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Volume(\"this\", {\n    name: \"quickstart_volume\",\n    catalogName: sandbox.name,\n    schemaName: things.name,\n    volumeType: \"EXTERNAL\",\n    storageLocation: some.url,\n    comment: \"this volume is managed by terraform\",\n});\nconst volume = new databricks.Grants(\"volume\", {\n    volume: _this.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"WRITE_VOLUME\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Volume(\"this\",\n    name=\"quickstart_volume\",\n    catalog_name=sandbox[\"name\"],\n    schema_name=things[\"name\"],\n    volume_type=\"EXTERNAL\",\n    storage_location=some[\"url\"],\n    comment=\"this volume is managed by terraform\")\nvolume = databricks.Grants(\"volume\",\n    volume=this.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"WRITE_VOLUME\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Volume(\"this\", new()\n    {\n        Name = \"quickstart_volume\",\n        CatalogName = sandbox.Name,\n        SchemaName = things.Name,\n        VolumeType = \"EXTERNAL\",\n        StorageLocation = some.Url,\n        Comment = \"this volume is managed by terraform\",\n    });\n\n    var volume = new Databricks.Grants(\"volume\", new()\n    {\n        Volume = @this.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"WRITE_VOLUME\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewVolume(ctx, \"this\", \u0026databricks.VolumeArgs{\n\t\t\tName:            pulumi.String(\"quickstart_volume\"),\n\t\t\tCatalogName:     pulumi.Any(sandbox.Name),\n\t\t\tSchemaName:      pulumi.Any(things.Name),\n\t\t\tVolumeType:      pulumi.String(\"EXTERNAL\"),\n\t\t\tStorageLocation: pulumi.Any(some.Url),\n\t\t\tComment:         pulumi.String(\"this volume is managed by terraform\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"volume\", \u0026databricks.GrantsArgs{\n\t\t\tVolume: this.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"WRITE_VOLUME\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Volume;\nimport com.pulumi.databricks.VolumeArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Volume(\"this\", VolumeArgs.builder()\n            .name(\"quickstart_volume\")\n            .catalogName(sandbox.name())\n            .schemaName(things.name())\n            .volumeType(\"EXTERNAL\")\n            .storageLocation(some.url())\n            .comment(\"this volume is managed by terraform\")\n            .build());\n\n        var volume = new Grants(\"volume\", GrantsArgs.builder()\n            .volume(this_.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"WRITE_VOLUME\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Volume\n    properties:\n      name: quickstart_volume\n      catalogName: ${sandbox.name}\n      schemaName: ${things.name}\n      volumeType: EXTERNAL\n      storageLocation: ${some.url}\n      comment: this volume is managed by terraform\n  volume:\n    type: databricks:Grants\n    properties:\n      volume: ${this.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - WRITE_VOLUME\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Registered model grants\n\nYou can grant `ALL_PRIVILEGES`, `APPLY_TAG`, `EXECUTE`, and `MANAGE` privileges to *`catalog.schema.model`* specified in the `model` attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst customers = new databricks.Grants(\"customers\", {\n    model: \"main.reporting.customer_model\",\n    grants: [\n        {\n            principal: \"Data Engineers\",\n            privileges: [\n                \"APPLY_TAG\",\n                \"EXECUTE\",\n            ],\n        },\n        {\n            principal: \"Data Analysts\",\n            privileges: [\"EXECUTE\"],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncustomers = databricks.Grants(\"customers\",\n    model=\"main.reporting.customer_model\",\n    grants=[\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\n                \"APPLY_TAG\",\n                \"EXECUTE\",\n            ],\n        },\n        {\n            \"principal\": \"Data Analysts\",\n            \"privileges\": [\"EXECUTE\"],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var customers = new Databricks.Grants(\"customers\", new()\n    {\n        Model = \"main.reporting.customer_model\",\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"APPLY_TAG\",\n                    \"EXECUTE\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Analysts\",\n                Privileges = new[]\n                {\n                    \"EXECUTE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrants(ctx, \"customers\", \u0026databricks.GrantsArgs{\n\t\t\tModel: pulumi.String(\"main.reporting.customer_model\"),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"APPLY_TAG\"),\n\t\t\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var customers = new Grants(\"customers\", GrantsArgs.builder()\n            .model(\"main.reporting.customer_model\")\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(                    \n                        \"APPLY_TAG\",\n                        \"EXECUTE\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Analysts\")\n                    .privileges(\"EXECUTE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  customers:\n    type: databricks:Grants\n    properties:\n      model: main.reporting.customer_model\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - APPLY_TAG\n            - EXECUTE\n        - principal: Data Analysts\n          privileges:\n            - EXECUTE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Function grants\n\nYou can grant `ALL_PRIVILEGES`, `EXECUTE`, and `MANAGE` privileges to *`catalog.schema.function`* specified in the `function` attribute.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst udf = new databricks.Grants(\"udf\", {\n    \"function\": \"main.reporting.udf\",\n    grants: [\n        {\n            principal: \"Data Engineers\",\n            privileges: [\"EXECUTE\"],\n        },\n        {\n            principal: \"Data Analysts\",\n            privileges: [\"EXECUTE\"],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nudf = databricks.Grants(\"udf\",\n    function=\"main.reporting.udf\",\n    grants=[\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\"EXECUTE\"],\n        },\n        {\n            \"principal\": \"Data Analysts\",\n            \"privileges\": [\"EXECUTE\"],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var udf = new Databricks.Grants(\"udf\", new()\n    {\n        Function = \"main.reporting.udf\",\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"EXECUTE\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Analysts\",\n                Privileges = new[]\n                {\n                    \"EXECUTE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGrants(ctx, \"udf\", \u0026databricks.GrantsArgs{\n\t\t\tFunction: pulumi.String(\"main.reporting.udf\"),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Analysts\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"EXECUTE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var udf = new Grants(\"udf\", GrantsArgs.builder()\n            .function(\"main.reporting.udf\")\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(\"EXECUTE\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(\"Data Analysts\")\n                    .privileges(\"EXECUTE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  udf:\n    type: databricks:Grants\n    properties:\n      function: main.reporting.udf\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - EXECUTE\n        - principal: Data Analysts\n          privileges:\n            - EXECUTE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Service credential grants\n\nYou can grant `ALL_PRIVILEGES`, `ACCESS`, `CREATE_CONNECTION`, and `MANAGE` privileges to databricks.Credential id specified in `credential` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.Credential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    purpose: \"SERVICE\",\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    credential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_CONNECTION\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.Credential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    purpose=\"SERVICE\",\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    credential=external.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"CREATE_CONNECTION\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.Credential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.CredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Purpose = \"SERVICE\",\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        Credential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_CONNECTION\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewCredential(ctx, \"external\", \u0026databricks.CredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.CredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tPurpose: pulumi.String(\"SERVICE\"),\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_CONNECTION\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Credential;\nimport com.pulumi.databricks.CredentialArgs;\nimport com.pulumi.databricks.inputs.CredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new Credential(\"external\", CredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(CredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .purpose(\"SERVICE\")\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .credential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_CONNECTION\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:Credential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      purpose: SERVICE\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      credential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_CONNECTION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Storage credential grants\n\nYou can grant `ALL_PRIVILEGES`, `CREATE_EXTERNAL_LOCATION`, `CREATE_EXTERNAL_TABLE`, `MANAGE`, `READ_FILES` and `WRITE_FILES` privileges to databricks.StorageCredential id specified in `storage_credential` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"CREATE_EXTERNAL_TABLE\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## External location grants\n\nYou can grant `ALL_PRIVILEGES`, `CREATE_EXTERNAL_TABLE`, `CREATE_MANAGED_STORAGE`, `CREATE EXTERNAL VOLUME`, `MANAGE`, `READ_FILES` and `WRITE_FILES` privileges to databricks.ExternalLocation id specified in `external_location` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.id,\n    comment: \"Managed by TF\",\n});\nconst someGrants = new databricks.Grants(\"some\", {\n    externalLocation: some.id,\n    grants: [\n        {\n            principal: \"Data Engineers\",\n            privileges: [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            principal: mySp.applicationId,\n            privileges: [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            principal: myGroup.displayName,\n            privileges: [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            principal: myUser.userName,\n            privileges: [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external[\"id\"],\n    comment=\"Managed by TF\")\nsome_grants = databricks.Grants(\"some\",\n    external_location=some.id,\n    grants=[\n        {\n            \"principal\": \"Data Engineers\",\n            \"privileges\": [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            \"principal\": my_sp[\"applicationId\"],\n            \"privileges\": [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            \"principal\": my_group[\"displayName\"],\n            \"privileges\": [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n        {\n            \"principal\": my_user[\"userName\"],\n            \"privileges\": [\n                \"CREATE_EXTERNAL_TABLE\",\n                \"READ_FILES\",\n            ],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Id,\n        Comment = \"Managed by TF\",\n    });\n\n    var someGrants = new Databricks.Grants(\"some\", new()\n    {\n        ExternalLocation = some.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = mySp.ApplicationId,\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = myGroup.DisplayName,\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = myUser.UserName,\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external\"),\n\t\t\tUrl:            pulumi.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id),\n\t\t\tCredentialName: pulumi.Any(external.Id),\n\t\t\tComment:        pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"some\", \u0026databricks.GrantsArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.Any(mySp.ApplicationId),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.Any(myGroup.DisplayName),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.Any(myUser.UserName),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()\n            .name(\"external\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.id())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var someGrants = new Grants(\"someGrants\", GrantsArgs.builder()\n            .externalLocation(some.id())\n            .grants(            \n                GrantsGrantArgs.builder()\n                    .principal(\"Data Engineers\")\n                    .privileges(                    \n                        \"CREATE_EXTERNAL_TABLE\",\n                        \"READ_FILES\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(mySp.applicationId())\n                    .privileges(                    \n                        \"CREATE_EXTERNAL_TABLE\",\n                        \"READ_FILES\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(myGroup.displayName())\n                    .privileges(                    \n                        \"CREATE_EXTERNAL_TABLE\",\n                        \"READ_FILES\")\n                    .build(),\n                GrantsGrantArgs.builder()\n                    .principal(myUser.userName())\n                    .privileges(                    \n                        \"CREATE_EXTERNAL_TABLE\",\n                        \"READ_FILES\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.id}\n      comment: Managed by TF\n  someGrants:\n    type: databricks:Grants\n    name: some\n    properties:\n      externalLocation: ${some.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n        - principal: ${mySp.applicationId}\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n        - principal: ${myGroup.displayName}\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n        - principal: ${myUser.userName}\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Connection grants\n\nYou can grant `ALL_PRIVILEGES`, `MANAGE`, `USE_CONNECTION` and `CREATE_FOREIGN_CATALOG` to databricks.Connection specified in `foreign_connection` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst mysql = new databricks.Connection(\"mysql\", {\n    name: \"mysql_connection\",\n    connectionType: \"MYSQL\",\n    comment: \"this is a connection to mysql db\",\n    options: {\n        host: \"test.mysql.database.azure.com\",\n        port: \"3306\",\n        user: \"user\",\n        password: \"password\",\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst some = new databricks.Grants(\"some\", {\n    foreignConnection: mysql.name,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\n            \"CREATE_FOREIGN_CATALOG\",\n            \"USE_CONNECTION\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmysql = databricks.Connection(\"mysql\",\n    name=\"mysql_connection\",\n    connection_type=\"MYSQL\",\n    comment=\"this is a connection to mysql db\",\n    options={\n        \"host\": \"test.mysql.database.azure.com\",\n        \"port\": \"3306\",\n        \"user\": \"user\",\n        \"password\": \"password\",\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\nsome = databricks.Grants(\"some\",\n    foreign_connection=mysql.name,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\n            \"CREATE_FOREIGN_CATALOG\",\n            \"USE_CONNECTION\",\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var mysql = new Databricks.Connection(\"mysql\", new()\n    {\n        Name = \"mysql_connection\",\n        ConnectionType = \"MYSQL\",\n        Comment = \"this is a connection to mysql db\",\n        Options = \n        {\n            { \"host\", \"test.mysql.database.azure.com\" },\n            { \"port\", \"3306\" },\n            { \"user\", \"user\" },\n            { \"password\", \"password\" },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var some = new Databricks.Grants(\"some\", new()\n    {\n        ForeignConnection = mysql.Name,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_FOREIGN_CATALOG\",\n                    \"USE_CONNECTION\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmysql, err := databricks.NewConnection(ctx, \"mysql\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"mysql_connection\"),\n\t\t\tConnectionType: pulumi.String(\"MYSQL\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to mysql db\"),\n\t\t\tOptions: pulumi.StringMap{\n\t\t\t\t\"host\":     pulumi.String(\"test.mysql.database.azure.com\"),\n\t\t\t\t\"port\":     pulumi.String(\"3306\"),\n\t\t\t\t\"user\":     pulumi.String(\"user\"),\n\t\t\t\t\"password\": pulumi.String(\"password\"),\n\t\t\t},\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"some\", \u0026databricks.GrantsArgs{\n\t\t\tForeignConnection: mysql.Name,\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_FOREIGN_CATALOG\"),\n\t\t\t\t\t\tpulumi.String(\"USE_CONNECTION\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mysql = new Connection(\"mysql\", ConnectionArgs.builder()\n            .name(\"mysql_connection\")\n            .connectionType(\"MYSQL\")\n            .comment(\"this is a connection to mysql db\")\n            .options(Map.ofEntries(\n                Map.entry(\"host\", \"test.mysql.database.azure.com\"),\n                Map.entry(\"port\", \"3306\"),\n                Map.entry(\"user\", \"user\"),\n                Map.entry(\"password\", \"password\")\n            ))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var some = new Grants(\"some\", GrantsArgs.builder()\n            .foreignConnection(mysql.name())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(                \n                    \"CREATE_FOREIGN_CATALOG\",\n                    \"USE_CONNECTION\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  mysql:\n    type: databricks:Connection\n    properties:\n      name: mysql_connection\n      connectionType: MYSQL\n      comment: this is a connection to mysql db\n      options:\n        host: test.mysql.database.azure.com\n        port: '3306'\n        user: user\n        password: password\n      properties:\n        purpose: testing\n  some:\n    type: databricks:Grants\n    properties:\n      foreignConnection: ${mysql.name}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_FOREIGN_CATALOG\n            - USE_CONNECTION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Delta Sharing share grants\n\nYou can grant `SELECT` to databricks.Recipient on databricks.Share name specified in `share` attribute:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst some = new databricks.Share(\"some\", {name: \"my_share\"});\nconst someRecipient = new databricks.Recipient(\"some\", {name: \"my_recipient\"});\nconst someGrants = new databricks.Grants(\"some\", {\n    share: some.name,\n    grants: [{\n        principal: someRecipient.name,\n        privileges: [\"SELECT\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsome = databricks.Share(\"some\", name=\"my_share\")\nsome_recipient = databricks.Recipient(\"some\", name=\"my_recipient\")\nsome_grants = databricks.Grants(\"some\",\n    share=some.name,\n    grants=[{\n        \"principal\": some_recipient.name,\n        \"privileges\": [\"SELECT\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var some = new Databricks.Share(\"some\", new()\n    {\n        Name = \"my_share\",\n    });\n\n    var someRecipient = new Databricks.Recipient(\"some\", new()\n    {\n        Name = \"my_recipient\",\n    });\n\n    var someGrants = new Databricks.Grants(\"some\", new()\n    {\n        Share = some.Name,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = someRecipient.Name,\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsome, err := databricks.NewShare(ctx, \"some\", \u0026databricks.ShareArgs{\n\t\t\tName: pulumi.String(\"my_share\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsomeRecipient, err := databricks.NewRecipient(ctx, \"some\", \u0026databricks.RecipientArgs{\n\t\t\tName: pulumi.String(\"my_recipient\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"some\", \u0026databricks.GrantsArgs{\n\t\t\tShare: some.Name,\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: someRecipient.Name,\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Share;\nimport com.pulumi.databricks.ShareArgs;\nimport com.pulumi.databricks.Recipient;\nimport com.pulumi.databricks.RecipientArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var some = new Share(\"some\", ShareArgs.builder()\n            .name(\"my_share\")\n            .build());\n\n        var someRecipient = new Recipient(\"someRecipient\", RecipientArgs.builder()\n            .name(\"my_recipient\")\n            .build());\n\n        var someGrants = new Grants(\"someGrants\", GrantsArgs.builder()\n            .share(some.name())\n            .grants(GrantsGrantArgs.builder()\n                .principal(someRecipient.name())\n                .privileges(\"SELECT\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:Share\n    properties:\n      name: my_share\n  someRecipient:\n    type: databricks:Recipient\n    name: some\n    properties:\n      name: my_recipient\n  someGrants:\n    type: databricks:Grants\n    name: some\n    properties:\n      share: ${some.name}\n      grants:\n        - principal: ${someRecipient.name}\n          privileges:\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Other access control\n\nYou can control Databricks General Permissions through databricks.Permissions resource.\n\n## Import\n\nThe resource can be imported using combination of securable type (`table`, `catalog`, `foreign_connection`, ...) and it's name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/grants:Grants this catalog/abc\n```\n\n",
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "credential": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "foreignConnection": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string"
                },
                "model": {
                    "type": "string"
                },
                "pipeline": {
                    "type": "string"
                },
                "recipient": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "share": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "volume": {
                    "type": "string"
                }
            },
            "required": [
                "grants"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "credential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "foreignConnection": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "model": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "pipeline": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "recipient": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "share": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "volume": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "grants"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grants resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "credential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "foreignConnection": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "grants": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                        },
                        "language": {
                            "csharp": {
                                "name": "GrantDetails"
                            }
                        }
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "model": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "pipeline": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "recipient": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "share": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "volume": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/group:Group": {
            "description": "This resource allows you to manage both [account groups and workspace-local groups](https://docs.databricks.com/administration-guide/users-groups/groups.html). You can use the databricks.GroupMember resource to assign Databricks users, service principals as well as other groups as members of the group. This is useful if you are using an application to sync users \u0026 groups with SCIM API.\n\n\u003e To assign an account level group to a workspace use databricks_mws_permission_assignment.\n\n\u003e Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level groups.  Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level groups.\n\nTo create account groups in the Databricks account, the provider must be configured accordingly. On AWS deployment with `host = \"https://accounts.cloud.databricks.com\"` and `account_id = \"00000000-0000-0000-0000-000000000000\"`. On Azure deployments `host = \"https://accounts.azuredatabricks.net\"`, `account_id = \"00000000-0000-0000-0000-000000000000\"` and using AAD tokens as authentication.\n\nRecommended to use along with Identity Provider SCIM provisioning to populate users into those groups:\n\n* [Azure Active Directory](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad)\n* [Okta](https://docs.databricks.com/administration-guide/users-groups/scim/okta.html)\n* [OneLogin](https://docs.databricks.com/administration-guide/users-groups/scim/onelogin.html)\n\n## Example Usage\n\nCreating some group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {\n    displayName: \"Some Group\",\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\",\n    display_name=\"Some Group\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName:             pulumi.String(\"Some Group\"),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()\n            .displayName(\"Some Group\")\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAdding databricks.User as databricks.GroupMember of some group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {\n    displayName: \"Some Group\",\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\nconst thisUser = new databricks.User(\"this\", {userName: \"someone@example.com\"});\nconst vipMember = new databricks.GroupMember(\"vip_member\", {\n    groupId: _this.id,\n    memberId: thisUser.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\",\n    display_name=\"Some Group\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\nthis_user = databricks.User(\"this\", user_name=\"someone@example.com\")\nvip_member = databricks.GroupMember(\"vip_member\",\n    group_id=this.id,\n    member_id=this_user.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n    var thisUser = new Databricks.User(\"this\", new()\n    {\n        UserName = \"someone@example.com\",\n    });\n\n    var vipMember = new Databricks.GroupMember(\"vip_member\", new()\n    {\n        GroupId = @this.Id,\n        MemberId = thisUser.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName:             pulumi.String(\"Some Group\"),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisUser, err := databricks.NewUser(ctx, \"this\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"someone@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"vip_member\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  this.ID(),\n\t\t\tMemberId: thisUser.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()\n            .displayName(\"Some Group\")\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n        var thisUser = new User(\"thisUser\", UserArgs.builder()\n            .userName(\"someone@example.com\")\n            .build());\n\n        var vipMember = new GroupMember(\"vipMember\", GroupMemberArgs.builder()\n            .groupId(this_.id())\n            .memberId(thisUser.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n  thisUser:\n    type: databricks:User\n    name: this\n    properties:\n      userName: someone@example.com\n  vipMember:\n    type: databricks:GroupMember\n    name: vip_member\n    properties:\n      groupId: ${this.id}\n      memberId: ${thisUser.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating group in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {displayName: \"Some Group\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\", display_name=\"Some Group\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Some Group\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()\n            .displayName(\"Some Group\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating group in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {displayName: \"Some Group\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\", display_name=\"Some Group\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Some Group\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()\n            .displayName(\"Some Group\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nYou can import a `databricks_group` resource with the name `my_group` like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/group:Group my_group \u003cgroup_id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create group: Group with name X already exists.` errors and implicitly import the specific group into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "aclPrincipalId",
                "displayName",
                "url"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n",
                    "willReplaceOnChanges": true
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create group: Group with name X already exists.` errors and implicitly import the specific group into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Group resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is the display name for the given group.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n",
                        "willReplaceOnChanges": true
                    },
                    "force": {
                        "type": "boolean",
                        "description": "Ignore `cannot create group: Group with name X already exists.` errors and implicitly import the specific group into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupInstanceProfile:GroupInstanceProfile": {
            "description": "\u003e **Deprecated** Please migrate to databricks_group_role.\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_group.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupInstanceProfile = new databricks.GroupInstanceProfile(\"my_group_instance_profile\", {\n    groupId: myGroup.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_instance_profile = databricks.GroupInstanceProfile(\"my_group_instance_profile\",\n    group_id=my_group.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupInstanceProfile = new Databricks.GroupInstanceProfile(\"my_group_instance_profile\", new()\n    {\n        GroupId = myGroup.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"my_group_instance_profile\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           myGroup.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()\n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()\n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupInstanceProfile = new GroupInstanceProfile(\"myGroupInstanceProfile\", GroupInstanceProfileArgs.builder()\n            .groupId(myGroup.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupInstanceProfile:\n    type: databricks:GroupInstanceProfile\n    name: my_group_instance_profile\n    properties:\n      groupId: ${myGroup.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "instanceProfileId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "instanceProfileId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupInstanceProfile resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupMember:GroupMember": {
            "description": "This resource allows you to attach users, service_principal, and groups as group members.\n\nTo attach members to groups in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments\n\n## Example Usage\n\nAfter the following example, Bradley would have direct membership in group B and transitive membership in group A.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst a = new databricks.Group(\"a\", {displayName: \"A\"});\nconst b = new databricks.Group(\"b\", {displayName: \"B\"});\nconst ab = new databricks.GroupMember(\"ab\", {\n    groupId: a.id,\n    memberId: b.id,\n});\nconst bradley = new databricks.User(\"bradley\", {userName: \"bradley@example.com\"});\nconst bb = new databricks.GroupMember(\"bb\", {\n    groupId: b.id,\n    memberId: bradley.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\na = databricks.Group(\"a\", display_name=\"A\")\nb = databricks.Group(\"b\", display_name=\"B\")\nab = databricks.GroupMember(\"ab\",\n    group_id=a.id,\n    member_id=b.id)\nbradley = databricks.User(\"bradley\", user_name=\"bradley@example.com\")\nbb = databricks.GroupMember(\"bb\",\n    group_id=b.id,\n    member_id=bradley.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var a = new Databricks.Group(\"a\", new()\n    {\n        DisplayName = \"A\",\n    });\n\n    var b = new Databricks.Group(\"b\", new()\n    {\n        DisplayName = \"B\",\n    });\n\n    var ab = new Databricks.GroupMember(\"ab\", new()\n    {\n        GroupId = a.Id,\n        MemberId = b.Id,\n    });\n\n    var bradley = new Databricks.User(\"bradley\", new()\n    {\n        UserName = \"bradley@example.com\",\n    });\n\n    var bb = new Databricks.GroupMember(\"bb\", new()\n    {\n        GroupId = b.Id,\n        MemberId = bradley.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ta, err := databricks.NewGroup(ctx, \"a\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"A\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tb, err := databricks.NewGroup(ctx, \"b\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"B\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"ab\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  a.ID(),\n\t\t\tMemberId: b.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbradley, err := databricks.NewUser(ctx, \"bradley\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"bradley@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"bb\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  b.ID(),\n\t\t\tMemberId: bradley.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var a = new Group(\"a\", GroupArgs.builder()\n            .displayName(\"A\")\n            .build());\n\n        var b = new Group(\"b\", GroupArgs.builder()\n            .displayName(\"B\")\n            .build());\n\n        var ab = new GroupMember(\"ab\", GroupMemberArgs.builder()\n            .groupId(a.id())\n            .memberId(b.id())\n            .build());\n\n        var bradley = new User(\"bradley\", UserArgs.builder()\n            .userName(\"bradley@example.com\")\n            .build());\n\n        var bb = new GroupMember(\"bb\", GroupMemberArgs.builder()\n            .groupId(b.id())\n            .memberId(bradley.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  a:\n    type: databricks:Group\n    properties:\n      displayName: A\n  b:\n    type: databricks:Group\n    properties:\n      displayName: B\n  ab:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${a.id}\n      memberId: ${b.id}\n  bradley:\n    type: databricks:User\n    properties:\n      userName: bradley@example.com\n  bb:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${b.id}\n      memberId: ${bradley.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.ServicePrincipal to grant access to a workspace to an automation tool or application.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\nYou can import a `databricks_group_member` resource with name `my_group_member` like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/groupMember:GroupMember my_group_member \"\u003cgroup_id\u003e|\u003cmember_id\u003e\"\n```\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the group, service principal, or user.\n"
                }
            },
            "required": [
                "groupId",
                "memberId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the group, service principal, or user.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "memberId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupMember resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "memberId": {
                        "type": "string",
                        "description": "This is the id of the group, service principal, or user.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupRole:GroupRole": {
            "description": "This resource allows you to attach a role to databricks_group. This role could be a pre-defined role such as account admin, or an instance profile ARN.\n\n## Example Usage\n\nAttach an instance profile to a group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupInstanceProfile = new databricks.GroupRole(\"my_group_instance_profile\", {\n    groupId: myGroup.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_instance_profile = databricks.GroupRole(\"my_group_instance_profile\",\n    group_id=my_group.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupInstanceProfile = new Databricks.GroupRole(\"my_group_instance_profile\", new()\n    {\n        GroupId = myGroup.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupRole(ctx, \"my_group_instance_profile\", \u0026databricks.GroupRoleArgs{\n\t\t\tGroupId: myGroup.ID(),\n\t\t\tRole:    instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupRole;\nimport com.pulumi.databricks.GroupRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()\n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()\n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupInstanceProfile = new GroupRole(\"myGroupInstanceProfile\", GroupRoleArgs.builder()\n            .groupId(myGroup.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupInstanceProfile:\n    type: databricks:GroupRole\n    name: my_group_instance_profile\n    properties:\n      groupId: ${myGroup.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAttach account admin role to an account-level group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupAccountAdmin = new databricks.GroupRole(\"my_group_account_admin\", {\n    groupId: myGroup.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_account_admin = databricks.GroupRole(\"my_group_account_admin\",\n    group_id=my_group.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupAccountAdmin = new Databricks.GroupRole(\"my_group_account_admin\", new()\n    {\n        GroupId = myGroup.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupRole(ctx, \"my_group_account_admin\", \u0026databricks.GroupRoleArgs{\n\t\t\tGroupId: myGroup.ID(),\n\t\t\tRole:    pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupRole;\nimport com.pulumi.databricks.GroupRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()\n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupAccountAdmin = new GroupRole(\"myGroupAccountAdmin\", GroupRoleArgs.builder()\n            .groupId(myGroup.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupAccountAdmin:\n    type: databricks:GroupRole\n    name: my_group_account_admin\n    properties:\n      groupId: ${myGroup.id}\n      role: account_admin\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "role"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "role"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupRole resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instancePool:InstancePool": {
            "description": "This resource allows you to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. An instance pool reduces cluster start and auto-scaling times by maintaining a set of idle, ready-to-use cloud instances. When a cluster attached to a pool needs an instance, it first attempts to allocate one of the pool’s idle instances. If the pool has no idle instances, it expands by allocating a new instance from the instance provider in order to accommodate the cluster’s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool’s idle instances.\n\n\u003e It is important to know that different cloud service providers have different `node_type_id`, `disk_specs` and potentially other configurations.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({});\nconst smallestNodes = new databricks.InstancePool(\"smallest_nodes\", {\n    instancePoolName: \"Smallest Nodes\",\n    minIdleInstances: 0,\n    maxCapacity: 300,\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    awsAttributes: {\n        availability: \"ON_DEMAND\",\n        zoneId: \"us-east-1a\",\n        spotBidPricePercent: 100,\n    },\n    idleInstanceAutoterminationMinutes: 10,\n    diskSpec: {\n        diskType: {\n            ebsVolumeType: \"GENERAL_PURPOSE_SSD\",\n        },\n        diskSize: 80,\n        diskCount: 1,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type()\nsmallest_nodes = databricks.InstancePool(\"smallest_nodes\",\n    instance_pool_name=\"Smallest Nodes\",\n    min_idle_instances=0,\n    max_capacity=300,\n    node_type_id=smallest.id,\n    aws_attributes={\n        \"availability\": \"ON_DEMAND\",\n        \"zone_id\": \"us-east-1a\",\n        \"spot_bid_price_percent\": 100,\n    },\n    idle_instance_autotermination_minutes=10,\n    disk_spec={\n        \"disk_type\": {\n            \"ebs_volume_type\": \"GENERAL_PURPOSE_SSD\",\n        },\n        \"disk_size\": 80,\n        \"disk_count\": 1,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke();\n\n    var smallestNodes = new Databricks.InstancePool(\"smallest_nodes\", new()\n    {\n        InstancePoolName = \"Smallest Nodes\",\n        MinIdleInstances = 0,\n        MaxCapacity = 300,\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AwsAttributes = new Databricks.Inputs.InstancePoolAwsAttributesArgs\n        {\n            Availability = \"ON_DEMAND\",\n            ZoneId = \"us-east-1a\",\n            SpotBidPricePercent = 100,\n        },\n        IdleInstanceAutoterminationMinutes = 10,\n        DiskSpec = new Databricks.Inputs.InstancePoolDiskSpecArgs\n        {\n            DiskType = new Databricks.Inputs.InstancePoolDiskSpecDiskTypeArgs\n            {\n                EbsVolumeType = \"GENERAL_PURPOSE_SSD\",\n            },\n            DiskSize = 80,\n            DiskCount = 1,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstancePool(ctx, \"smallest_nodes\", \u0026databricks.InstancePoolArgs{\n\t\t\tInstancePoolName: pulumi.String(\"Smallest Nodes\"),\n\t\t\tMinIdleInstances: pulumi.Int(0),\n\t\t\tMaxCapacity:      pulumi.Int(300),\n\t\t\tNodeTypeId:       pulumi.String(smallest.Id),\n\t\t\tAwsAttributes: \u0026databricks.InstancePoolAwsAttributesArgs{\n\t\t\t\tAvailability:        pulumi.String(\"ON_DEMAND\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1a\"),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t\tIdleInstanceAutoterminationMinutes: pulumi.Int(10),\n\t\t\tDiskSpec: \u0026databricks.InstancePoolDiskSpecArgs{\n\t\t\t\tDiskType: \u0026databricks.InstancePoolDiskSpecDiskTypeArgs{\n\t\t\t\t\tEbsVolumeType: pulumi.String(\"GENERAL_PURPOSE_SSD\"),\n\t\t\t\t},\n\t\t\t\tDiskSize:  pulumi.Int(80),\n\t\t\t\tDiskCount: pulumi.Int(1),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.InstancePool;\nimport com.pulumi.databricks.InstancePoolArgs;\nimport com.pulumi.databricks.inputs.InstancePoolAwsAttributesArgs;\nimport com.pulumi.databricks.inputs.InstancePoolDiskSpecArgs;\nimport com.pulumi.databricks.inputs.InstancePoolDiskSpecDiskTypeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .build());\n\n        var smallestNodes = new InstancePool(\"smallestNodes\", InstancePoolArgs.builder()\n            .instancePoolName(\"Smallest Nodes\")\n            .minIdleInstances(0)\n            .maxCapacity(300)\n            .nodeTypeId(smallest.id())\n            .awsAttributes(InstancePoolAwsAttributesArgs.builder()\n                .availability(\"ON_DEMAND\")\n                .zoneId(\"us-east-1a\")\n                .spotBidPricePercent(100)\n                .build())\n            .idleInstanceAutoterminationMinutes(10)\n            .diskSpec(InstancePoolDiskSpecArgs.builder()\n                .diskType(InstancePoolDiskSpecDiskTypeArgs.builder()\n                    .ebsVolumeType(\"GENERAL_PURPOSE_SSD\")\n                    .build())\n                .diskSize(80)\n                .diskCount(1)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  smallestNodes:\n    type: databricks:InstancePool\n    name: smallest_nodes\n    properties:\n      instancePoolName: Smallest Nodes\n      minIdleInstances: 0\n      maxCapacity: 300\n      nodeTypeId: ${smallest.id}\n      awsAttributes:\n        availability: ON_DEMAND\n        zoneId: us-east-1a\n        spotBidPricePercent: '100'\n      idleInstanceAutoterminationMinutes: 10\n      diskSpec:\n        diskType:\n          ebsVolumeType: GENERAL_PURPOSE_SSD\n        diskSize: 80\n        diskCount: 1\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Group and databricks.User can control which groups or individual users can create instance pools.\n* databricks.Permissions can control which groups or individual users can *Manage* or *Attach to* individual instance pools.\n\n## Import\n\nThe resource instance pool can be imported using it's id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/instancePool:InstancePool this \u003cinstance-pool-id\u003e\n```\n\n",
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n"
                }
            },
            "required": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "inputProperties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                    "willReplaceOnChanges": true
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                    "willReplaceOnChanges": true
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                    "willReplaceOnChanges": true
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                    "willReplaceOnChanges": true
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                    "willReplaceOnChanges": true
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                    "willReplaceOnChanges": true
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    },
                    "willReplaceOnChanges": true
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstancePool resources.\n",
                "properties": {
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                        "willReplaceOnChanges": true
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                        "willReplaceOnChanges": true
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                    },
                    "diskSpec": {
                        "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                        "willReplaceOnChanges": true
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                        "willReplaceOnChanges": true
                    },
                    "idleInstanceAutoterminationMinutes": {
                        "type": "integer",
                        "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                    },
                    "instancePoolFleetAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string"
                    },
                    "instancePoolName": {
                        "type": "string",
                        "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                    },
                    "maxCapacity": {
                        "type": "integer",
                        "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                    },
                    "minIdleInstances": {
                        "type": "integer",
                        "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                        "willReplaceOnChanges": true
                    },
                    "preloadedDockerImages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                        },
                        "willReplaceOnChanges": true
                    },
                    "preloadedSparkVersions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instanceProfile:InstanceProfile": {
            "description": "This resource allows you to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount. The following example demonstrates how to create an instance profile and create a cluster with it. When creating a new `databricks.InstanceProfile`, Databricks validates that it has sufficient permissions to launch instances with the instance profile. This validation uses AWS dry-run mode for the [AWS EC2 RunInstances API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html).\n\n\u003e Please switch to databricks.StorageCredential with Unity Catalog to manage storage credentials, which provides a better and faster way for managing credential security.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Role that you've specified on https://accounts.cloud.databricks.com/#aws\nconst crossaccountRoleName = config.require(\"crossaccountRoleName\");\nconst assumeRoleForEc2 = aws.iam.getPolicyDocument({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            identifiers: [\"ec2.amazonaws.com\"],\n            type: \"Service\",\n        }],\n    }],\n});\nconst roleForS3Access = new aws.iam.Role(\"role_for_s3_access\", {\n    name: \"shared-ec2-role-for-s3\",\n    description: \"Role for shared access\",\n    assumeRolePolicy: assumeRoleForEc2.then(assumeRoleForEc2 =\u003e assumeRoleForEc2.json),\n});\nconst passRoleForS3Access = aws.iam.getPolicyDocumentOutput({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"iam:PassRole\"],\n        resources: [roleForS3Access.arn],\n    }],\n});\nconst passRoleForS3AccessPolicy = new aws.iam.Policy(\"pass_role_for_s3_access\", {\n    name: \"shared-pass-role-for-s3-access\",\n    path: \"/\",\n    policy: passRoleForS3Access.apply(passRoleForS3Access =\u003e passRoleForS3Access.json),\n});\nconst crossAccount = new aws.iam.RolePolicyAttachment(\"cross_account\", {\n    policyArn: passRoleForS3AccessPolicy.arn,\n    role: crossaccountRoleName,\n});\nconst shared = new aws.iam.InstanceProfile(\"shared\", {\n    name: \"shared-instance-profile\",\n    role: roleForS3Access.name,\n});\nconst sharedInstanceProfile = new databricks.InstanceProfile(\"shared\", {instanceProfileArn: shared.arn});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Cluster(\"this\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    awsAttributes: {\n        instanceProfileArn: sharedInstanceProfile.id,\n        availability: \"SPOT\",\n        zoneId: \"us-east-1\",\n        firstOnDemand: 1,\n        spotBidPricePercent: 100,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Role that you've specified on https://accounts.cloud.databricks.com/#aws\ncrossaccount_role_name = config.require(\"crossaccountRoleName\")\nassume_role_for_ec2 = aws.iam.get_policy_document(statements=[{\n    \"effect\": \"Allow\",\n    \"actions\": [\"sts:AssumeRole\"],\n    \"principals\": [{\n        \"identifiers\": [\"ec2.amazonaws.com\"],\n        \"type\": \"Service\",\n    }],\n}])\nrole_for_s3_access = aws.iam.Role(\"role_for_s3_access\",\n    name=\"shared-ec2-role-for-s3\",\n    description=\"Role for shared access\",\n    assume_role_policy=assume_role_for_ec2.json)\npass_role_for_s3_access = aws.iam.get_policy_document_output(statements=[{\n    \"effect\": \"Allow\",\n    \"actions\": [\"iam:PassRole\"],\n    \"resources\": [role_for_s3_access.arn],\n}])\npass_role_for_s3_access_policy = aws.iam.Policy(\"pass_role_for_s3_access\",\n    name=\"shared-pass-role-for-s3-access\",\n    path=\"/\",\n    policy=pass_role_for_s3_access.json)\ncross_account = aws.iam.RolePolicyAttachment(\"cross_account\",\n    policy_arn=pass_role_for_s3_access_policy.arn,\n    role=crossaccount_role_name)\nshared = aws.iam.InstanceProfile(\"shared\",\n    name=\"shared-instance-profile\",\n    role=role_for_s3_access.name)\nshared_instance_profile = databricks.InstanceProfile(\"shared\", instance_profile_arn=shared.arn)\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Cluster(\"this\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    },\n    aws_attributes={\n        \"instance_profile_arn\": shared_instance_profile.id,\n        \"availability\": \"SPOT\",\n        \"zone_id\": \"us-east-1\",\n        \"first_on_demand\": 1,\n        \"spot_bid_price_percent\": 100,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Role that you've specified on https://accounts.cloud.databricks.com/#aws\n    var crossaccountRoleName = config.Require(\"crossaccountRoleName\");\n    var assumeRoleForEc2 = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Identifiers = new[]\n                        {\n                            \"ec2.amazonaws.com\",\n                        },\n                        Type = \"Service\",\n                    },\n                },\n            },\n        },\n    });\n\n    var roleForS3Access = new Aws.Iam.Role(\"role_for_s3_access\", new()\n    {\n        Name = \"shared-ec2-role-for-s3\",\n        Description = \"Role for shared access\",\n        AssumeRolePolicy = assumeRoleForEc2.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var passRoleForS3Access = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"iam:PassRole\",\n                },\n                Resources = new[]\n                {\n                    roleForS3Access.Arn,\n                },\n            },\n        },\n    });\n\n    var passRoleForS3AccessPolicy = new Aws.Iam.Policy(\"pass_role_for_s3_access\", new()\n    {\n        Name = \"shared-pass-role-for-s3-access\",\n        Path = \"/\",\n        PolicyDocument = passRoleForS3Access.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var crossAccount = new Aws.Iam.RolePolicyAttachment(\"cross_account\", new()\n    {\n        PolicyArn = passRoleForS3AccessPolicy.Arn,\n        Role = crossaccountRoleName,\n    });\n\n    var shared = new Aws.Iam.InstanceProfile(\"shared\", new()\n    {\n        Name = \"shared-instance-profile\",\n        Role = roleForS3Access.Name,\n    });\n\n    var sharedInstanceProfile = new Databricks.InstanceProfile(\"shared\", new()\n    {\n        InstanceProfileArn = shared.Arn,\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        AwsAttributes = new Databricks.Inputs.ClusterAwsAttributesArgs\n        {\n            InstanceProfileArn = sharedInstanceProfile.Id,\n            Availability = \"SPOT\",\n            ZoneId = \"us-east-1\",\n            FirstOnDemand = 1,\n            SpotBidPricePercent = 100,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Role that you've specified on https://accounts.cloud.databricks.com/#aws\n\t\tcrossaccountRoleName := cfg.Require(\"crossaccountRoleName\")\n\t\tassumeRoleForEc2, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tEffect: pulumi.StringRef(\"Allow\"),\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"ec2.amazonaws.com\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tType: \"Service\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\troleForS3Access, err := iam.NewRole(ctx, \"role_for_s3_access\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(\"shared-ec2-role-for-s3\"),\n\t\t\tDescription:      pulumi.String(\"Role for shared access\"),\n\t\t\tAssumeRolePolicy: pulumi.String(assumeRoleForEc2.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpassRoleForS3Access := iam.GetPolicyDocumentOutput(ctx, iam.GetPolicyDocumentOutputArgs{\n\t\t\tStatements: iam.GetPolicyDocumentStatementArray{\n\t\t\t\t\u0026iam.GetPolicyDocumentStatementArgs{\n\t\t\t\t\tEffect: pulumi.String(\"Allow\"),\n\t\t\t\t\tActions: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"iam:PassRole\"),\n\t\t\t\t\t},\n\t\t\t\t\tResources: pulumi.StringArray{\n\t\t\t\t\t\troleForS3Access.Arn,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tpassRoleForS3AccessPolicy, err := iam.NewPolicy(ctx, \"pass_role_for_s3_access\", \u0026iam.PolicyArgs{\n\t\t\tName: pulumi.String(\"shared-pass-role-for-s3-access\"),\n\t\t\tPath: pulumi.String(\"/\"),\n\t\t\tPolicy: pulumi.String(passRoleForS3Access.ApplyT(func(passRoleForS3Access iam.GetPolicyDocumentResult) (*string, error) {\n\t\t\t\treturn \u0026passRoleForS3Access.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput)),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"cross_account\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: passRoleForS3AccessPolicy.Arn,\n\t\t\tRole:      pulumi.String(crossaccountRoleName),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tshared, err := iam.NewInstanceProfile(ctx, \"shared\", \u0026iam.InstanceProfileArgs{\n\t\t\tName: pulumi.String(\"shared-instance-profile\"),\n\t\t\tRole: roleForS3Access.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedInstanceProfile, err := databricks.NewInstanceProfile(ctx, \"shared\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: shared.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tAwsAttributes: \u0026databricks.ClusterAwsAttributesArgs{\n\t\t\t\tInstanceProfileArn:  sharedInstanceProfile.ID(),\n\t\t\t\tAvailability:        pulumi.String(\"SPOT\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1\"),\n\t\t\t\tFirstOnDemand:       pulumi.Int(1),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport com.pulumi.databricks.inputs.ClusterAwsAttributesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var crossaccountRoleName = config.get(\"crossaccountRoleName\");\n        final var assumeRoleForEc2 = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .identifiers(\"ec2.amazonaws.com\")\n                    .type(\"Service\")\n                    .build())\n                .build())\n            .build());\n\n        var roleForS3Access = new Role(\"roleForS3Access\", RoleArgs.builder()\n            .name(\"shared-ec2-role-for-s3\")\n            .description(\"Role for shared access\")\n            .assumeRolePolicy(assumeRoleForEc2.json())\n            .build());\n\n        final var passRoleForS3Access = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"iam:PassRole\")\n                .resources(roleForS3Access.arn())\n                .build())\n            .build());\n\n        var passRoleForS3AccessPolicy = new Policy(\"passRoleForS3AccessPolicy\", PolicyArgs.builder()\n            .name(\"shared-pass-role-for-s3-access\")\n            .path(\"/\")\n            .policy(passRoleForS3Access.applyValue(_passRoleForS3Access -\u003e _passRoleForS3Access.json()))\n            .build());\n\n        var crossAccount = new RolePolicyAttachment(\"crossAccount\", RolePolicyAttachmentArgs.builder()\n            .policyArn(passRoleForS3AccessPolicy.arn())\n            .role(crossaccountRoleName)\n            .build());\n\n        var shared = new com.pulumi.aws.iam.InstanceProfile(\"shared\", com.pulumi.aws.iam.InstanceProfileArgs.builder()\n            .name(\"shared-instance-profile\")\n            .role(roleForS3Access.name())\n            .build());\n\n        var sharedInstanceProfile = new com.pulumi.databricks.InstanceProfile(\"sharedInstanceProfile\", com.pulumi.databricks.InstanceProfileArgs.builder()\n            .instanceProfileArn(shared.arn())\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()\n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latest.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .awsAttributes(ClusterAwsAttributesArgs.builder()\n                .instanceProfileArn(sharedInstanceProfile.id())\n                .availability(\"SPOT\")\n                .zoneId(\"us-east-1\")\n                .firstOnDemand(1)\n                .spotBidPricePercent(100)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  crossaccountRoleName:\n    type: string\nresources:\n  roleForS3Access:\n    type: aws:iam:Role\n    name: role_for_s3_access\n    properties:\n      name: shared-ec2-role-for-s3\n      description: Role for shared access\n      assumeRolePolicy: ${assumeRoleForEc2.json}\n  passRoleForS3AccessPolicy:\n    type: aws:iam:Policy\n    name: pass_role_for_s3_access\n    properties:\n      name: shared-pass-role-for-s3-access\n      path: /\n      policy: ${passRoleForS3Access.json}\n  crossAccount:\n    type: aws:iam:RolePolicyAttachment\n    name: cross_account\n    properties:\n      policyArn: ${passRoleForS3AccessPolicy.arn}\n      role: ${crossaccountRoleName}\n  shared:\n    type: aws:iam:InstanceProfile\n    properties:\n      name: shared-instance-profile\n      role: ${roleForS3Access.name}\n  sharedInstanceProfile:\n    type: databricks:InstanceProfile\n    name: shared\n    properties:\n      instanceProfileArn: ${shared.arn}\n  this:\n    type: databricks:Cluster\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latest.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      awsAttributes:\n        instanceProfileArn: ${sharedInstanceProfile.id}\n        availability: SPOT\n        zoneId: us-east-1\n        firstOnDemand: 1\n        spotBidPricePercent: 100\nvariables:\n  assumeRoleForEc2:\n    fn::invoke:\n      function: aws:iam:getPolicyDocument\n      arguments:\n        statements:\n          - effect: Allow\n            actions:\n              - sts:AssumeRole\n            principals:\n              - identifiers:\n                  - ec2.amazonaws.com\n                type: Service\n  passRoleForS3Access:\n    fn::invoke:\n      function: aws:iam:getPolicyDocument\n      arguments:\n        statements:\n          - effect: Allow\n            actions:\n              - iam:PassRole\n            resources:\n              - ${roleForS3Access.arn}\n  latest:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments: {}\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Usage with Cluster Policies\n\nIt is advised to keep all common configurations in Cluster Policies to maintain control of the environments launched, so `databricks.Cluster` above could be replaced with `databricks.ClusterPolicy`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ClusterPolicy(\"this\", {\n    name: \"Policy with predefined instance profile\",\n    definition: JSON.stringify({\n        \"aws_attributes.instance_profile_arn\": {\n            type: \"fixed\",\n            value: shared.id,\n        },\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nthis = databricks.ClusterPolicy(\"this\",\n    name=\"Policy with predefined instance profile\",\n    definition=json.dumps({\n        \"aws_attributes.instance_profile_arn\": {\n            \"type\": \"fixed\",\n            \"value\": shared[\"id\"],\n        },\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ClusterPolicy(\"this\", new()\n    {\n        Name = \"Policy with predefined instance profile\",\n        Definition = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"aws_attributes.instance_profile_arn\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"fixed\",\n                [\"value\"] = shared.Id,\n            },\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"aws_attributes.instance_profile_arn\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": shared.Id,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"this\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tName:       pulumi.String(\"Policy with predefined instance profile\"),\n\t\t\tDefinition: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ClusterPolicy(\"this\", ClusterPolicyArgs.builder()\n            .name(\"Policy with predefined instance profile\")\n            .definition(serializeJson(\n                jsonObject(\n                    jsonProperty(\"aws_attributes.instance_profile_arn\", jsonObject(\n                        jsonProperty(\"type\", \"fixed\"),\n                        jsonProperty(\"value\", shared.id())\n                    ))\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ClusterPolicy\n    properties:\n      name: Policy with predefined instance profile\n      definition:\n        fn::toJSON:\n          aws_attributes.instance_profile_arn:\n            type: fixed\n            value: ${shared.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Granting access to all users\n\nYou can make instance profile available to all users by associating it with the special group called `users` through databricks.Group data source.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.InstanceProfile(\"this\", {instanceProfileArn: shared.id});\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst all = new databricks.GroupInstanceProfile(\"all\", {\n    groupId: users.then(users =\u003e users.id),\n    instanceProfileId: _this.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.InstanceProfile(\"this\", instance_profile_arn=shared[\"id\"])\nusers = databricks.get_group(display_name=\"users\")\nall = databricks.GroupInstanceProfile(\"all\",\n    group_id=users.id,\n    instance_profile_id=this.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = shared.Id,\n    });\n\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var all = new Databricks.GroupInstanceProfile(\"all\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        InstanceProfileId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.Any(shared.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"all\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           pulumi.String(users.Id),\n\t\t\tInstanceProfileId: this.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new InstanceProfile(\"this\", InstanceProfileArgs.builder()\n            .instanceProfileArn(shared.id())\n            .build());\n\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var all = new GroupInstanceProfile(\"all\", GroupInstanceProfileArgs.builder()\n            .groupId(users.id())\n            .instanceProfileId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: ${shared.id}\n  all:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${users.id}\n      instanceProfileId: ${this.id}\nvariables:\n  users:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: users\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Usage with Databricks SQL serverless\n\nWhen the instance profile ARN and its associated IAM role ARN don't match and the instance profile is intended for use with Databricks SQL serverless, the `iam_role_arn` parameter can be specified.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlServerlessAssumeRole = aws.iam.getPolicyDocument({\n    statements: [{\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            type: \"AWS\",\n            identifiers: [\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n        }],\n        conditions: [{\n            test: \"StringEquals\",\n            variable: \"sts:ExternalID\",\n            values: [\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n            ],\n        }],\n    }],\n});\nconst _this = new aws.iam.Role(\"this\", {\n    name: \"my-databricks-sql-serverless-role\",\n    assumeRolePolicy: sqlServerlessAssumeRole.then(sqlServerlessAssumeRole =\u003e sqlServerlessAssumeRole.json),\n});\nconst thisInstanceProfile = new aws.iam.InstanceProfile(\"this\", {\n    name: \"my-databricks-sql-serverless-instance-profile\",\n    role: _this.name,\n});\nconst thisInstanceProfile2 = new databricks.InstanceProfile(\"this\", {\n    instanceProfileArn: thisInstanceProfile.arn,\n    iamRoleArn: _this.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nsql_serverless_assume_role = aws.iam.get_policy_document(statements=[{\n    \"actions\": [\"sts:AssumeRole\"],\n    \"principals\": [{\n        \"type\": \"AWS\",\n        \"identifiers\": [\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n    }],\n    \"conditions\": [{\n        \"test\": \"StringEquals\",\n        \"variable\": \"sts:ExternalID\",\n        \"values\": [\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n        ],\n    }],\n}])\nthis = aws.iam.Role(\"this\",\n    name=\"my-databricks-sql-serverless-role\",\n    assume_role_policy=sql_serverless_assume_role.json)\nthis_instance_profile = aws.iam.InstanceProfile(\"this\",\n    name=\"my-databricks-sql-serverless-instance-profile\",\n    role=this.name)\nthis_instance_profile2 = databricks.InstanceProfile(\"this\",\n    instance_profile_arn=this_instance_profile.arn,\n    iam_role_arn=this.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlServerlessAssumeRole = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n                        },\n                    },\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"StringEquals\",\n                        Variable = \"sts:ExternalID\",\n                        Values = new[]\n                        {\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var @this = new Aws.Iam.Role(\"this\", new()\n    {\n        Name = \"my-databricks-sql-serverless-role\",\n        AssumeRolePolicy = sqlServerlessAssumeRole.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var thisInstanceProfile = new Aws.Iam.InstanceProfile(\"this\", new()\n    {\n        Name = \"my-databricks-sql-serverless-instance-profile\",\n        Role = @this.Name,\n    });\n\n    var thisInstanceProfile2 = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = thisInstanceProfile.Arn,\n        IamRoleArn = @this.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsqlServerlessAssumeRole, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tType: \"AWS\",\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tConditions: []iam.GetPolicyDocumentStatementCondition{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tTest:     \"StringEquals\",\n\t\t\t\t\t\t\tVariable: \"sts:ExternalID\",\n\t\t\t\t\t\t\tValues: []string{\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := iam.NewRole(ctx, \"this\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(\"my-databricks-sql-serverless-role\"),\n\t\t\tAssumeRolePolicy: pulumi.String(sqlServerlessAssumeRole.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisInstanceProfile, err := iam.NewInstanceProfile(ctx, \"this\", \u0026iam.InstanceProfileArgs{\n\t\t\tName: pulumi.String(\"my-databricks-sql-serverless-instance-profile\"),\n\t\t\tRole: this.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: thisInstanceProfile.Arn,\n\t\t\tIamRoleArn:         this.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sqlServerlessAssumeRole = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .type(\"AWS\")\n                    .identifiers(\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\")\n                    .build())\n                .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                    .test(\"StringEquals\")\n                    .variable(\"sts:ExternalID\")\n                    .values(                    \n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\")\n                    .build())\n                .build())\n            .build());\n\n        var this_ = new Role(\"this\", RoleArgs.builder()\n            .name(\"my-databricks-sql-serverless-role\")\n            .assumeRolePolicy(sqlServerlessAssumeRole.json())\n            .build());\n\n        var thisInstanceProfile = new com.pulumi.aws.iam.InstanceProfile(\"thisInstanceProfile\", com.pulumi.aws.iam.InstanceProfileArgs.builder()\n            .name(\"my-databricks-sql-serverless-instance-profile\")\n            .role(this_.name())\n            .build());\n\n        var thisInstanceProfile2 = new com.pulumi.databricks.InstanceProfile(\"thisInstanceProfile2\", com.pulumi.databricks.InstanceProfileArgs.builder()\n            .instanceProfileArn(thisInstanceProfile.arn())\n            .iamRoleArn(this_.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: aws:iam:Role\n    properties:\n      name: my-databricks-sql-serverless-role\n      assumeRolePolicy: ${sqlServerlessAssumeRole.json}\n  thisInstanceProfile:\n    type: aws:iam:InstanceProfile\n    name: this\n    properties:\n      name: my-databricks-sql-serverless-instance-profile\n      role: ${this.name}\n  thisInstanceProfile2:\n    type: databricks:InstanceProfile\n    name: this\n    properties:\n      instanceProfileArn: ${thisInstanceProfile.arn}\n      iamRoleArn: ${this.arn}\nvariables:\n  sqlServerlessAssumeRole:\n    fn::invoke:\n      function: aws:iam:getPolicyDocument\n      arguments:\n        statements:\n          - actions:\n              - sts:AssumeRole\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::790110701330:role/serverless-customer-resource-role\n            conditions:\n              - test: StringEquals\n                variable: sts:ExternalID\n                values:\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource instance profile can be imported using the ARN of it\n\nbash\n\n```sh\n$ pulumi import databricks:index/instanceProfile:InstanceProfile this \u003cinstance-profile-arn\u003e\n```\n\n",
            "properties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "required": [
                "instanceProfileArn",
                "skipValidation"
            ],
            "inputProperties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "requiredInputs": [
                "instanceProfileArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstanceProfile resources.\n",
                "properties": {
                    "iamRoleArn": {
                        "type": "string",
                        "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                    },
                    "isMetaInstanceProfile": {
                        "type": "boolean",
                        "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/ipAccessList:IpAccessList": {
            "description": "Security-conscious enterprises that use cloud SaaS applications need to restrict access to their own employees. Authentication helps to prove user identity, but that does not enforce network location of the users. Accessing a cloud service from an unsecured network can pose security risks to an enterprise, especially when the user may have authorized access to sensitive or personal data. Enterprise network perimeters apply security policies and limit access to external services (for example, firewalls, proxies, DLP, and logging), so access beyond these controls are assumed to be untrusted. Please see [IP Access List](https://docs.databricks.com/security/network/ip-access-list.html) for full feature documentation.\n\n\u003e The total number of IP addresses and CIDR scopes provided across all ACL Lists in a workspace can not exceed 1000.  Refer to the docs above for specifics.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: \"true\",\n}});\nconst allowed_list = new databricks.IpAccessList(\"allowed-list\", {\n    label: \"allow_in\",\n    listType: \"ALLOW\",\n    ipAddresses: [\n        \"1.1.1.1\",\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n}, {\n    dependsOn: [_this],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": \"true\",\n})\nallowed_list = databricks.IpAccessList(\"allowed-list\",\n    label=\"allow_in\",\n    list_type=\"ALLOW\",\n    ip_addresses=[\n        \"1.1.1.1\",\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n    opts = pulumi.ResourceOptions(depends_on=[this]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", \"true\" },\n        },\n    });\n\n    var allowed_list = new Databricks.IpAccessList(\"allowed-list\", new()\n    {\n        Label = \"allow_in\",\n        ListType = \"ALLOW\",\n        IpAddresses = new[]\n        {\n            \"1.1.1.1\",\n            \"1.2.3.0/24\",\n            \"1.2.5.0/24\",\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            @this,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.StringMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.String(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewIpAccessList(ctx, \"allowed-list\", \u0026databricks.IpAccessListArgs{\n\t\t\tLabel:    pulumi.String(\"allow_in\"),\n\t\t\tListType: pulumi.String(\"ALLOW\"),\n\t\t\tIpAddresses: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"1.1.1.1\"),\n\t\t\t\tpulumi.String(\"1.2.3.0/24\"),\n\t\t\t\tpulumi.String(\"1.2.5.0/24\"),\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthis,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport com.pulumi.databricks.IpAccessList;\nimport com.pulumi.databricks.IpAccessListArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()\n            .customConfig(Map.of(\"enableIpAccessLists\", \"true\"))\n            .build());\n\n        var allowed_list = new IpAccessList(\"allowed-list\", IpAccessListArgs.builder()\n            .label(\"allow_in\")\n            .listType(\"ALLOW\")\n            .ipAddresses(            \n                \"1.1.1.1\",\n                \"1.2.3.0/24\",\n                \"1.2.5.0/24\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(this_)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n  allowed-list:\n    type: databricks:IpAccessList\n    properties:\n      label: allow_in\n      listType: ALLOW\n      ipAddresses:\n        - 1.1.1.1\n        - 1.2.3.0/24\n        - 1.2.5.0/24\n    options:\n      dependsOn:\n        - ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsPrivateAccessSettings to create a [Private Access Setting](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-5-create-a-private-access-settings-configuration-using-the-databricks-account-api) that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nThe databricks_ip_access_list can be imported using id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/ipAccessList:IpAccessList this \u003clist-id\u003e\n```\n\n",
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A string list of IP addresses and CIDR ranges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                }
            },
            "required": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "inputProperties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A string list of IP addresses and CIDR ranges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                }
            },
            "requiredInputs": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering IpAccessList resources.\n",
                "properties": {
                    "enabled": {
                        "type": "boolean",
                        "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                    },
                    "ipAddresses": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "A string list of IP addresses and CIDR ranges.\n"
                    },
                    "label": {
                        "type": "string",
                        "description": "This is the display name for the given IP ACL List.\n"
                    },
                    "listType": {
                        "type": "string",
                        "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/job:Job": {
            "description": "The `databricks.Job` resource allows you to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n\n## Example Usage\n\n\u003e In Pulumi configuration, it is recommended to define tasks in alphabetical order of their `task_key` arguments, so that you get consistent and readable diff. Whenever tasks are added or removed, or `task_key` is renamed, you'll observe a change in the majority of tasks. It's related to the fact that the current version of the provider treats `task` blocks as an ordered list. Alternatively, `task` block could have been an unordered set, though end-users would see the entire block replaced upon a change in single property of the task.\n\nIt is possible to create [a Databricks job](https://docs.databricks.com/data-engineering/jobs/jobs-user-guide.html) using `task` blocks. A single task is defined with the `task` block containing one of the `*_task` blocks, `task_key`, and additional arguments described below.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Job(\"this\", {\n    name: \"Job with multiple tasks\",\n    description: \"This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\",\n    jobClusters: [{\n        jobClusterKey: \"j\",\n        newCluster: {\n            numWorkers: 2,\n            sparkVersion: latest.id,\n            nodeTypeId: smallest.id,\n        },\n    }],\n    tasks: [\n        {\n            taskKey: \"a\",\n            newCluster: {\n                numWorkers: 1,\n                sparkVersion: latest.id,\n                nodeTypeId: smallest.id,\n            },\n            notebookTask: {\n                notebookPath: thisDatabricksNotebook.path,\n            },\n        },\n        {\n            taskKey: \"b\",\n            dependsOns: [{\n                taskKey: \"a\",\n            }],\n            existingClusterId: shared.id,\n            sparkJarTask: {\n                mainClassName: \"com.acme.data.Main\",\n            },\n        },\n        {\n            taskKey: \"c\",\n            jobClusterKey: \"j\",\n            notebookTask: {\n                notebookPath: thisDatabricksNotebook.path,\n            },\n        },\n        {\n            taskKey: \"d\",\n            pipelineTask: {\n                pipelineId: thisDatabricksPipeline.id,\n            },\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Job(\"this\",\n    name=\"Job with multiple tasks\",\n    description=\"This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\",\n    job_clusters=[{\n        \"job_cluster_key\": \"j\",\n        \"new_cluster\": {\n            \"num_workers\": 2,\n            \"spark_version\": latest[\"id\"],\n            \"node_type_id\": smallest[\"id\"],\n        },\n    }],\n    tasks=[\n        {\n            \"task_key\": \"a\",\n            \"new_cluster\": {\n                \"num_workers\": 1,\n                \"spark_version\": latest[\"id\"],\n                \"node_type_id\": smallest[\"id\"],\n            },\n            \"notebook_task\": {\n                \"notebook_path\": this_databricks_notebook[\"path\"],\n            },\n        },\n        {\n            \"task_key\": \"b\",\n            \"depends_ons\": [{\n                \"task_key\": \"a\",\n            }],\n            \"existing_cluster_id\": shared[\"id\"],\n            \"spark_jar_task\": {\n                \"main_class_name\": \"com.acme.data.Main\",\n            },\n        },\n        {\n            \"task_key\": \"c\",\n            \"job_cluster_key\": \"j\",\n            \"notebook_task\": {\n                \"notebook_path\": this_databricks_notebook[\"path\"],\n            },\n        },\n        {\n            \"task_key\": \"d\",\n            \"pipeline_task\": {\n                \"pipeline_id\": this_databricks_pipeline[\"id\"],\n            },\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Job(\"this\", new()\n    {\n        Name = \"Job with multiple tasks\",\n        Description = \"This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\",\n        JobClusters = new[]\n        {\n            new Databricks.Inputs.JobJobClusterArgs\n            {\n                JobClusterKey = \"j\",\n                NewCluster = new Databricks.Inputs.JobJobClusterNewClusterArgs\n                {\n                    NumWorkers = 2,\n                    SparkVersion = latest.Id,\n                    NodeTypeId = smallest.Id,\n                },\n            },\n        },\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"a\",\n                NewCluster = new Databricks.Inputs.JobTaskNewClusterArgs\n                {\n                    NumWorkers = 1,\n                    SparkVersion = latest.Id,\n                    NodeTypeId = smallest.Id,\n                },\n                NotebookTask = new Databricks.Inputs.JobTaskNotebookTaskArgs\n                {\n                    NotebookPath = thisDatabricksNotebook.Path,\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"b\",\n                DependsOns = new[]\n                {\n                    new Databricks.Inputs.JobTaskDependsOnArgs\n                    {\n                        TaskKey = \"a\",\n                    },\n                },\n                ExistingClusterId = shared.Id,\n                SparkJarTask = new Databricks.Inputs.JobTaskSparkJarTaskArgs\n                {\n                    MainClassName = \"com.acme.data.Main\",\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"c\",\n                JobClusterKey = \"j\",\n                NotebookTask = new Databricks.Inputs.JobTaskNotebookTaskArgs\n                {\n                    NotebookPath = thisDatabricksNotebook.Path,\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"d\",\n                PipelineTask = new Databricks.Inputs.JobTaskPipelineTaskArgs\n                {\n                    PipelineId = thisDatabricksPipeline.Id,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tName:        pulumi.String(\"Job with multiple tasks\"),\n\t\t\tDescription: pulumi.String(\"This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\"),\n\t\t\tJobClusters: databricks.JobJobClusterArray{\n\t\t\t\t\u0026databricks.JobJobClusterArgs{\n\t\t\t\t\tJobClusterKey: pulumi.String(\"j\"),\n\t\t\t\t\tNewCluster: \u0026databricks.JobJobClusterNewClusterArgs{\n\t\t\t\t\t\tNumWorkers:   pulumi.Int(2),\n\t\t\t\t\t\tSparkVersion: pulumi.Any(latest.Id),\n\t\t\t\t\t\tNodeTypeId:   pulumi.Any(smallest.Id),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"a\"),\n\t\t\t\t\tNewCluster: \u0026databricks.JobTaskNewClusterArgs{\n\t\t\t\t\t\tNumWorkers:   pulumi.Int(1),\n\t\t\t\t\t\tSparkVersion: pulumi.Any(latest.Id),\n\t\t\t\t\t\tNodeTypeId:   pulumi.Any(smallest.Id),\n\t\t\t\t\t},\n\t\t\t\t\tNotebookTask: \u0026databricks.JobTaskNotebookTaskArgs{\n\t\t\t\t\t\tNotebookPath: pulumi.Any(thisDatabricksNotebook.Path),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"b\"),\n\t\t\t\t\tDependsOns: databricks.JobTaskDependsOnArray{\n\t\t\t\t\t\t\u0026databricks.JobTaskDependsOnArgs{\n\t\t\t\t\t\t\tTaskKey: pulumi.String(\"a\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tExistingClusterId: pulumi.Any(shared.Id),\n\t\t\t\t\tSparkJarTask: \u0026databricks.JobTaskSparkJarTaskArgs{\n\t\t\t\t\t\tMainClassName: pulumi.String(\"com.acme.data.Main\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey:       pulumi.String(\"c\"),\n\t\t\t\t\tJobClusterKey: pulumi.String(\"j\"),\n\t\t\t\t\tNotebookTask: \u0026databricks.JobTaskNotebookTaskArgs{\n\t\t\t\t\t\tNotebookPath: pulumi.Any(thisDatabricksNotebook.Path),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"d\"),\n\t\t\t\t\tPipelineTask: \u0026databricks.JobTaskPipelineTaskArgs{\n\t\t\t\t\t\tPipelineId: pulumi.Any(thisDatabricksPipeline.Id),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobJobClusterArgs;\nimport com.pulumi.databricks.inputs.JobJobClusterNewClusterArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskNewClusterArgs;\nimport com.pulumi.databricks.inputs.JobTaskNotebookTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSparkJarTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskPipelineTaskArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Job(\"this\", JobArgs.builder()\n            .name(\"Job with multiple tasks\")\n            .description(\"This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\")\n            .jobClusters(JobJobClusterArgs.builder()\n                .jobClusterKey(\"j\")\n                .newCluster(JobJobClusterNewClusterArgs.builder()\n                    .numWorkers(2)\n                    .sparkVersion(latest.id())\n                    .nodeTypeId(smallest.id())\n                    .build())\n                .build())\n            .tasks(            \n                JobTaskArgs.builder()\n                    .taskKey(\"a\")\n                    .newCluster(JobTaskNewClusterArgs.builder()\n                        .numWorkers(1)\n                        .sparkVersion(latest.id())\n                        .nodeTypeId(smallest.id())\n                        .build())\n                    .notebookTask(JobTaskNotebookTaskArgs.builder()\n                        .notebookPath(thisDatabricksNotebook.path())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"b\")\n                    .dependsOns(JobTaskDependsOnArgs.builder()\n                        .taskKey(\"a\")\n                        .build())\n                    .existingClusterId(shared.id())\n                    .sparkJarTask(JobTaskSparkJarTaskArgs.builder()\n                        .mainClassName(\"com.acme.data.Main\")\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"c\")\n                    .jobClusterKey(\"j\")\n                    .notebookTask(JobTaskNotebookTaskArgs.builder()\n                        .notebookPath(thisDatabricksNotebook.path())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"d\")\n                    .pipelineTask(JobTaskPipelineTaskArgs.builder()\n                        .pipelineId(thisDatabricksPipeline.id())\n                        .build())\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Job\n    properties:\n      name: Job with multiple tasks\n      description: This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.\n      jobClusters:\n        - jobClusterKey: j\n          newCluster:\n            numWorkers: 2\n            sparkVersion: ${latest.id}\n            nodeTypeId: ${smallest.id}\n      tasks:\n        - taskKey: a\n          newCluster:\n            numWorkers: 1\n            sparkVersion: ${latest.id}\n            nodeTypeId: ${smallest.id}\n          notebookTask:\n            notebookPath: ${thisDatabricksNotebook.path}\n        - taskKey: b\n          dependsOns:\n            - taskKey: a\n          existingClusterId: ${shared.id}\n          sparkJarTask:\n            mainClassName: com.acme.data.Main\n        - taskKey: c\n          jobClusterKey: j\n          notebookTask:\n            notebookPath: ${thisDatabricksNotebook.path}\n        - taskKey: d\n          pipelineTask:\n            pipelineId: ${thisDatabricksPipeline.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\nBy default, all users can create and modify jobs unless an administrator [enables jobs access control](https://docs.databricks.com/administration-guide/access-control/jobs-acl.html). With jobs access control, individual permissions determine a user’s abilities.\n\n* databricks.Permissions can control which groups or individual users can *Can View*, *Can Manage Run*, and *Can Manage*.\n* databricks.ClusterPolicy can control which kinds of clusters users can create for jobs.\n\n## Import\n\nThe resource job can be imported using the id of the job\n\nbash\n\n```sh\n$ pulumi import databricks:index/job:Job this \u003cjob-id\u003e\n```\n\n",
            "properties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                    "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.\n"
                },
                "continuous": {
                    "$ref": "#/types/databricks:index/JobContinuous:JobContinuous",
                    "description": "Configuration block to configure pause status. See continuous Configuration Block.\n"
                },
                "controlRunState": {
                    "type": "boolean",
                    "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "editMode": {
                    "type": "string",
                    "description": "If `\"UI_LOCKED\"`, the user interface for the job will be locked. If `\"EDITABLE\"` (the default), the user interface will be editable.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource",
                    "description": "Specifices the a Git repository for task source code. See git_source Configuration Block below.\n"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                    "description": "An optional block that specifies the health conditions for the job documented below.\n"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    },
                    "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level documented below.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                    },
                    "description": "Specifices job parameter for the job. See parameter Configuration Block\n"
                },
                "performanceTarget": {
                    "type": "string"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/JobQueue:JobQueue",
                    "description": "The queue status for the job. See queue Configuration Block below.\n"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/JobRunAs:JobRunAs",
                    "description": "The user or the service prinicipal the job runs as. See run_as Configuration Block below.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "An optional map of the tags associated with the job. See tags Configuration Map\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    },
                    "description": "A list of task specification that the job will execute. See task Configuration Block below.\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/JobTrigger:JobTrigger",
                    "description": "The conditions that triggers the job to start. See trigger Configuration Block below.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "required": [
                "format",
                "name",
                "runAs",
                "url"
            ],
            "inputProperties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                    "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.\n"
                },
                "continuous": {
                    "$ref": "#/types/databricks:index/JobContinuous:JobContinuous",
                    "description": "Configuration block to configure pause status. See continuous Configuration Block.\n"
                },
                "controlRunState": {
                    "type": "boolean",
                    "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "editMode": {
                    "type": "string",
                    "description": "If `\"UI_LOCKED\"`, the user interface for the job will be locked. If `\"EDITABLE\"` (the default), the user interface will be editable.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource",
                    "description": "Specifices the a Git repository for task source code. See git_source Configuration Block below.\n"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                    "description": "An optional block that specifies the health conditions for the job documented below.\n"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    },
                    "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level documented below.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                    },
                    "description": "Specifices job parameter for the job. See parameter Configuration Block\n"
                },
                "performanceTarget": {
                    "type": "string"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/JobQueue:JobQueue",
                    "description": "The queue status for the job. See queue Configuration Block below.\n"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/JobRunAs:JobRunAs",
                    "description": "The user or the service prinicipal the job runs as. See run_as Configuration Block below.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "An optional map of the tags associated with the job. See tags Configuration Map\n"
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    },
                    "description": "A list of task specification that the job will execute. See task Configuration Block below.\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/JobTrigger:JobTrigger",
                    "description": "The conditions that triggers the job to start. See trigger Configuration Block below.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Job resources.\n",
                "properties": {
                    "alwaysRunning": {
                        "type": "boolean",
                        "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                        "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                    },
                    "budgetPolicyId": {
                        "type": "string",
                        "description": "The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.\n"
                    },
                    "continuous": {
                        "$ref": "#/types/databricks:index/JobContinuous:JobContinuous",
                        "description": "Configuration block to configure pause status. See continuous Configuration Block.\n"
                    },
                    "controlRunState": {
                        "type": "boolean",
                        "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                    },
                    "dbtTask": {
                        "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "deployment": {
                        "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                    },
                    "description": {
                        "type": "string",
                        "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                    },
                    "editMode": {
                        "type": "string",
                        "description": "If `\"UI_LOCKED\"`, the user interface for the job will be locked. If `\"EDITABLE\"` (the default), the user interface will be editable.\n"
                    },
                    "emailNotifications": {
                        "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                        "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                    },
                    "environments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                        }
                    },
                    "existingClusterId": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "gitSource": {
                        "$ref": "#/types/databricks:index/JobGitSource:JobGitSource",
                        "description": "Specifices the a Git repository for task source code. See git_source Configuration Block below.\n"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                        "description": "An optional block that specifies the health conditions for the job documented below.\n"
                    },
                    "jobClusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                        },
                        "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                        },
                        "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.\n"
                    },
                    "maxConcurrentRuns": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                    },
                    "maxRetries": {
                        "type": "integer",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "minRetryIntervalMillis": {
                        "type": "integer",
                        "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "name": {
                        "type": "string",
                        "description": "An optional name for the job. The default value is Untitled.\n"
                    },
                    "newCluster": {
                        "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                    },
                    "notebookTask": {
                        "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "notificationSettings": {
                        "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                        "description": "An optional block controlling the notification settings on the job level documented below.\n"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                        },
                        "description": "Specifices job parameter for the job. See parameter Configuration Block\n"
                    },
                    "performanceTarget": {
                        "type": "string"
                    },
                    "pipelineTask": {
                        "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "pythonWheelTask": {
                        "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "queue": {
                        "$ref": "#/types/databricks:index/JobQueue:JobQueue",
                        "description": "The queue status for the job. See queue Configuration Block below.\n"
                    },
                    "retryOnTimeout": {
                        "type": "boolean",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "runAs": {
                        "$ref": "#/types/databricks:index/JobRunAs:JobRunAs",
                        "description": "The user or the service prinicipal the job runs as. See run_as Configuration Block below.\n"
                    },
                    "runJobTask": {
                        "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                        "description": "An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.\n"
                    },
                    "sparkJarTask": {
                        "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "sparkPythonTask": {
                        "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "sparkSubmitTask": {
                        "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "tags": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "An optional map of the tags associated with the job. See tags Configuration Map\n"
                    },
                    "tasks": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobTask:JobTask"
                        },
                        "description": "A list of task specification that the job will execute. See task Configuration Block below.\n"
                    },
                    "timeoutSeconds": {
                        "type": "integer",
                        "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                    },
                    "trigger": {
                        "$ref": "#/types/databricks:index/JobTrigger:JobTrigger",
                        "description": "The conditions that triggers the job to start. See trigger Configuration Block below.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the job on the given workspace\n"
                    },
                    "webhookNotifications": {
                        "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                        "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/lakehouseMonitor:LakehouseMonitor": {
            "description": "NOTE: This resource has been deprecated and will be removed soon. Please use the databricks.QualityMonitor resource instead.\n\nThis resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks. \n\nA `databricks.LakehouseMonitor` is attached to a databricks.SqlTable and can be of type timeseries, snapshot or inference. \n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst myTestTable = new databricks.SqlTable(\"myTestTable\", {\n    catalogName: \"main\",\n    schemaName: things.name,\n    name: \"bar\",\n    tableType: \"MANAGED\",\n    dataSourceFormat: \"DELTA\",\n    columns: [{\n        name: \"timestamp\",\n        type: \"int\",\n    }],\n});\nconst testTimeseriesMonitor = new databricks.LakehouseMonitor(\"testTimeseriesMonitor\", {\n    tableName: pulumi.interpolate`${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: pulumi.interpolate`/Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}`,\n    outputSchemaName: pulumi.interpolate`${sandbox.name}.${things.name}`,\n    timeSeries: {\n        granularities: [\"1 hour\"],\n        timestampCol: \"timestamp\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    name=\"things\",\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nmy_test_table = databricks.SqlTable(\"myTestTable\",\n    catalog_name=\"main\",\n    schema_name=things.name,\n    name=\"bar\",\n    table_type=\"MANAGED\",\n    data_source_format=\"DELTA\",\n    columns=[{\n        \"name\": \"timestamp\",\n        \"type\": \"int\",\n    }])\ntest_timeseries_monitor = databricks.LakehouseMonitor(\"testTimeseriesMonitor\",\n    table_name=pulumi.Output.all(\n        sandboxName=sandbox.name,\n        thingsName=things.name,\n        myTestTableName=my_test_table.name\n).apply(lambda resolved_outputs: f\"{resolved_outputs['sandboxName']}.{resolved_outputs['thingsName']}.{resolved_outputs['myTestTableName']}\")\n,\n    assets_dir=my_test_table.name.apply(lambda name: f\"/Shared/provider-test/databricks_lakehouse_monitoring/{name}\"),\n    output_schema_name=pulumi.Output.all(\n        sandboxName=sandbox.name,\n        thingsName=things.name\n).apply(lambda resolved_outputs: f\"{resolved_outputs['sandboxName']}.{resolved_outputs['thingsName']}\")\n,\n    time_series={\n        \"granularities\": [\"1 hour\"],\n        \"timestamp_col\": \"timestamp\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var myTestTable = new Databricks.SqlTable(\"myTestTable\", new()\n    {\n        CatalogName = \"main\",\n        SchemaName = things.Name,\n        Name = \"bar\",\n        TableType = \"MANAGED\",\n        DataSourceFormat = \"DELTA\",\n        Columns = new[]\n        {\n            new Databricks.Inputs.SqlTableColumnArgs\n            {\n                Name = \"timestamp\",\n                Type = \"int\",\n            },\n        },\n    });\n\n    var testTimeseriesMonitor = new Databricks.LakehouseMonitor(\"testTimeseriesMonitor\", new()\n    {\n        TableName = Output.Tuple(sandbox.Name, things.Name, myTestTable.Name).Apply(values =\u003e\n        {\n            var sandboxName = values.Item1;\n            var thingsName = values.Item2;\n            var myTestTableName = values.Item3;\n            return $\"{sandboxName}.{thingsName}.{myTestTableName}\";\n        }),\n        AssetsDir = myTestTable.Name.Apply(name =\u003e $\"/Shared/provider-test/databricks_lakehouse_monitoring/{name}\"),\n        OutputSchemaName = Output.Tuple(sandbox.Name, things.Name).Apply(values =\u003e\n        {\n            var sandboxName = values.Item1;\n            var thingsName = values.Item2;\n            return $\"{sandboxName}.{thingsName}\";\n        }),\n        TimeSeries = new Databricks.Inputs.LakehouseMonitorTimeSeriesArgs\n        {\n            Granularities = new[]\n            {\n                \"1 hour\",\n            },\n            TimestampCol = \"timestamp\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyTestTable, err := databricks.NewSqlTable(ctx, \"myTestTable\", \u0026databricks.SqlTableArgs{\n\t\t\tCatalogName:      pulumi.String(\"main\"),\n\t\t\tSchemaName:       things.Name,\n\t\t\tName:             pulumi.String(\"bar\"),\n\t\t\tTableType:        pulumi.String(\"MANAGED\"),\n\t\t\tDataSourceFormat: pulumi.String(\"DELTA\"),\n\t\t\tColumns: databricks.SqlTableColumnArray{\n\t\t\t\t\u0026databricks.SqlTableColumnArgs{\n\t\t\t\t\tName: pulumi.String(\"timestamp\"),\n\t\t\t\t\tType: pulumi.String(\"int\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewLakehouseMonitor(ctx, \"testTimeseriesMonitor\", \u0026databricks.LakehouseMonitorArgs{\n\t\t\tTableName: pulumi.All(sandbox.Name, things.Name, myTestTable.Name).ApplyT(func(_args []interface{}) (string, error) {\n\t\t\t\tsandboxName := _args[0].(string)\n\t\t\t\tthingsName := _args[1].(string)\n\t\t\t\tmyTestTableName := _args[2].(string)\n\t\t\t\treturn fmt.Sprintf(\"%v.%v.%v\", sandboxName, thingsName, myTestTableName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tAssetsDir: myTestTable.Name.ApplyT(func(name string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"/Shared/provider-test/databricks_lakehouse_monitoring/%v\", name), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tOutputSchemaName: pulumi.All(sandbox.Name, things.Name).ApplyT(func(_args []interface{}) (string, error) {\n\t\t\t\tsandboxName := _args[0].(string)\n\t\t\t\tthingsName := _args[1].(string)\n\t\t\t\treturn fmt.Sprintf(\"%v.%v\", sandboxName, thingsName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tTimeSeries: \u0026databricks.LakehouseMonitorTimeSeriesArgs{\n\t\t\t\tGranularities: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"1 hour\"),\n\t\t\t\t},\n\t\t\t\tTimestampCol: pulumi.String(\"timestamp\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.SqlTable;\nimport com.pulumi.databricks.SqlTableArgs;\nimport com.pulumi.databricks.inputs.SqlTableColumnArgs;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorTimeSeriesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var myTestTable = new SqlTable(\"myTestTable\", SqlTableArgs.builder()\n            .catalogName(\"main\")\n            .schemaName(things.name())\n            .name(\"bar\")\n            .tableType(\"MANAGED\")\n            .dataSourceFormat(\"DELTA\")\n            .columns(SqlTableColumnArgs.builder()\n                .name(\"timestamp\")\n                .type(\"int\")\n                .build())\n            .build());\n\n        var testTimeseriesMonitor = new LakehouseMonitor(\"testTimeseriesMonitor\", LakehouseMonitorArgs.builder()\n            .tableName(Output.tuple(sandbox.name(), things.name(), myTestTable.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                var myTestTableName = values.t3;\n                return String.format(\"%s.%s.%s\", sandboxName,thingsName,myTestTableName);\n            }))\n            .assetsDir(myTestTable.name().applyValue(_name -\u003e String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", _name)))\n            .outputSchemaName(Output.tuple(sandbox.name(), things.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                return String.format(\"%s.%s\", sandboxName,thingsName);\n            }))\n            .timeSeries(LakehouseMonitorTimeSeriesArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n  myTestTable:\n    type: databricks:SqlTable\n    properties:\n      catalogName: main\n      schemaName: ${things.name}\n      name: bar\n      tableType: MANAGED\n      dataSourceFormat: DELTA\n      columns:\n        - name: timestamp\n          type: int\n  testTimeseriesMonitor:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      timeSeries:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Inference Monitor\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.LakehouseMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    inferenceLog: {\n        granularities: [\"1 hour\"],\n        timestampCol: \"timestamp\",\n        predictionCol: \"prediction\",\n        modelIdCol: \"model_id\",\n        problemType: \"PROBLEM_TYPE_REGRESSION\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.LakehouseMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_lakehouse_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    inference_log={\n        \"granularities\": [\"1 hour\"],\n        \"timestamp_col\": \"timestamp\",\n        \"prediction_col\": \"prediction\",\n        \"model_id_col\": \"model_id\",\n        \"problem_type\": \"PROBLEM_TYPE_REGRESSION\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.LakehouseMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_lakehouse_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        InferenceLog = new Databricks.Inputs.LakehouseMonitorInferenceLogArgs\n        {\n            Granularities = new[]\n            {\n                \"1 hour\",\n            },\n            TimestampCol = \"timestamp\",\n            PredictionCol = \"prediction\",\n            ModelIdCol = \"model_id\",\n            ProblemType = \"PROBLEM_TYPE_REGRESSION\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLakehouseMonitor(ctx, \"testMonitorInference\", \u0026databricks.LakehouseMonitorArgs{\n\t\t\tTableName:        pulumi.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name),\n\t\t\tAssetsDir:        pulumi.Sprintf(\"/Shared/provider-test/databricks_lakehouse_monitoring/%v\", myTestTable.Name),\n\t\t\tOutputSchemaName: pulumi.Sprintf(\"%v.%v\", sandbox.Name, things.Name),\n\t\t\tInferenceLog: \u0026databricks.LakehouseMonitorInferenceLogArgs{\n\t\t\t\tGranularities: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"1 hour\"),\n\t\t\t\t},\n\t\t\t\tTimestampCol:  pulumi.String(\"timestamp\"),\n\t\t\t\tPredictionCol: pulumi.String(\"prediction\"),\n\t\t\t\tModelIdCol:    pulumi.String(\"model_id\"),\n\t\t\t\tProblemType:   pulumi.String(\"PROBLEM_TYPE_REGRESSION\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorInferenceLogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new LakehouseMonitor(\"testMonitorInference\", LakehouseMonitorArgs.builder()\n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .inferenceLog(LakehouseMonitorInferenceLogArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .predictionCol(\"prediction\")\n                .modelIdCol(\"model_id\")\n                .problemType(\"PROBLEM_TYPE_REGRESSION\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      inferenceLog:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n        predictionCol: prediction\n        modelIdCol: model_id\n        problemType: PROBLEM_TYPE_REGRESSION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n### Snapshot Monitor\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.LakehouseMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    snapshot: {},\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.LakehouseMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_lakehouse_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    snapshot={})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.LakehouseMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_lakehouse_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        Snapshot = null,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLakehouseMonitor(ctx, \"testMonitorInference\", \u0026databricks.LakehouseMonitorArgs{\n\t\t\tTableName:        pulumi.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name),\n\t\t\tAssetsDir:        pulumi.Sprintf(\"/Shared/provider-test/databricks_lakehouse_monitoring/%v\", myTestTable.Name),\n\t\t\tOutputSchemaName: pulumi.Sprintf(\"%v.%v\", sandbox.Name, things.Name),\n\t\t\tSnapshot:         \u0026databricks.LakehouseMonitorSnapshotArgs{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorSnapshotArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new LakehouseMonitor(\"testMonitorInference\", LakehouseMonitorArgs.builder()\n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .snapshot(LakehouseMonitorSnapshotArgs.builder()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      snapshot: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Catalog\n* databricks.Schema\n* databricks.SqlTable\n",
            "properties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "The ID of the generated dashboard.\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "driftMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "monitorVersion": {
                    "type": "string",
                    "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "profileMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of the Monitor\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                }
            },
            "required": [
                "assetsDir",
                "dashboardId",
                "driftMetricsTableName",
                "monitorVersion",
                "outputSchemaName",
                "profileMetricsTableName",
                "status",
                "tableName"
            ],
            "inputProperties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                }
            },
            "requiredInputs": [
                "assetsDir",
                "outputSchemaName",
                "tableName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering LakehouseMonitor resources.\n",
                "properties": {
                    "assetsDir": {
                        "type": "string",
                        "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                    },
                    "baselineTableName": {
                        "type": "string",
                        "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                    },
                    "customMetrics": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                        },
                        "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                    },
                    "dashboardId": {
                        "type": "string",
                        "description": "The ID of the generated dashboard.\n"
                    },
                    "dataClassificationConfig": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                        "description": "The data classification config for the monitor\n"
                    },
                    "driftMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "inferenceLog": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                        "description": "Configuration for the inference log monitor\n"
                    },
                    "latestMonitorFailureMsg": {
                        "type": "string"
                    },
                    "monitorVersion": {
                        "type": "string",
                        "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                    },
                    "notifications": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                        "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                    },
                    "outputSchemaName": {
                        "type": "string",
                        "description": "Schema where output metric tables are created\n"
                    },
                    "profileMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                        "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                    },
                    "skipBuiltinDashboard": {
                        "type": "boolean",
                        "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                    },
                    "slicingExprs": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                    },
                    "snapshot": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                        "description": "Configuration for monitoring snapshot tables.\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of the Monitor\n"
                    },
                    "tableName": {
                        "type": "string",
                        "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                    },
                    "timeSeries": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                        "description": "Configuration for monitoring timeseries tables.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/library:Library": {
            "description": "Installs a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster. Each different type of library has a slightly different syntax. It's possible to set only one type of library within one resource. Otherwise, the plan will fail with an error.\n\n\u003e `databricks.Library` resource would always start the associated cluster if it's not running, so make sure to have auto-termination configured. It's not possible to atomically change the version of the same library without cluster restart. Libraries are fully removed from the cluster only after restart.\n\n## Plugin Framework Migration\n\nThe library resource has been migrated from sdkv2 to plugin framework。 If you encounter any problem with this resource and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_RESOURCES=\"databricks.Library\"`.\n\n## Installing library on all clusters\n\nYou can install libraries on all clusters with the help of databricks.getClusters data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const all = await databricks.getClusters({});\n    const cli: databricks.Library[] = [];\n    for (const range of all.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        cli.push(new databricks.Library(`cli-${range.key}`, {\n            clusterId: range.key,\n            pypi: {\n                \"package\": \"databricks-cli\",\n            },\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\ncli = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(all.ids)]:\n    cli.append(databricks.Library(f\"cli-{range['key']}\",\n        cluster_id=range[\"key\"],\n        pypi={\n            \"package\": \"databricks-cli\",\n        }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var all = await Databricks.GetClusters.InvokeAsync();\n\n    var cli = new List\u003cDatabricks.Library\u003e();\n    foreach (var range in )\n    {\n        cli.Add(new Databricks.Library($\"cli-{range.Key}\", new()\n        {\n            ClusterId = range.Key,\n            Pypi = new Databricks.Inputs.LibraryPypiArgs\n            {\n                Package = \"databricks-cli\",\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar cli []*databricks.Library\n\t\tfor key0, _ := range all.Ids {\n\t\t\t__res, err := databricks.NewLibrary(ctx, fmt.Sprintf(\"cli-%v\", key0), \u0026databricks.LibraryArgs{\n\t\t\t\tClusterId: pulumi.Float64(key0),\n\t\t\t\tPypi: \u0026databricks.LibraryPypiArgs{\n\t\t\t\t\tPackage: pulumi.String(\"databricks-cli\"),\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcli = append(cli, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .build());\n\n        final var cli = all.applyValue(getClustersResult -\u003e {\n            final var resources = new ArrayList\u003cLibrary\u003e();\n            for (var range : KeyedValue.of(getClustersResult.ids())) {\n                var resource = new Library(\"cli-\" + range.key(), LibraryArgs.builder()\n                    .clusterId(range.key())\n                    .pypi(LibraryPypiArgs.builder()\n                        .package_(\"databricks-cli\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  cli:\n    type: databricks:Library\n    properties:\n      clusterId: ${range.key}\n      pypi:\n        package: databricks-cli\n    options: {}\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Java/Scala Maven\n\nInstalling artifacts from Maven repository. You can also optionally specify a `repo` parameter for a custom Maven-style repository, that should be accessible without any authentication. Maven libraries are resolved in Databricks Control Plane, so repo should be accessible from it. It can even be properly configured [maven s3 wagon](https://github.com/seahen/maven-s3-wagon), [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) or [Azure Artifacts](https://azure.microsoft.com/en-us/services/devops/artifacts/).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst deequ = new databricks.Library(\"deequ\", {\n    clusterId: _this.id,\n    maven: {\n        coordinates: \"com.amazon.deequ:deequ:1.0.4\",\n        exclusions: [\"org.apache.avro:avro\"],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndeequ = databricks.Library(\"deequ\",\n    cluster_id=this[\"id\"],\n    maven={\n        \"coordinates\": \"com.amazon.deequ:deequ:1.0.4\",\n        \"exclusions\": [\"org.apache.avro:avro\"],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var deequ = new Databricks.Library(\"deequ\", new()\n    {\n        ClusterId = @this.Id,\n        Maven = new Databricks.Inputs.LibraryMavenArgs\n        {\n            Coordinates = \"com.amazon.deequ:deequ:1.0.4\",\n            Exclusions = new[]\n            {\n                \"org.apache.avro:avro\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"deequ\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tMaven: \u0026databricks.LibraryMavenArgs{\n\t\t\t\tCoordinates: pulumi.String(\"com.amazon.deequ:deequ:1.0.4\"),\n\t\t\t\tExclusions: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"org.apache.avro:avro\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryMavenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var deequ = new Library(\"deequ\", LibraryArgs.builder()\n            .clusterId(this_.id())\n            .maven(LibraryMavenArgs.builder()\n                .coordinates(\"com.amazon.deequ:deequ:1.0.4\")\n                .exclusions(\"org.apache.avro:avro\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  deequ:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      maven:\n        coordinates: com.amazon.deequ:deequ:1.0.4\n        exclusions:\n          - org.apache.avro:avro\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Python PyPI\n\nInstalling Python PyPI artifacts. You can optionally also specify the `repo` parameter for a custom PyPI mirror, which should be accessible without any authentication for the network that cluster runs in.\n\n\u003e `repo` host should be accessible from the Internet by Databricks control plane. If connectivity to custom PyPI repositories is required, please modify cluster-node `/etc/pip.conf` through databricks_global_init_script.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fbprophet = new databricks.Library(\"fbprophet\", {\n    clusterId: _this.id,\n    pypi: {\n        \"package\": \"fbprophet==0.6\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfbprophet = databricks.Library(\"fbprophet\",\n    cluster_id=this[\"id\"],\n    pypi={\n        \"package\": \"fbprophet==0.6\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fbprophet = new Databricks.Library(\"fbprophet\", new()\n    {\n        ClusterId = @this.Id,\n        Pypi = new Databricks.Inputs.LibraryPypiArgs\n        {\n            Package = \"fbprophet==0.6\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"fbprophet\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tPypi: \u0026databricks.LibraryPypiArgs{\n\t\t\t\tPackage: pulumi.String(\"fbprophet==0.6\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fbprophet = new Library(\"fbprophet\", LibraryArgs.builder()\n            .clusterId(this_.id())\n            .pypi(LibraryPypiArgs.builder()\n                .package_(\"fbprophet==0.6\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fbprophet:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      pypi:\n        package: fbprophet==0.6\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Python requirements files\n\nInstalling Python libraries listed in the `requirements.txt` file.  Only Workspace paths and Unity Catalog Volumes paths are supported.  Requires a cluster with DBR 15.0+.\n\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst libraries = new databricks.Library(\"libraries\", {\n    clusterId: _this.id,\n    requirements: \"/Workspace/path/to/requirements.txt\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nlibraries = databricks.Library(\"libraries\",\n    cluster_id=this[\"id\"],\n    requirements=\"/Workspace/path/to/requirements.txt\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var libraries = new Databricks.Library(\"libraries\", new()\n    {\n        ClusterId = @this.Id,\n        Requirements = \"/Workspace/path/to/requirements.txt\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"libraries\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId:    pulumi.Any(this.Id),\n\t\t\tRequirements: pulumi.String(\"/Workspace/path/to/requirements.txt\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var libraries = new Library(\"libraries\", LibraryArgs.builder()\n            .clusterId(this_.id())\n            .requirements(\"/Workspace/path/to/requirements.txt\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  libraries:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      requirements: /Workspace/path/to/requirements.txt\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\n## R CRan\n\nInstalling artifacts from CRan. You can also optionally specify a `repo` parameter for a custom cran mirror.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst rkeops = new databricks.Library(\"rkeops\", {\n    clusterId: _this.id,\n    cran: {\n        \"package\": \"rkeops\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nrkeops = databricks.Library(\"rkeops\",\n    cluster_id=this[\"id\"],\n    cran={\n        \"package\": \"rkeops\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var rkeops = new Databricks.Library(\"rkeops\", new()\n    {\n        ClusterId = @this.Id,\n        Cran = new Databricks.Inputs.LibraryCranArgs\n        {\n            Package = \"rkeops\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"rkeops\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tCran: \u0026databricks.LibraryCranArgs{\n\t\t\t\tPackage: pulumi.String(\"rkeops\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryCranArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var rkeops = new Library(\"rkeops\", LibraryArgs.builder()\n            .clusterId(this_.id())\n            .cran(LibraryCranArgs.builder()\n                .package_(\"rkeops\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  rkeops:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      cran:\n        package: rkeops\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "clusterId": {
                    "type": "string"
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "libraryId": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "required": [
                "clusterId",
                "libraryId"
            ],
            "inputProperties": {
                "clusterId": {
                    "type": "string"
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "libraryId": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                },
                "requirements": {
                    "type": "string"
                },
                "whl": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "clusterId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Library resources.\n",
                "properties": {
                    "clusterId": {
                        "type": "string"
                    },
                    "cran": {
                        "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                    },
                    "egg": {
                        "type": "string"
                    },
                    "jar": {
                        "type": "string"
                    },
                    "libraryId": {
                        "type": "string"
                    },
                    "maven": {
                        "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                    },
                    "pypi": {
                        "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                    },
                    "requirements": {
                        "type": "string"
                    },
                    "whl": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastore:Metastore": {
            "description": "\u003e This resource can be used with an account or workspace-level provider.\n\nA metastore is the top-level container of objects in Unity Catalog. It stores data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create metastores and assign them to Databricks workspaces in order to control which workloads use each metastore.\n\nUnity Catalog offers a new metastore with built in security and auditing. This is distinct to the metastore used in previous versions of Databricks (based on the Hive Metastore).\n\nA Unity Catalog metastore can be created without a root location \u0026 credential to maintain strict separation of storage across catalogs or environments.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"this\", {\n    metastoreId: _this.id,\n    workspaceId: workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"this\",\n    metastore_id=this.id,\n    workspace_id=workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        WorkspaceId = workspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.Sprintf(\"s3://%v/metastore\", metastore.Id),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"this\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tWorkspaceId: pulumi.Any(workspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()\n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()\n            .metastoreId(this_.id())\n            .workspaceId(workspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      workspaceId: ${workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n## Import\n\nThis resource can be imported by ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastore:Metastore this \u003cid\u003e\n```\n\n",
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string",
                    "description": "The region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n"
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "required": [
                "cloud",
                "createdAt",
                "createdBy",
                "globalMetastoreId",
                "metastoreId",
                "name",
                "owner",
                "region",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string",
                    "description": "The region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n",
                    "willReplaceOnChanges": true
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Metastore resources.\n",
                "properties": {
                    "cloud": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer"
                    },
                    "createdBy": {
                        "type": "string"
                    },
                    "defaultDataAccessConfigId": {
                        "type": "string"
                    },
                    "deltaSharingOrganizationName": {
                        "type": "string",
                        "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                    },
                    "deltaSharingRecipientTokenLifetimeInSeconds": {
                        "type": "integer",
                        "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                    },
                    "deltaSharingScope": {
                        "type": "string",
                        "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy metastore regardless of its contents.\n"
                    },
                    "globalMetastoreId": {
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of metastore.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the metastore owner.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "The region of the metastore\n"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageRootCredentialId": {
                        "type": "string"
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreAssignment:MetastoreAssignment": {
            "description": "\u003e This resource can be used with an account or workspace-level provider.\n\nA single databricks.Metastore can be shared across Databricks workspaces, and each linked workspace has a consistent view of the data and a single set of access policies. You can only create a single metastore for each region in which your organization operates.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"this\", {\n    metastoreId: _this.id,\n    workspaceId: workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"this\",\n    metastore_id=this.id,\n    workspace_id=workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        WorkspaceId = workspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.Sprintf(\"s3://%v/metastore\", metastore.Id),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"this\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tWorkspaceId: pulumi.Any(workspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()\n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()\n            .metastoreId(this_.id())\n            .workspaceId(workspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      workspaceId: ${workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by combination of workspace id and metastore id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastoreAssignment:MetastoreAssignment this '\u003cworkspace_id\u003e|\u003cmetastore_id\u003e'\n```\n\n",
            "properties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment. Please use databricks.DefaultNamespaceSetting instead.\n",
                    "deprecationMessage": "Use databricks.DefaultNamespaceSetting resource instead"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "id of the workspace for the assignment\n"
                }
            },
            "required": [
                "defaultCatalogName",
                "metastoreId",
                "workspaceId"
            ],
            "inputProperties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment. Please use databricks.DefaultNamespaceSetting instead.\n",
                    "deprecationMessage": "Use databricks.DefaultNamespaceSetting resource instead"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "id of the workspace for the assignment\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreAssignment resources.\n",
                "properties": {
                    "defaultCatalogName": {
                        "type": "string",
                        "description": "Default catalog used for this assignment. Please use databricks.DefaultNamespaceSetting instead.\n",
                        "deprecationMessage": "Use databricks.DefaultNamespaceSetting resource instead"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "id of the workspace for the assignment\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreDataAccess:MetastoreDataAccess": {
            "description": "\u003e This resource can be used with an account or workspace-level provider.\n\nOptionally, each databricks.Metastore can have a default databricks.StorageCredential defined as `databricks.MetastoreDataAccess`. This will be used by Unity Catalog to access data in the root storage location if defined.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreDataAccess = new databricks.MetastoreDataAccess(\"this\", {\n    metastoreId: _this.id,\n    name: metastoreDataAccess.name,\n    awsIamRole: {\n        roleArn: metastoreDataAccess.arn,\n    },\n    isDefault: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_data_access = databricks.MetastoreDataAccess(\"this\",\n    metastore_id=this.id,\n    name=metastore_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": metastore_data_access[\"arn\"],\n    },\n    is_default=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreDataAccess = new Databricks.MetastoreDataAccess(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        Name = metastoreDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.MetastoreDataAccessAwsIamRoleArgs\n        {\n            RoleArn = metastoreDataAccess.Arn,\n        },\n        IsDefault = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.Sprintf(\"s3://%v/metastore\", metastore.Id),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreDataAccess(ctx, \"this\", \u0026databricks.MetastoreDataAccessArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tName:        pulumi.Any(metastoreDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.MetastoreDataAccessAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(metastoreDataAccess.Arn),\n\t\t\t},\n\t\t\tIsDefault: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreDataAccess;\nimport com.pulumi.databricks.MetastoreDataAccessArgs;\nimport com.pulumi.databricks.inputs.MetastoreDataAccessAwsIamRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()\n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreDataAccess = new MetastoreDataAccess(\"thisMetastoreDataAccess\", MetastoreDataAccessArgs.builder()\n            .metastoreId(this_.id())\n            .name(metastoreDataAccess.name())\n            .awsIamRole(MetastoreDataAccessAwsIamRoleArgs.builder()\n                .roleArn(metastoreDataAccess.arn())\n                .build())\n            .isDefault(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreDataAccess:\n    type: databricks:MetastoreDataAccess\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      name: ${metastoreDataAccess.name}\n      awsIamRole:\n        roleArn: ${metastoreDataAccess.arn}\n      isDefault: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure using managed identity as credential (recommended)\n\n## Import\n\nThis resource can be imported by combination of metastore id and the data access name.\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastoreDataAccess:MetastoreDataAccess this '\u003cmetastore_id\u003e|\u003cname\u003e'\n```\n\n",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal"
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessCloudflareApiToken:MetastoreDataAccessCloudflareApiToken"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean"
                },
                "forceUpdate": {
                    "type": "boolean"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey"
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n"
                },
                "isolationMode": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "readOnly": {
                    "type": "boolean"
                },
                "skipValidation": {
                    "type": "boolean"
                }
            },
            "required": [
                "databricksGcpServiceAccount",
                "isolationMode",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                    "willReplaceOnChanges": true
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                    "willReplaceOnChanges": true
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                    "willReplaceOnChanges": true
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessCloudflareApiToken:MetastoreDataAccessCloudflareApiToken",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "forceUpdate": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                    "willReplaceOnChanges": true
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n",
                    "willReplaceOnChanges": true
                },
                "isolationMode": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string"
                },
                "readOnly": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "skipValidation": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreDataAccess resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                        "willReplaceOnChanges": true
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                        "willReplaceOnChanges": true
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                        "willReplaceOnChanges": true
                    },
                    "cloudflareApiToken": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessCloudflareApiToken:MetastoreDataAccessCloudflareApiToken",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "databricksGcpServiceAccount": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                        "willReplaceOnChanges": true
                    },
                    "isDefault": {
                        "type": "boolean",
                        "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n",
                        "willReplaceOnChanges": true
                    },
                    "isolationMode": {
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreProvider:MetastoreProvider": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nIn Delta Sharing, a provider is an entity that shares data with a recipient. Within a metastore, Unity Catalog provides the ability to create a provider which contains a list of shares that have been shared with you.\n\nA `databricks.MetastoreProvider` is contained within databricks.Metastore and can contain a list of shares that have been shared with you.\n\n\u003e Databricks to Databricks sharing automatically creates the provider.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dbprovider = new databricks.MetastoreProvider(\"dbprovider\", {\n    name: \"terraform-test-provider\",\n    comment: \"made by terraform 2\",\n    authenticationType: \"TOKEN\",\n    recipientProfileStr: JSON.stringify({\n        shareCredentialsVersion: 1,\n        bearerToken: \"token\",\n        endpoint: \"endpoint\",\n        expirationTime: \"expiration-time\",\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\ndbprovider = databricks.MetastoreProvider(\"dbprovider\",\n    name=\"terraform-test-provider\",\n    comment=\"made by terraform 2\",\n    authentication_type=\"TOKEN\",\n    recipient_profile_str=json.dumps({\n        \"shareCredentialsVersion\": 1,\n        \"bearerToken\": \"token\",\n        \"endpoint\": \"endpoint\",\n        \"expirationTime\": \"expiration-time\",\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dbprovider = new Databricks.MetastoreProvider(\"dbprovider\", new()\n    {\n        Name = \"terraform-test-provider\",\n        Comment = \"made by terraform 2\",\n        AuthenticationType = \"TOKEN\",\n        RecipientProfileStr = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"shareCredentialsVersion\"] = 1,\n            [\"bearerToken\"] = \"token\",\n            [\"endpoint\"] = \"endpoint\",\n            [\"expirationTime\"] = \"expiration-time\",\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"shareCredentialsVersion\": 1,\n\t\t\t\"bearerToken\":             \"token\",\n\t\t\t\"endpoint\":                \"endpoint\",\n\t\t\t\"expirationTime\":          \"expiration-time\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewMetastoreProvider(ctx, \"dbprovider\", \u0026databricks.MetastoreProviderArgs{\n\t\t\tName:                pulumi.String(\"terraform-test-provider\"),\n\t\t\tComment:             pulumi.String(\"made by terraform 2\"),\n\t\t\tAuthenticationType:  pulumi.String(\"TOKEN\"),\n\t\t\tRecipientProfileStr: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MetastoreProvider;\nimport com.pulumi.databricks.MetastoreProviderArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dbprovider = new MetastoreProvider(\"dbprovider\", MetastoreProviderArgs.builder()\n            .name(\"terraform-test-provider\")\n            .comment(\"made by terraform 2\")\n            .authenticationType(\"TOKEN\")\n            .recipientProfileStr(serializeJson(\n                jsonObject(\n                    jsonProperty(\"shareCredentialsVersion\", 1),\n                    jsonProperty(\"bearerToken\", \"token\"),\n                    jsonProperty(\"endpoint\", \"endpoint\"),\n                    jsonProperty(\"expirationTime\", \"expiration-time\")\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dbprovider:\n    type: databricks:MetastoreProvider\n    properties:\n      name: terraform-test-provider\n      comment: made by terraform 2\n      authenticationType: TOKEN\n      recipientProfileStr:\n        fn::toJSON:\n          shareCredentialsVersion: 1\n          bearerToken: token\n          endpoint: endpoint\n          expirationTime: expiration-time\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n",
            "properties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n"
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "required": [
                "authenticationType",
                "name",
                "recipientProfileStr"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "requiredInputs": [
                "authenticationType",
                "recipientProfileStr"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreProvider resources.\n",
                "properties": {
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the provider.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "recipientProfileStr": {
                        "type": "string",
                        "description": "This is the json file that is created from a recipient url.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowExperiment:MlflowExperiment": {
            "description": "This resource allows you to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.MlflowExperiment(\"this\", {\n    name: me.then(me =\u003e `${me.home}/Sample`),\n    artifactLocation: \"dbfs:/tmp/my-experiment\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.MlflowExperiment(\"this\",\n    name=f\"{me.home}/Sample\",\n    artifact_location=\"dbfs:/tmp/my-experiment\",\n    tags=[\n        {\n            \"key\": \"key1\",\n            \"value\": \"value1\",\n        },\n        {\n            \"key\": \"key2\",\n            \"value\": \"value2\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.MlflowExperiment(\"this\", new()\n    {\n        Name = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/Sample\",\n        ArtifactLocation = \"dbfs:/tmp/my-experiment\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowExperimentTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowExperimentTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowExperiment(ctx, \"this\", \u0026databricks.MlflowExperimentArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v/Sample\", me.Home),\n\t\t\tArtifactLocation: pulumi.String(\"dbfs:/tmp/my-experiment\"),\n\t\t\tTags: databricks.MlflowExperimentTagArray{\n\t\t\t\t\u0026databricks.MlflowExperimentTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowExperimentTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.MlflowExperiment;\nimport com.pulumi.databricks.MlflowExperimentArgs;\nimport com.pulumi.databricks.inputs.MlflowExperimentTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var this_ = new MlflowExperiment(\"this\", MlflowExperimentArgs.builder()\n            .name(String.format(\"%s/Sample\", me.home()))\n            .artifactLocation(\"dbfs:/tmp/my-experiment\")\n            .tags(            \n                MlflowExperimentTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowExperimentTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowExperiment\n    properties:\n      name: ${me.home}/Sample\n      artifactLocation: dbfs:/tmp/my-experiment\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, or *Manage* individual experiments.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create models in the [workspace model registry](https://docs.databricks.com/en/mlflow/model-registry.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe experiment resource can be imported using the id of the experiment\n\nbash\n\n```sh\n$ pulumi import databricks:index/mlflowExperiment:MlflowExperiment this \u003cexperiment-id\u003e\n```\n\n",
            "properties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "deprecationMessage": "Remove the description attribute as it no longer is used and will be removed in a future version."
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowExperimentTag:MlflowExperimentTag"
                    },
                    "description": "Tags for the MLflow experiment.\n"
                }
            },
            "required": [
                "creationTime",
                "experimentId",
                "lastUpdateTime",
                "lifecycleStage",
                "name",
                "tags"
            ],
            "inputProperties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "deprecationMessage": "Remove the description attribute as it no longer is used and will be removed in a future version."
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowExperimentTag:MlflowExperimentTag"
                    },
                    "description": "Tags for the MLflow experiment.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowExperiment resources.\n",
                "properties": {
                    "artifactLocation": {
                        "type": "string",
                        "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "deprecationMessage": "Remove the description attribute as it no longer is used and will be removed in a future version."
                    },
                    "experimentId": {
                        "type": "string"
                    },
                    "lastUpdateTime": {
                        "type": "integer"
                    },
                    "lifecycleStage": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MlflowExperimentTag:MlflowExperimentTag"
                        },
                        "description": "Tags for the MLflow experiment.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowModel:MlflowModel": {
            "description": "This resource allows you to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n\n\u003e This documentation covers the Workspace Model Registry. Databricks recommends using Models in Unity Catalog. Models in Unity Catalog provides centralized model governance, cross-workspace access, lineage, and deployment.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst test = new databricks.MlflowModel(\"test\", {\n    name: \"My MLflow Model\",\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest = databricks.MlflowModel(\"test\",\n    name=\"My MLflow Model\",\n    description=\"My MLflow model description\",\n    tags=[\n        {\n            \"key\": \"key1\",\n            \"value\": \"value1\",\n        },\n        {\n            \"key\": \"key2\",\n            \"value\": \"value2\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var test = new Databricks.MlflowModel(\"test\", new()\n    {\n        Name = \"My MLflow Model\",\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"test\", \u0026databricks.MlflowModelArgs{\n\t\t\tName:        pulumi.String(\"My MLflow Model\"),\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: databricks.MlflowModelTagArray{\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var test = new MlflowModel(\"test\", MlflowModelArgs.builder()\n            .name(\"My MLflow Model\")\n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  test:\n    type: databricks:MlflowModel\n    properties:\n      name: My MLflow Model\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, *Manage Staging Versions*, *Manage Production Versions*, and *Manage* individual models.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.Directory to manage directories in [Databricks Workspace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe model resource can be imported using the name\n\nbash\n\n```sh\n$ pulumi import databricks:index/mlflowModel:MlflowModel this \u003cname\u003e\n```\n\n",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                }
            },
            "required": [
                "name",
                "registeredModelId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n",
                    "willReplaceOnChanges": true
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowModel resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow model.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow model. Change of name triggers new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                        },
                        "description": "Tags for the MLflow model.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowWebhook:MlflowWebhook": {
            "description": "This resource allows you to create [MLflow Model Registry Webhooks](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) in Databricks.  Webhooks enable you to listen for Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. Webhooks allow trigger execution of a Databricks job or call a web service on specific event(s) that is generated in the MLflow Registry - stage transitioning, creation of registered model, creation of transition request, etc.\n\n## Example Usage\n\n### Triggering Databricks job\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst me = databricks.getCurrentUser({});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Notebook(\"this\", {\n    path: me.then(me =\u003e `${me.home}/MLFlowWebhook`),\n    language: \"PYTHON\",\n    contentBase64: std.base64encode({\n        input: `import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n`,\n    }).then(invoke =\u003e invoke.result),\n});\nconst thisJob = new databricks.Job(\"this\", {\n    name: me.then(me =\u003e `Pulumi MLflowWebhook Demo (${me.alphanumeric})`),\n    tasks: [{\n        taskKey: \"task1\",\n        newCluster: {\n            numWorkers: 1,\n            sparkVersion: latest.then(latest =\u003e latest.id),\n            nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n        },\n        notebookTask: {\n            notebookPath: _this.path,\n        },\n    }],\n});\nconst patForWebhook = new databricks.Token(\"pat_for_webhook\", {\n    comment: \"MLflow Webhook\",\n    lifetimeSeconds: 86400000,\n});\nconst job = new databricks.MlflowWebhook(\"job\", {\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    description: \"Databricks Job webhook trigger\",\n    status: \"ACTIVE\",\n    jobSpec: {\n        jobId: thisJob.id,\n        workspaceUrl: me.then(me =\u003e me.workspaceUrl),\n        accessToken: patForWebhook.tokenValue,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\nme = databricks.get_current_user()\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Notebook(\"this\",\n    path=f\"{me.home}/MLFlowWebhook\",\n    language=\"PYTHON\",\n    content_base64=std.base64encode(input=\"\"\"import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n\"\"\").result)\nthis_job = databricks.Job(\"this\",\n    name=f\"Pulumi MLflowWebhook Demo ({me.alphanumeric})\",\n    tasks=[{\n        \"task_key\": \"task1\",\n        \"new_cluster\": {\n            \"num_workers\": 1,\n            \"spark_version\": latest.id,\n            \"node_type_id\": smallest.id,\n        },\n        \"notebook_task\": {\n            \"notebook_path\": this.path,\n        },\n    }])\npat_for_webhook = databricks.Token(\"pat_for_webhook\",\n    comment=\"MLflow Webhook\",\n    lifetime_seconds=86400000)\njob = databricks.MlflowWebhook(\"job\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    description=\"Databricks Job webhook trigger\",\n    status=\"ACTIVE\",\n    job_spec={\n        \"job_id\": this_job.id,\n        \"workspace_url\": me.workspace_url,\n        \"access_token\": pat_for_webhook.token_value,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Notebook(\"this\", new()\n    {\n        Path = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/MLFlowWebhook\",\n        Language = \"PYTHON\",\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = @\"import json\n \nevent_message = dbutils.widgets.get(\"\"event_message\"\")\nevent_message_dict = json.loads(event_message)\nprint(f\"\"event data={event_message_dict}\"\")\n\",\n        }).Apply(invoke =\u003e invoke.Result),\n    });\n\n    var thisJob = new Databricks.Job(\"this\", new()\n    {\n        Name = $\"Pulumi MLflowWebhook Demo ({me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)})\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"task1\",\n                NewCluster = new Databricks.Inputs.JobTaskNewClusterArgs\n                {\n                    NumWorkers = 1,\n                    SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n                    NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n                },\n                NotebookTask = new Databricks.Inputs.JobTaskNotebookTaskArgs\n                {\n                    NotebookPath = @this.Path,\n                },\n            },\n        },\n    });\n\n    var patForWebhook = new Databricks.Token(\"pat_for_webhook\", new()\n    {\n        Comment = \"MLflow Webhook\",\n        LifetimeSeconds = 86400000,\n    });\n\n    var job = new Databricks.MlflowWebhook(\"job\", new()\n    {\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        Description = \"Databricks Job webhook trigger\",\n        Status = \"ACTIVE\",\n        JobSpec = new Databricks.Inputs.MlflowWebhookJobSpecArgs\n        {\n            JobId = thisJob.Id,\n            WorkspaceUrl = me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.WorkspaceUrl),\n            AccessToken = patForWebhook.TokenValue,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: `import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n`,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewNotebook(ctx, \"this\", \u0026databricks.NotebookArgs{\n\t\t\tPath:          pulumi.Sprintf(\"%v/MLFlowWebhook\", me.Home),\n\t\t\tLanguage:      pulumi.String(\"PYTHON\"),\n\t\t\tContentBase64: pulumi.String(invokeBase64encode.Result),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisJob, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.Sprintf(\"Pulumi MLflowWebhook Demo (%v)\", me.Alphanumeric),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"task1\"),\n\t\t\t\t\tNewCluster: \u0026databricks.JobTaskNewClusterArgs{\n\t\t\t\t\t\tNumWorkers:   pulumi.Int(1),\n\t\t\t\t\t\tSparkVersion: pulumi.String(latest.Id),\n\t\t\t\t\t\tNodeTypeId:   pulumi.String(smallest.Id),\n\t\t\t\t\t},\n\t\t\t\t\tNotebookTask: \u0026databricks.JobTaskNotebookTaskArgs{\n\t\t\t\t\t\tNotebookPath: this.Path,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpatForWebhook, err := databricks.NewToken(ctx, \"pat_for_webhook\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"MLflow Webhook\"),\n\t\t\tLifetimeSeconds: pulumi.Int(86400000),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowWebhook(ctx, \"job\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tDescription: pulumi.String(\"Databricks Job webhook trigger\"),\n\t\t\tStatus:      pulumi.String(\"ACTIVE\"),\n\t\t\tJobSpec: \u0026databricks.MlflowWebhookJobSpecArgs{\n\t\t\t\tJobId:        thisJob.ID(),\n\t\t\t\tWorkspaceUrl: pulumi.String(me.WorkspaceUrl),\n\t\t\t\tAccessToken:  patForWebhook.TokenValue,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.NotebookArgs;\nimport com.pulumi.std.StdFunctions;\nimport com.pulumi.std.inputs.Base64encodeArgs;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskNewClusterArgs;\nimport com.pulumi.databricks.inputs.JobTaskNotebookTaskArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookJobSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Notebook(\"this\", NotebookArgs.builder()\n            .path(String.format(\"%s/MLFlowWebhook\", me.home()))\n            .language(\"PYTHON\")\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"\"\"\nimport json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n                \"\"\")\n                .build()).result())\n            .build());\n\n        var thisJob = new Job(\"thisJob\", JobArgs.builder()\n            .name(String.format(\"Pulumi MLflowWebhook Demo (%s)\", me.alphanumeric()))\n            .tasks(JobTaskArgs.builder()\n                .taskKey(\"task1\")\n                .newCluster(JobTaskNewClusterArgs.builder()\n                    .numWorkers(1)\n                    .sparkVersion(latest.id())\n                    .nodeTypeId(smallest.id())\n                    .build())\n                .notebookTask(JobTaskNotebookTaskArgs.builder()\n                    .notebookPath(this_.path())\n                    .build())\n                .build())\n            .build());\n\n        var patForWebhook = new Token(\"patForWebhook\", TokenArgs.builder()\n            .comment(\"MLflow Webhook\")\n            .lifetimeSeconds(86400000)\n            .build());\n\n        var job = new MlflowWebhook(\"job\", MlflowWebhookArgs.builder()\n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .description(\"Databricks Job webhook trigger\")\n            .status(\"ACTIVE\")\n            .jobSpec(MlflowWebhookJobSpecArgs.builder()\n                .jobId(thisJob.id())\n                .workspaceUrl(me.workspaceUrl())\n                .accessToken(patForWebhook.tokenValue())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Notebook\n    properties:\n      path: ${me.home}/MLFlowWebhook\n      language: PYTHON\n      contentBase64:\n        fn::invoke:\n          function: std:base64encode\n          arguments:\n            input: \"import json\\n \\nevent_message = dbutils.widgets.get(\\\"event_message\\\")\\nevent_message_dict = json.loads(event_message)\\nprint(f\\\"event data={event_message_dict}\\\")\\n\"\n          return: result\n  thisJob:\n    type: databricks:Job\n    name: this\n    properties:\n      name: Pulumi MLflowWebhook Demo (${me.alphanumeric})\n      tasks:\n        - taskKey: task1\n          newCluster:\n            numWorkers: 1\n            sparkVersion: ${latest.id}\n            nodeTypeId: ${smallest.id}\n          notebookTask:\n            notebookPath: ${this.path}\n  patForWebhook:\n    type: databricks:Token\n    name: pat_for_webhook\n    properties:\n      comment: MLflow Webhook\n      lifetimeSeconds: 8.64e+07\n  job:\n    type: databricks:MlflowWebhook\n    properties:\n      events:\n        - TRANSITION_REQUEST_CREATED\n      description: Databricks Job webhook trigger\n      status: ACTIVE\n      jobSpec:\n        jobId: ${thisJob.id}\n        workspaceUrl: ${me.workspaceUrl}\n        accessToken: ${patForWebhook.tokenValue}\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n  latest:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments: {}\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### POSTing to URL\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst url = new databricks.MlflowWebhook(\"url\", {\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    description: \"URL webhook trigger\",\n    httpUrlSpec: {\n        url: \"https://my_cool_host/webhook\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nurl = databricks.MlflowWebhook(\"url\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    description=\"URL webhook trigger\",\n    http_url_spec={\n        \"url\": \"https://my_cool_host/webhook\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var url = new Databricks.MlflowWebhook(\"url\", new()\n    {\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        Description = \"URL webhook trigger\",\n        HttpUrlSpec = new Databricks.Inputs.MlflowWebhookHttpUrlSpecArgs\n        {\n            Url = \"https://my_cool_host/webhook\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowWebhook(ctx, \"url\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tDescription: pulumi.String(\"URL webhook trigger\"),\n\t\t\tHttpUrlSpec: \u0026databricks.MlflowWebhookHttpUrlSpecArgs{\n\t\t\t\tUrl: pulumi.String(\"https://my_cool_host/webhook\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookHttpUrlSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var url = new MlflowWebhook(\"url\", MlflowWebhookArgs.builder()\n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .description(\"URL webhook trigger\")\n            .httpUrlSpec(MlflowWebhookHttpUrlSpecArgs.builder()\n                .url(\"https://my_cool_host/webhook\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  url:\n    type: databricks:MlflowWebhook\n    properties:\n      events:\n        - TRANSITION_REQUEST_CREATED\n      description: URL webhook trigger\n      httpUrlSpec:\n        url: https://my_cool_host/webhook\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* MLflow webhooks could be configured only by workspace admins.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "required": [
                "events"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "requiredInputs": [
                "events"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowWebhook resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Optional description of the MLflow webhook.\n"
                    },
                    "events": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                    },
                    "httpUrlSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                    },
                    "jobSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                    },
                    "modelName": {
                        "type": "string",
                        "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/modelServing:ModelServing": {
            "description": "This resource allows you to manage [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.\n\n\u003e If you replace `served_models` with `served_entities` in an existing serving endpoint, the serving endpoint will briefly go into an update state (~30 seconds) and increment the config version.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ModelServing(\"this\", {\n    name: \"ads-serving-endpoint\",\n    config: {\n        servedEntities: [\n            {\n                name: \"prod_model\",\n                entityName: \"ads-model\",\n                entityVersion: \"2\",\n                workloadSize: \"Small\",\n                scaleToZeroEnabled: true,\n            },\n            {\n                name: \"candidate_model\",\n                entityName: \"ads-model\",\n                entityVersion: \"4\",\n                workloadSize: \"Small\",\n                scaleToZeroEnabled: false,\n            },\n        ],\n        trafficConfig: {\n            routes: [\n                {\n                    servedModelName: \"prod_model\",\n                    trafficPercentage: 90,\n                },\n                {\n                    servedModelName: \"candidate_model\",\n                    trafficPercentage: 10,\n                },\n            ],\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.ModelServing(\"this\",\n    name=\"ads-serving-endpoint\",\n    config={\n        \"served_entities\": [\n            {\n                \"name\": \"prod_model\",\n                \"entity_name\": \"ads-model\",\n                \"entity_version\": \"2\",\n                \"workload_size\": \"Small\",\n                \"scale_to_zero_enabled\": True,\n            },\n            {\n                \"name\": \"candidate_model\",\n                \"entity_name\": \"ads-model\",\n                \"entity_version\": \"4\",\n                \"workload_size\": \"Small\",\n                \"scale_to_zero_enabled\": False,\n            },\n        ],\n        \"traffic_config\": {\n            \"routes\": [\n                {\n                    \"served_model_name\": \"prod_model\",\n                    \"traffic_percentage\": 90,\n                },\n                {\n                    \"served_model_name\": \"candidate_model\",\n                    \"traffic_percentage\": 10,\n                },\n            ],\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ModelServing(\"this\", new()\n    {\n        Name = \"ads-serving-endpoint\",\n        Config = new Databricks.Inputs.ModelServingConfigArgs\n        {\n            ServedEntities = new[]\n            {\n                new Databricks.Inputs.ModelServingConfigServedEntityArgs\n                {\n                    Name = \"prod_model\",\n                    EntityName = \"ads-model\",\n                    EntityVersion = \"2\",\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = true,\n                },\n                new Databricks.Inputs.ModelServingConfigServedEntityArgs\n                {\n                    Name = \"candidate_model\",\n                    EntityName = \"ads-model\",\n                    EntityVersion = \"4\",\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = false,\n                },\n            },\n            TrafficConfig = new Databricks.Inputs.ModelServingConfigTrafficConfigArgs\n            {\n                Routes = new[]\n                {\n                    new Databricks.Inputs.ModelServingConfigTrafficConfigRouteArgs\n                    {\n                        ServedModelName = \"prod_model\",\n                        TrafficPercentage = 90,\n                    },\n                    new Databricks.Inputs.ModelServingConfigTrafficConfigRouteArgs\n                    {\n                        ServedModelName = \"candidate_model\",\n                        TrafficPercentage = 10,\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewModelServing(ctx, \"this\", \u0026databricks.ModelServingArgs{\n\t\t\tName: pulumi.String(\"ads-serving-endpoint\"),\n\t\t\tConfig: \u0026databricks.ModelServingConfigArgs{\n\t\t\t\tServedEntities: databricks.ModelServingConfigServedEntityArray{\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedEntityArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"prod_model\"),\n\t\t\t\t\t\tEntityName:         pulumi.String(\"ads-model\"),\n\t\t\t\t\t\tEntityVersion:      pulumi.String(\"2\"),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(true),\n\t\t\t\t\t},\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedEntityArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"candidate_model\"),\n\t\t\t\t\t\tEntityName:         pulumi.String(\"ads-model\"),\n\t\t\t\t\t\tEntityVersion:      pulumi.String(\"4\"),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(false),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tTrafficConfig: \u0026databricks.ModelServingConfigTrafficConfigArgs{\n\t\t\t\t\tRoutes: databricks.ModelServingConfigTrafficConfigRouteArray{\n\t\t\t\t\t\t\u0026databricks.ModelServingConfigTrafficConfigRouteArgs{\n\t\t\t\t\t\t\tServedModelName:   pulumi.String(\"prod_model\"),\n\t\t\t\t\t\t\tTrafficPercentage: pulumi.Int(90),\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\u0026databricks.ModelServingConfigTrafficConfigRouteArgs{\n\t\t\t\t\t\t\tServedModelName:   pulumi.String(\"candidate_model\"),\n\t\t\t\t\t\t\tTrafficPercentage: pulumi.Int(10),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ModelServing;\nimport com.pulumi.databricks.ModelServingArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigTrafficConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ModelServing(\"this\", ModelServingArgs.builder()\n            .name(\"ads-serving-endpoint\")\n            .config(ModelServingConfigArgs.builder()\n                .servedEntities(                \n                    ModelServingConfigServedEntityArgs.builder()\n                        .name(\"prod_model\")\n                        .entityName(\"ads-model\")\n                        .entityVersion(\"2\")\n                        .workloadSize(\"Small\")\n                        .scaleToZeroEnabled(true)\n                        .build(),\n                    ModelServingConfigServedEntityArgs.builder()\n                        .name(\"candidate_model\")\n                        .entityName(\"ads-model\")\n                        .entityVersion(\"4\")\n                        .workloadSize(\"Small\")\n                        .scaleToZeroEnabled(false)\n                        .build())\n                .trafficConfig(ModelServingConfigTrafficConfigArgs.builder()\n                    .routes(                    \n                        ModelServingConfigTrafficConfigRouteArgs.builder()\n                            .servedModelName(\"prod_model\")\n                            .trafficPercentage(90)\n                            .build(),\n                        ModelServingConfigTrafficConfigRouteArgs.builder()\n                            .servedModelName(\"candidate_model\")\n                            .trafficPercentage(10)\n                            .build())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ModelServing\n    properties:\n      name: ads-serving-endpoint\n      config:\n        servedEntities:\n          - name: prod_model\n            entityName: ads-model\n            entityVersion: '2'\n            workloadSize: Small\n            scaleToZeroEnabled: true\n          - name: candidate_model\n            entityName: ads-model\n            entityVersion: '4'\n            workloadSize: Small\n            scaleToZeroEnabled: false\n        trafficConfig:\n          routes:\n            - servedModelName: prod_model\n              trafficPercentage: 90\n            - servedModelName: candidate_model\n              trafficPercentage: 10\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workspace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create models in the [workspace model registry](https://docs.databricks.com/en/mlflow/model-registry.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe model serving resource can be imported using the name of the endpoint.\n\nbash\n\n```sh\n$ pulumi import databricks:index/modelServing:ModelServing this \u003cmodel-serving-endpoint-name\u003e\n```\n\n",
            "properties": {
                "aiGateway": {
                    "$ref": "#/types/databricks:index/ModelServingAiGateway:ModelServingAiGateway",
                    "description": "A block with AI Gateway configuration for the serving endpoint. *Note: only external model endpoints are supported as of now.*\n"
                },
                "budgetPolicyId": {
                    "type": "string"
                },
                "config": {
                    "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                    "description": "The model serving endpoint configuration. This is optional and can be added and modified after creation. If `config` was provided in a previous apply but is not provided in the current apply, no change to the model serving endpoint will occur. To recreate the model serving endpoint without the `config` block, the model serving endpoint must be destroyed and recreated.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.\n"
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                    },
                    "description": "A list of rate limit blocks to be applied to the serving endpoint. *Note: only external and foundation model endpoints are supported as of now.*\n"
                },
                "routeOptimized": {
                    "type": "boolean",
                    "description": "A boolean enabling route optimization for the endpoint. *Note: only available for custom models.*\n"
                },
                "servingEndpointId": {
                    "type": "string",
                    "description": "Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                    },
                    "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                }
            },
            "required": [
                "config",
                "name",
                "servingEndpointId"
            ],
            "inputProperties": {
                "aiGateway": {
                    "$ref": "#/types/databricks:index/ModelServingAiGateway:ModelServingAiGateway",
                    "description": "A block with AI Gateway configuration for the serving endpoint. *Note: only external model endpoints are supported as of now.*\n"
                },
                "budgetPolicyId": {
                    "type": "string"
                },
                "config": {
                    "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                    "description": "The model serving endpoint configuration. This is optional and can be added and modified after creation. If `config` was provided in a previous apply but is not provided in the current apply, no change to the model serving endpoint will occur. To recreate the model serving endpoint without the `config` block, the model serving endpoint must be destroyed and recreated.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.\n",
                    "willReplaceOnChanges": true
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                    },
                    "description": "A list of rate limit blocks to be applied to the serving endpoint. *Note: only external and foundation model endpoints are supported as of now.*\n"
                },
                "routeOptimized": {
                    "type": "boolean",
                    "description": "A boolean enabling route optimization for the endpoint. *Note: only available for custom models.*\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                    },
                    "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ModelServing resources.\n",
                "properties": {
                    "aiGateway": {
                        "$ref": "#/types/databricks:index/ModelServingAiGateway:ModelServingAiGateway",
                        "description": "A block with AI Gateway configuration for the serving endpoint. *Note: only external model endpoints are supported as of now.*\n"
                    },
                    "budgetPolicyId": {
                        "type": "string"
                    },
                    "config": {
                        "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                        "description": "The model serving endpoint configuration. This is optional and can be added and modified after creation. If `config` was provided in a previous apply but is not provided in the current apply, no change to the model serving endpoint will occur. To recreate the model serving endpoint without the `config` block, the model serving endpoint must be destroyed and recreated.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the updated name.\n",
                        "willReplaceOnChanges": true
                    },
                    "rateLimits": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                        },
                        "description": "A list of rate limit blocks to be applied to the serving endpoint. *Note: only external and foundation model endpoints are supported as of now.*\n"
                    },
                    "routeOptimized": {
                        "type": "boolean",
                        "description": "A boolean enabling route optimization for the endpoint. *Note: only available for custom models.*\n"
                    },
                    "servingEndpointId": {
                        "type": "string",
                        "description": "Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                        },
                        "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mount:Mount": {
            "description": "\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs"
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl"
                },
                "clusterId": {
                    "type": "string"
                },
                "encryptionType": {
                    "type": "string"
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs"
                },
                "name": {
                    "type": "string"
                },
                "resourceId": {
                    "type": "string"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3"
                },
                "source": {
                    "type": "string",
                    "description": "(String) HDFS-compatible url\n"
                },
                "uri": {
                    "type": "string"
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb"
                }
            },
            "required": [
                "clusterId",
                "name",
                "source"
            ],
            "inputProperties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                    "willReplaceOnChanges": true
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "encryptionType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3",
                    "willReplaceOnChanges": true
                },
                "uri": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Mount resources.\n",
                "properties": {
                    "abfs": {
                        "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                        "willReplaceOnChanges": true
                    },
                    "adl": {
                        "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "encryptionType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "extraConfigs": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "willReplaceOnChanges": true
                    },
                    "gs": {
                        "$ref": "#/types/databricks:index/MountGs:MountGs",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "s3": {
                        "$ref": "#/types/databricks:index/MountS3:MountS3",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "(String) HDFS-compatible url\n"
                    },
                    "uri": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "wasb": {
                        "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCredentials:MwsCredentials": {
            "description": "## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\n// Names of created resources will be prefixed with this value\nconst prefix = config.requireObject\u003cany\u003e(\"prefix\");\nconst _this = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccountRole = new aws.iam.Role(\"cross_account_role\", {\n    name: `${prefix}-crossaccount`,\n    assumeRolePolicy: _this.then(_this =\u003e _this.json),\n    tags: tags,\n});\nconst thisGetAwsCrossAccountPolicy = databricks.getAwsCrossAccountPolicy({});\nconst thisRolePolicy = new aws.iam.RolePolicy(\"this\", {\n    name: `${prefix}-policy`,\n    role: crossAccountRole.id,\n    policy: thisGetAwsCrossAccountPolicy.then(thisGetAwsCrossAccountPolicy =\u003e thisGetAwsCrossAccountPolicy.json),\n});\nconst thisMwsCredentials = new databricks.MwsCredentials(\"this\", {\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossAccountRole.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# Names of created resources will be prefixed with this value\nprefix = config.require_object(\"prefix\")\nthis = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account_role = aws.iam.Role(\"cross_account_role\",\n    name=f\"{prefix}-crossaccount\",\n    assume_role_policy=this.json,\n    tags=tags)\nthis_get_aws_cross_account_policy = databricks.get_aws_cross_account_policy()\nthis_role_policy = aws.iam.RolePolicy(\"this\",\n    name=f\"{prefix}-policy\",\n    role=cross_account_role.id,\n    policy=this_get_aws_cross_account_policy.json)\nthis_mws_credentials = databricks.MwsCredentials(\"this\",\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=cross_account_role.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // Names of created resources will be prefixed with this value\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var @this = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccountRole = new Aws.Iam.Role(\"cross_account_role\", new()\n    {\n        Name = $\"{prefix}-crossaccount\",\n        AssumeRolePolicy = @this.Apply(@this =\u003e @this.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json)),\n        Tags = tags,\n    });\n\n    var thisGetAwsCrossAccountPolicy = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var thisRolePolicy = new Aws.Iam.RolePolicy(\"this\", new()\n    {\n        Name = $\"{prefix}-policy\",\n        Role = crossAccountRole.Id,\n        Policy = thisGetAwsCrossAccountPolicy.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json),\n    });\n\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"this\", new()\n    {\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossAccountRole.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// Names of created resources will be prefixed with this value\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tthis, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountRole, err := iam.NewRole(ctx, \"cross_account_role\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v-crossaccount\", prefix),\n\t\t\tAssumeRolePolicy: pulumi.String(this.Json),\n\t\t\tTags:             pulumi.Any(tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsCrossAccountPolicy, err := databricks.GetAwsCrossAccountPolicy(ctx, \u0026databricks.GetAwsCrossAccountPolicyArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicy(ctx, \"this\", \u0026iam.RolePolicyArgs{\n\t\t\tName:   pulumi.Sprintf(\"%v-policy\", prefix),\n\t\t\tRole:   crossAccountRole.ID(),\n\t\t\tPolicy: pulumi.String(thisGetAwsCrossAccountPolicy.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tCredentialsName: pulumi.Sprintf(\"%v-creds\", prefix),\n\t\t\tRoleArn:         crossAccountRole.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.RolePolicy;\nimport com.pulumi.aws.iam.RolePolicyArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var prefix = config.get(\"prefix\");\n        final var this = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccountRole = new Role(\"crossAccountRole\", RoleArgs.builder()\n            .name(String.format(\"%s-crossaccount\", prefix))\n            .assumeRolePolicy(this_.json())\n            .tags(tags)\n            .build());\n\n        final var thisGetAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()\n            .build());\n\n        var thisRolePolicy = new RolePolicy(\"thisRolePolicy\", RolePolicyArgs.builder()\n            .name(String.format(\"%s-policy\", prefix))\n            .role(crossAccountRole.id())\n            .policy(thisGetAwsCrossAccountPolicy.json())\n            .build());\n\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossAccountRole.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  crossAccountRole:\n    type: aws:iam:Role\n    name: cross_account_role\n    properties:\n      name: ${prefix}-crossaccount\n      assumeRolePolicy: ${this.json}\n      tags: ${tags}\n  thisRolePolicy:\n    type: aws:iam:RolePolicy\n    name: this\n    properties:\n      name: ${prefix}-policy\n      role: ${crossAccountRole.id}\n      policy: ${thisGetAwsCrossAccountPolicy.json}\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    name: this\n    properties:\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossAccountRole.arn}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsAssumeRolePolicy\n      arguments:\n        externalId: ${databricksAccountId}\n  thisGetAwsCrossAccountPolicy:\n    fn::invoke:\n      function: databricks:getAwsCrossAccountPolicy\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by the combination of its identifier and the account id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/mwsCredentials:MwsCredentials this \u003caccount_id\u003e/\u003ccredentials_id\u003e\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "**(Deprecated)** Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where `host = \"https://accounts.cloud.databricks.com\"`\n",
                    "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource."
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n"
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n"
                }
            },
            "required": [
                "creationTime",
                "credentialsId",
                "credentialsName",
                "externalId",
                "roleArn"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "**(Deprecated)** Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where `host = \"https://accounts.cloud.databricks.com\"`\n",
                    "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource.",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string",
                    "description": "name of credentials to register\n",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of cross-account role\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "credentialsName",
                "roleArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCredentials resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "**(Deprecated)** Maintained for backwards compatibility and will be removed in a later version. It should now be specified under a provider instance where `host = \"https://accounts.cloud.databricks.com\"`\n",
                        "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource.",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time of credentials registration\n"
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "(String) identifier of credentials\n"
                    },
                    "credentialsName": {
                        "type": "string",
                        "description": "name of credentials to register\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "roleArn": {
                        "type": "string",
                        "description": "ARN of cross-account role\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCustomerManagedKeys:MwsCustomerManagedKeys": {
            "description": "## Example Usage\n\n\u003e If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.\n\n### Customer-managed key for managed services\n\nYou must configure this during workspace creation\n\n### For AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst current = aws.getCallerIdentity({});\nconst databricksManagedServicesCmk = current.then(current =\u003e aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for control plane managed services\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources: [\"*\"],\n        },\n    ],\n}));\nconst managedServicesCustomerManagedKey = new aws.kms.Key(\"managed_services_customer_managed_key\", {policy: databricksManagedServicesCmk.then(databricksManagedServicesCmk =\u003e databricksManagedServicesCmk.json)});\nconst managedServicesCustomerManagedKeyAlias = new aws.kms.Alias(\"managed_services_customer_managed_key_alias\", {\n    name: \"alias/managed-services-customer-managed-key-alias\",\n    targetKeyId: managedServicesCustomerManagedKey.keyId,\n});\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managed_services\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: managedServicesCustomerManagedKey.arn,\n        keyAlias: managedServicesCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ncurrent = aws.get_caller_identity()\ndatabricks_managed_services_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        {\n            \"sid\": \"Enable IAM User Permissions\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [current.account_id],\n            }],\n            \"actions\": [\"kms:*\"],\n            \"resources\": [\"*\"],\n        },\n        {\n            \"sid\": \"Allow Databricks to use KMS key for control plane managed services\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [\"arn:aws:iam::414351767826:root\"],\n            }],\n            \"actions\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            \"resources\": [\"*\"],\n        },\n    ])\nmanaged_services_customer_managed_key = aws.kms.Key(\"managed_services_customer_managed_key\", policy=databricks_managed_services_cmk.json)\nmanaged_services_customer_managed_key_alias = aws.kms.Alias(\"managed_services_customer_managed_key_alias\",\n    name=\"alias/managed-services-customer-managed-key-alias\",\n    target_key_id=managed_services_customer_managed_key.key_id)\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managed_services\",\n    account_id=databricks_account_id,\n    aws_key_info={\n        \"key_arn\": managed_services_customer_managed_key.arn,\n        \"key_alias\": managed_services_customer_managed_key_alias.name,\n    },\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var current = Aws.GetCallerIdentity.Invoke();\n\n    var databricksManagedServicesCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.Apply(getCallerIdentityResult =\u003e getCallerIdentityResult.AccountId),\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for control plane managed services\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n        },\n    });\n\n    var managedServicesCustomerManagedKey = new Aws.Kms.Key(\"managed_services_customer_managed_key\", new()\n    {\n        Policy = databricksManagedServicesCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var managedServicesCustomerManagedKeyAlias = new Aws.Kms.Alias(\"managed_services_customer_managed_key_alias\", new()\n    {\n        Name = \"alias/managed-services-customer-managed-key-alias\",\n        TargetKeyId = managedServicesCustomerManagedKey.KeyId,\n    });\n\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managed_services\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = managedServicesCustomerManagedKey.Arn,\n            KeyAlias = managedServicesCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/kms\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\nfunc main() {\npulumi.Run(func(ctx *pulumi.Context) error {\ncfg := config.New(ctx, \"\")\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\ncurrent, err := aws.GetCallerIdentity(ctx, \u0026aws.GetCallerIdentityArgs{\n}, nil);\nif err != nil {\nreturn err\n}\ndatabricksManagedServicesCmk, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\nVersion: pulumi.StringRef(\"2012-10-17\"),\nStatements: []iam.GetPolicyDocumentStatement{\n{\nSid: pulumi.StringRef(\"Enable IAM User Permissions\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ncurrent.AccountId,\n},\n},\n},\nActions: []string{\n\"kms:*\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for control plane managed services\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n},\nResources: []string{\n\"*\",\n},\n},\n},\n}, nil);\nif err != nil {\nreturn err\n}\nmanagedServicesCustomerManagedKey, err := kms.NewKey(ctx, \"managed_services_customer_managed_key\", \u0026kms.KeyArgs{\nPolicy: pulumi.String(databricksManagedServicesCmk.Json),\n})\nif err != nil {\nreturn err\n}\nmanagedServicesCustomerManagedKeyAlias, err := kms.NewAlias(ctx, \"managed_services_customer_managed_key_alias\", \u0026kms.AliasArgs{\nName: pulumi.String(\"alias/managed-services-customer-managed-key-alias\"),\nTargetKeyId: managedServicesCustomerManagedKey.KeyId,\n})\nif err != nil {\nreturn err\n}\n_, err = databricks.NewMwsCustomerManagedKeys(ctx, \"managed_services\", \u0026databricks.MwsCustomerManagedKeysArgs{\nAccountId: pulumi.Any(databricksAccountId),\nAwsKeyInfo: \u0026databricks.MwsCustomerManagedKeysAwsKeyInfoArgs{\nKeyArn: managedServicesCustomerManagedKey.Arn,\nKeyAlias: managedServicesCustomerManagedKeyAlias.Name,\n},\nUseCases: pulumi.StringArray{\npulumi.String(\"MANAGED_SERVICES\"),\n},\n})\nif err != nil {\nreturn err\n}\nreturn nil\n})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.AwsFunctions;\nimport com.pulumi.aws.inputs.GetCallerIdentityArgs;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var current = AwsFunctions.getCallerIdentity(GetCallerIdentityArgs.builder()\n            .build());\n\n        final var databricksManagedServicesCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.accountId())\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for control plane managed services\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\")\n                    .resources(\"*\")\n                    .build())\n            .build());\n\n        var managedServicesCustomerManagedKey = new Key(\"managedServicesCustomerManagedKey\", KeyArgs.builder()\n            .policy(databricksManagedServicesCmk.json())\n            .build());\n\n        var managedServicesCustomerManagedKeyAlias = new Alias(\"managedServicesCustomerManagedKeyAlias\", AliasArgs.builder()\n            .name(\"alias/managed-services-customer-managed-key-alias\")\n            .targetKeyId(managedServicesCustomerManagedKey.keyId())\n            .build());\n\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()\n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(managedServicesCustomerManagedKey.arn())\n                .keyAlias(managedServicesCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  managedServicesCustomerManagedKey:\n    type: aws:kms:Key\n    name: managed_services_customer_managed_key\n    properties:\n      policy: ${databricksManagedServicesCmk.json}\n  managedServicesCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    name: managed_services_customer_managed_key_alias\n    properties:\n      name: alias/managed-services-customer-managed-key-alias\n      targetKeyId: ${managedServicesCustomerManagedKey.keyId}\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    name: managed_services\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${managedServicesCustomerManagedKey.arn}\n        keyAlias: ${managedServicesCustomerManagedKeyAlias.name}\n      useCases:\n        - MANAGED_SERVICES\nvariables:\n  current:\n    fn::invoke:\n      function: aws:getCallerIdentity\n      arguments: {}\n  databricksManagedServicesCmk:\n    fn::invoke:\n      function: aws:iam:getPolicyDocument\n      arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for control plane managed services\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n            resources:\n              - '*'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### For GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\n// Id of a google_kms_crypto_key\nconst cmekResourceId = config.requireObject\u003cany\u003e(\"cmekResourceId\");\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managed_services\", {\n    accountId: databricksAccountId,\n    gcpKeyInfo: {\n        kmsKeyId: cmekResourceId,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# Id of a google_kms_crypto_key\ncmek_resource_id = config.require_object(\"cmekResourceId\")\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managed_services\",\n    account_id=databricks_account_id,\n    gcp_key_info={\n        \"kms_key_id\": cmek_resource_id,\n    },\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // Id of a google_kms_crypto_key\n    var cmekResourceId = config.RequireObject\u003cdynamic\u003e(\"cmekResourceId\");\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managed_services\", new()\n    {\n        AccountId = databricksAccountId,\n        GcpKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysGcpKeyInfoArgs\n        {\n            KmsKeyId = cmekResourceId,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// Id of a google_kms_crypto_key\n\t\tcmekResourceId := cfg.RequireObject(\"cmekResourceId\")\n\t\t_, err := databricks.NewMwsCustomerManagedKeys(ctx, \"managed_services\", \u0026databricks.MwsCustomerManagedKeysArgs{\n\t\t\tAccountId: pulumi.Any(databricksAccountId),\n\t\t\tGcpKeyInfo: \u0026databricks.MwsCustomerManagedKeysGcpKeyInfoArgs{\n\t\t\t\tKmsKeyId: pulumi.Any(cmekResourceId),\n\t\t\t},\n\t\t\tUseCases: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"MANAGED_SERVICES\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysGcpKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var cmekResourceId = config.get(\"cmekResourceId\");\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()\n            .accountId(databricksAccountId)\n            .gcpKeyInfo(MwsCustomerManagedKeysGcpKeyInfoArgs.builder()\n                .kmsKeyId(cmekResourceId)\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  cmekResourceId:\n    type: dynamic\nresources:\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    name: managed_services\n    properties:\n      accountId: ${databricksAccountId}\n      gcpKeyInfo:\n        kmsKeyId: ${cmekResourceId}\n      useCases:\n        - MANAGED_SERVICES\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Customer-managed key for workspace storage\n\n### For AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\n// AWS ARN for the Databricks cross account role\nconst databricksCrossAccountRole = config.requireObject\u003cany\u003e(\"databricksCrossAccountRole\");\nconst current = aws.getCallerIdentity({});\nconst databricksStorageCmk = current.then(current =\u003e aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"Bool\",\n                variable: \"kms:GrantIsForAWSResource\",\n                values: [\"true\"],\n            }],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for EBS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [databricksCrossAccountRole],\n            }],\n            actions: [\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"ForAnyValue:StringLike\",\n                variable: \"kms:ViaService\",\n                values: [\"ec2.*.amazonaws.com\"],\n            }],\n        },\n    ],\n}));\nconst storageCustomerManagedKey = new aws.kms.Key(\"storage_customer_managed_key\", {policy: databricksStorageCmk.then(databricksStorageCmk =\u003e databricksStorageCmk.json)});\nconst storageCustomerManagedKeyAlias = new aws.kms.Alias(\"storage_customer_managed_key_alias\", {\n    name: \"alias/storage-customer-managed-key-alias\",\n    targetKeyId: storageCustomerManagedKey.keyId,\n});\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: storageCustomerManagedKey.arn,\n        keyAlias: storageCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# AWS ARN for the Databricks cross account role\ndatabricks_cross_account_role = config.require_object(\"databricksCrossAccountRole\")\ncurrent = aws.get_caller_identity()\ndatabricks_storage_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        {\n            \"sid\": \"Enable IAM User Permissions\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [current.account_id],\n            }],\n            \"actions\": [\"kms:*\"],\n            \"resources\": [\"*\"],\n        },\n        {\n            \"sid\": \"Allow Databricks to use KMS key for DBFS\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [\"arn:aws:iam::414351767826:root\"],\n            }],\n            \"actions\": [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            \"resources\": [\"*\"],\n        },\n        {\n            \"sid\": \"Allow Databricks to use KMS key for DBFS (Grants)\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [\"arn:aws:iam::414351767826:root\"],\n            }],\n            \"actions\": [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            \"resources\": [\"*\"],\n            \"conditions\": [{\n                \"test\": \"Bool\",\n                \"variable\": \"kms:GrantIsForAWSResource\",\n                \"values\": [\"true\"],\n            }],\n        },\n        {\n            \"sid\": \"Allow Databricks to use KMS key for EBS\",\n            \"effect\": \"Allow\",\n            \"principals\": [{\n                \"type\": \"AWS\",\n                \"identifiers\": [databricks_cross_account_role],\n            }],\n            \"actions\": [\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            \"resources\": [\"*\"],\n            \"conditions\": [{\n                \"test\": \"ForAnyValue:StringLike\",\n                \"variable\": \"kms:ViaService\",\n                \"values\": [\"ec2.*.amazonaws.com\"],\n            }],\n        },\n    ])\nstorage_customer_managed_key = aws.kms.Key(\"storage_customer_managed_key\", policy=databricks_storage_cmk.json)\nstorage_customer_managed_key_alias = aws.kms.Alias(\"storage_customer_managed_key_alias\",\n    name=\"alias/storage-customer-managed-key-alias\",\n    target_key_id=storage_customer_managed_key.key_id)\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    aws_key_info={\n        \"key_arn\": storage_customer_managed_key.arn,\n        \"key_alias\": storage_customer_managed_key_alias.name,\n    },\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // AWS ARN for the Databricks cross account role\n    var databricksCrossAccountRole = config.RequireObject\u003cdynamic\u003e(\"databricksCrossAccountRole\");\n    var current = Aws.GetCallerIdentity.Invoke();\n\n    var databricksStorageCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.Apply(getCallerIdentityResult =\u003e getCallerIdentityResult.AccountId),\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                    \"kms:ReEncrypt*\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS (Grants)\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:CreateGrant\",\n                    \"kms:ListGrants\",\n                    \"kms:RevokeGrant\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"Bool\",\n                        Variable = \"kms:GrantIsForAWSResource\",\n                        Values = new[]\n                        {\n                            \"true\",\n                        },\n                    },\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for EBS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            databricksCrossAccountRole,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Decrypt\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:CreateGrant\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"ForAnyValue:StringLike\",\n                        Variable = \"kms:ViaService\",\n                        Values = new[]\n                        {\n                            \"ec2.*.amazonaws.com\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var storageCustomerManagedKey = new Aws.Kms.Key(\"storage_customer_managed_key\", new()\n    {\n        Policy = databricksStorageCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var storageCustomerManagedKeyAlias = new Aws.Kms.Alias(\"storage_customer_managed_key_alias\", new()\n    {\n        Name = \"alias/storage-customer-managed-key-alias\",\n        TargetKeyId = storageCustomerManagedKey.KeyId,\n    });\n\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = storageCustomerManagedKey.Arn,\n            KeyAlias = storageCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/kms\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\nfunc main() {\npulumi.Run(func(ctx *pulumi.Context) error {\ncfg := config.New(ctx, \"\")\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n// AWS ARN for the Databricks cross account role\ndatabricksCrossAccountRole := cfg.RequireObject(\"databricksCrossAccountRole\")\ncurrent, err := aws.GetCallerIdentity(ctx, \u0026aws.GetCallerIdentityArgs{\n}, nil);\nif err != nil {\nreturn err\n}\ndatabricksStorageCmk, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\nVersion: pulumi.StringRef(\"2012-10-17\"),\nStatements: []iam.GetPolicyDocumentStatement{\n{\nSid: pulumi.StringRef(\"Enable IAM User Permissions\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ncurrent.AccountId,\n},\n},\n},\nActions: []string{\n\"kms:*\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for DBFS\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n\"kms:ReEncrypt*\",\n\"kms:GenerateDataKey*\",\n\"kms:DescribeKey\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for DBFS (Grants)\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:CreateGrant\",\n\"kms:ListGrants\",\n\"kms:RevokeGrant\",\n},\nResources: []string{\n\"*\",\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"Bool\",\nVariable: \"kms:GrantIsForAWSResource\",\nValues: []string{\n\"true\",\n},\n},\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for EBS\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ndatabricksCrossAccountRole,\n},\n},\n},\nActions: []string{\n\"kms:Decrypt\",\n\"kms:GenerateDataKey*\",\n\"kms:CreateGrant\",\n\"kms:DescribeKey\",\n},\nResources: []string{\n\"*\",\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"ForAnyValue:StringLike\",\nVariable: \"kms:ViaService\",\nValues: []string{\n\"ec2.*.amazonaws.com\",\n},\n},\n},\n},\n},\n}, nil);\nif err != nil {\nreturn err\n}\nstorageCustomerManagedKey, err := kms.NewKey(ctx, \"storage_customer_managed_key\", \u0026kms.KeyArgs{\nPolicy: pulumi.String(databricksStorageCmk.Json),\n})\nif err != nil {\nreturn err\n}\nstorageCustomerManagedKeyAlias, err := kms.NewAlias(ctx, \"storage_customer_managed_key_alias\", \u0026kms.AliasArgs{\nName: pulumi.String(\"alias/storage-customer-managed-key-alias\"),\nTargetKeyId: storageCustomerManagedKey.KeyId,\n})\nif err != nil {\nreturn err\n}\n_, err = databricks.NewMwsCustomerManagedKeys(ctx, \"storage\", \u0026databricks.MwsCustomerManagedKeysArgs{\nAccountId: pulumi.Any(databricksAccountId),\nAwsKeyInfo: \u0026databricks.MwsCustomerManagedKeysAwsKeyInfoArgs{\nKeyArn: storageCustomerManagedKey.Arn,\nKeyAlias: storageCustomerManagedKeyAlias.Name,\n},\nUseCases: pulumi.StringArray{\npulumi.String(\"STORAGE\"),\n},\n})\nif err != nil {\nreturn err\n}\nreturn nil\n})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.AwsFunctions;\nimport com.pulumi.aws.inputs.GetCallerIdentityArgs;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksCrossAccountRole = config.get(\"databricksCrossAccountRole\");\n        final var current = AwsFunctions.getCallerIdentity(GetCallerIdentityArgs.builder()\n            .build());\n\n        final var databricksStorageCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.accountId())\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\",\n                        \"kms:ReEncrypt*\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS (Grants)\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:CreateGrant\",\n                        \"kms:ListGrants\",\n                        \"kms:RevokeGrant\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"Bool\")\n                        .variable(\"kms:GrantIsForAWSResource\")\n                        .values(\"true\")\n                        .build())\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for EBS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(databricksCrossAccountRole)\n                        .build())\n                    .actions(                    \n                        \"kms:Decrypt\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:CreateGrant\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"ForAnyValue:StringLike\")\n                        .variable(\"kms:ViaService\")\n                        .values(\"ec2.*.amazonaws.com\")\n                        .build())\n                    .build())\n            .build());\n\n        var storageCustomerManagedKey = new Key(\"storageCustomerManagedKey\", KeyArgs.builder()\n            .policy(databricksStorageCmk.json())\n            .build());\n\n        var storageCustomerManagedKeyAlias = new Alias(\"storageCustomerManagedKeyAlias\", AliasArgs.builder()\n            .name(\"alias/storage-customer-managed-key-alias\")\n            .targetKeyId(storageCustomerManagedKey.keyId())\n            .build());\n\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()\n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(storageCustomerManagedKey.arn())\n                .keyAlias(storageCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksCrossAccountRole:\n    type: dynamic\nresources:\n  storageCustomerManagedKey:\n    type: aws:kms:Key\n    name: storage_customer_managed_key\n    properties:\n      policy: ${databricksStorageCmk.json}\n  storageCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    name: storage_customer_managed_key_alias\n    properties:\n      name: alias/storage-customer-managed-key-alias\n      targetKeyId: ${storageCustomerManagedKey.keyId}\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${storageCustomerManagedKey.arn}\n        keyAlias: ${storageCustomerManagedKeyAlias.name}\n      useCases:\n        - STORAGE\nvariables:\n  current:\n    fn::invoke:\n      function: aws:getCallerIdentity\n      arguments: {}\n  databricksStorageCmk:\n    fn::invoke:\n      function: aws:iam:getPolicyDocument\n      arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n              - kms:ReEncrypt*\n              - kms:GenerateDataKey*\n              - kms:DescribeKey\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS (Grants)\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:CreateGrant\n              - kms:ListGrants\n              - kms:RevokeGrant\n            resources:\n              - '*'\n            conditions:\n              - test: Bool\n                variable: kms:GrantIsForAWSResource\n                values:\n                  - 'true'\n          - sid: Allow Databricks to use KMS key for EBS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${databricksCrossAccountRole}\n            actions:\n              - kms:Decrypt\n              - kms:GenerateDataKey*\n              - kms:CreateGrant\n              - kms:DescribeKey\n            resources:\n              - '*'\n            conditions:\n              - test: ForAnyValue:StringLike\n                variable: kms:ViaService\n                values:\n                  - ec2.*.amazonaws.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### For GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\n// Id of a google_kms_crypto_key\nconst cmekResourceId = config.requireObject\u003cany\u003e(\"cmekResourceId\");\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    gcpKeyInfo: {\n        kmsKeyId: cmekResourceId,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# Id of a google_kms_crypto_key\ncmek_resource_id = config.require_object(\"cmekResourceId\")\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    gcp_key_info={\n        \"kms_key_id\": cmek_resource_id,\n    },\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // Id of a google_kms_crypto_key\n    var cmekResourceId = config.RequireObject\u003cdynamic\u003e(\"cmekResourceId\");\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        GcpKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysGcpKeyInfoArgs\n        {\n            KmsKeyId = cmekResourceId,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// Id of a google_kms_crypto_key\n\t\tcmekResourceId := cfg.RequireObject(\"cmekResourceId\")\n\t\t_, err := databricks.NewMwsCustomerManagedKeys(ctx, \"storage\", \u0026databricks.MwsCustomerManagedKeysArgs{\n\t\t\tAccountId: pulumi.Any(databricksAccountId),\n\t\t\tGcpKeyInfo: \u0026databricks.MwsCustomerManagedKeysGcpKeyInfoArgs{\n\t\t\t\tKmsKeyId: pulumi.Any(cmekResourceId),\n\t\t\t},\n\t\t\tUseCases: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"STORAGE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysGcpKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var cmekResourceId = config.get(\"cmekResourceId\");\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()\n            .accountId(databricksAccountId)\n            .gcpKeyInfo(MwsCustomerManagedKeysGcpKeyInfoArgs.builder()\n                .kmsKeyId(cmekResourceId)\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  cmekResourceId:\n    type: dynamic\nresources:\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      gcpKeyInfo:\n        kmsKeyId: ${cmekResourceId}\n      useCases:\n        - STORAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by Databricks account ID and customer managed key ID.\n\n```sh\n$ pulumi import databricks:index/mwsCustomerManagedKeys:MwsCustomerManagedKeys this '\u003caccount_id\u003e/\u003ccustomer_managed_key_id\u003e'\n```\n\n~\u003e This resource does not support updates. If your configuration does not match the existing resource,\n\n   the next `pulumi up` will cause the resource to be destroyed and recreated. After importing,\n\n   verify that the configuration matches the existing resource by running `pulumi preview`.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "gcpKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n* `MANAGED_SERVICES` - for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane\n* `STORAGE` - for encryption of the DBFS Storage \u0026 Cluster EBS Volumes\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "customerManagedKeyId",
                "useCases"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "gcpKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n",
                    "willReplaceOnChanges": true
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n* `MANAGED_SERVICES` - for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane\n* `STORAGE` - for encryption of the DBFS Storage \u0026 Cluster EBS Volumes\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "useCases"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCustomerManagedKeys resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                        "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "description": "(String) ID of the encryption key configuration object.\n"
                    },
                    "gcpKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                        "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n",
                        "willReplaceOnChanges": true
                    },
                    "useCases": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n* `MANAGED_SERVICES` - for encryption of the workspace objects (notebooks, secrets) that are stored in the control plane\n* `STORAGE` - for encryption of the DBFS Storage \u0026 Cluster EBS Volumes\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsLogDelivery:MwsLogDelivery": {
            "description": "\u003e Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nThis resource configures the delivery of the two supported log types from Databricks workspaces: [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n\nYou cannot delete a log delivery configuration, but you can disable it when you no longer need it. This fact is important because there is a limit to the number of enabled log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.\n\n## Billable Usage\n\nCSV files are delivered to `\u003cdelivery_path_prefix\u003e/billable-usage/csv/` and are named `workspaceId=\u003cworkspace-id\u003e-usageMonth=\u003cmonth\u003e.csv`, which are delivered daily by overwriting the month's CSV file for each workspace. Format of CSV file, as well as some usage examples, can be found [here](https://docs.databricks.com/administration-guide/account-settings/usage.html#download-usage-as-a-csv-file).\n\nCommon processing scenario is to apply [cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html), that could be enforced by setting custom_tags on a cluster or through cluster policy. Report contains `clusterId` field, that could be joined with data from AWS [cost and usage reports](https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html), that can be joined with `user:ClusterId` tag from AWS usage report.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst usageLogs = new databricks.MwsLogDelivery(\"usage_logs\", {\n    accountId: databricksAccountId,\n    credentialsId: logWriter.credentialsId,\n    storageConfigurationId: logBucket.storageConfigurationId,\n    deliveryPathPrefix: \"billable-usage\",\n    configName: \"Usage Logs\",\n    logType: \"BILLABLE_USAGE\",\n    outputFormat: \"CSV\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusage_logs = databricks.MwsLogDelivery(\"usage_logs\",\n    account_id=databricks_account_id,\n    credentials_id=log_writer[\"credentialsId\"],\n    storage_configuration_id=log_bucket[\"storageConfigurationId\"],\n    delivery_path_prefix=\"billable-usage\",\n    config_name=\"Usage Logs\",\n    log_type=\"BILLABLE_USAGE\",\n    output_format=\"CSV\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var usageLogs = new Databricks.MwsLogDelivery(\"usage_logs\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsId = logWriter.CredentialsId,\n        StorageConfigurationId = logBucket.StorageConfigurationId,\n        DeliveryPathPrefix = \"billable-usage\",\n        ConfigName = \"Usage Logs\",\n        LogType = \"BILLABLE_USAGE\",\n        OutputFormat = \"CSV\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"usage_logs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tCredentialsId:          pulumi.Any(logWriter.CredentialsId),\n\t\t\tStorageConfigurationId: pulumi.Any(logBucket.StorageConfigurationId),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"billable-usage\"),\n\t\t\tConfigName:             pulumi.String(\"Usage Logs\"),\n\t\t\tLogType:                pulumi.String(\"BILLABLE_USAGE\"),\n\t\t\tOutputFormat:           pulumi.String(\"CSV\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var usageLogs = new MwsLogDelivery(\"usageLogs\", MwsLogDeliveryArgs.builder()\n            .accountId(databricksAccountId)\n            .credentialsId(logWriter.credentialsId())\n            .storageConfigurationId(logBucket.storageConfigurationId())\n            .deliveryPathPrefix(\"billable-usage\")\n            .configName(\"Usage Logs\")\n            .logType(\"BILLABLE_USAGE\")\n            .outputFormat(\"CSV\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  usageLogs:\n    type: databricks:MwsLogDelivery\n    name: usage_logs\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsId: ${logWriter.credentialsId}\n      storageConfigurationId: ${logBucket.storageConfigurationId}\n      deliveryPathPrefix: billable-usage\n      configName: Usage Logs\n      logType: BILLABLE_USAGE\n      outputFormat: CSV\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Audit Logs\n\nJSON files with [static schema](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#audit-log-schema) are delivered to `\u003cdelivery_path_prefix\u003e/workspaceId=\u003cworkspaceId\u003e/date=\u003cyyyy-mm-dd\u003e/auditlogs_\u003cinternal-id\u003e.json`. Logs are available within 15 minutes of activation for audit logs. New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. Sometimes data may arrive later than 15 minutes. Databricks can overwrite the delivered log files in your bucket at any time. If a file is overwritten, the existing content remains, but there may be additional lines for more auditable events. Overwriting ensures exactly-once semantics without requiring read or delete access to your account.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auditLogs = new databricks.MwsLogDelivery(\"audit_logs\", {\n    accountId: databricksAccountId,\n    credentialsId: logWriter.credentialsId,\n    storageConfigurationId: logBucket.storageConfigurationId,\n    deliveryPathPrefix: \"audit-logs\",\n    configName: \"Audit Logs\",\n    logType: \"AUDIT_LOGS\",\n    outputFormat: \"JSON\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naudit_logs = databricks.MwsLogDelivery(\"audit_logs\",\n    account_id=databricks_account_id,\n    credentials_id=log_writer[\"credentialsId\"],\n    storage_configuration_id=log_bucket[\"storageConfigurationId\"],\n    delivery_path_prefix=\"audit-logs\",\n    config_name=\"Audit Logs\",\n    log_type=\"AUDIT_LOGS\",\n    output_format=\"JSON\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auditLogs = new Databricks.MwsLogDelivery(\"audit_logs\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsId = logWriter.CredentialsId,\n        StorageConfigurationId = logBucket.StorageConfigurationId,\n        DeliveryPathPrefix = \"audit-logs\",\n        ConfigName = \"Audit Logs\",\n        LogType = \"AUDIT_LOGS\",\n        OutputFormat = \"JSON\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"audit_logs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tCredentialsId:          pulumi.Any(logWriter.CredentialsId),\n\t\t\tStorageConfigurationId: pulumi.Any(logBucket.StorageConfigurationId),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"audit-logs\"),\n\t\t\tConfigName:             pulumi.String(\"Audit Logs\"),\n\t\t\tLogType:                pulumi.String(\"AUDIT_LOGS\"),\n\t\t\tOutputFormat:           pulumi.String(\"JSON\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auditLogs = new MwsLogDelivery(\"auditLogs\", MwsLogDeliveryArgs.builder()\n            .accountId(databricksAccountId)\n            .credentialsId(logWriter.credentialsId())\n            .storageConfigurationId(logBucket.storageConfigurationId())\n            .deliveryPathPrefix(\"audit-logs\")\n            .configName(\"Audit Logs\")\n            .logType(\"AUDIT_LOGS\")\n            .outputFormat(\"JSON\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auditLogs:\n    type: databricks:MwsLogDelivery\n    name: audit_logs\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsId: ${logWriter.credentialsId}\n      storageConfigurationId: ${logBucket.storageConfigurationId}\n      deliveryPathPrefix: audit-logs\n      configName: Audit Logs\n      logType: AUDIT_LOGS\n      outputFormat: JSON\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n"
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n"
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n"
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n"
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n"
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n"
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n"
                }
            },
            "required": [
                "accountId",
                "configId",
                "credentialsId",
                "deliveryStartTime",
                "logType",
                "outputFormat",
                "status",
                "storageConfigurationId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "willReplaceOnChanges": true
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n",
                    "willReplaceOnChanges": true
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                    "willReplaceOnChanges": true
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                    "willReplaceOnChanges": true
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsId",
                "logType",
                "outputFormat",
                "storageConfigurationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsLogDelivery resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "configId": {
                        "type": "string",
                        "description": "Databricks log delivery configuration ID.\n",
                        "willReplaceOnChanges": true
                    },
                    "configName": {
                        "type": "string",
                        "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                        "willReplaceOnChanges": true
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryPathPrefix": {
                        "type": "string",
                        "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryStartTime": {
                        "type": "string",
                        "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                        "willReplaceOnChanges": true
                    },
                    "logType": {
                        "type": "string",
                        "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "outputFormat": {
                        "type": "string",
                        "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceIdsFilters": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        },
                        "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the multitenant version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNccBinding:MwsNccBinding": {
            "description": "\u003e Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e This feature is available for AWS \u0026 Azure only, and is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html) in AWS.\n\nAllows you to attach a Network Connectivity Config object to a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages serverless network connectivity configs](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/serverless-firewall).\n\nThe NCC and workspace must be in the same region.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject\u003cany\u003e(\"region\");\nconst prefix = config.requireObject\u003cany\u003e(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `ncc-for-${prefix}`,\n    region: region,\n});\nconst nccBinding = new databricks.MwsNccBinding(\"ncc_binding\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    workspaceId: databricksWorkspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"ncc-for-{prefix}\",\n    region=region)\nncc_binding = databricks.MwsNccBinding(\"ncc_binding\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    workspace_id=databricks_workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"ncc-for-{prefix}\",\n        Region = region,\n    });\n\n    var nccBinding = new Databricks.MwsNccBinding(\"ncc_binding\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        WorkspaceId = databricksWorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.Sprintf(\"ncc-for-%v\", prefix),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccBinding(ctx, \"ncc_binding\", \u0026databricks.MwsNccBindingArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tWorkspaceId:                 pulumi.Any(databricksWorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccBinding;\nimport com.pulumi.databricks.MwsNccBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()\n            .name(String.format(\"ncc-for-%s\", prefix))\n            .region(region)\n            .build());\n\n        var nccBinding = new MwsNccBinding(\"nccBinding\", MwsNccBindingArgs.builder()\n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .workspaceId(databricksWorkspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: ncc-for-${prefix}\n      region: ${region}\n  nccBinding:\n    type: databricks:MwsNccBinding\n    name: ncc_binding\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      workspaceId: ${databricksWorkspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n* databricks.MwsNetworkConnectivityConfig to create Network Connectivity Config objects.\n",
            "properties": {
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "networkConnectivityConfigId",
                "workspaceId"
            ],
            "inputProperties": {
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "networkConnectivityConfigId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNccBinding resources.\n",
                "properties": {
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNccPrivateEndpointRule:MwsNccPrivateEndpointRule": {
            "description": "\u003e Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e This feature is only available in Azure.\n\nAllows you to create a private endpoint in a Network Connectivity Config that can be used to [configure private connectivity from serverless compute](https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/serverless-private-link).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject\u003cany\u003e(\"region\");\nconst prefix = config.requireObject\u003cany\u003e(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `ncc-for-${prefix}`,\n    region: region,\n});\nconst storage = new databricks.MwsNccPrivateEndpointRule(\"storage\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    resourceId: \"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n    groupId: \"blob\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"ncc-for-{prefix}\",\n    region=region)\nstorage = databricks.MwsNccPrivateEndpointRule(\"storage\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    resource_id=\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n    group_id=\"blob\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"ncc-for-{prefix}\",\n        Region = region,\n    });\n\n    var storage = new Databricks.MwsNccPrivateEndpointRule(\"storage\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        ResourceId = \"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n        GroupId = \"blob\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.Sprintf(\"ncc-for-%v\", prefix),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccPrivateEndpointRule(ctx, \"storage\", \u0026databricks.MwsNccPrivateEndpointRuleArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tResourceId:                  pulumi.String(\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\"),\n\t\t\tGroupId:                     pulumi.String(\"blob\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccPrivateEndpointRule;\nimport com.pulumi.databricks.MwsNccPrivateEndpointRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()\n            .name(String.format(\"ncc-for-%s\", prefix))\n            .region(region)\n            .build());\n\n        var storage = new MwsNccPrivateEndpointRule(\"storage\", MwsNccPrivateEndpointRuleArgs.builder()\n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .resourceId(\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\")\n            .groupId(\"blob\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: ncc-for-${prefix}\n      region: ${region}\n  storage:\n    type: databricks:MwsNccPrivateEndpointRule\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      resourceId: /subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\n      groupId: blob\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsNetworkConnectivityConfig to create Network Connectivity Config objects.\n* databricks.MwsNccBinding to attach an NCC to a workspace.\n\n## Import\n\nThis resource can be imported by Databricks account ID and Network Connectivity Config ID.\n\n```sh\n$ pulumi import databricks:index/mwsNccPrivateEndpointRule:MwsNccPrivateEndpointRule rule \u003cnetwork_connectivity_config_id\u003e/\u003crule_id\u003e\n```\n\n",
            "properties": {
                "connectionState": {
                    "type": "string",
                    "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n* `PENDING`: The endpoint has been created and pending approval.\n* `ESTABLISHED`: The endpoint has been approved and is ready to be used in your serverless compute resources.\n* `REJECTED`: Connection was rejected by the private link resource owner.\n* `DISCONNECTED`: Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was created.\n"
                },
                "deactivated": {
                    "type": "boolean",
                    "description": "Whether this private endpoint is deactivated.\n"
                },
                "deactivatedAt": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was deactivated.\n"
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "The sub-resource type (group ID) of the target resource. Must be one of supported resource types (i.e., `blob`, `dfs`, `sqlServer` , etc. Consult the [Azure documentation](https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview#private-link-resource) for full list of supported resources). Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for `blob` and one for `dfs`. Change forces creation of a new resource.\n"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n"
                },
                "resourceId": {
                    "type": "string",
                    "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n"
                },
                "ruleId": {
                    "type": "string",
                    "description": "the ID of a private endpoint rule.\n"
                },
                "updatedTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was updated.\n"
                }
            },
            "required": [
                "connectionState",
                "creationTime",
                "endpointName",
                "groupId",
                "networkConnectivityConfigId",
                "resourceId",
                "ruleId",
                "updatedTime"
            ],
            "inputProperties": {
                "connectionState": {
                    "type": "string",
                    "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n* `PENDING`: The endpoint has been created and pending approval.\n* `ESTABLISHED`: The endpoint has been approved and is ready to be used in your serverless compute resources.\n* `REJECTED`: Connection was rejected by the private link resource owner.\n* `DISCONNECTED`: Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was created.\n"
                },
                "deactivated": {
                    "type": "boolean",
                    "description": "Whether this private endpoint is deactivated.\n",
                    "willReplaceOnChanges": true
                },
                "deactivatedAt": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was deactivated.\n",
                    "willReplaceOnChanges": true
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "The sub-resource type (group ID) of the target resource. Must be one of supported resource types (i.e., `blob`, `dfs`, `sqlServer` , etc. Consult the [Azure documentation](https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview#private-link-resource) for full list of supported resources). Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for `blob` and one for `dfs`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "ruleId": {
                    "type": "string",
                    "description": "the ID of a private endpoint rule.\n"
                },
                "updatedTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was updated.\n"
                }
            },
            "requiredInputs": [
                "groupId",
                "networkConnectivityConfigId",
                "resourceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNccPrivateEndpointRule resources.\n",
                "properties": {
                    "connectionState": {
                        "type": "string",
                        "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n* `PENDING`: The endpoint has been created and pending approval.\n* `ESTABLISHED`: The endpoint has been approved and is ready to be used in your serverless compute resources.\n* `REJECTED`: Connection was rejected by the private link resource owner.\n* `DISCONNECTED`: Connection was removed by the private link resource owner, the private endpoint becomes informative and should be deleted for clean-up.\n"
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was created.\n"
                    },
                    "deactivated": {
                        "type": "boolean",
                        "description": "Whether this private endpoint is deactivated.\n",
                        "willReplaceOnChanges": true
                    },
                    "deactivatedAt": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was deactivated.\n",
                        "willReplaceOnChanges": true
                    },
                    "endpointName": {
                        "type": "string",
                        "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                    },
                    "groupId": {
                        "type": "string",
                        "description": "The sub-resource type (group ID) of the target resource. Must be one of supported resource types (i.e., `blob`, `dfs`, `sqlServer` , etc. Consult the [Azure documentation](https://learn.microsoft.com/en-us/azure/private-link/private-endpoint-overview#private-link-resource) for full list of supported resources). Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for `blob` and one for `dfs`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "ruleId": {
                        "type": "string",
                        "description": "the ID of a private endpoint rule.\n"
                    },
                    "updatedTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was updated.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworkConnectivityConfig:MwsNetworkConnectivityConfig": {
            "description": "\u003e Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e **Public Preview** This feature is available for AWS \u0026 Azure only, and is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html) in AWS.\n\nAllows you to create a Network Connectivity Config that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages serverless network connectivity configs](https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/serverless-firewall).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject\u003cany\u003e(\"region\");\nconst prefix = config.requireObject\u003cany\u003e(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `ncc-for-${prefix}`,\n    region: region,\n});\nconst nccBinding = new databricks.MwsNccBinding(\"ncc_binding\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    workspaceId: databricksWorkspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"ncc-for-{prefix}\",\n    region=region)\nncc_binding = databricks.MwsNccBinding(\"ncc_binding\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    workspace_id=databricks_workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"ncc-for-{prefix}\",\n        Region = region,\n    });\n\n    var nccBinding = new Databricks.MwsNccBinding(\"ncc_binding\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        WorkspaceId = databricksWorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.Sprintf(\"ncc-for-%v\", prefix),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccBinding(ctx, \"ncc_binding\", \u0026databricks.MwsNccBindingArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tWorkspaceId:                 pulumi.Any(databricksWorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccBinding;\nimport com.pulumi.databricks.MwsNccBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()\n            .name(String.format(\"ncc-for-%s\", prefix))\n            .region(region)\n            .build());\n\n        var nccBinding = new MwsNccBinding(\"nccBinding\", MwsNccBindingArgs.builder()\n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .workspaceId(databricksWorkspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: ncc-for-${prefix}\n      region: ${region}\n  nccBinding:\n    type: databricks:MwsNccBinding\n    name: ncc_binding\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      workspaceId: ${databricksWorkspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n* databricks.MwsNccBinding to attach an NCC to a workspace.\n* databricks.MwsNccPrivateEndpointRule to create a private endpoint rule.\n\n## Import\n\nThis resource can be imported by Databricks account ID and Network Connectivity Config ID.\n\n```sh\n$ pulumi import databricks:index/mwsNetworkConnectivityConfig:MwsNetworkConnectivityConfig ncc \u003caccount_id\u003e/\u003cnetwork_connectivity_config_id\u003e\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "egressConfig": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n"
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "egressConfig",
                "name",
                "networkConnectivityConfigId",
                "region",
                "updatedTime"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "egressConfig": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "requiredInputs": [
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworkConnectivityConfig resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "egressConfig": {
                        "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "updatedTime": {
                        "type": "integer"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworks:MwsNetworks": {
            "description": "## Example Usage\n\n### Creating a Databricks on GCP workspace\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as google from \"@pulumi/google\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst dbxPrivateVpc = new google.index.ComputeNetwork(\"dbx_private_vpc\", {\n    project: googleProject,\n    name: `tf-network-${suffix.result}`,\n    autoCreateSubnetworks: false,\n});\nconst network_with_private_secondary_ip_ranges = new google.index.ComputeSubnetwork(\"network-with-private-secondary-ip-ranges\", {\n    name: `test-dbx-${suffix.result}`,\n    ipCidrRange: \"10.0.0.0/16\",\n    region: \"us-central1\",\n    network: dbxPrivateVpc.id,\n    secondaryIpRange: [\n        {\n            rangeName: \"pods\",\n            ipCidrRange: \"10.1.0.0/16\",\n        },\n        {\n            rangeName: \"svc\",\n            ipCidrRange: \"10.2.0.0/20\",\n        },\n    ],\n    privateIpGoogleAccess: true,\n});\nconst router = new google.index.ComputeRouter(\"router\", {\n    name: `my-router-${suffix.result}`,\n    region: network_with_private_secondary_ip_ranges.region,\n    network: dbxPrivateVpc.id,\n});\nconst nat = new google.index.ComputeRouterNat(\"nat\", {\n    name: `my-router-nat-${suffix.result}`,\n    router: router.name,\n    region: router.region,\n    natIpAllocateOption: \"AUTO_ONLY\",\n    sourceSubnetworkIpRangesToNat: \"ALL_SUBNETWORKS_ALL_IP_RANGES\",\n});\nconst _this = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `test-demo-${suffix.result}`,\n    gcpNetworkInfo: {\n        networkProjectId: googleProject,\n        vpcId: dbxPrivateVpc.name,\n        subnetId: networkWithPrivateSecondaryIpRanges.name,\n        subnetRegion: networkWithPrivateSecondaryIpRanges.region,\n        podIpRangeName: \"pods\",\n        serviceIpRangeName: \"svc\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_google as google\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndbx_private_vpc = google.index.ComputeNetwork(\"dbx_private_vpc\",\n    project=google_project,\n    name=ftf-network-{suffix.result},\n    auto_create_subnetworks=False)\nnetwork_with_private_secondary_ip_ranges = google.index.ComputeSubnetwork(\"network-with-private-secondary-ip-ranges\",\n    name=ftest-dbx-{suffix.result},\n    ip_cidr_range=10.0.0.0/16,\n    region=us-central1,\n    network=dbx_private_vpc.id,\n    secondary_ip_range=[\n        {\n            rangeName: pods,\n            ipCidrRange: 10.1.0.0/16,\n        },\n        {\n            rangeName: svc,\n            ipCidrRange: 10.2.0.0/20,\n        },\n    ],\n    private_ip_google_access=True)\nrouter = google.index.ComputeRouter(\"router\",\n    name=fmy-router-{suffix.result},\n    region=network_with_private_secondary_ip_ranges.region,\n    network=dbx_private_vpc.id)\nnat = google.index.ComputeRouterNat(\"nat\",\n    name=fmy-router-nat-{suffix.result},\n    router=router.name,\n    region=router.region,\n    nat_ip_allocate_option=AUTO_ONLY,\n    source_subnetwork_ip_ranges_to_nat=ALL_SUBNETWORKS_ALL_IP_RANGES)\nthis = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"test-demo-{suffix['result']}\",\n    gcp_network_info={\n        \"network_project_id\": google_project,\n        \"vpc_id\": dbx_private_vpc[\"name\"],\n        \"subnet_id\": network_with_private_secondary_ip_ranges[\"name\"],\n        \"subnet_region\": network_with_private_secondary_ip_ranges[\"region\"],\n        \"pod_ip_range_name\": \"pods\",\n        \"service_ip_range_name\": \"svc\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Google = Pulumi.Google;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var dbxPrivateVpc = new Google.Index.ComputeNetwork(\"dbx_private_vpc\", new()\n    {\n        Project = googleProject,\n        Name = $\"tf-network-{suffix.Result}\",\n        AutoCreateSubnetworks = false,\n    });\n\n    var network_with_private_secondary_ip_ranges = new Google.Index.ComputeSubnetwork(\"network-with-private-secondary-ip-ranges\", new()\n    {\n        Name = $\"test-dbx-{suffix.Result}\",\n        IpCidrRange = \"10.0.0.0/16\",\n        Region = \"us-central1\",\n        Network = dbxPrivateVpc.Id,\n        SecondaryIpRange = new[]\n        {\n            \n            {\n                { \"rangeName\", \"pods\" },\n                { \"ipCidrRange\", \"10.1.0.0/16\" },\n            },\n            \n            {\n                { \"rangeName\", \"svc\" },\n                { \"ipCidrRange\", \"10.2.0.0/20\" },\n            },\n        },\n        PrivateIpGoogleAccess = true,\n    });\n\n    var router = new Google.Index.ComputeRouter(\"router\", new()\n    {\n        Name = $\"my-router-{suffix.Result}\",\n        Region = network_with_private_secondary_ip_ranges.Region,\n        Network = dbxPrivateVpc.Id,\n    });\n\n    var nat = new Google.Index.ComputeRouterNat(\"nat\", new()\n    {\n        Name = $\"my-router-nat-{suffix.Result}\",\n        Router = router.Name,\n        Region = router.Region,\n        NatIpAllocateOption = \"AUTO_ONLY\",\n        SourceSubnetworkIpRangesToNat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\",\n    });\n\n    var @this = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"test-demo-{suffix.Result}\",\n        GcpNetworkInfo = new Databricks.Inputs.MwsNetworksGcpNetworkInfoArgs\n        {\n            NetworkProjectId = googleProject,\n            VpcId = dbxPrivateVpc.Name,\n            SubnetId = networkWithPrivateSecondaryIpRanges.Name,\n            SubnetRegion = networkWithPrivateSecondaryIpRanges.Region,\n            PodIpRangeName = \"pods\",\n            ServiceIpRangeName = \"svc\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-google/sdk/go/google\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tdbxPrivateVpc, err := google.NewComputeNetwork(ctx, \"dbx_private_vpc\", \u0026google.ComputeNetworkArgs{\n\t\t\tProject:               googleProject,\n\t\t\tName:                  fmt.Sprintf(\"tf-network-%v\", suffix.Result),\n\t\t\tAutoCreateSubnetworks: false,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tnetwork_with_private_secondary_ip_ranges, err := google.NewComputeSubnetwork(ctx, \"network-with-private-secondary-ip-ranges\", \u0026google.ComputeSubnetworkArgs{\n\t\t\tName:        fmt.Sprintf(\"test-dbx-%v\", suffix.Result),\n\t\t\tIpCidrRange: \"10.0.0.0/16\",\n\t\t\tRegion:      \"us-central1\",\n\t\t\tNetwork:     dbxPrivateVpc.Id,\n\t\t\tSecondaryIpRange: []map[string]interface{}{\n\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\"rangeName\":   \"pods\",\n\t\t\t\t\t\"ipCidrRange\": \"10.1.0.0/16\",\n\t\t\t\t},\n\t\t\t\tmap[string]interface{}{\n\t\t\t\t\t\"rangeName\":   \"svc\",\n\t\t\t\t\t\"ipCidrRange\": \"10.2.0.0/20\",\n\t\t\t\t},\n\t\t\t},\n\t\t\tPrivateIpGoogleAccess: true,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trouter, err := google.NewComputeRouter(ctx, \"router\", \u0026google.ComputeRouterArgs{\n\t\t\tName:    fmt.Sprintf(\"my-router-%v\", suffix.Result),\n\t\t\tRegion:  network_with_private_secondary_ip_ranges.Region,\n\t\t\tNetwork: dbxPrivateVpc.Id,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = google.NewComputeRouterNat(ctx, \"nat\", \u0026google.ComputeRouterNatArgs{\n\t\t\tName:                          fmt.Sprintf(\"my-router-nat-%v\", suffix.Result),\n\t\t\tRouter:                        router.Name,\n\t\t\tRegion:                        router.Region,\n\t\t\tNatIpAllocateOption:           \"AUTO_ONLY\",\n\t\t\tSourceSubnetworkIpRangesToNat: \"ALL_SUBNETWORKS_ALL_IP_RANGES\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.Sprintf(\"test-demo-%v\", suffix.Result),\n\t\t\tGcpNetworkInfo: \u0026databricks.MwsNetworksGcpNetworkInfoArgs{\n\t\t\t\tNetworkProjectId:   pulumi.Any(googleProject),\n\t\t\t\tVpcId:              dbxPrivateVpc.Name,\n\t\t\t\tSubnetId:           pulumi.Any(networkWithPrivateSecondaryIpRanges.Name),\n\t\t\t\tSubnetRegion:       pulumi.Any(networkWithPrivateSecondaryIpRanges.Region),\n\t\t\t\tPodIpRangeName:     pulumi.String(\"pods\"),\n\t\t\t\tServiceIpRangeName: pulumi.String(\"svc\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.google.computeNetwork;\nimport com.pulumi.google.computeNetworkArgs;\nimport com.pulumi.google.computeSubnetwork;\nimport com.pulumi.google.computeSubnetworkArgs;\nimport com.pulumi.google.computeRouter;\nimport com.pulumi.google.computeRouterArgs;\nimport com.pulumi.google.computeRouterNat;\nimport com.pulumi.google.computeRouterNatArgs;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.inputs.MwsNetworksGcpNetworkInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var dbxPrivateVpc = new ComputeNetwork(\"dbxPrivateVpc\", ComputeNetworkArgs.builder()\n            .project(googleProject)\n            .name(String.format(\"tf-network-%s\", suffix.result()))\n            .autoCreateSubnetworks(false)\n            .build());\n\n        var network_with_private_secondary_ip_ranges = new ComputeSubnetwork(\"network-with-private-secondary-ip-ranges\", ComputeSubnetworkArgs.builder()\n            .name(String.format(\"test-dbx-%s\", suffix.result()))\n            .ipCidrRange(\"10.0.0.0/16\")\n            .region(\"us-central1\")\n            .network(dbxPrivateVpc.id())\n            .secondaryIpRange(List.of(            \n                Map.ofEntries(\n                    Map.entry(\"rangeName\", \"pods\"),\n                    Map.entry(\"ipCidrRange\", \"10.1.0.0/16\")\n                ),\n                Map.ofEntries(\n                    Map.entry(\"rangeName\", \"svc\"),\n                    Map.entry(\"ipCidrRange\", \"10.2.0.0/20\")\n                )))\n            .privateIpGoogleAccess(true)\n            .build());\n\n        var router = new ComputeRouter(\"router\", ComputeRouterArgs.builder()\n            .name(String.format(\"my-router-%s\", suffix.result()))\n            .region(network_with_private_secondary_ip_ranges.region())\n            .network(dbxPrivateVpc.id())\n            .build());\n\n        var nat = new ComputeRouterNat(\"nat\", ComputeRouterNatArgs.builder()\n            .name(String.format(\"my-router-nat-%s\", suffix.result()))\n            .router(router.name())\n            .region(router.region())\n            .natIpAllocateOption(\"AUTO_ONLY\")\n            .sourceSubnetworkIpRangesToNat(\"ALL_SUBNETWORKS_ALL_IP_RANGES\")\n            .build());\n\n        var this_ = new MwsNetworks(\"this\", MwsNetworksArgs.builder()\n            .accountId(databricksAccountId)\n            .networkName(String.format(\"test-demo-%s\", suffix.result()))\n            .gcpNetworkInfo(MwsNetworksGcpNetworkInfoArgs.builder()\n                .networkProjectId(googleProject)\n                .vpcId(dbxPrivateVpc.name())\n                .subnetId(networkWithPrivateSecondaryIpRanges.name())\n                .subnetRegion(networkWithPrivateSecondaryIpRanges.region())\n                .podIpRangeName(\"pods\")\n                .serviceIpRangeName(\"svc\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  dbxPrivateVpc:\n    type: google:computeNetwork\n    name: dbx_private_vpc\n    properties:\n      project: ${googleProject}\n      name: tf-network-${suffix.result}\n      autoCreateSubnetworks: false\n  network-with-private-secondary-ip-ranges:\n    type: google:computeSubnetwork\n    properties:\n      name: test-dbx-${suffix.result}\n      ipCidrRange: 10.0.0.0/16\n      region: us-central1\n      network: ${dbxPrivateVpc.id}\n      secondaryIpRange:\n        - rangeName: pods\n          ipCidrRange: 10.1.0.0/16\n        - rangeName: svc\n          ipCidrRange: 10.2.0.0/20\n      privateIpGoogleAccess: true\n  router:\n    type: google:computeRouter\n    properties:\n      name: my-router-${suffix.result}\n      region: ${[\"network-with-private-secondary-ip-ranges\"].region}\n      network: ${dbxPrivateVpc.id}\n  nat:\n    type: google:computeRouterNat\n    properties:\n      name: my-router-nat-${suffix.result}\n      router: ${router.name}\n      region: ${router.region}\n      natIpAllocateOption: AUTO_ONLY\n      sourceSubnetworkIpRangesToNat: ALL_SUBNETWORKS_ALL_IP_RANGES\n  this:\n    type: databricks:MwsNetworks\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: test-demo-${suffix.result}\n      gcpNetworkInfo:\n        networkProjectId: ${googleProject}\n        vpcId: ${dbxPrivateVpc.name}\n        subnetId: ${networkWithPrivateSecondaryIpRanges.name}\n        subnetRegion: ${networkWithPrivateSecondaryIpRanges.region}\n        podIpRangeName: pods\n        serviceIpRangeName: svc\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn order to create a VPC [that leverages GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) you would need to add the `vpc_endpoint_id` Attributes from mws_vpc_endpoint resources into the databricks.MwsNetworks resource. For example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `test-demo-${suffix.result}`,\n    gcpNetworkInfo: {\n        networkProjectId: googleProject,\n        vpcId: dbxPrivateVpc.name,\n        subnetId: networkWithPrivateSecondaryIpRanges.name,\n        subnetRegion: networkWithPrivateSecondaryIpRanges.region,\n        podIpRangeName: \"pods\",\n        serviceIpRangeName: \"svc\",\n    },\n    vpcEndpoints: {\n        dataplaneRelays: [relay.vpcEndpointId],\n        restApis: [workspace.vpcEndpointId],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"test-demo-{suffix['result']}\",\n    gcp_network_info={\n        \"network_project_id\": google_project,\n        \"vpc_id\": dbx_private_vpc[\"name\"],\n        \"subnet_id\": network_with_private_secondary_ip_ranges[\"name\"],\n        \"subnet_region\": network_with_private_secondary_ip_ranges[\"region\"],\n        \"pod_ip_range_name\": \"pods\",\n        \"service_ip_range_name\": \"svc\",\n    },\n    vpc_endpoints={\n        \"dataplane_relays\": [relay[\"vpcEndpointId\"]],\n        \"rest_apis\": [workspace[\"vpcEndpointId\"]],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"test-demo-{suffix.Result}\",\n        GcpNetworkInfo = new Databricks.Inputs.MwsNetworksGcpNetworkInfoArgs\n        {\n            NetworkProjectId = googleProject,\n            VpcId = dbxPrivateVpc.Name,\n            SubnetId = networkWithPrivateSecondaryIpRanges.Name,\n            SubnetRegion = networkWithPrivateSecondaryIpRanges.Region,\n            PodIpRangeName = \"pods\",\n            ServiceIpRangeName = \"svc\",\n        },\n        VpcEndpoints = new Databricks.Inputs.MwsNetworksVpcEndpointsArgs\n        {\n            DataplaneRelays = new[]\n            {\n                relay.VpcEndpointId,\n            },\n            RestApis = new[]\n            {\n                workspace.VpcEndpointId,\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.Sprintf(\"test-demo-%v\", suffix.Result),\n\t\t\tGcpNetworkInfo: \u0026databricks.MwsNetworksGcpNetworkInfoArgs{\n\t\t\t\tNetworkProjectId:   pulumi.Any(googleProject),\n\t\t\t\tVpcId:              pulumi.Any(dbxPrivateVpc.Name),\n\t\t\t\tSubnetId:           pulumi.Any(networkWithPrivateSecondaryIpRanges.Name),\n\t\t\t\tSubnetRegion:       pulumi.Any(networkWithPrivateSecondaryIpRanges.Region),\n\t\t\t\tPodIpRangeName:     pulumi.String(\"pods\"),\n\t\t\t\tServiceIpRangeName: pulumi.String(\"svc\"),\n\t\t\t},\n\t\t\tVpcEndpoints: \u0026databricks.MwsNetworksVpcEndpointsArgs{\n\t\t\t\tDataplaneRelays: pulumi.StringArray{\n\t\t\t\t\trelay.VpcEndpointId,\n\t\t\t\t},\n\t\t\t\tRestApis: pulumi.StringArray{\n\t\t\t\t\tworkspace.VpcEndpointId,\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.inputs.MwsNetworksGcpNetworkInfoArgs;\nimport com.pulumi.databricks.inputs.MwsNetworksVpcEndpointsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsNetworks(\"this\", MwsNetworksArgs.builder()\n            .accountId(databricksAccountId)\n            .networkName(String.format(\"test-demo-%s\", suffix.result()))\n            .gcpNetworkInfo(MwsNetworksGcpNetworkInfoArgs.builder()\n                .networkProjectId(googleProject)\n                .vpcId(dbxPrivateVpc.name())\n                .subnetId(networkWithPrivateSecondaryIpRanges.name())\n                .subnetRegion(networkWithPrivateSecondaryIpRanges.region())\n                .podIpRangeName(\"pods\")\n                .serviceIpRangeName(\"svc\")\n                .build())\n            .vpcEndpoints(MwsNetworksVpcEndpointsArgs.builder()\n                .dataplaneRelays(relay.vpcEndpointId())\n                .restApis(workspace.vpcEndpointId())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsNetworks\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: test-demo-${suffix.result}\n      gcpNetworkInfo:\n        networkProjectId: ${googleProject}\n        vpcId: ${dbxPrivateVpc.name}\n        subnetId: ${networkWithPrivateSecondaryIpRanges.name}\n        subnetRegion: ${networkWithPrivateSecondaryIpRanges.region}\n        podIpRangeName: pods\n        serviceIpRangeName: svc\n      vpcEndpoints:\n        dataplaneRelays:\n          - ${relay.vpcEndpointId}\n        restApis:\n          - ${workspace.vpcEndpointId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Modifying networks on running workspaces (AWS only)\n\nDue to specifics of platform APIs, changing any attribute of network configuration would cause `databricks.MwsNetworks` to be re-created - deleted \u0026 added again with special case for running workspaces. Once network configuration is attached to a running databricks_mws_workspaces, you cannot delete it and `pulumi up` would result in `INVALID_STATE: Unable to delete, Network is being used by active workspace X` error. In order to modify any attributes of a network, you have to perform three different `pulumi up` steps:\n\n1. Create a new `databricks.MwsNetworks` resource.\n2. Update the `databricks.MwsWorkspaces` to point to the new `network_id`.\n3. Delete the old `databricks.MwsNetworks` resource.\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with Private Link guide.\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* Provisioning Databricks on GCP guide.\n* Provisioning Databricks workspaces on GCP with Private Service Connect guide.\n* databricks.MwsVpcEndpoint to register aws_vpc_endpoint resources with Databricks such that they can be used as part of a databricks.MwsNetworks configuration.\n* databricks.MwsPrivateAccessSettings to create a Private Access Setting that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) or [GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html).\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by Databricks account ID and network ID.\n\n```sh\n$ pulumi import databricks:index/mwsNetworks:MwsNetworks this '\u003caccount_id\u003e/\u003cnetwork_id\u003e'\n```\n\n~\u003e This resource does not support updates. If your configuration does not match the existing resource,\n\n   the next `pulumi up` will cause the resource to be destroyed and recreated. After importing,\n\n   verify that the configuration matches the existing resource by running `pulumi preview`.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n"
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_security_group\n"
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_subnet\n"
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n"
                },
                "vpcId": {
                    "type": "string",
                    "description": "aws_vpc id\n"
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "errorMessages",
                "networkId",
                "networkName",
                "vpcEndpoints",
                "vpcStatus",
                "workspaceId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n",
                    "willReplaceOnChanges": true
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_security_group\n",
                    "willReplaceOnChanges": true
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_subnet\n",
                    "willReplaceOnChanges": true
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "aws_vpc id\n",
                    "willReplaceOnChanges": true
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "networkName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworks resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "errorMessages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                        }
                    },
                    "gcpNetworkInfo": {
                        "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                        "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n"
                    },
                    "networkId": {
                        "type": "string",
                        "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                    },
                    "networkName": {
                        "type": "string",
                        "description": "name under which this network is registered\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "ids of aws_security_group\n",
                        "willReplaceOnChanges": true
                    },
                    "subnetIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "ids of aws_subnet\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcEndpoints": {
                        "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                        "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "description": "aws_vpc id\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcStatus": {
                        "type": "string",
                        "description": "(String) VPC attachment status\n"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "(Integer) id of associated workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPermissionAssignment:MwsPermissionAssignment": {
            "description": "These resources are invoked in the account context. Permission Assignment Account API endpoints are restricted to account admins. Provider must have `account_id` attribute configured. Account Id that could be found in the top right corner of Accounts Console\n\n## Example Usage\n\nIn account context, adding account-level group to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dataEng = new databricks.Group(\"data_eng\", {displayName: \"Data Engineering\"});\nconst addAdminGroup = new databricks.MwsPermissionAssignment(\"add_admin_group\", {\n    workspaceId: _this.workspaceId,\n    principalId: dataEng.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndata_eng = databricks.Group(\"data_eng\", display_name=\"Data Engineering\")\nadd_admin_group = databricks.MwsPermissionAssignment(\"add_admin_group\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=data_eng.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dataEng = new Databricks.Group(\"data_eng\", new()\n    {\n        DisplayName = \"Data Engineering\",\n    });\n\n    var addAdminGroup = new Databricks.MwsPermissionAssignment(\"add_admin_group\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = dataEng.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdataEng, err := databricks.NewGroup(ctx, \"data_eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_admin_group\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: dataEng.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dataEng = new Group(\"dataEng\", GroupArgs.builder()\n            .displayName(\"Data Engineering\")\n            .build());\n\n        var addAdminGroup = new MwsPermissionAssignment(\"addAdminGroup\", MwsPermissionAssignmentArgs.builder()\n            .workspaceId(this_.workspaceId())\n            .principalId(dataEng.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dataEng:\n    type: databricks:Group\n    name: data_eng\n    properties:\n      displayName: Data Engineering\n  addAdminGroup:\n    type: databricks:MwsPermissionAssignment\n    name: add_admin_group\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${dataEng.id}\n      permissions:\n        - ADMIN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn account context, adding account-level user to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst addUser = new databricks.MwsPermissionAssignment(\"add_user\", {\n    workspaceId: _this.workspaceId,\n    principalId: me.id,\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nadd_user = databricks.MwsPermissionAssignment(\"add_user\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.MwsPermissionAssignment(\"add_user\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = me.Id,\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_user\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: me.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new MwsPermissionAssignment(\"addUser\", MwsPermissionAssignmentArgs.builder()\n            .workspaceId(this_.workspaceId())\n            .principalId(me.id())\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  addUser:\n    type: databricks:MwsPermissionAssignment\n    name: add_user\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${me.id}\n      permissions:\n        - USER\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn account context, adding account-level service principal to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\nconst addAdminSpn = new databricks.MwsPermissionAssignment(\"add_admin_spn\", {\n    workspaceId: _this.workspaceId,\n    principalId: sp.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.MwsPermissionAssignment(\"add_admin_spn\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.MwsPermissionAssignment(\"add_admin_spn\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = sp.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_admin_spn\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: sp.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new MwsPermissionAssignment(\"addAdminSpn\", MwsPermissionAssignmentArgs.builder()\n            .workspaceId(this_.workspaceId())\n            .principalId(sp.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n  addAdminSpn:\n    type: databricks:MwsPermissionAssignment\n    name: add_admin_spn\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.PermissionAssignment to manage permission assignment from a workspace context\n\n## Import\n\nThe resource `databricks_mws_permission_assignment` can be imported using the workspace id and principal id\n\nbash\n\n```sh\n$ pulumi import databricks:index/mwsPermissionAssignment:MwsPermissionAssignment this \"workspace_id|principal_id\"\n```\n\n",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n"
                },
                "principalId": {
                    "type": "string",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Databricks workspace ID.\n"
                }
            },
            "required": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "string",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Databricks workspace ID.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "string",
                        "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "Databricks workspace ID.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPrivateAccessSettings:MwsPrivateAccessSettings": {
            "description": "Allows you to create a Private Access Setting resource that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) or [GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html)\n\nIt is strongly recommended that customers read the [Enable AWS Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) [Enable GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) documentation before trying to leverage this resource.\n\n## Databricks on AWS usage\n\n\u003e Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst pas = new databricks.MwsPrivateAccessSettings(\"pas\", {\n    accountId: databricksAccountId,\n    privateAccessSettingsName: `Private Access Settings for ${prefix}`,\n    region: region,\n    publicAccessEnabled: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npas = databricks.MwsPrivateAccessSettings(\"pas\",\n    account_id=databricks_account_id,\n    private_access_settings_name=f\"Private Access Settings for {prefix}\",\n    region=region,\n    public_access_enabled=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var pas = new Databricks.MwsPrivateAccessSettings(\"pas\", new()\n    {\n        AccountId = databricksAccountId,\n        PrivateAccessSettingsName = $\"Private Access Settings for {prefix}\",\n        Region = region,\n        PublicAccessEnabled = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsPrivateAccessSettings(ctx, \"pas\", \u0026databricks.MwsPrivateAccessSettingsArgs{\n\t\t\tAccountId:                 pulumi.Any(databricksAccountId),\n\t\t\tPrivateAccessSettingsName: pulumi.Sprintf(\"Private Access Settings for %v\", prefix),\n\t\t\tRegion:                    pulumi.Any(region),\n\t\t\tPublicAccessEnabled:       pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsPrivateAccessSettings;\nimport com.pulumi.databricks.MwsPrivateAccessSettingsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var pas = new MwsPrivateAccessSettings(\"pas\", MwsPrivateAccessSettingsArgs.builder()\n            .accountId(databricksAccountId)\n            .privateAccessSettingsName(String.format(\"Private Access Settings for %s\", prefix))\n            .region(region)\n            .publicAccessEnabled(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  pas:\n    type: databricks:MwsPrivateAccessSettings\n    properties:\n      accountId: ${databricksAccountId}\n      privateAccessSettingsName: Private Access Settings for ${prefix}\n      region: ${region}\n      publicAccessEnabled: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThe `databricks_mws_private_access_settings.pas.private_access_settings_id` can then be used as part of a databricks.MwsWorkspaces resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    awsRegion: region,\n    workspaceName: prefix,\n    credentialsId: thisDatabricksMwsCredentials.credentialsId,\n    storageConfigurationId: thisDatabricksMwsStorageConfigurations.storageConfigurationId,\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"ENTERPRISE\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    aws_region=region,\n    workspace_name=prefix,\n    credentials_id=this_databricks_mws_credentials[\"credentialsId\"],\n    storage_configuration_id=this_databricks_mws_storage_configurations[\"storageConfigurationId\"],\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"ENTERPRISE\",\n    opts = pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AwsRegion = region,\n        WorkspaceName = prefix,\n        CredentialsId = thisDatabricksMwsCredentials.CredentialsId,\n        StorageConfigurationId = thisDatabricksMwsStorageConfigurations.StorageConfigurationId,\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"ENTERPRISE\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAwsRegion:               pulumi.Any(region),\n\t\t\tWorkspaceName:           pulumi.Any(prefix),\n\t\t\tCredentialsId:           pulumi.Any(thisDatabricksMwsCredentials.CredentialsId),\n\t\t\tStorageConfigurationId:  pulumi.Any(thisDatabricksMwsStorageConfigurations.StorageConfigurationId),\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"ENTERPRISE\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()\n            .awsRegion(region)\n            .workspaceName(prefix)\n            .credentialsId(thisDatabricksMwsCredentials.credentialsId())\n            .storageConfigurationId(thisDatabricksMwsStorageConfigurations.storageConfigurationId())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"ENTERPRISE\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      awsRegion: ${region}\n      workspaceName: ${prefix}\n      credentialsId: ${thisDatabricksMwsCredentials.credentialsId}\n      storageConfigurationId: ${thisDatabricksMwsStorageConfigurations.storageConfigurationId}\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: ENTERPRISE\n    options:\n      dependsOn:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Databricks on GCP usage\n\n\u003e Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.gcp.databricks.com\"` and use `provider = databricks.mws`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    workspaceName: \"gcp-workspace\",\n    location: subnetRegion,\n    cloudResourceContainer: {\n        gcp: {\n            projectId: googleProject,\n        },\n    },\n    gkeConfig: {\n        connectivityType: \"PRIVATE_NODE_PUBLIC_MASTER\",\n        masterIpRange: \"10.3.0.0/28\",\n    },\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"PREMIUM\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    workspace_name=\"gcp-workspace\",\n    location=subnet_region,\n    cloud_resource_container={\n        \"gcp\": {\n            \"project_id\": google_project,\n        },\n    },\n    gke_config={\n        \"connectivity_type\": \"PRIVATE_NODE_PUBLIC_MASTER\",\n        \"master_ip_range\": \"10.3.0.0/28\",\n    },\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"PREMIUM\",\n    opts = pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        WorkspaceName = \"gcp-workspace\",\n        Location = subnetRegion,\n        CloudResourceContainer = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerArgs\n        {\n            Gcp = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerGcpArgs\n            {\n                ProjectId = googleProject,\n            },\n        },\n        GkeConfig = new Databricks.Inputs.MwsWorkspacesGkeConfigArgs\n        {\n            ConnectivityType = \"PRIVATE_NODE_PUBLIC_MASTER\",\n            MasterIpRange = \"10.3.0.0/28\",\n        },\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"PREMIUM\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tWorkspaceName: pulumi.String(\"gcp-workspace\"),\n\t\t\tLocation:      pulumi.Any(subnetRegion),\n\t\t\tCloudResourceContainer: \u0026databricks.MwsWorkspacesCloudResourceContainerArgs{\n\t\t\t\tGcp: \u0026databricks.MwsWorkspacesCloudResourceContainerGcpArgs{\n\t\t\t\t\tProjectId: pulumi.Any(googleProject),\n\t\t\t\t},\n\t\t\t},\n\t\t\tGkeConfig: \u0026databricks.MwsWorkspacesGkeConfigArgs{\n\t\t\t\tConnectivityType: pulumi.String(\"PRIVATE_NODE_PUBLIC_MASTER\"),\n\t\t\t\tMasterIpRange:    pulumi.String(\"10.3.0.0/28\"),\n\t\t\t},\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"PREMIUM\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerGcpArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesGkeConfigArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()\n            .workspaceName(\"gcp-workspace\")\n            .location(subnetRegion)\n            .cloudResourceContainer(MwsWorkspacesCloudResourceContainerArgs.builder()\n                .gcp(MwsWorkspacesCloudResourceContainerGcpArgs.builder()\n                    .projectId(googleProject)\n                    .build())\n                .build())\n            .gkeConfig(MwsWorkspacesGkeConfigArgs.builder()\n                .connectivityType(\"PRIVATE_NODE_PUBLIC_MASTER\")\n                .masterIpRange(\"10.3.0.0/28\")\n                .build())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"PREMIUM\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      workspaceName: gcp-workspace\n      location: ${subnetRegion}\n      cloudResourceContainer:\n        gcp:\n          projectId: ${googleProject}\n      gkeConfig:\n        connectivityType: PRIVATE_NODE_PUBLIC_MASTER\n        masterIpRange: 10.3.0.0/28\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: PREMIUM\n    options:\n      dependsOn:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with Private Link guide.\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* Provisioning Databricks workspaces on GCP with Private Service Connect guide.\n* databricks.MwsVpcEndpoint to register aws_vpc_endpoint resources with Databricks such that they can be used as part of a databricks.MwsNetworks configuration.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by Databricks account ID and private access settings ID.\n\n```sh\n$ pulumi import databricks:index/mwsPrivateAccessSettings:MwsPrivateAccessSettings this '\u003caccount_id\u003e/\u003cprivate_access_settings_id\u003e'\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                }
            },
            "required": [
                "accountId",
                "privateAccessSettingsId",
                "privateAccessSettingsName",
                "region"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                }
            },
            "requiredInputs": [
                "privateAccessSettingsName",
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPrivateAccessSettings resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                    },
                    "allowedVpcEndpointIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                    },
                    "privateAccessLevel": {
                        "type": "string",
                        "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                    },
                    "privateAccessSettingsName": {
                        "type": "string",
                        "description": "Name of Private Access Settings in Databricks Account\n"
                    },
                    "publicAccessEnabled": {
                        "type": "boolean",
                        "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsStorageConfigurations:MwsStorageConfigurations": {
            "description": "## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst rootStorageBucket = new aws.s3.BucketV2(\"root_storage_bucket\", {\n    bucket: `${prefix}-rootbucket`,\n    acl: \"private\",\n});\nconst rootVersioning = new aws.s3.BucketVersioningV2(\"root_versioning\", {\n    bucket: rootStorageBucket.id,\n    versioningConfiguration: {\n        status: \"Disabled\",\n    },\n});\nconst _this = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${prefix}-storage`,\n    bucketName: rootStorageBucket.bucket,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nroot_storage_bucket = aws.s3.BucketV2(\"root_storage_bucket\",\n    bucket=f\"{prefix}-rootbucket\",\n    acl=\"private\")\nroot_versioning = aws.s3.BucketVersioningV2(\"root_versioning\",\n    bucket=root_storage_bucket.id,\n    versioning_configuration={\n        \"status\": \"Disabled\",\n    })\nthis = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{prefix}-storage\",\n    bucket_name=root_storage_bucket.bucket)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var rootStorageBucket = new Aws.S3.BucketV2(\"root_storage_bucket\", new()\n    {\n        Bucket = $\"{prefix}-rootbucket\",\n        Acl = \"private\",\n    });\n\n    var rootVersioning = new Aws.S3.BucketVersioningV2(\"root_versioning\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        VersioningConfiguration = new Aws.S3.Inputs.BucketVersioningV2VersioningConfigurationArgs\n        {\n            Status = \"Disabled\",\n        },\n    });\n\n    var @this = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{prefix}-storage\",\n        BucketName = rootStorageBucket.Bucket,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\trootStorageBucket, err := s3.NewBucketV2(ctx, \"root_storage_bucket\", \u0026s3.BucketV2Args{\n\t\t\tBucket: pulumi.Sprintf(\"%v-rootbucket\", prefix),\n\t\t\tAcl:    pulumi.String(\"private\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = s3.NewBucketVersioningV2(ctx, \"root_versioning\", \u0026s3.BucketVersioningV2Args{\n\t\t\tBucket: rootStorageBucket.ID(),\n\t\t\tVersioningConfiguration: \u0026s3.BucketVersioningV2VersioningConfigurationArgs{\n\t\t\t\tStatus: pulumi.String(\"Disabled\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.Sprintf(\"%v-storage\", prefix),\n\t\t\tBucketName:               rootStorageBucket.Bucket,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.aws.s3.BucketVersioningV2;\nimport com.pulumi.aws.s3.BucketVersioningV2Args;\nimport com.pulumi.aws.s3.inputs.BucketVersioningV2VersioningConfigurationArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var rootStorageBucket = new BucketV2(\"rootStorageBucket\", BucketV2Args.builder()\n            .bucket(String.format(\"%s-rootbucket\", prefix))\n            .acl(\"private\")\n            .build());\n\n        var rootVersioning = new BucketVersioningV2(\"rootVersioning\", BucketVersioningV2Args.builder()\n            .bucket(rootStorageBucket.id())\n            .versioningConfiguration(BucketVersioningV2VersioningConfigurationArgs.builder()\n                .status(\"Disabled\")\n                .build())\n            .build());\n\n        var this_ = new MwsStorageConfigurations(\"this\", MwsStorageConfigurationsArgs.builder()\n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", prefix))\n            .bucketName(rootStorageBucket.bucket())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  rootStorageBucket:\n    type: aws:s3:BucketV2\n    name: root_storage_bucket\n    properties:\n      bucket: ${prefix}-rootbucket\n      acl: private\n  rootVersioning:\n    type: aws:s3:BucketVersioningV2\n    name: root_versioning\n    properties:\n      bucket: ${rootStorageBucket.id}\n      versioningConfiguration:\n        status: Disabled\n  this:\n    type: databricks:MwsStorageConfigurations\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${prefix}-storage\n      bucketName: ${rootStorageBucket.bucket}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with Private Link guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by Databricks account ID and storage configuration ID.\n\n```sh\n$ pulumi import databricks:index/mwsStorageConfigurations:MwsStorageConfigurations this '\u003caccount_id\u003e/\u003cstorage_configuration_id\u003e'\n```\n\n~\u003e This resource does not support updates. If your configuration does not match the existing resource,\n\n   the next `pulumi up` will cause the resource to be destroyed and recreated. After importing,\n\n   verify that the configuration matches the existing resource by running `pulumi preview`.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n"
                }
            },
            "required": [
                "accountId",
                "bucketName",
                "creationTime",
                "storageConfigurationId",
                "storageConfigurationName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "bucketName": {
                    "type": "string",
                    "description": "name of AWS S3 bucket\n",
                    "willReplaceOnChanges": true
                },
                "storageConfigurationName": {
                    "type": "string",
                    "description": "name under which this storage configuration is stored\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "bucketName",
                "storageConfigurationName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsStorageConfigurations resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "description": "name of AWS S3 bucket\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                    },
                    "storageConfigurationName": {
                        "type": "string",
                        "description": "name under which this storage configuration is stored\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsVpcEndpoint:MwsVpcEndpoint": {
            "description": "\u003e Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nEnables you to register aws_vpc_endpoint resources or gcp vpc_endpoint resources with Databricks such that they can be used as part of a databricks.MwsNetworks configuration.\n\nIt is strongly recommended that customers read the [Enable AWS Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) or the [Enable GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) documentation before trying to leverage this resource.\n\n## Example Usage\n\n### Databricks on AWS usage\n\nBefore using this resource, you will need to create the necessary VPC Endpoints as per your [VPC endpoint requirements](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#vpc-endpoint-requirements). You can use the aws_vpc_endpoint resource for this, for example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\n\nconst workspace = new aws.ec2.VpcEndpoint(\"workspace\", {\n    vpcId: vpc.vpcId,\n    serviceName: privateLink.workspaceService,\n    vpcEndpointType: \"Interface\",\n    securityGroupIds: [vpc.defaultSecurityGroupId],\n    subnetIds: [plSubnet.id],\n    privateDnsEnabled: true,\n}, {\n    dependsOn: [plSubnet],\n});\nconst relay = new aws.ec2.VpcEndpoint(\"relay\", {\n    vpcId: vpc.vpcId,\n    serviceName: privateLink.relayService,\n    vpcEndpointType: \"Interface\",\n    securityGroupIds: [vpc.defaultSecurityGroupId],\n    subnetIds: [plSubnet.id],\n    privateDnsEnabled: true,\n}, {\n    dependsOn: [plSubnet],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\n\nworkspace = aws.ec2.VpcEndpoint(\"workspace\",\n    vpc_id=vpc[\"vpcId\"],\n    service_name=private_link[\"workspaceService\"],\n    vpc_endpoint_type=\"Interface\",\n    security_group_ids=[vpc[\"defaultSecurityGroupId\"]],\n    subnet_ids=[pl_subnet[\"id\"]],\n    private_dns_enabled=True,\n    opts = pulumi.ResourceOptions(depends_on=[pl_subnet]))\nrelay = aws.ec2.VpcEndpoint(\"relay\",\n    vpc_id=vpc[\"vpcId\"],\n    service_name=private_link[\"relayService\"],\n    vpc_endpoint_type=\"Interface\",\n    security_group_ids=[vpc[\"defaultSecurityGroupId\"]],\n    subnet_ids=[pl_subnet[\"id\"]],\n    private_dns_enabled=True,\n    opts = pulumi.ResourceOptions(depends_on=[pl_subnet]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var workspace = new Aws.Ec2.VpcEndpoint(\"workspace\", new()\n    {\n        VpcId = vpc.VpcId,\n        ServiceName = privateLink.WorkspaceService,\n        VpcEndpointType = \"Interface\",\n        SecurityGroupIds = new[]\n        {\n            vpc.DefaultSecurityGroupId,\n        },\n        SubnetIds = new[]\n        {\n            plSubnet.Id,\n        },\n        PrivateDnsEnabled = true,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            plSubnet,\n        },\n    });\n\n    var relay = new Aws.Ec2.VpcEndpoint(\"relay\", new()\n    {\n        VpcId = vpc.VpcId,\n        ServiceName = privateLink.RelayService,\n        VpcEndpointType = \"Interface\",\n        SecurityGroupIds = new[]\n        {\n            vpc.DefaultSecurityGroupId,\n        },\n        SubnetIds = new[]\n        {\n            plSubnet.Id,\n        },\n        PrivateDnsEnabled = true,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            plSubnet,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/ec2\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := ec2.NewVpcEndpoint(ctx, \"workspace\", \u0026ec2.VpcEndpointArgs{\n\t\t\tVpcId:           pulumi.Any(vpc.VpcId),\n\t\t\tServiceName:     pulumi.Any(privateLink.WorkspaceService),\n\t\t\tVpcEndpointType: pulumi.String(\"Interface\"),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tvpc.DefaultSecurityGroupId,\n\t\t\t},\n\t\t\tSubnetIds: pulumi.StringArray{\n\t\t\t\tplSubnet.Id,\n\t\t\t},\n\t\t\tPrivateDnsEnabled: pulumi.Bool(true),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tplSubnet,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = ec2.NewVpcEndpoint(ctx, \"relay\", \u0026ec2.VpcEndpointArgs{\n\t\t\tVpcId:           pulumi.Any(vpc.VpcId),\n\t\t\tServiceName:     pulumi.Any(privateLink.RelayService),\n\t\t\tVpcEndpointType: pulumi.String(\"Interface\"),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tvpc.DefaultSecurityGroupId,\n\t\t\t},\n\t\t\tSubnetIds: pulumi.StringArray{\n\t\t\t\tplSubnet.Id,\n\t\t\t},\n\t\t\tPrivateDnsEnabled: pulumi.Bool(true),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tplSubnet,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.ec2.VpcEndpoint;\nimport com.pulumi.aws.ec2.VpcEndpointArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var workspace = new VpcEndpoint(\"workspace\", VpcEndpointArgs.builder()\n            .vpcId(vpc.vpcId())\n            .serviceName(privateLink.workspaceService())\n            .vpcEndpointType(\"Interface\")\n            .securityGroupIds(vpc.defaultSecurityGroupId())\n            .subnetIds(plSubnet.id())\n            .privateDnsEnabled(true)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(plSubnet)\n                .build());\n\n        var relay = new VpcEndpoint(\"relay\", VpcEndpointArgs.builder()\n            .vpcId(vpc.vpcId())\n            .serviceName(privateLink.relayService())\n            .vpcEndpointType(\"Interface\")\n            .securityGroupIds(vpc.defaultSecurityGroupId())\n            .subnetIds(plSubnet.id())\n            .privateDnsEnabled(true)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(plSubnet)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  workspace:\n    type: aws:ec2:VpcEndpoint\n    properties:\n      vpcId: ${vpc.vpcId}\n      serviceName: ${privateLink.workspaceService}\n      vpcEndpointType: Interface\n      securityGroupIds:\n        - ${vpc.defaultSecurityGroupId}\n      subnetIds:\n        - ${plSubnet.id}\n      privateDnsEnabled: true\n    options:\n      dependsOn:\n        - ${plSubnet}\n  relay:\n    type: aws:ec2:VpcEndpoint\n    properties:\n      vpcId: ${vpc.vpcId}\n      serviceName: ${privateLink.relayService}\n      vpcEndpointType: Interface\n      securityGroupIds:\n        - ${vpc.defaultSecurityGroupId}\n      subnetIds:\n        - ${plSubnet.id}\n      privateDnsEnabled: true\n    options:\n      dependsOn:\n        - ${plSubnet}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nDepending on your use case, you may need or choose to add VPC Endpoints for the AWS Services Databricks uses. See [Add VPC endpoints for other AWS services (recommended but optional)\n](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-9-add-vpc-endpoints-for-other-aws-services-recommended-but-optional) for more information. For example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\n\nconst s3 = new aws.ec2.VpcEndpoint(\"s3\", {\n    vpcId: vpc.vpcId,\n    routeTableIds: vpc.privateRouteTableIds,\n    serviceName: `com.amazonaws.${region}.s3`,\n}, {\n    dependsOn: [vpc],\n});\nconst sts = new aws.ec2.VpcEndpoint(\"sts\", {\n    vpcId: vpc.vpcId,\n    serviceName: `com.amazonaws.${region}.sts`,\n    vpcEndpointType: \"Interface\",\n    subnetIds: vpc.privateSubnets,\n    securityGroupIds: [vpc.defaultSecurityGroupId],\n    privateDnsEnabled: true,\n}, {\n    dependsOn: [vpc],\n});\nconst kinesis_streams = new aws.ec2.VpcEndpoint(\"kinesis-streams\", {\n    vpcId: vpc.vpcId,\n    serviceName: `com.amazonaws.${region}.kinesis-streams`,\n    vpcEndpointType: \"Interface\",\n    subnetIds: vpc.privateSubnets,\n    securityGroupIds: [vpc.defaultSecurityGroupId],\n}, {\n    dependsOn: [vpc],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\n\ns3 = aws.ec2.VpcEndpoint(\"s3\",\n    vpc_id=vpc[\"vpcId\"],\n    route_table_ids=vpc[\"privateRouteTableIds\"],\n    service_name=f\"com.amazonaws.{region}.s3\",\n    opts = pulumi.ResourceOptions(depends_on=[vpc]))\nsts = aws.ec2.VpcEndpoint(\"sts\",\n    vpc_id=vpc[\"vpcId\"],\n    service_name=f\"com.amazonaws.{region}.sts\",\n    vpc_endpoint_type=\"Interface\",\n    subnet_ids=vpc[\"privateSubnets\"],\n    security_group_ids=[vpc[\"defaultSecurityGroupId\"]],\n    private_dns_enabled=True,\n    opts = pulumi.ResourceOptions(depends_on=[vpc]))\nkinesis_streams = aws.ec2.VpcEndpoint(\"kinesis-streams\",\n    vpc_id=vpc[\"vpcId\"],\n    service_name=f\"com.amazonaws.{region}.kinesis-streams\",\n    vpc_endpoint_type=\"Interface\",\n    subnet_ids=vpc[\"privateSubnets\"],\n    security_group_ids=[vpc[\"defaultSecurityGroupId\"]],\n    opts = pulumi.ResourceOptions(depends_on=[vpc]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var s3 = new Aws.Ec2.VpcEndpoint(\"s3\", new()\n    {\n        VpcId = vpc.VpcId,\n        RouteTableIds = vpc.PrivateRouteTableIds,\n        ServiceName = $\"com.amazonaws.{region}.s3\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            vpc,\n        },\n    });\n\n    var sts = new Aws.Ec2.VpcEndpoint(\"sts\", new()\n    {\n        VpcId = vpc.VpcId,\n        ServiceName = $\"com.amazonaws.{region}.sts\",\n        VpcEndpointType = \"Interface\",\n        SubnetIds = vpc.PrivateSubnets,\n        SecurityGroupIds = new[]\n        {\n            vpc.DefaultSecurityGroupId,\n        },\n        PrivateDnsEnabled = true,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            vpc,\n        },\n    });\n\n    var kinesis_streams = new Aws.Ec2.VpcEndpoint(\"kinesis-streams\", new()\n    {\n        VpcId = vpc.VpcId,\n        ServiceName = $\"com.amazonaws.{region}.kinesis-streams\",\n        VpcEndpointType = \"Interface\",\n        SubnetIds = vpc.PrivateSubnets,\n        SecurityGroupIds = new[]\n        {\n            vpc.DefaultSecurityGroupId,\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            vpc,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/ec2\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := ec2.NewVpcEndpoint(ctx, \"s3\", \u0026ec2.VpcEndpointArgs{\n\t\t\tVpcId:         pulumi.Any(vpc.VpcId),\n\t\t\tRouteTableIds: pulumi.Any(vpc.PrivateRouteTableIds),\n\t\t\tServiceName:   pulumi.Sprintf(\"com.amazonaws.%v.s3\", region),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tvpc,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = ec2.NewVpcEndpoint(ctx, \"sts\", \u0026ec2.VpcEndpointArgs{\n\t\t\tVpcId:           pulumi.Any(vpc.VpcId),\n\t\t\tServiceName:     pulumi.Sprintf(\"com.amazonaws.%v.sts\", region),\n\t\t\tVpcEndpointType: pulumi.String(\"Interface\"),\n\t\t\tSubnetIds:       pulumi.Any(vpc.PrivateSubnets),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tvpc.DefaultSecurityGroupId,\n\t\t\t},\n\t\t\tPrivateDnsEnabled: pulumi.Bool(true),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tvpc,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = ec2.NewVpcEndpoint(ctx, \"kinesis-streams\", \u0026ec2.VpcEndpointArgs{\n\t\t\tVpcId:           pulumi.Any(vpc.VpcId),\n\t\t\tServiceName:     pulumi.Sprintf(\"com.amazonaws.%v.kinesis-streams\", region),\n\t\t\tVpcEndpointType: pulumi.String(\"Interface\"),\n\t\t\tSubnetIds:       pulumi.Any(vpc.PrivateSubnets),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tvpc.DefaultSecurityGroupId,\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tvpc,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.ec2.VpcEndpoint;\nimport com.pulumi.aws.ec2.VpcEndpointArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var s3 = new VpcEndpoint(\"s3\", VpcEndpointArgs.builder()\n            .vpcId(vpc.vpcId())\n            .routeTableIds(vpc.privateRouteTableIds())\n            .serviceName(String.format(\"com.amazonaws.%s.s3\", region))\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(vpc)\n                .build());\n\n        var sts = new VpcEndpoint(\"sts\", VpcEndpointArgs.builder()\n            .vpcId(vpc.vpcId())\n            .serviceName(String.format(\"com.amazonaws.%s.sts\", region))\n            .vpcEndpointType(\"Interface\")\n            .subnetIds(vpc.privateSubnets())\n            .securityGroupIds(vpc.defaultSecurityGroupId())\n            .privateDnsEnabled(true)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(vpc)\n                .build());\n\n        var kinesis_streams = new VpcEndpoint(\"kinesis-streams\", VpcEndpointArgs.builder()\n            .vpcId(vpc.vpcId())\n            .serviceName(String.format(\"com.amazonaws.%s.kinesis-streams\", region))\n            .vpcEndpointType(\"Interface\")\n            .subnetIds(vpc.privateSubnets())\n            .securityGroupIds(vpc.defaultSecurityGroupId())\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(vpc)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  s3:\n    type: aws:ec2:VpcEndpoint\n    properties:\n      vpcId: ${vpc.vpcId}\n      routeTableIds: ${vpc.privateRouteTableIds}\n      serviceName: com.amazonaws.${region}.s3\n    options:\n      dependsOn:\n        - ${vpc}\n  sts:\n    type: aws:ec2:VpcEndpoint\n    properties:\n      vpcId: ${vpc.vpcId}\n      serviceName: com.amazonaws.${region}.sts\n      vpcEndpointType: Interface\n      subnetIds: ${vpc.privateSubnets}\n      securityGroupIds:\n        - ${vpc.defaultSecurityGroupId}\n      privateDnsEnabled: true\n    options:\n      dependsOn:\n        - ${vpc}\n  kinesis-streams:\n    type: aws:ec2:VpcEndpoint\n    properties:\n      vpcId: ${vpc.vpcId}\n      serviceName: com.amazonaws.${region}.kinesis-streams\n      vpcEndpointType: Interface\n      subnetIds: ${vpc.privateSubnets}\n      securityGroupIds:\n        - ${vpc.defaultSecurityGroupId}\n    options:\n      dependsOn:\n        - ${vpc}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nOnce you have created the necessary endpoints, you need to register each of them via *this* Pulumi resource, which calls out to the [Databricks Account API](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-3-register-your-vpc-endpoint-ids-with-the-account-api)):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst workspace = new databricks.MwsVpcEndpoint(\"workspace\", {\n    accountId: databricksAccountId,\n    awsVpcEndpointId: workspaceAwsVpcEndpoint.id,\n    vpcEndpointName: `VPC Relay for ${vpc.vpcId}`,\n    region: region,\n}, {\n    dependsOn: [workspaceAwsVpcEndpoint],\n});\nconst relay = new databricks.MwsVpcEndpoint(\"relay\", {\n    accountId: databricksAccountId,\n    awsVpcEndpointId: relayAwsVpcEndpoint.id,\n    vpcEndpointName: `VPC Relay for ${vpc.vpcId}`,\n    region: region,\n}, {\n    dependsOn: [relayAwsVpcEndpoint],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nworkspace = databricks.MwsVpcEndpoint(\"workspace\",\n    account_id=databricks_account_id,\n    aws_vpc_endpoint_id=workspace_aws_vpc_endpoint[\"id\"],\n    vpc_endpoint_name=f\"VPC Relay for {vpc['vpcId']}\",\n    region=region,\n    opts = pulumi.ResourceOptions(depends_on=[workspace_aws_vpc_endpoint]))\nrelay = databricks.MwsVpcEndpoint(\"relay\",\n    account_id=databricks_account_id,\n    aws_vpc_endpoint_id=relay_aws_vpc_endpoint[\"id\"],\n    vpc_endpoint_name=f\"VPC Relay for {vpc['vpcId']}\",\n    region=region,\n    opts = pulumi.ResourceOptions(depends_on=[relay_aws_vpc_endpoint]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var workspace = new Databricks.MwsVpcEndpoint(\"workspace\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsVpcEndpointId = workspaceAwsVpcEndpoint.Id,\n        VpcEndpointName = $\"VPC Relay for {vpc.VpcId}\",\n        Region = region,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            workspaceAwsVpcEndpoint,\n        },\n    });\n\n    var relay = new Databricks.MwsVpcEndpoint(\"relay\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsVpcEndpointId = relayAwsVpcEndpoint.Id,\n        VpcEndpointName = $\"VPC Relay for {vpc.VpcId}\",\n        Region = region,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            relayAwsVpcEndpoint,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsVpcEndpoint(ctx, \"workspace\", \u0026databricks.MwsVpcEndpointArgs{\n\t\t\tAccountId:        pulumi.Any(databricksAccountId),\n\t\t\tAwsVpcEndpointId: pulumi.Any(workspaceAwsVpcEndpoint.Id),\n\t\t\tVpcEndpointName:  pulumi.Sprintf(\"VPC Relay for %v\", vpc.VpcId),\n\t\t\tRegion:           pulumi.Any(region),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tworkspaceAwsVpcEndpoint,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsVpcEndpoint(ctx, \"relay\", \u0026databricks.MwsVpcEndpointArgs{\n\t\t\tAccountId:        pulumi.Any(databricksAccountId),\n\t\t\tAwsVpcEndpointId: pulumi.Any(relayAwsVpcEndpoint.Id),\n\t\t\tVpcEndpointName:  pulumi.Sprintf(\"VPC Relay for %v\", vpc.VpcId),\n\t\t\tRegion:           pulumi.Any(region),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\trelayAwsVpcEndpoint,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsVpcEndpoint;\nimport com.pulumi.databricks.MwsVpcEndpointArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var workspace = new MwsVpcEndpoint(\"workspace\", MwsVpcEndpointArgs.builder()\n            .accountId(databricksAccountId)\n            .awsVpcEndpointId(workspaceAwsVpcEndpoint.id())\n            .vpcEndpointName(String.format(\"VPC Relay for %s\", vpc.vpcId()))\n            .region(region)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(workspaceAwsVpcEndpoint)\n                .build());\n\n        var relay = new MwsVpcEndpoint(\"relay\", MwsVpcEndpointArgs.builder()\n            .accountId(databricksAccountId)\n            .awsVpcEndpointId(relayAwsVpcEndpoint.id())\n            .vpcEndpointName(String.format(\"VPC Relay for %s\", vpc.vpcId()))\n            .region(region)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(relayAwsVpcEndpoint)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  workspace:\n    type: databricks:MwsVpcEndpoint\n    properties:\n      accountId: ${databricksAccountId}\n      awsVpcEndpointId: ${workspaceAwsVpcEndpoint.id}\n      vpcEndpointName: VPC Relay for ${vpc.vpcId}\n      region: ${region}\n    options:\n      dependsOn:\n        - ${workspaceAwsVpcEndpoint}\n  relay:\n    type: databricks:MwsVpcEndpoint\n    properties:\n      accountId: ${databricksAccountId}\n      awsVpcEndpointId: ${relayAwsVpcEndpoint.id}\n      vpcEndpointName: VPC Relay for ${vpc.vpcId}\n      region: ${region}\n    options:\n      dependsOn:\n        - ${relayAwsVpcEndpoint}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nTypically the next steps after this would be to create a databricks.MwsPrivateAccessSettings and databricks.MwsNetworks configuration, before passing the `databricks_mws_private_access_settings.pas.private_access_settings_id` and `databricks_mws_networks.this.network_id` into a databricks.MwsWorkspaces resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    awsRegion: region,\n    workspaceName: prefix,\n    credentialsId: thisDatabricksMwsCredentials.credentialsId,\n    storageConfigurationId: thisDatabricksMwsStorageConfigurations.storageConfigurationId,\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"ENTERPRISE\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    aws_region=region,\n    workspace_name=prefix,\n    credentials_id=this_databricks_mws_credentials[\"credentialsId\"],\n    storage_configuration_id=this_databricks_mws_storage_configurations[\"storageConfigurationId\"],\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"ENTERPRISE\",\n    opts = pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsRegion = region,\n        WorkspaceName = prefix,\n        CredentialsId = thisDatabricksMwsCredentials.CredentialsId,\n        StorageConfigurationId = thisDatabricksMwsStorageConfigurations.StorageConfigurationId,\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"ENTERPRISE\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:               pulumi.Any(databricksAccountId),\n\t\t\tAwsRegion:               pulumi.Any(region),\n\t\t\tWorkspaceName:           pulumi.Any(prefix),\n\t\t\tCredentialsId:           pulumi.Any(thisDatabricksMwsCredentials.CredentialsId),\n\t\t\tStorageConfigurationId:  pulumi.Any(thisDatabricksMwsStorageConfigurations.StorageConfigurationId),\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"ENTERPRISE\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()\n            .accountId(databricksAccountId)\n            .awsRegion(region)\n            .workspaceName(prefix)\n            .credentialsId(thisDatabricksMwsCredentials.credentialsId())\n            .storageConfigurationId(thisDatabricksMwsStorageConfigurations.storageConfigurationId())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"ENTERPRISE\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      accountId: ${databricksAccountId}\n      awsRegion: ${region}\n      workspaceName: ${prefix}\n      credentialsId: ${thisDatabricksMwsCredentials.credentialsId}\n      storageConfigurationId: ${thisDatabricksMwsStorageConfigurations.storageConfigurationId}\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: ENTERPRISE\n    options:\n      dependsOn:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Databricks on GCP usage\n\nBefore using this resource, you will need to create the necessary Private Service Connect (PSC) connections on your Google Cloud VPC networks. You can see [Enable Private Service Connect for your workspace](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) for more details.\n\nOnce you have created the necessary PSC connections, you need to register each of them via *this* Pulumi resource, which calls out to the Databricks Account API.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in https://accounts.gcp.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst databricksGoogleServiceAccount = config.requireObject\u003cany\u003e(\"databricksGoogleServiceAccount\");\nconst googleProject = config.requireObject\u003cany\u003e(\"googleProject\");\nconst subnetRegion = config.requireObject\u003cany\u003e(\"subnetRegion\");\nconst workspace = new databricks.MwsVpcEndpoint(\"workspace\", {\n    accountId: databricksAccountId,\n    vpcEndpointName: \"PSC Rest API endpoint\",\n    gcpVpcEndpointInfo: {\n        projectId: googleProject,\n        pscEndpointName: \"PSC Rest API endpoint\",\n        endpointRegion: subnetRegion,\n    },\n});\nconst relay = new databricks.MwsVpcEndpoint(\"relay\", {\n    accountId: databricksAccountId,\n    vpcEndpointName: \"PSC Relay endpoint\",\n    gcpVpcEndpointInfo: {\n        projectId: googleProject,\n        pscEndpointName: \"PSC Relay endpoint\",\n        endpointRegion: subnetRegion,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in https://accounts.gcp.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndatabricks_google_service_account = config.require_object(\"databricksGoogleServiceAccount\")\ngoogle_project = config.require_object(\"googleProject\")\nsubnet_region = config.require_object(\"subnetRegion\")\nworkspace = databricks.MwsVpcEndpoint(\"workspace\",\n    account_id=databricks_account_id,\n    vpc_endpoint_name=\"PSC Rest API endpoint\",\n    gcp_vpc_endpoint_info={\n        \"project_id\": google_project,\n        \"psc_endpoint_name\": \"PSC Rest API endpoint\",\n        \"endpoint_region\": subnet_region,\n    })\nrelay = databricks.MwsVpcEndpoint(\"relay\",\n    account_id=databricks_account_id,\n    vpc_endpoint_name=\"PSC Relay endpoint\",\n    gcp_vpc_endpoint_info={\n        \"project_id\": google_project,\n        \"psc_endpoint_name\": \"PSC Relay endpoint\",\n        \"endpoint_region\": subnet_region,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in https://accounts.gcp.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var databricksGoogleServiceAccount = config.RequireObject\u003cdynamic\u003e(\"databricksGoogleServiceAccount\");\n    var googleProject = config.RequireObject\u003cdynamic\u003e(\"googleProject\");\n    var subnetRegion = config.RequireObject\u003cdynamic\u003e(\"subnetRegion\");\n    var workspace = new Databricks.MwsVpcEndpoint(\"workspace\", new()\n    {\n        AccountId = databricksAccountId,\n        VpcEndpointName = \"PSC Rest API endpoint\",\n        GcpVpcEndpointInfo = new Databricks.Inputs.MwsVpcEndpointGcpVpcEndpointInfoArgs\n        {\n            ProjectId = googleProject,\n            PscEndpointName = \"PSC Rest API endpoint\",\n            EndpointRegion = subnetRegion,\n        },\n    });\n\n    var relay = new Databricks.MwsVpcEndpoint(\"relay\", new()\n    {\n        AccountId = databricksAccountId,\n        VpcEndpointName = \"PSC Relay endpoint\",\n        GcpVpcEndpointInfo = new Databricks.Inputs.MwsVpcEndpointGcpVpcEndpointInfoArgs\n        {\n            ProjectId = googleProject,\n            PscEndpointName = \"PSC Relay endpoint\",\n            EndpointRegion = subnetRegion,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in https://accounts.gcp.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tdatabricksGoogleServiceAccount := cfg.RequireObject(\"databricksGoogleServiceAccount\")\n\t\tgoogleProject := cfg.RequireObject(\"googleProject\")\n\t\tsubnetRegion := cfg.RequireObject(\"subnetRegion\")\n\t\t_, err := databricks.NewMwsVpcEndpoint(ctx, \"workspace\", \u0026databricks.MwsVpcEndpointArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tVpcEndpointName: pulumi.String(\"PSC Rest API endpoint\"),\n\t\t\tGcpVpcEndpointInfo: \u0026databricks.MwsVpcEndpointGcpVpcEndpointInfoArgs{\n\t\t\t\tProjectId:       pulumi.Any(googleProject),\n\t\t\t\tPscEndpointName: pulumi.String(\"PSC Rest API endpoint\"),\n\t\t\t\tEndpointRegion:  pulumi.Any(subnetRegion),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsVpcEndpoint(ctx, \"relay\", \u0026databricks.MwsVpcEndpointArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tVpcEndpointName: pulumi.String(\"PSC Relay endpoint\"),\n\t\t\tGcpVpcEndpointInfo: \u0026databricks.MwsVpcEndpointGcpVpcEndpointInfoArgs{\n\t\t\t\tProjectId:       pulumi.Any(googleProject),\n\t\t\t\tPscEndpointName: pulumi.String(\"PSC Relay endpoint\"),\n\t\t\t\tEndpointRegion:  pulumi.Any(subnetRegion),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsVpcEndpoint;\nimport com.pulumi.databricks.MwsVpcEndpointArgs;\nimport com.pulumi.databricks.inputs.MwsVpcEndpointGcpVpcEndpointInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksGoogleServiceAccount = config.get(\"databricksGoogleServiceAccount\");\n        final var googleProject = config.get(\"googleProject\");\n        final var subnetRegion = config.get(\"subnetRegion\");\n        var workspace = new MwsVpcEndpoint(\"workspace\", MwsVpcEndpointArgs.builder()\n            .accountId(databricksAccountId)\n            .vpcEndpointName(\"PSC Rest API endpoint\")\n            .gcpVpcEndpointInfo(MwsVpcEndpointGcpVpcEndpointInfoArgs.builder()\n                .projectId(googleProject)\n                .pscEndpointName(\"PSC Rest API endpoint\")\n                .endpointRegion(subnetRegion)\n                .build())\n            .build());\n\n        var relay = new MwsVpcEndpoint(\"relay\", MwsVpcEndpointArgs.builder()\n            .accountId(databricksAccountId)\n            .vpcEndpointName(\"PSC Relay endpoint\")\n            .gcpVpcEndpointInfo(MwsVpcEndpointGcpVpcEndpointInfoArgs.builder()\n                .projectId(googleProject)\n                .pscEndpointName(\"PSC Relay endpoint\")\n                .endpointRegion(subnetRegion)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksGoogleServiceAccount:\n    type: dynamic\n  googleProject:\n    type: dynamic\n  subnetRegion:\n    type: dynamic\nresources:\n  workspace:\n    type: databricks:MwsVpcEndpoint\n    properties:\n      accountId: ${databricksAccountId}\n      vpcEndpointName: PSC Rest API endpoint\n      gcpVpcEndpointInfo:\n        projectId: ${googleProject}\n        pscEndpointName: PSC Rest API endpoint\n        endpointRegion: ${subnetRegion}\n  relay:\n    type: databricks:MwsVpcEndpoint\n    properties:\n      accountId: ${databricksAccountId}\n      vpcEndpointName: PSC Relay endpoint\n      gcpVpcEndpointInfo:\n        projectId: ${googleProject}\n        pscEndpointName: PSC Relay endpoint\n        endpointRegion: ${subnetRegion}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nTypically the next steps after this would be to create a databricks.MwsPrivateAccessSettings and databricks.MwsNetworks configuration, before passing the `databricks_mws_private_access_settings.pas.private_access_settings_id` and `databricks_mws_networks.this.network_id` into a databricks.MwsWorkspaces resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: \"gcp workspace\",\n    location: subnetRegion,\n    cloudResourceContainer: {\n        gcp: {\n            projectId: googleProject,\n        },\n    },\n    gkeConfig: {\n        connectivityType: \"PRIVATE_NODE_PUBLIC_MASTER\",\n        masterIpRange: \"10.3.0.0/28\",\n    },\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"PREMIUM\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=\"gcp workspace\",\n    location=subnet_region,\n    cloud_resource_container={\n        \"gcp\": {\n            \"project_id\": google_project,\n        },\n    },\n    gke_config={\n        \"connectivity_type\": \"PRIVATE_NODE_PUBLIC_MASTER\",\n        \"master_ip_range\": \"10.3.0.0/28\",\n    },\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"PREMIUM\",\n    opts = pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = \"gcp workspace\",\n        Location = subnetRegion,\n        CloudResourceContainer = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerArgs\n        {\n            Gcp = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerGcpArgs\n            {\n                ProjectId = googleProject,\n            },\n        },\n        GkeConfig = new Databricks.Inputs.MwsWorkspacesGkeConfigArgs\n        {\n            ConnectivityType = \"PRIVATE_NODE_PUBLIC_MASTER\",\n            MasterIpRange = \"10.3.0.0/28\",\n        },\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"PREMIUM\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:     pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName: pulumi.String(\"gcp workspace\"),\n\t\t\tLocation:      pulumi.Any(subnetRegion),\n\t\t\tCloudResourceContainer: \u0026databricks.MwsWorkspacesCloudResourceContainerArgs{\n\t\t\t\tGcp: \u0026databricks.MwsWorkspacesCloudResourceContainerGcpArgs{\n\t\t\t\t\tProjectId: pulumi.Any(googleProject),\n\t\t\t\t},\n\t\t\t},\n\t\t\tGkeConfig: \u0026databricks.MwsWorkspacesGkeConfigArgs{\n\t\t\t\tConnectivityType: pulumi.String(\"PRIVATE_NODE_PUBLIC_MASTER\"),\n\t\t\t\tMasterIpRange:    pulumi.String(\"10.3.0.0/28\"),\n\t\t\t},\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"PREMIUM\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerGcpArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesGkeConfigArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()\n            .accountId(databricksAccountId)\n            .workspaceName(\"gcp workspace\")\n            .location(subnetRegion)\n            .cloudResourceContainer(MwsWorkspacesCloudResourceContainerArgs.builder()\n                .gcp(MwsWorkspacesCloudResourceContainerGcpArgs.builder()\n                    .projectId(googleProject)\n                    .build())\n                .build())\n            .gkeConfig(MwsWorkspacesGkeConfigArgs.builder()\n                .connectivityType(\"PRIVATE_NODE_PUBLIC_MASTER\")\n                .masterIpRange(\"10.3.0.0/28\")\n                .build())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"PREMIUM\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: gcp workspace\n      location: ${subnetRegion}\n      cloudResourceContainer:\n        gcp:\n          projectId: ${googleProject}\n      gkeConfig:\n        connectivityType: PRIVATE_NODE_PUBLIC_MASTER\n        masterIpRange: 10.3.0.0/28\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: PREMIUM\n    options:\n      dependsOn:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with Private Link guide.\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* Provisioning Databricks workspaces on GCP with Private Service Connect guide.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsPrivateAccessSettings to create a [Private Access Setting](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-5-create-a-private-access-settings-configuration-using-the-databricks-account-api) that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n"
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "description": "ID of configured aws_vpc_endpoint\n"
                },
                "gcpVpcEndpointInfo": {
                    "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                    "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "state": {
                    "type": "string",
                    "description": "(AWS Only) State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n"
                }
            },
            "required": [
                "awsAccountId",
                "awsEndpointServiceId",
                "state",
                "useCase",
                "vpcEndpointId",
                "vpcEndpointName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "description": "ID of configured aws_vpc_endpoint\n",
                    "willReplaceOnChanges": true
                },
                "gcpVpcEndpointInfo": {
                    "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                    "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n",
                    "willReplaceOnChanges": true
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n",
                    "willReplaceOnChanges": true
                },
                "state": {
                    "type": "string",
                    "description": "(AWS Only) State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "vpcEndpointName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsVpcEndpoint resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsEndpointServiceId": {
                        "type": "string",
                        "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                    },
                    "awsVpcEndpointId": {
                        "type": "string",
                        "description": "ID of configured aws_vpc_endpoint\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpVpcEndpointInfo": {
                        "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                        "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "state": {
                        "type": "string",
                        "description": "(AWS Only) State of VPC Endpoint\n"
                    },
                    "useCase": {
                        "type": "string"
                    },
                    "vpcEndpointId": {
                        "type": "string",
                        "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                    },
                    "vpcEndpointName": {
                        "type": "string",
                        "description": "Name of VPC Endpoint in Databricks Account\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsWorkspaces:MwsWorkspaces": {
            "description": "## Example Usage\n\n### Creating a Databricks on AWS workspace\n\n!Simplest multiworkspace\n\nTo get workspace running, you have to configure a couple of things:\n\n* databricks.MwsCredentials - You can share a credentials (cross-account IAM role) configuration ID with multiple workspaces. It is not required to create a new one for each workspace.\n* databricks.MwsStorageConfigurations - You can share a root S3 bucket with multiple workspaces in a single account. You do not have to create new ones for each workspace. If you share a root S3 bucket for multiple workspaces in an account, data on the root S3 bucket is partitioned into separate directories by workspace.\n* databricks.MwsNetworks - (optional, but recommended) You can share one [customer-managed VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) with multiple workspaces in a single account. However, Databricks recommends using unique subnets and security groups for each workspace. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks databricks.MwsNetworks encapsulates this information, you cannot reuse it across workspaces.\n* databricks.MwsCustomerManagedKeys - You can share a customer-managed key across workspaces.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\n// register cross-account ARN\nconst _this = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossaccountArn,\n});\n// register root bucket\nconst thisMwsStorageConfigurations = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${prefix}-storage`,\n    bucketName: rootBucket,\n});\n// register VPC\nconst thisMwsNetworks = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `${prefix}-network`,\n    vpcId: vpcId,\n    subnetIds: subnetsPrivate,\n    securityGroupIds: [securityGroup],\n});\n// create workspace in given VPC with DBFS on root bucket\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    awsRegion: region,\n    credentialsId: _this.credentialsId,\n    storageConfigurationId: thisMwsStorageConfigurations.storageConfigurationId,\n    networkId: thisMwsNetworks.networkId,\n    token: {},\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# register cross-account ARN\nthis = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=crossaccount_arn)\n# register root bucket\nthis_mws_storage_configurations = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{prefix}-storage\",\n    bucket_name=root_bucket)\n# register VPC\nthis_mws_networks = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"{prefix}-network\",\n    vpc_id=vpc_id,\n    subnet_ids=subnets_private,\n    security_group_ids=[security_group])\n# create workspace in given VPC with DBFS on root bucket\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    aws_region=region,\n    credentials_id=this.credentials_id,\n    storage_configuration_id=this_mws_storage_configurations.storage_configuration_id,\n    network_id=this_mws_networks.network_id,\n    token={})\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // register cross-account ARN\n    var @this = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossaccountArn,\n    });\n\n    // register root bucket\n    var thisMwsStorageConfigurations = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{prefix}-storage\",\n        BucketName = rootBucket,\n    });\n\n    // register VPC\n    var thisMwsNetworks = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"{prefix}-network\",\n        VpcId = vpcId,\n        SubnetIds = subnetsPrivate,\n        SecurityGroupIds = new[]\n        {\n            securityGroup,\n        },\n    });\n\n    // create workspace in given VPC with DBFS on root bucket\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        AwsRegion = region,\n        CredentialsId = @this.CredentialsId,\n        StorageConfigurationId = thisMwsStorageConfigurations.StorageConfigurationId,\n        NetworkId = thisMwsNetworks.NetworkId,\n        Token = null,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// register cross-account ARN\n\t\tthis, err := databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.Sprintf(\"%v-creds\", prefix),\n\t\t\tRoleArn:         pulumi.Any(crossaccountArn),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// register root bucket\n\t\tthisMwsStorageConfigurations, err := databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.Sprintf(\"%v-storage\", prefix),\n\t\t\tBucketName:               pulumi.Any(rootBucket),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// register VPC\n\t\tthisMwsNetworks, err := databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.Sprintf(\"%v-network\", prefix),\n\t\t\tVpcId:       pulumi.Any(vpcId),\n\t\t\tSubnetIds:   pulumi.Any(subnetsPrivate),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tsecurityGroup,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// create workspace in given VPC with DBFS on root bucket\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName:          pulumi.Any(prefix),\n\t\t\tAwsRegion:              pulumi.Any(region),\n\t\t\tCredentialsId:          this.CredentialsId,\n\t\t\tStorageConfigurationId: thisMwsStorageConfigurations.StorageConfigurationId,\n\t\t\tNetworkId:              thisMwsNetworks.NetworkId,\n\t\t\tToken:                  \u0026databricks.MwsWorkspacesTokenArgs{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        // register cross-account ARN\n        var this_ = new MwsCredentials(\"this\", MwsCredentialsArgs.builder()\n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossaccountArn)\n            .build());\n\n        // register root bucket\n        var thisMwsStorageConfigurations = new MwsStorageConfigurations(\"thisMwsStorageConfigurations\", MwsStorageConfigurationsArgs.builder()\n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", prefix))\n            .bucketName(rootBucket)\n            .build());\n\n        // register VPC\n        var thisMwsNetworks = new MwsNetworks(\"thisMwsNetworks\", MwsNetworksArgs.builder()\n            .accountId(databricksAccountId)\n            .networkName(String.format(\"%s-network\", prefix))\n            .vpcId(vpcId)\n            .subnetIds(subnetsPrivate)\n            .securityGroupIds(securityGroup)\n            .build());\n\n        // create workspace in given VPC with DBFS on root bucket\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()\n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .awsRegion(region)\n            .credentialsId(this_.credentialsId())\n            .storageConfigurationId(thisMwsStorageConfigurations.storageConfigurationId())\n            .networkId(thisMwsNetworks.networkId())\n            .token(MwsWorkspacesTokenArgs.builder()\n                .build())\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(_token -\u003e _token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  # register cross-account ARN\n  this:\n    type: databricks:MwsCredentials\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossaccountArn}\n  # register root bucket\n  thisMwsStorageConfigurations:\n    type: databricks:MwsStorageConfigurations\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${prefix}-storage\n      bucketName: ${rootBucket}\n  # register VPC\n  thisMwsNetworks:\n    type: databricks:MwsNetworks\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: ${prefix}-network\n      vpcId: ${vpcId}\n      subnetIds: ${subnetsPrivate}\n      securityGroupIds:\n        - ${securityGroup}\n  # create workspace in given VPC with DBFS on root bucket\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      awsRegion: ${region}\n      credentialsId: ${this.credentialsId}\n      storageConfigurationId: ${thisMwsStorageConfigurations.storageConfigurationId}\n      networkId: ${thisMwsNetworks.networkId}\n      token: {}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Creating a Databricks on AWS workspace with Databricks-Managed VPC\n\n![VPCs](https://docs.databricks.com/_images/customer-managed-vpc.png)\n\nBy default, Databricks creates a VPC in your AWS account for each workspace. Databricks uses it for running clusters in the workspace. Optionally, you can use your VPC for the workspace, using the feature customer-managed VPC. Databricks recommends that you provide your VPC with databricks.MwsNetworks so that you can configure it according to your organization’s enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your VPC. Please see the difference described through IAM policy actions [on this page](https://docs.databricks.com/administration-guide/account-api/iam-role.html).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as random from \"@pulumi/random\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst naming = new random.index.String(\"naming\", {\n    special: false,\n    upper: false,\n    length: 6,\n});\nconst prefix = `dltp${naming.result}`;\nconst _this = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccountRole = new aws.iam.Role(\"cross_account_role\", {\n    name: `${prefix}-crossaccount`,\n    assumeRolePolicy: _this.then(_this =\u003e _this.json),\n    tags: tags,\n});\nconst thisGetAwsCrossAccountPolicy = databricks.getAwsCrossAccountPolicy({});\nconst thisRolePolicy = new aws.iam.RolePolicy(\"this\", {\n    name: `${prefix}-policy`,\n    role: crossAccountRole.id,\n    policy: thisGetAwsCrossAccountPolicy.then(thisGetAwsCrossAccountPolicy =\u003e thisGetAwsCrossAccountPolicy.json),\n});\nconst thisMwsCredentials = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossAccountRole.arn,\n});\nconst rootStorageBucket = new aws.s3.BucketV2(\"root_storage_bucket\", {\n    bucket: `${prefix}-rootbucket`,\n    acl: \"private\",\n    forceDestroy: true,\n    tags: tags,\n});\nconst rootVersioning = new aws.s3.BucketVersioningV2(\"root_versioning\", {\n    bucket: rootStorageBucket.id,\n    versioningConfiguration: {\n        status: \"Disabled\",\n    },\n});\nconst rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new aws.s3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\", {\n    bucket: rootStorageBucket.bucket,\n    rules: [{\n        applyServerSideEncryptionByDefault: {\n            sseAlgorithm: \"AES256\",\n        },\n    }],\n});\nconst rootStorageBucketBucketPublicAccessBlock = new aws.s3.BucketPublicAccessBlock(\"root_storage_bucket\", {\n    bucket: rootStorageBucket.id,\n    blockPublicAcls: true,\n    blockPublicPolicy: true,\n    ignorePublicAcls: true,\n    restrictPublicBuckets: true,\n}, {\n    dependsOn: [rootStorageBucket],\n});\nconst thisGetAwsBucketPolicy = databricks.getAwsBucketPolicyOutput({\n    bucket: rootStorageBucket.bucket,\n});\nconst rootBucketPolicy = new aws.s3.BucketPolicy(\"root_bucket_policy\", {\n    bucket: rootStorageBucket.id,\n    policy: thisGetAwsBucketPolicy.apply(thisGetAwsBucketPolicy =\u003e thisGetAwsBucketPolicy.json),\n}, {\n    dependsOn: [rootStorageBucketBucketPublicAccessBlock],\n});\nconst thisMwsStorageConfigurations = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${prefix}-storage`,\n    bucketName: rootStorageBucket.bucket,\n});\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    awsRegion: \"us-east-1\",\n    credentialsId: thisMwsCredentials.credentialsId,\n    storageConfigurationId: thisMwsStorageConfigurations.storageConfigurationId,\n    token: {},\n    customTags: {\n        SoldToCode: \"1234\",\n    },\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\nimport pulumi_random as random\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nnaming = random.index.String(\"naming\",\n    special=False,\n    upper=False,\n    length=6)\nprefix = f\"dltp{naming['result']}\"\nthis = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account_role = aws.iam.Role(\"cross_account_role\",\n    name=f\"{prefix}-crossaccount\",\n    assume_role_policy=this.json,\n    tags=tags)\nthis_get_aws_cross_account_policy = databricks.get_aws_cross_account_policy()\nthis_role_policy = aws.iam.RolePolicy(\"this\",\n    name=f\"{prefix}-policy\",\n    role=cross_account_role.id,\n    policy=this_get_aws_cross_account_policy.json)\nthis_mws_credentials = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=cross_account_role.arn)\nroot_storage_bucket = aws.s3.BucketV2(\"root_storage_bucket\",\n    bucket=f\"{prefix}-rootbucket\",\n    acl=\"private\",\n    force_destroy=True,\n    tags=tags)\nroot_versioning = aws.s3.BucketVersioningV2(\"root_versioning\",\n    bucket=root_storage_bucket.id,\n    versioning_configuration={\n        \"status\": \"Disabled\",\n    })\nroot_storage_bucket_bucket_server_side_encryption_configuration_v2 = aws.s3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\",\n    bucket=root_storage_bucket.bucket,\n    rules=[{\n        \"apply_server_side_encryption_by_default\": {\n            \"sse_algorithm\": \"AES256\",\n        },\n    }])\nroot_storage_bucket_bucket_public_access_block = aws.s3.BucketPublicAccessBlock(\"root_storage_bucket\",\n    bucket=root_storage_bucket.id,\n    block_public_acls=True,\n    block_public_policy=True,\n    ignore_public_acls=True,\n    restrict_public_buckets=True,\n    opts = pulumi.ResourceOptions(depends_on=[root_storage_bucket]))\nthis_get_aws_bucket_policy = databricks.get_aws_bucket_policy_output(bucket=root_storage_bucket.bucket)\nroot_bucket_policy = aws.s3.BucketPolicy(\"root_bucket_policy\",\n    bucket=root_storage_bucket.id,\n    policy=this_get_aws_bucket_policy.json,\n    opts = pulumi.ResourceOptions(depends_on=[root_storage_bucket_bucket_public_access_block]))\nthis_mws_storage_configurations = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{prefix}-storage\",\n    bucket_name=root_storage_bucket.bucket)\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    aws_region=\"us-east-1\",\n    credentials_id=this_mws_credentials.credentials_id,\n    storage_configuration_id=this_mws_storage_configurations.storage_configuration_id,\n    token={},\n    custom_tags={\n        \"SoldToCode\": \"1234\",\n    })\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\nusing Random = Pulumi.Random;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var naming = new Random.Index.String(\"naming\", new()\n    {\n        Special = false,\n        Upper = false,\n        Length = 6,\n    });\n\n    var prefix = $\"dltp{naming.Result}\";\n\n    var @this = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccountRole = new Aws.Iam.Role(\"cross_account_role\", new()\n    {\n        Name = $\"{prefix}-crossaccount\",\n        AssumeRolePolicy = @this.Apply(@this =\u003e @this.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json)),\n        Tags = tags,\n    });\n\n    var thisGetAwsCrossAccountPolicy = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var thisRolePolicy = new Aws.Iam.RolePolicy(\"this\", new()\n    {\n        Name = $\"{prefix}-policy\",\n        Role = crossAccountRole.Id,\n        Policy = thisGetAwsCrossAccountPolicy.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json),\n    });\n\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossAccountRole.Arn,\n    });\n\n    var rootStorageBucket = new Aws.S3.BucketV2(\"root_storage_bucket\", new()\n    {\n        Bucket = $\"{prefix}-rootbucket\",\n        Acl = \"private\",\n        ForceDestroy = true,\n        Tags = tags,\n    });\n\n    var rootVersioning = new Aws.S3.BucketVersioningV2(\"root_versioning\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        VersioningConfiguration = new Aws.S3.Inputs.BucketVersioningV2VersioningConfigurationArgs\n        {\n            Status = \"Disabled\",\n        },\n    });\n\n    var rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new Aws.S3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\", new()\n    {\n        Bucket = rootStorageBucket.Bucket,\n        Rules = new[]\n        {\n            new Aws.S3.Inputs.BucketServerSideEncryptionConfigurationV2RuleArgs\n            {\n                ApplyServerSideEncryptionByDefault = new Aws.S3.Inputs.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs\n                {\n                    SseAlgorithm = \"AES256\",\n                },\n            },\n        },\n    });\n\n    var rootStorageBucketBucketPublicAccessBlock = new Aws.S3.BucketPublicAccessBlock(\"root_storage_bucket\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        BlockPublicAcls = true,\n        BlockPublicPolicy = true,\n        IgnorePublicAcls = true,\n        RestrictPublicBuckets = true,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            rootStorageBucket,\n        },\n    });\n\n    var thisGetAwsBucketPolicy = Databricks.GetAwsBucketPolicy.Invoke(new()\n    {\n        Bucket = rootStorageBucket.Bucket,\n    });\n\n    var rootBucketPolicy = new Aws.S3.BucketPolicy(\"root_bucket_policy\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        Policy = thisGetAwsBucketPolicy.Apply(getAwsBucketPolicyResult =\u003e getAwsBucketPolicyResult.Json),\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            rootStorageBucketBucketPublicAccessBlock,\n        },\n    });\n\n    var thisMwsStorageConfigurations = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{prefix}-storage\",\n        BucketName = rootStorageBucket.Bucket,\n    });\n\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        AwsRegion = \"us-east-1\",\n        CredentialsId = thisMwsCredentials.CredentialsId,\n        StorageConfigurationId = thisMwsStorageConfigurations.StorageConfigurationId,\n        Token = null,\n        CustomTags = \n        {\n            { \"SoldToCode\", \"1234\" },\n        },\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-random/sdk/v4/go/random\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tnaming, err := random.NewString(ctx, \"naming\", \u0026random.StringArgs{\n\t\t\tSpecial: false,\n\t\t\tUpper:   false,\n\t\t\tLength:  6,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tprefix := fmt.Sprintf(\"dltp%v\", naming.Result)\n\t\tthis, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountRole, err := iam.NewRole(ctx, \"cross_account_role\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v-crossaccount\", prefix),\n\t\t\tAssumeRolePolicy: pulumi.String(this.Json),\n\t\t\tTags:             pulumi.Any(tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsCrossAccountPolicy, err := databricks.GetAwsCrossAccountPolicy(ctx, \u0026databricks.GetAwsCrossAccountPolicyArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicy(ctx, \"this\", \u0026iam.RolePolicyArgs{\n\t\t\tName:   pulumi.Sprintf(\"%v-policy\", prefix),\n\t\t\tRole:   crossAccountRole.ID(),\n\t\t\tPolicy: pulumi.String(thisGetAwsCrossAccountPolicy.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsCredentials, err := databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.Sprintf(\"%v-creds\", prefix),\n\t\t\tRoleArn:         crossAccountRole.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trootStorageBucket, err := s3.NewBucketV2(ctx, \"root_storage_bucket\", \u0026s3.BucketV2Args{\n\t\t\tBucket:       pulumi.Sprintf(\"%v-rootbucket\", prefix),\n\t\t\tAcl:          pulumi.String(\"private\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t\tTags:         pulumi.Any(tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = s3.NewBucketVersioningV2(ctx, \"root_versioning\", \u0026s3.BucketVersioningV2Args{\n\t\t\tBucket: rootStorageBucket.ID(),\n\t\t\tVersioningConfiguration: \u0026s3.BucketVersioningV2VersioningConfigurationArgs{\n\t\t\t\tStatus: pulumi.String(\"Disabled\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = s3.NewBucketServerSideEncryptionConfigurationV2(ctx, \"root_storage_bucket\", \u0026s3.BucketServerSideEncryptionConfigurationV2Args{\n\t\t\tBucket: rootStorageBucket.Bucket,\n\t\t\tRules: s3.BucketServerSideEncryptionConfigurationV2RuleArray{\n\t\t\t\t\u0026s3.BucketServerSideEncryptionConfigurationV2RuleArgs{\n\t\t\t\t\tApplyServerSideEncryptionByDefault: \u0026s3.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs{\n\t\t\t\t\t\tSseAlgorithm: pulumi.String(\"AES256\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trootStorageBucketBucketPublicAccessBlock, err := s3.NewBucketPublicAccessBlock(ctx, \"root_storage_bucket\", \u0026s3.BucketPublicAccessBlockArgs{\n\t\t\tBucket:                rootStorageBucket.ID(),\n\t\t\tBlockPublicAcls:       pulumi.Bool(true),\n\t\t\tBlockPublicPolicy:     pulumi.Bool(true),\n\t\t\tIgnorePublicAcls:      pulumi.Bool(true),\n\t\t\tRestrictPublicBuckets: pulumi.Bool(true),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\trootStorageBucket,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsBucketPolicy := databricks.GetAwsBucketPolicyOutput(ctx, databricks.GetAwsBucketPolicyOutputArgs{\n\t\t\tBucket: rootStorageBucket.Bucket,\n\t\t}, nil)\n\t\t_, err = s3.NewBucketPolicy(ctx, \"root_bucket_policy\", \u0026s3.BucketPolicyArgs{\n\t\t\tBucket: rootStorageBucket.ID(),\n\t\t\tPolicy: pulumi.String(thisGetAwsBucketPolicy.ApplyT(func(thisGetAwsBucketPolicy databricks.GetAwsBucketPolicyResult) (*string, error) {\n\t\t\t\treturn \u0026thisGetAwsBucketPolicy.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput)),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\trootStorageBucketBucketPublicAccessBlock,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsStorageConfigurations, err := databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.Sprintf(\"%v-storage\", prefix),\n\t\t\tBucketName:               rootStorageBucket.Bucket,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName:          pulumi.String(prefix),\n\t\t\tAwsRegion:              pulumi.String(\"us-east-1\"),\n\t\t\tCredentialsId:          thisMwsCredentials.CredentialsId,\n\t\t\tStorageConfigurationId: thisMwsStorageConfigurations.StorageConfigurationId,\n\t\t\tToken:                  \u0026databricks.MwsWorkspacesTokenArgs{},\n\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\"SoldToCode\": pulumi.String(\"1234\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.random.string;\nimport com.pulumi.random.stringArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.RolePolicy;\nimport com.pulumi.aws.iam.RolePolicyArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.aws.s3.BucketVersioningV2;\nimport com.pulumi.aws.s3.BucketVersioningV2Args;\nimport com.pulumi.aws.s3.inputs.BucketVersioningV2VersioningConfigurationArgs;\nimport com.pulumi.aws.s3.BucketServerSideEncryptionConfigurationV2;\nimport com.pulumi.aws.s3.BucketServerSideEncryptionConfigurationV2Args;\nimport com.pulumi.aws.s3.inputs.BucketServerSideEncryptionConfigurationV2RuleArgs;\nimport com.pulumi.aws.s3.inputs.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs;\nimport com.pulumi.aws.s3.BucketPublicAccessBlock;\nimport com.pulumi.aws.s3.BucketPublicAccessBlockArgs;\nimport com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;\nimport com.pulumi.aws.s3.BucketPolicy;\nimport com.pulumi.aws.s3.BucketPolicyArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var naming = new String(\"naming\", StringArgs.builder()\n            .special(false)\n            .upper(false)\n            .length(6)\n            .build());\n\n        final var prefix = String.format(\"dltp%s\", naming.result());\n\n        final var this = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccountRole = new Role(\"crossAccountRole\", RoleArgs.builder()\n            .name(String.format(\"%s-crossaccount\", prefix))\n            .assumeRolePolicy(this_.json())\n            .tags(tags)\n            .build());\n\n        final var thisGetAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()\n            .build());\n\n        var thisRolePolicy = new RolePolicy(\"thisRolePolicy\", RolePolicyArgs.builder()\n            .name(String.format(\"%s-policy\", prefix))\n            .role(crossAccountRole.id())\n            .policy(thisGetAwsCrossAccountPolicy.json())\n            .build());\n\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()\n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossAccountRole.arn())\n            .build());\n\n        var rootStorageBucket = new BucketV2(\"rootStorageBucket\", BucketV2Args.builder()\n            .bucket(String.format(\"%s-rootbucket\", prefix))\n            .acl(\"private\")\n            .forceDestroy(true)\n            .tags(tags)\n            .build());\n\n        var rootVersioning = new BucketVersioningV2(\"rootVersioning\", BucketVersioningV2Args.builder()\n            .bucket(rootStorageBucket.id())\n            .versioningConfiguration(BucketVersioningV2VersioningConfigurationArgs.builder()\n                .status(\"Disabled\")\n                .build())\n            .build());\n\n        var rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new BucketServerSideEncryptionConfigurationV2(\"rootStorageBucketBucketServerSideEncryptionConfigurationV2\", BucketServerSideEncryptionConfigurationV2Args.builder()\n            .bucket(rootStorageBucket.bucket())\n            .rules(BucketServerSideEncryptionConfigurationV2RuleArgs.builder()\n                .applyServerSideEncryptionByDefault(BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs.builder()\n                    .sseAlgorithm(\"AES256\")\n                    .build())\n                .build())\n            .build());\n\n        var rootStorageBucketBucketPublicAccessBlock = new BucketPublicAccessBlock(\"rootStorageBucketBucketPublicAccessBlock\", BucketPublicAccessBlockArgs.builder()\n            .bucket(rootStorageBucket.id())\n            .blockPublicAcls(true)\n            .blockPublicPolicy(true)\n            .ignorePublicAcls(true)\n            .restrictPublicBuckets(true)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(rootStorageBucket)\n                .build());\n\n        final var thisGetAwsBucketPolicy = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()\n            .bucket(rootStorageBucket.bucket())\n            .build());\n\n        var rootBucketPolicy = new BucketPolicy(\"rootBucketPolicy\", BucketPolicyArgs.builder()\n            .bucket(rootStorageBucket.id())\n            .policy(thisGetAwsBucketPolicy.applyValue(_thisGetAwsBucketPolicy -\u003e _thisGetAwsBucketPolicy.json()))\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(rootStorageBucketBucketPublicAccessBlock)\n                .build());\n\n        var thisMwsStorageConfigurations = new MwsStorageConfigurations(\"thisMwsStorageConfigurations\", MwsStorageConfigurationsArgs.builder()\n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", prefix))\n            .bucketName(rootStorageBucket.bucket())\n            .build());\n\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()\n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .awsRegion(\"us-east-1\")\n            .credentialsId(thisMwsCredentials.credentialsId())\n            .storageConfigurationId(thisMwsStorageConfigurations.storageConfigurationId())\n            .token(MwsWorkspacesTokenArgs.builder()\n                .build())\n            .customTags(Map.of(\"SoldToCode\", \"1234\"))\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(_token -\u003e _token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  naming:\n    type: random:string\n    properties:\n      special: false\n      upper: false\n      length: 6\n  crossAccountRole:\n    type: aws:iam:Role\n    name: cross_account_role\n    properties:\n      name: ${prefix}-crossaccount\n      assumeRolePolicy: ${this.json}\n      tags: ${tags}\n  thisRolePolicy:\n    type: aws:iam:RolePolicy\n    name: this\n    properties:\n      name: ${prefix}-policy\n      role: ${crossAccountRole.id}\n      policy: ${thisGetAwsCrossAccountPolicy.json}\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossAccountRole.arn}\n  rootStorageBucket:\n    type: aws:s3:BucketV2\n    name: root_storage_bucket\n    properties:\n      bucket: ${prefix}-rootbucket\n      acl: private\n      forceDestroy: true\n      tags: ${tags}\n  rootVersioning:\n    type: aws:s3:BucketVersioningV2\n    name: root_versioning\n    properties:\n      bucket: ${rootStorageBucket.id}\n      versioningConfiguration:\n        status: Disabled\n  rootStorageBucketBucketServerSideEncryptionConfigurationV2:\n    type: aws:s3:BucketServerSideEncryptionConfigurationV2\n    name: root_storage_bucket\n    properties:\n      bucket: ${rootStorageBucket.bucket}\n      rules:\n        - applyServerSideEncryptionByDefault:\n            sseAlgorithm: AES256\n  rootStorageBucketBucketPublicAccessBlock:\n    type: aws:s3:BucketPublicAccessBlock\n    name: root_storage_bucket\n    properties:\n      bucket: ${rootStorageBucket.id}\n      blockPublicAcls: true\n      blockPublicPolicy: true\n      ignorePublicAcls: true\n      restrictPublicBuckets: true\n    options:\n      dependsOn:\n        - ${rootStorageBucket}\n  rootBucketPolicy:\n    type: aws:s3:BucketPolicy\n    name: root_bucket_policy\n    properties:\n      bucket: ${rootStorageBucket.id}\n      policy: ${thisGetAwsBucketPolicy.json}\n    options:\n      dependsOn:\n        - ${rootStorageBucketBucketPublicAccessBlock}\n  thisMwsStorageConfigurations:\n    type: databricks:MwsStorageConfigurations\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${prefix}-storage\n      bucketName: ${rootStorageBucket.bucket}\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      awsRegion: us-east-1\n      credentialsId: ${thisMwsCredentials.credentialsId}\n      storageConfigurationId: ${thisMwsStorageConfigurations.storageConfigurationId}\n      token: {}\n      customTags:\n        SoldToCode: '1234'\nvariables:\n  prefix: dltp${naming.result}\n  this:\n    fn::invoke:\n      function: databricks:getAwsAssumeRolePolicy\n      arguments:\n        externalId: ${databricksAccountId}\n  thisGetAwsCrossAccountPolicy:\n    fn::invoke:\n      function: databricks:getAwsCrossAccountPolicy\n      arguments: {}\n  thisGetAwsBucketPolicy:\n    fn::invoke:\n      function: databricks:getAwsBucketPolicy\n      arguments:\n        bucket: ${rootStorageBucket.bucket}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn order to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) please ensure that you have read and understood the [Enable Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) documentation and then customise the example above with the relevant examples from mws_vpc_endpoint, mws_private_access_settings and mws_networks.\n\n### Creating a Databricks on GCP workspace\n\nTo get workspace running, you have to configure a network object:\n\n* databricks.MwsNetworks - (optional, but recommended) You can share one [customer-managed VPC](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/customer-managed-vpc.html) with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you cannot reuse subnets with other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks databricks.MwsNetworks encapsulates this information, you cannot reuse it across workspaces.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst databricksGoogleServiceAccount = config.requireObject\u003cany\u003e(\"databricksGoogleServiceAccount\");\nconst googleProject = config.requireObject\u003cany\u003e(\"googleProject\");\n// register VPC\nconst _this = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `${prefix}-network`,\n    gcpNetworkInfo: {\n        networkProjectId: googleProject,\n        vpcId: vpcId,\n        subnetId: subnetId,\n        subnetRegion: subnetRegion,\n        podIpRangeName: \"pods\",\n        serviceIpRangeName: \"svc\",\n    },\n});\n// create workspace in given VPC\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    location: subnetRegion,\n    cloudResourceContainer: {\n        gcp: {\n            projectId: googleProject,\n        },\n    },\n    networkId: _this.networkId,\n    gkeConfig: {\n        connectivityType: \"PRIVATE_NODE_PUBLIC_MASTER\",\n        masterIpRange: \"10.3.0.0/28\",\n    },\n    token: {},\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndatabricks_google_service_account = config.require_object(\"databricksGoogleServiceAccount\")\ngoogle_project = config.require_object(\"googleProject\")\n# register VPC\nthis = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"{prefix}-network\",\n    gcp_network_info={\n        \"network_project_id\": google_project,\n        \"vpc_id\": vpc_id,\n        \"subnet_id\": subnet_id,\n        \"subnet_region\": subnet_region,\n        \"pod_ip_range_name\": \"pods\",\n        \"service_ip_range_name\": \"svc\",\n    })\n# create workspace in given VPC\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    location=subnet_region,\n    cloud_resource_container={\n        \"gcp\": {\n            \"project_id\": google_project,\n        },\n    },\n    network_id=this.network_id,\n    gke_config={\n        \"connectivity_type\": \"PRIVATE_NODE_PUBLIC_MASTER\",\n        \"master_ip_range\": \"10.3.0.0/28\",\n    },\n    token={})\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var databricksGoogleServiceAccount = config.RequireObject\u003cdynamic\u003e(\"databricksGoogleServiceAccount\");\n    var googleProject = config.RequireObject\u003cdynamic\u003e(\"googleProject\");\n    // register VPC\n    var @this = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"{prefix}-network\",\n        GcpNetworkInfo = new Databricks.Inputs.MwsNetworksGcpNetworkInfoArgs\n        {\n            NetworkProjectId = googleProject,\n            VpcId = vpcId,\n            SubnetId = subnetId,\n            SubnetRegion = subnetRegion,\n            PodIpRangeName = \"pods\",\n            ServiceIpRangeName = \"svc\",\n        },\n    });\n\n    // create workspace in given VPC\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        Location = subnetRegion,\n        CloudResourceContainer = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerArgs\n        {\n            Gcp = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerGcpArgs\n            {\n                ProjectId = googleProject,\n            },\n        },\n        NetworkId = @this.NetworkId,\n        GkeConfig = new Databricks.Inputs.MwsWorkspacesGkeConfigArgs\n        {\n            ConnectivityType = \"PRIVATE_NODE_PUBLIC_MASTER\",\n            MasterIpRange = \"10.3.0.0/28\",\n        },\n        Token = null,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tdatabricksGoogleServiceAccount := cfg.RequireObject(\"databricksGoogleServiceAccount\")\n\t\tgoogleProject := cfg.RequireObject(\"googleProject\")\n\t\t// register VPC\n\t\tthis, err := databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.Sprintf(\"%v-network\", prefix),\n\t\t\tGcpNetworkInfo: \u0026databricks.MwsNetworksGcpNetworkInfoArgs{\n\t\t\t\tNetworkProjectId:   pulumi.Any(googleProject),\n\t\t\t\tVpcId:              pulumi.Any(vpcId),\n\t\t\t\tSubnetId:           pulumi.Any(subnetId),\n\t\t\t\tSubnetRegion:       pulumi.Any(subnetRegion),\n\t\t\t\tPodIpRangeName:     pulumi.String(\"pods\"),\n\t\t\t\tServiceIpRangeName: pulumi.String(\"svc\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// create workspace in given VPC\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:     pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName: pulumi.Any(prefix),\n\t\t\tLocation:      pulumi.Any(subnetRegion),\n\t\t\tCloudResourceContainer: \u0026databricks.MwsWorkspacesCloudResourceContainerArgs{\n\t\t\t\tGcp: \u0026databricks.MwsWorkspacesCloudResourceContainerGcpArgs{\n\t\t\t\t\tProjectId: pulumi.Any(googleProject),\n\t\t\t\t},\n\t\t\t},\n\t\t\tNetworkId: this.NetworkId,\n\t\t\tGkeConfig: \u0026databricks.MwsWorkspacesGkeConfigArgs{\n\t\t\t\tConnectivityType: pulumi.String(\"PRIVATE_NODE_PUBLIC_MASTER\"),\n\t\t\t\tMasterIpRange:    pulumi.String(\"10.3.0.0/28\"),\n\t\t\t},\n\t\t\tToken: \u0026databricks.MwsWorkspacesTokenArgs{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.inputs.MwsNetworksGcpNetworkInfoArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerGcpArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesGkeConfigArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksGoogleServiceAccount = config.get(\"databricksGoogleServiceAccount\");\n        final var googleProject = config.get(\"googleProject\");\n        // register VPC\n        var this_ = new MwsNetworks(\"this\", MwsNetworksArgs.builder()\n            .accountId(databricksAccountId)\n            .networkName(String.format(\"%s-network\", prefix))\n            .gcpNetworkInfo(MwsNetworksGcpNetworkInfoArgs.builder()\n                .networkProjectId(googleProject)\n                .vpcId(vpcId)\n                .subnetId(subnetId)\n                .subnetRegion(subnetRegion)\n                .podIpRangeName(\"pods\")\n                .serviceIpRangeName(\"svc\")\n                .build())\n            .build());\n\n        // create workspace in given VPC\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()\n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .location(subnetRegion)\n            .cloudResourceContainer(MwsWorkspacesCloudResourceContainerArgs.builder()\n                .gcp(MwsWorkspacesCloudResourceContainerGcpArgs.builder()\n                    .projectId(googleProject)\n                    .build())\n                .build())\n            .networkId(this_.networkId())\n            .gkeConfig(MwsWorkspacesGkeConfigArgs.builder()\n                .connectivityType(\"PRIVATE_NODE_PUBLIC_MASTER\")\n                .masterIpRange(\"10.3.0.0/28\")\n                .build())\n            .token(MwsWorkspacesTokenArgs.builder()\n                .build())\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(_token -\u003e _token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksGoogleServiceAccount:\n    type: dynamic\n  googleProject:\n    type: dynamic\nresources:\n  # register VPC\n  this:\n    type: databricks:MwsNetworks\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: ${prefix}-network\n      gcpNetworkInfo:\n        networkProjectId: ${googleProject}\n        vpcId: ${vpcId}\n        subnetId: ${subnetId}\n        subnetRegion: ${subnetRegion}\n        podIpRangeName: pods\n        serviceIpRangeName: svc\n  # create workspace in given VPC\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      location: ${subnetRegion}\n      cloudResourceContainer:\n        gcp:\n          projectId: ${googleProject}\n      networkId: ${this.networkId}\n      gkeConfig:\n        connectivityType: PRIVATE_NODE_PUBLIC_MASTER\n        masterIpRange: 10.3.0.0/28\n      token: {}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn order to create a [Databricks Workspace that leverages GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) please ensure that you have read and understood the [Enable Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) documentation and then customise the example above with the relevant examples from mws_vpc_endpoint, mws_private_access_settings and mws_networks.\n\n## Import\n\nThis resource can be imported by Databricks account ID and workspace ID.\n\n```sh\n$ pulumi import databricks:index/mwsWorkspaces:MwsWorkspaces this '\u003caccount_id\u003e/\u003cworkspace_id\u003e'\n```\n\n~\u003e Not all fields of `databricks_mws_workspaces` can be updated without causing the workspace to be recreated.\n\n   If the configuration for these immutable fields does not match the existing workspace, the workspace will\n\n   be deleted and recreated in the next `pulumi up`. After importing, verify that the configuration\n\n   matches the existing resource by running `pulumi preview`. The only fields that can be updated are\n\n   `credentials_id`, `network_id`, `storage_customer_managed_key_id`, `private_access_settings_id`,\n\n   `managed_services_customer_managed_key_id`, and `custom_tags`.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC.\n"
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead"
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n"
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo"
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig"
                },
                "gcpWorkspaceSa": {
                    "type": "string",
                    "description": "(String, GCP only) identifier of a service account created for the workspace in form of `db-\u003cworkspace-id\u003e@prod-gcp-\u003cregion\u003e.iam.gserviceaccount.com`\n"
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                    "deprecationMessage": "gke_config is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-databricks-workspace"
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean"
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet.\n"
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string",
                    "description": "The pricing tier of the workspace.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration.\n"
                },
                "storageCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(String) workspace id\n"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI.\n"
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "required": [
                "accountId",
                "cloud",
                "creationTime",
                "gcpWorkspaceSa",
                "pricingTier",
                "workspaceId",
                "workspaceName",
                "workspaceStatus",
                "workspaceStatusMessage",
                "workspaceUrl"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC.\n",
                    "willReplaceOnChanges": true
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                    "willReplaceOnChanges": true
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                    "willReplaceOnChanges": true
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                    "willReplaceOnChanges": true
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                    "willReplaceOnChanges": true
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                    "deprecationMessage": "gke_config is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-databricks-workspace",
                    "willReplaceOnChanges": true
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet.\n",
                    "willReplaceOnChanges": true
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string",
                    "description": "The pricing tier of the workspace.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration.\n",
                    "willReplaceOnChanges": true
                },
                "storageCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(String) workspace id\n"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "workspaceName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsWorkspaces resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "awsRegion": {
                        "type": "string",
                        "description": "region of VPC.\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string"
                    },
                    "cloudResourceContainer": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                        "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time when workspace was created\n"
                    },
                    "credentialsId": {
                        "type": "string"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                        "willReplaceOnChanges": true
                    },
                    "deploymentName": {
                        "type": "string",
                        "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalCustomerInfo": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                        "willReplaceOnChanges": true
                    },
                    "gcpManagedNetworkConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                        "willReplaceOnChanges": true
                    },
                    "gcpWorkspaceSa": {
                        "type": "string",
                        "description": "(String, GCP only) identifier of a service account created for the workspace in form of `db-\u003cworkspace-id\u003e@prod-gcp-\u003cregion\u003e.iam.gserviceaccount.com`\n"
                    },
                    "gkeConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                        "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                        "deprecationMessage": "gke_config is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.72.0/docs/guides/gcp-workspace#creating-a-databricks-workspace",
                        "willReplaceOnChanges": true
                    },
                    "isNoPublicIpEnabled": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "location": {
                        "type": "string",
                        "description": "region of the subnet.\n",
                        "willReplaceOnChanges": true
                    },
                    "managedServicesCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                    },
                    "networkId": {
                        "type": "string",
                        "description": "`network_id` from networks.\n"
                    },
                    "pricingTier": {
                        "type": "string",
                        "description": "The pricing tier of the workspace.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "`storage_configuration_id` from storage configuration.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                    },
                    "token": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "(String) workspace id\n"
                    },
                    "workspaceName": {
                        "type": "string",
                        "description": "name of the workspace, will appear on UI.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceStatus": {
                        "type": "string",
                        "description": "(String) workspace status\n"
                    },
                    "workspaceStatusMessage": {
                        "type": "string",
                        "description": "(String) updates on workspace status\n"
                    },
                    "workspaceUrl": {
                        "type": "string",
                        "description": "(String) URL of the workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/notebook:Notebook": {
            "description": "This resource allows you to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html). You can also work with databricks.Notebook and databricks.getNotebookPaths data sources.\n\n## Import\n\nThe resource notebook can be imported using notebook path\n\nbash\n\n```sh\n$ pulumi import databricks:index/notebook:Notebook this /path/to/notebook\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded notebook source code. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.\n"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the notebook\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "format",
                "language",
                "objectId",
                "objectType",
                "path",
                "url",
                "workspacePath"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded notebook source code. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.\n"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Notebook resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "The base64-encoded notebook source code. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a notebook with configuration properties for a data pipeline.\n"
                    },
                    "format": {
                        "type": "string"
                    },
                    "language": {
                        "type": "string",
                        "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a NOTEBOOK\n"
                    },
                    "objectType": {
                        "type": "string",
                        "deprecationMessage": "Always is a notebook"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the notebook\n"
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/notificationDestination:NotificationDestination": {
            "description": "This resource allows you to manage [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification destinations are used to send notifications for query alerts and jobs to destinations outside of Databricks. Only workspace admins can create, update, and delete notification destinations.\n\n## Example Usage\n\n`Email` notification destination:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ndresource = new databricks.NotificationDestination(\"ndresource\", {\n    displayName: \"Notification Destination\",\n    config: {\n        email: {\n            addresses: [\"abc@gmail.com\"],\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nndresource = databricks.NotificationDestination(\"ndresource\",\n    display_name=\"Notification Destination\",\n    config={\n        \"email\": {\n            \"addresses\": [\"abc@gmail.com\"],\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ndresource = new Databricks.NotificationDestination(\"ndresource\", new()\n    {\n        DisplayName = \"Notification Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            Email = new Databricks.Inputs.NotificationDestinationConfigEmailArgs\n            {\n                Addresses = new[]\n                {\n                    \"abc@gmail.com\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"ndresource\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Notification Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tEmail: \u0026databricks.NotificationDestinationConfigEmailArgs{\n\t\t\t\t\tAddresses: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"abc@gmail.com\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ndresource = new NotificationDestination(\"ndresource\", NotificationDestinationArgs.builder()\n            .displayName(\"Notification Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .email(NotificationDestinationConfigEmailArgs.builder()\n                    .addresses(\"abc@gmail.com\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ndresource:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Notification Destination\n      config:\n        email:\n          addresses:\n            - abc@gmail.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n`Slack` notification destination:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ndresource = new databricks.NotificationDestination(\"ndresource\", {\n    displayName: \"Notification Destination\",\n    config: {\n        slack: {\n            url: \"https://hooks.slack.com/services/...\",\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nndresource = databricks.NotificationDestination(\"ndresource\",\n    display_name=\"Notification Destination\",\n    config={\n        \"slack\": {\n            \"url\": \"https://hooks.slack.com/services/...\",\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ndresource = new Databricks.NotificationDestination(\"ndresource\", new()\n    {\n        DisplayName = \"Notification Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            Slack = new Databricks.Inputs.NotificationDestinationConfigSlackArgs\n            {\n                Url = \"https://hooks.slack.com/services/...\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"ndresource\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Notification Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tSlack: \u0026databricks.NotificationDestinationConfigSlackArgs{\n\t\t\t\t\tUrl: pulumi.String(\"https://hooks.slack.com/services/...\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ndresource = new NotificationDestination(\"ndresource\", NotificationDestinationArgs.builder()\n            .displayName(\"Notification Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .slack(NotificationDestinationConfigSlackArgs.builder()\n                    .url(\"https://hooks.slack.com/services/...\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ndresource:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Notification Destination\n      config:\n        slack:\n          url: https://hooks.slack.com/services/...\n```\n\u003c!--End PulumiCodeChooser --\u003e\n`PagerDuty` notification destination:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ndresource = new databricks.NotificationDestination(\"ndresource\", {\n    displayName: \"Notification Destination\",\n    config: {\n        pagerduty: {\n            integrationKey: \"xxxxxx\",\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nndresource = databricks.NotificationDestination(\"ndresource\",\n    display_name=\"Notification Destination\",\n    config={\n        \"pagerduty\": {\n            \"integration_key\": \"xxxxxx\",\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ndresource = new Databricks.NotificationDestination(\"ndresource\", new()\n    {\n        DisplayName = \"Notification Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            Pagerduty = new Databricks.Inputs.NotificationDestinationConfigPagerdutyArgs\n            {\n                IntegrationKey = \"xxxxxx\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"ndresource\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Notification Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tPagerduty: \u0026databricks.NotificationDestinationConfigPagerdutyArgs{\n\t\t\t\t\tIntegrationKey: pulumi.String(\"xxxxxx\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigPagerdutyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ndresource = new NotificationDestination(\"ndresource\", NotificationDestinationArgs.builder()\n            .displayName(\"Notification Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .pagerduty(NotificationDestinationConfigPagerdutyArgs.builder()\n                    .integrationKey(\"xxxxxx\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ndresource:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Notification Destination\n      config:\n        pagerduty:\n          integrationKey: xxxxxx\n```\n\u003c!--End PulumiCodeChooser --\u003e\n`Microsoft Teams` notification destination:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ndresource = new databricks.NotificationDestination(\"ndresource\", {\n    displayName: \"Notification Destination\",\n    config: {\n        microsoftTeams: {\n            url: \"https://outlook.office.com/webhook/...\",\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nndresource = databricks.NotificationDestination(\"ndresource\",\n    display_name=\"Notification Destination\",\n    config={\n        \"microsoft_teams\": {\n            \"url\": \"https://outlook.office.com/webhook/...\",\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ndresource = new Databricks.NotificationDestination(\"ndresource\", new()\n    {\n        DisplayName = \"Notification Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            MicrosoftTeams = new Databricks.Inputs.NotificationDestinationConfigMicrosoftTeamsArgs\n            {\n                Url = \"https://outlook.office.com/webhook/...\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"ndresource\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Notification Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tMicrosoftTeams: \u0026databricks.NotificationDestinationConfigMicrosoftTeamsArgs{\n\t\t\t\t\tUrl: pulumi.String(\"https://outlook.office.com/webhook/...\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigMicrosoftTeamsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ndresource = new NotificationDestination(\"ndresource\", NotificationDestinationArgs.builder()\n            .displayName(\"Notification Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .microsoftTeams(NotificationDestinationConfigMicrosoftTeamsArgs.builder()\n                    .url(\"https://outlook.office.com/webhook/...\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ndresource:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Notification Destination\n      config:\n        microsoftTeams:\n          url: https://outlook.office.com/webhook/...\n```\n\u003c!--End PulumiCodeChooser --\u003e\n`Generic Webhook` notification destination:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ndresource = new databricks.NotificationDestination(\"ndresource\", {\n    displayName: \"Notification Destination\",\n    config: {\n        genericWebhook: {\n            url: \"https://example.com/webhook\",\n            username: \"username\",\n            password: \"password\",\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nndresource = databricks.NotificationDestination(\"ndresource\",\n    display_name=\"Notification Destination\",\n    config={\n        \"generic_webhook\": {\n            \"url\": \"https://example.com/webhook\",\n            \"username\": \"username\",\n            \"password\": \"password\",\n        },\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ndresource = new Databricks.NotificationDestination(\"ndresource\", new()\n    {\n        DisplayName = \"Notification Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            GenericWebhook = new Databricks.Inputs.NotificationDestinationConfigGenericWebhookArgs\n            {\n                Url = \"https://example.com/webhook\",\n                Username = \"username\",\n                Password = \"password\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"ndresource\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Notification Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tGenericWebhook: \u0026databricks.NotificationDestinationConfigGenericWebhookArgs{\n\t\t\t\t\tUrl:      pulumi.String(\"https://example.com/webhook\"),\n\t\t\t\t\tUsername: pulumi.String(\"username\"),\n\t\t\t\t\tPassword: pulumi.String(\"password\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigGenericWebhookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ndresource = new NotificationDestination(\"ndresource\", NotificationDestinationArgs.builder()\n            .displayName(\"Notification Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .genericWebhook(NotificationDestinationConfigGenericWebhookArgs.builder()\n                    .url(\"https://example.com/webhook\")\n                    .username(\"username\")\n                    .password(\"password\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ndresource:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Notification Destination\n      config:\n        genericWebhook:\n          url: https://example.com/webhook\n          username: username\n          password: password\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by notification ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/notificationDestination:NotificationDestination this \u003cnotification-id\u003e\n```\n\n",
            "properties": {
                "config": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfig:NotificationDestinationConfig",
                    "description": "The configuration of the Notification Destination. It must contain exactly one of the following blocks:\n"
                },
                "destinationType": {
                    "type": "string",
                    "description": "the type of Notification Destination.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the Notification Destination.\n"
                }
            },
            "required": [
                "destinationType",
                "displayName"
            ],
            "inputProperties": {
                "config": {
                    "$ref": "#/types/databricks:index/NotificationDestinationConfig:NotificationDestinationConfig",
                    "description": "The configuration of the Notification Destination. It must contain exactly one of the following blocks:\n"
                },
                "destinationType": {
                    "type": "string",
                    "description": "the type of Notification Destination.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "The display name of the Notification Destination.\n"
                }
            },
            "requiredInputs": [
                "displayName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering NotificationDestination resources.\n",
                "properties": {
                    "config": {
                        "$ref": "#/types/databricks:index/NotificationDestinationConfig:NotificationDestinationConfig",
                        "description": "The configuration of the Notification Destination. It must contain exactly one of the following blocks:\n"
                    },
                    "destinationType": {
                        "type": "string",
                        "description": "the type of Notification Destination.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "The display name of the Notification Destination.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/oboToken:OboToken": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource creates [On-Behalf-Of tokens](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-personal-access-tokens-for-a-service-principal) for a databricks.ServicePrincipal in Databricks workspaces on AWS and GCP.  In general it's best to use OAuth authentication using client ID and secret, and use this resource mostly for integrations that doesn't support OAuth.\n\n\u003e To create On-Behalf-Of token for Azure Service Principal, configure Pulumi provider to use Azure service principal's client ID and secret, and use `databricks.Token` resource to create a personal access token.\n\n## Example Usage\n\nCreating a token for a narrowly-scoped service principal, that would be the only one (besides admins) allowed to use PAT token in this given workspace, keeping your automated deployment highly secure.\n\n\u003e A given declaration of `databricks_permissions.token_usage` would OVERWRITE permissions to use PAT tokens from any existing groups with token usage permissions such as the `users` group. To avoid this, be sure to include any desired groups in additional `access_control` blocks in the Pulumi configuration file.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"Automation-only SP\"});\nconst tokenUsage = new databricks.Permissions(\"token_usage\", {\n    authorization: \"tokens\",\n    accessControls: [{\n        servicePrincipalName: _this.applicationId,\n        permissionLevel: \"CAN_USE\",\n    }],\n});\nconst thisOboToken = new databricks.OboToken(\"this\", {\n    applicationId: _this.applicationId,\n    comment: pulumi.interpolate`PAT on behalf of ${_this.displayName}`,\n    lifetimeSeconds: 3600,\n}, {\n    dependsOn: [tokenUsage],\n});\nexport const obo = thisOboToken.tokenValue;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.ServicePrincipal(\"this\", display_name=\"Automation-only SP\")\ntoken_usage = databricks.Permissions(\"token_usage\",\n    authorization=\"tokens\",\n    access_controls=[{\n        \"service_principal_name\": this.application_id,\n        \"permission_level\": \"CAN_USE\",\n    }])\nthis_obo_token = databricks.OboToken(\"this\",\n    application_id=this.application_id,\n    comment=this.display_name.apply(lambda display_name: f\"PAT on behalf of {display_name}\"),\n    lifetime_seconds=3600,\n    opts = pulumi.ResourceOptions(depends_on=[token_usage]))\npulumi.export(\"obo\", this_obo_token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var tokenUsage = new Databricks.Permissions(\"token_usage\", new()\n    {\n        Authorization = \"tokens\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                ServicePrincipalName = @this.ApplicationId,\n                PermissionLevel = \"CAN_USE\",\n            },\n        },\n    });\n\n    var thisOboToken = new Databricks.OboToken(\"this\", new()\n    {\n        ApplicationId = @this.ApplicationId,\n        Comment = @this.DisplayName.Apply(displayName =\u003e $\"PAT on behalf of {displayName}\"),\n        LifetimeSeconds = 3600,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            tokenUsage,\n        },\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"obo\"] = thisOboToken.TokenValue,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttokenUsage, err := databricks.NewPermissions(ctx, \"token_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tAuthorization: pulumi.String(\"tokens\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tServicePrincipalName: this.ApplicationId,\n\t\t\t\t\tPermissionLevel:      pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisOboToken, err := databricks.NewOboToken(ctx, \"this\", \u0026databricks.OboTokenArgs{\n\t\t\tApplicationId: this.ApplicationId,\n\t\t\tComment: this.DisplayName.ApplyT(func(displayName string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"PAT on behalf of %v\", displayName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tLifetimeSeconds: pulumi.Int(3600),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\ttokenUsage,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"obo\", thisOboToken.TokenValue)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport com.pulumi.databricks.OboToken;\nimport com.pulumi.databricks.OboTokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()\n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var tokenUsage = new Permissions(\"tokenUsage\", PermissionsArgs.builder()\n            .authorization(\"tokens\")\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .servicePrincipalName(this_.applicationId())\n                .permissionLevel(\"CAN_USE\")\n                .build())\n            .build());\n\n        var thisOboToken = new OboToken(\"thisOboToken\", OboTokenArgs.builder()\n            .applicationId(this_.applicationId())\n            .comment(this_.displayName().applyValue(_displayName -\u003e String.format(\"PAT on behalf of %s\", _displayName)))\n            .lifetimeSeconds(3600)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(tokenUsage)\n                .build());\n\n        ctx.export(\"obo\", thisOboToken.tokenValue());\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n  tokenUsage:\n    type: databricks:Permissions\n    name: token_usage\n    properties:\n      authorization: tokens\n      accessControls:\n        - servicePrincipalName: ${this.applicationId}\n          permissionLevel: CAN_USE\n  thisOboToken:\n    type: databricks:OboToken\n    name: this\n    properties:\n      applicationId: ${this.applicationId}\n      comment: PAT on behalf of ${this.displayName}\n      lifetimeSeconds: 3600\n    options:\n      dependsOn:\n        - ${tokenUsage}\noutputs:\n  obo: ${thisOboToken.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating a token for a service principal with admin privileges\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"Pulumi\"});\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst thisGroupMember = new databricks.GroupMember(\"this\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: _this.id,\n});\nconst thisOboToken = new databricks.OboToken(\"this\", {\n    applicationId: _this.applicationId,\n    comment: pulumi.interpolate`PAT on behalf of ${_this.displayName}`,\n    lifetimeSeconds: 3600,\n}, {\n    dependsOn: [thisGroupMember],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.ServicePrincipal(\"this\", display_name=\"Pulumi\")\nadmins = databricks.get_group(display_name=\"admins\")\nthis_group_member = databricks.GroupMember(\"this\",\n    group_id=admins.id,\n    member_id=this.id)\nthis_obo_token = databricks.OboToken(\"this\",\n    application_id=this.application_id,\n    comment=this.display_name.apply(lambda display_name: f\"PAT on behalf of {display_name}\"),\n    lifetime_seconds=3600,\n    opts = pulumi.ResourceOptions(depends_on=[this_group_member]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"Pulumi\",\n    });\n\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var thisGroupMember = new Databricks.GroupMember(\"this\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = @this.Id,\n    });\n\n    var thisOboToken = new Databricks.OboToken(\"this\", new()\n    {\n        ApplicationId = @this.ApplicationId,\n        Comment = @this.DisplayName.Apply(displayName =\u003e $\"PAT on behalf of {displayName}\"),\n        LifetimeSeconds = 3600,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisGroupMember,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Pulumi\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGroupMember, err := databricks.NewGroupMember(ctx, \"this\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: this.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewOboToken(ctx, \"this\", \u0026databricks.OboTokenArgs{\n\t\t\tApplicationId: this.ApplicationId,\n\t\t\tComment: this.DisplayName.ApplyT(func(displayName string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"PAT on behalf of %v\", displayName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tLifetimeSeconds: pulumi.Int(3600),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisGroupMember,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport com.pulumi.databricks.OboToken;\nimport com.pulumi.databricks.OboTokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()\n            .displayName(\"Pulumi\")\n            .build());\n\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var thisGroupMember = new GroupMember(\"thisGroupMember\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(this_.id())\n            .build());\n\n        var thisOboToken = new OboToken(\"thisOboToken\", OboTokenArgs.builder()\n            .applicationId(this_.applicationId())\n            .comment(this_.displayName().applyValue(_displayName -\u003e String.format(\"PAT on behalf of %s\", _displayName)))\n            .lifetimeSeconds(3600)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisGroupMember)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Pulumi\n  thisGroupMember:\n    type: databricks:GroupMember\n    name: this\n    properties:\n      groupId: ${admins.id}\n      memberId: ${this.id}\n  thisOboToken:\n    type: databricks:OboToken\n    name: this\n    properties:\n      applicationId: ${this.applicationId}\n      comment: PAT on behalf of ${this.displayName}\n      lifetimeSeconds: 3600\n    options:\n      dependsOn:\n        - ${thisGroupMember}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.ServicePrincipal to manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) that could be added to databricks.Group within workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "applicationId",
                "tokenValue"
            ],
            "inputProperties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n",
                    "willReplaceOnChanges": true
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "applicationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OboToken resources.\n",
                "properties": {
                    "applicationId": {
                        "type": "string",
                        "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Comment that describes the purpose of the token.\n",
                        "willReplaceOnChanges": true
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/onlineTable:OnlineTable": {
            "description": "\u003e This resource can only be used on a Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Online Table](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html) in Databricks.  An online table is a read-only copy of a Delta Table that is stored in row-oriented format optimized for online access. Online tables are fully serverless tables that auto-scale throughput capacity with the request load and provide low latency and high throughput access to data of any scale. Online tables are designed to work with Databricks Model Serving, Feature Serving, and retrieval-augmented generation (RAG) applications where they are used for fast data lookups.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.OnlineTable(\"this\", {\n    name: \"main.default.online_table\",\n    spec: {\n        sourceTableFullName: \"main.default.source_table\",\n        primaryKeyColumns: [\"id\"],\n        runTriggered: {},\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.OnlineTable(\"this\",\n    name=\"main.default.online_table\",\n    spec={\n        \"source_table_full_name\": \"main.default.source_table\",\n        \"primary_key_columns\": [\"id\"],\n        \"run_triggered\": {},\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.OnlineTable(\"this\", new()\n    {\n        Name = \"main.default.online_table\",\n        Spec = new Databricks.Inputs.OnlineTableSpecArgs\n        {\n            SourceTableFullName = \"main.default.source_table\",\n            PrimaryKeyColumns = new[]\n            {\n                \"id\",\n            },\n            RunTriggered = null,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewOnlineTable(ctx, \"this\", \u0026databricks.OnlineTableArgs{\n\t\t\tName: pulumi.String(\"main.default.online_table\"),\n\t\t\tSpec: \u0026databricks.OnlineTableSpecArgs{\n\t\t\t\tSourceTableFullName: pulumi.String(\"main.default.source_table\"),\n\t\t\t\tPrimaryKeyColumns: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"id\"),\n\t\t\t\t},\n\t\t\t\tRunTriggered: \u0026databricks.OnlineTableSpecRunTriggeredArgs{},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.OnlineTable;\nimport com.pulumi.databricks.OnlineTableArgs;\nimport com.pulumi.databricks.inputs.OnlineTableSpecArgs;\nimport com.pulumi.databricks.inputs.OnlineTableSpecRunTriggeredArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new OnlineTable(\"this\", OnlineTableArgs.builder()\n            .name(\"main.default.online_table\")\n            .spec(OnlineTableSpecArgs.builder()\n                .sourceTableFullName(\"main.default.source_table\")\n                .primaryKeyColumns(\"id\")\n                .runTriggered(OnlineTableSpecRunTriggeredArgs.builder()\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:OnlineTable\n    properties:\n      name: main.default.online_table\n      spec:\n        sourceTableFullName: main.default.source_table\n        primaryKeyColumns:\n          - id\n        runTriggered: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Online Table:\n\nbash\n\n```sh\n$ pulumi import databricks:index/onlineTable:OnlineTable this \u003cendpoint-name\u003e\n```\n\n",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "3-level name of the Online Table to create.\n"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                    "description": "object containing specification of the online table:\n"
                },
                "statuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/OnlineTableStatus:OnlineTableStatus"
                    },
                    "description": "object describing status of the online table:\n"
                },
                "tableServingUrl": {
                    "type": "string",
                    "description": "Data serving REST API URL for this table.\n"
                },
                "unityCatalogProvisioningState": {
                    "type": "string",
                    "description": "The provisioning state of the online table entity in Unity Catalog. This is distinct from the state of the data synchronization pipeline (i.e. the table may be in \"ACTIVE\" but the pipeline may be in \"PROVISIONING\" as it runs asynchronously).\n"
                }
            },
            "required": [
                "name",
                "statuses",
                "tableServingUrl",
                "unityCatalogProvisioningState"
            ],
            "inputProperties": {
                "name": {
                    "type": "string",
                    "description": "3-level name of the Online Table to create.\n",
                    "willReplaceOnChanges": true
                },
                "spec": {
                    "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                    "description": "object containing specification of the online table:\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OnlineTable resources.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "3-level name of the Online Table to create.\n",
                        "willReplaceOnChanges": true
                    },
                    "spec": {
                        "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                        "description": "object containing specification of the online table:\n",
                        "willReplaceOnChanges": true
                    },
                    "statuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/OnlineTableStatus:OnlineTableStatus"
                        },
                        "description": "object describing status of the online table:\n"
                    },
                    "tableServingUrl": {
                        "type": "string",
                        "description": "Data serving REST API URL for this table.\n"
                    },
                    "unityCatalogProvisioningState": {
                        "type": "string",
                        "description": "The provisioning state of the online table entity in Unity Catalog. This is distinct from the state of the data synchronization pipeline (i.e. the table may be in \"ACTIVE\" but the pipeline may be in \"PROVISIONING\" as it runs asynchronously).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissionAssignment:PermissionAssignment": {
            "description": "These resources are invoked in the workspace context.\n\n## Example Usage\n\nIn workspace context, adding account-level user to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst addUser = new databricks.PermissionAssignment(\"add_user\", {\n    principalId: me.then(me =\u003e me.id),\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\nme = databricks.get_user(user_name=\"me@example.com\")\nadd_user = databricks.PermissionAssignment(\"add_user\",\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.PermissionAssignment(\"add_user\", new()\n    {\n        PrincipalId = me.Apply(getUserResult =\u003e getUserResult.Id),\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"add_user\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(me.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new PermissionAssignment(\"addUser\", PermissionAssignmentArgs.builder()\n            .principalId(me.id())\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  addUser:\n    type: databricks:PermissionAssignment\n    name: add_user\n    properties:\n      principalId: ${me.id}\n      permissions:\n        - USER\nvariables:\n  # Use the account provider\n  me:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn workspace context, adding account-level service principal to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst sp = databricks.getServicePrincipal({\n    displayName: \"Automation-only SP\",\n});\nconst addAdminSpn = new databricks.PermissionAssignment(\"add_admin_spn\", {\n    principalId: sp.then(sp =\u003e sp.id),\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\nsp = databricks.get_service_principal(display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.PermissionAssignment(\"add_admin_spn\",\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var sp = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.PermissionAssignment(\"add_admin_spn\", new()\n    {\n        PrincipalId = sp.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.Id),\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\tsp, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.StringRef(\"Automation-only SP\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"add_admin_spn\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(sp.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var sp = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new PermissionAssignment(\"addAdminSpn\", PermissionAssignmentArgs.builder()\n            .principalId(sp.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  addAdminSpn:\n    type: databricks:PermissionAssignment\n    name: add_admin_spn\n    properties:\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\nvariables:\n  # Use the account provider\n  sp:\n    fn::invoke:\n      function: databricks:getServicePrincipal\n      arguments:\n        displayName: Automation-only SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn workspace context, adding account-level group to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst accountLevel = databricks.getGroup({\n    displayName: \"example-group\",\n});\n// Use the workspace provider\nconst _this = new databricks.PermissionAssignment(\"this\", {\n    principalId: accountLevel.then(accountLevel =\u003e accountLevel.id),\n    permissions: [\"USER\"],\n});\nconst workspaceLevel = databricks.getGroup({\n    displayName: \"example-group\",\n});\nexport const databricksGroupId = workspaceLevel.then(workspaceLevel =\u003e workspaceLevel.id);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\naccount_level = databricks.get_group(display_name=\"example-group\")\n# Use the workspace provider\nthis = databricks.PermissionAssignment(\"this\",\n    principal_id=account_level.id,\n    permissions=[\"USER\"])\nworkspace_level = databricks.get_group(display_name=\"example-group\")\npulumi.export(\"databricksGroupId\", workspace_level.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var accountLevel = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"example-group\",\n    });\n\n    // Use the workspace provider\n    var @this = new Databricks.PermissionAssignment(\"this\", new()\n    {\n        PrincipalId = accountLevel.Apply(getGroupResult =\u003e getGroupResult.Id),\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n    var workspaceLevel = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"example-group\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksGroupId\"] = workspaceLevel.Apply(getGroupResult =\u003e getGroupResult.Id),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\taccountLevel, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"example-group\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Use the workspace provider\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"this\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(accountLevel.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tworkspaceLevel, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"example-group\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksGroupId\", workspaceLevel.Id)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var accountLevel = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"example-group\")\n            .build());\n\n        // Use the workspace provider\n        var this_ = new PermissionAssignment(\"this\", PermissionAssignmentArgs.builder()\n            .principalId(accountLevel.id())\n            .permissions(\"USER\")\n            .build());\n\n        final var workspaceLevel = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"example-group\")\n            .build());\n\n        ctx.export(\"databricksGroupId\", workspaceLevel.id());\n    }\n}\n```\n```yaml\nresources:\n  # Use the workspace provider\n  this:\n    type: databricks:PermissionAssignment\n    properties:\n      principalId: ${accountLevel.id}\n      permissions:\n        - USER\nvariables:\n  # Use the account provider\n  accountLevel:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: example-group\n  workspaceLevel:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: example-group\noutputs:\n  databricksGroupId: ${workspaceLevel.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.MwsPermissionAssignment to manage permission assignment from an account context\n\n## Import\n\nThe resource `databricks_permission_assignment` can be imported using the principal id\n\nbash\n\n```sh\n$ pulumi import databricks:index/permissionAssignment:PermissionAssignment this principal_id\n```\n\n",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n"
                },
                "principalId": {
                    "type": "string",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of `principal_id` as outputs from another Pulumi stack.\n"
                }
            },
            "required": [
                "permissions",
                "principalId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "string",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of `principal_id` as outputs from another Pulumi stack.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering PermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "string",
                        "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the account-level SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources with account API (and has to be an account admin). A more sensible approach is to retrieve the list of `principal_id` as outputs from another Pulumi stack.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissions:Permissions": {
            "description": "This resource allows you to generically manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspaces. It ensures that only _admins_, _authenticated principal_ and those declared within `access_control` blocks would have specified access. It is not possible to remove management rights from _admins_ group.\n\n\u003e This resource is _authoritative_ for permissions on objects. Configuring this resource for an object will **OVERWRITE** any existing permissions of the same type unless imported, and changes made outside of Pulumi will be reset.\n\n\u003e It is not possible to lower permissions for `admins`, so Databricks Pulumi Provider removes those `access_control` blocks automatically.\n\n\u003e If multiple permission levels are specified for an identity (e.g. `CAN_RESTART` and `CAN_MANAGE` for a cluster), only the highest level permission is returned and will cause permanent drift.\n\n\u003e To manage access control on service principals, use databricks_access_control_rule_set.\n\n## Cluster usage\n\nIt's possible to separate [cluster access control](https://docs.databricks.com/security/access-control/cluster-acl.html) to three different permission levels: `CAN_ATTACH_TO`, `CAN_RESTART` and `CAN_MANAGE`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 60,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 10,\n    },\n});\nconst clusterUsage = new databricks.Permissions(\"cluster_usage\", {\n    clusterId: sharedAutoscaling.id,\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_ATTACH_TO\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_RESTART\",\n        },\n        {\n            groupName: ds.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=60,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 10,\n    })\ncluster_usage = databricks.Permissions(\"cluster_usage\",\n    cluster_id=shared_autoscaling.id,\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_ATTACH_TO\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_RESTART\",\n        },\n        {\n            \"group_name\": ds.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 60,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 10,\n        },\n    });\n\n    var clusterUsage = new Databricks.Permissions(\"cluster_usage\", new()\n    {\n        ClusterId = sharedAutoscaling.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_ATTACH_TO\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_RESTART\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = ds.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedAutoscaling, err := databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(60),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(10),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"cluster_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tClusterId: sharedAutoscaling.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_ATTACH_TO\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RESTART\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       ds.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()\n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latest.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(60)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(10)\n                .build())\n            .build());\n\n        var clusterUsage = new Permissions(\"clusterUsage\", PermissionsArgs.builder()\n            .clusterId(sharedAutoscaling.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_ATTACH_TO\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_RESTART\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(ds.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latest.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 60\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 10\n  clusterUsage:\n    type: databricks:Permissions\n    name: cluster_usage\n    properties:\n      clusterId: ${sharedAutoscaling.id}\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_ATTACH_TO\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_RESTART\n        - groupName: ${ds.displayName}\n          permissionLevel: CAN_MANAGE\nvariables:\n  latest:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments: {}\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Cluster Policy usage\n\nCluster policies allow creation of clusters, that match [given policy](https://docs.databricks.com/administration-guide/clusters/policies.html). It's possible to assign `CAN_USE` permission to users and groups:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst somethingSimple = new databricks.ClusterPolicy(\"something_simple\", {\n    name: \"Some simple policy\",\n    definition: JSON.stringify({\n        \"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\": {\n            type: \"forbidden\",\n        },\n        \"spark_conf.spark.secondkey\": {\n            type: \"forbidden\",\n        },\n    }),\n});\nconst policyUsage = new databricks.Permissions(\"policy_usage\", {\n    clusterPolicyId: somethingSimple.id,\n    accessControls: [\n        {\n            groupName: ds.displayName,\n            permissionLevel: \"CAN_USE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_USE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nsomething_simple = databricks.ClusterPolicy(\"something_simple\",\n    name=\"Some simple policy\",\n    definition=json.dumps({\n        \"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\": {\n            \"type\": \"forbidden\",\n        },\n        \"spark_conf.spark.secondkey\": {\n            \"type\": \"forbidden\",\n        },\n    }))\npolicy_usage = databricks.Permissions(\"policy_usage\",\n    cluster_policy_id=something_simple.id,\n    access_controls=[\n        {\n            \"group_name\": ds.display_name,\n            \"permission_level\": \"CAN_USE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_USE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var somethingSimple = new Databricks.ClusterPolicy(\"something_simple\", new()\n    {\n        Name = \"Some simple policy\",\n        Definition = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"forbidden\",\n            },\n            [\"spark_conf.spark.secondkey\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"forbidden\",\n            },\n        }),\n    });\n\n    var policyUsage = new Databricks.Permissions(\"policy_usage\", new()\n    {\n        ClusterPolicyId = somethingSimple.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = ds.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\": map[string]interface{}{\n\t\t\t\t\"type\": \"forbidden\",\n\t\t\t},\n\t\t\t\"spark_conf.spark.secondkey\": map[string]interface{}{\n\t\t\t\t\"type\": \"forbidden\",\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\tsomethingSimple, err := databricks.NewClusterPolicy(ctx, \"something_simple\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tName:       pulumi.String(\"Some simple policy\"),\n\t\t\tDefinition: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"policy_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tClusterPolicyId: somethingSimple.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       ds.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var somethingSimple = new ClusterPolicy(\"somethingSimple\", ClusterPolicyArgs.builder()\n            .name(\"Some simple policy\")\n            .definition(serializeJson(\n                jsonObject(\n                    jsonProperty(\"spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL\", jsonObject(\n                        jsonProperty(\"type\", \"forbidden\")\n                    )),\n                    jsonProperty(\"spark_conf.spark.secondkey\", jsonObject(\n                        jsonProperty(\"type\", \"forbidden\")\n                    ))\n                )))\n            .build());\n\n        var policyUsage = new Permissions(\"policyUsage\", PermissionsArgs.builder()\n            .clusterPolicyId(somethingSimple.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(ds.displayName())\n                    .permissionLevel(\"CAN_USE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_USE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  somethingSimple:\n    type: databricks:ClusterPolicy\n    name: something_simple\n    properties:\n      name: Some simple policy\n      definition:\n        fn::toJSON:\n          spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL:\n            type: forbidden\n          spark_conf.spark.secondkey:\n            type: forbidden\n  policyUsage:\n    type: databricks:Permissions\n    name: policy_usage\n    properties:\n      clusterPolicyId: ${somethingSimple.id}\n      accessControls:\n        - groupName: ${ds.displayName}\n          permissionLevel: CAN_USE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_USE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Instance Pool usage\n\nInstance Pools access control [allows to](https://docs.databricks.com/security/access-control/pool-acl.html) assign `CAN_ATTACH_TO` and `CAN_MANAGE` permissions to users, service principals, and groups. It's also possible to grant creation of Instance Pools to individual groups and users, service principals.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.InstancePool(\"this\", {\n    instancePoolName: \"Reserved Instances\",\n    idleInstanceAutoterminationMinutes: 60,\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    minIdleInstances: 0,\n    maxCapacity: 10,\n});\nconst poolUsage = new databricks.Permissions(\"pool_usage\", {\n    instancePoolId: _this.id,\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_ATTACH_TO\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.InstancePool(\"this\",\n    instance_pool_name=\"Reserved Instances\",\n    idle_instance_autotermination_minutes=60,\n    node_type_id=smallest.id,\n    min_idle_instances=0,\n    max_capacity=10)\npool_usage = databricks.Permissions(\"pool_usage\",\n    instance_pool_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_ATTACH_TO\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.InstancePool(\"this\", new()\n    {\n        InstancePoolName = \"Reserved Instances\",\n        IdleInstanceAutoterminationMinutes = 60,\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        MinIdleInstances = 0,\n        MaxCapacity = 10,\n    });\n\n    var poolUsage = new Databricks.Permissions(\"pool_usage\", new()\n    {\n        InstancePoolId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_ATTACH_TO\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewInstancePool(ctx, \"this\", \u0026databricks.InstancePoolArgs{\n\t\t\tInstancePoolName:                   pulumi.String(\"Reserved Instances\"),\n\t\t\tIdleInstanceAutoterminationMinutes: pulumi.Int(60),\n\t\t\tNodeTypeId:                         pulumi.String(smallest.Id),\n\t\t\tMinIdleInstances:                   pulumi.Int(0),\n\t\t\tMaxCapacity:                        pulumi.Int(10),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"pool_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tInstancePoolId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_ATTACH_TO\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.InstancePool;\nimport com.pulumi.databricks.InstancePoolArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new InstancePool(\"this\", InstancePoolArgs.builder()\n            .instancePoolName(\"Reserved Instances\")\n            .idleInstanceAutoterminationMinutes(60)\n            .nodeTypeId(smallest.id())\n            .minIdleInstances(0)\n            .maxCapacity(10)\n            .build());\n\n        var poolUsage = new Permissions(\"poolUsage\", PermissionsArgs.builder()\n            .instancePoolId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_ATTACH_TO\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:InstancePool\n    properties:\n      instancePoolName: Reserved Instances\n      idleInstanceAutoterminationMinutes: 60\n      nodeTypeId: ${smallest.id}\n      minIdleInstances: 0\n      maxCapacity: 10\n  poolUsage:\n    type: databricks:Permissions\n    name: pool_usage\n    properties:\n      instancePoolId: ${this.id}\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_ATTACH_TO\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\nvariables:\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Job usage\n\nThere are four assignable [permission levels](https://docs.databricks.com/security/access-control/jobs-acl.html#job-permissions) for databricks_job: `CAN_VIEW`, `CAN_MANAGE_RUN`, `IS_OWNER`, and `CAN_MANAGE`. Admins are granted the `CAN_MANAGE` permission by default, and they can assign that permission to non-admin users, and service principals.\n\n- The creator of a job has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a job would revert ownership to the creator.\n- A job must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the job. Nothing would change, per se, if the job was created through Pulumi.\n- A job cannot have a group as an owner.\n- Jobs triggered through _Run Now_ assume the permissions of the job owner and not the user, and service principal who issued Run Now.\n- Read [main documentation](https://docs.databricks.com/security/access-control/jobs-acl.html) for additional detail.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst awsPrincipal = new databricks.ServicePrincipal(\"aws_principal\", {displayName: \"main\"});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Job(\"this\", {\n    name: \"Featurization\",\n    maxConcurrentRuns: 1,\n    tasks: [{\n        taskKey: \"task1\",\n        newCluster: {\n            numWorkers: 300,\n            sparkVersion: latest.then(latest =\u003e latest.id),\n            nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n        },\n        notebookTask: {\n            notebookPath: \"/Production/MakeFeatures\",\n        },\n    }],\n});\nconst jobUsage = new databricks.Permissions(\"job_usage\", {\n    jobId: _this.id,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_VIEW\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_MANAGE_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n        {\n            servicePrincipalName: awsPrincipal.applicationId,\n            permissionLevel: \"IS_OWNER\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\naws_principal = databricks.ServicePrincipal(\"aws_principal\", display_name=\"main\")\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Job(\"this\",\n    name=\"Featurization\",\n    max_concurrent_runs=1,\n    tasks=[{\n        \"task_key\": \"task1\",\n        \"new_cluster\": {\n            \"num_workers\": 300,\n            \"spark_version\": latest.id,\n            \"node_type_id\": smallest.id,\n        },\n        \"notebook_task\": {\n            \"notebook_path\": \"/Production/MakeFeatures\",\n        },\n    }])\njob_usage = databricks.Permissions(\"job_usage\",\n    job_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_VIEW\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_MANAGE_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n        {\n            \"service_principal_name\": aws_principal.application_id,\n            \"permission_level\": \"IS_OWNER\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var awsPrincipal = new Databricks.ServicePrincipal(\"aws_principal\", new()\n    {\n        DisplayName = \"main\",\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Job(\"this\", new()\n    {\n        Name = \"Featurization\",\n        MaxConcurrentRuns = 1,\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"task1\",\n                NewCluster = new Databricks.Inputs.JobTaskNewClusterArgs\n                {\n                    NumWorkers = 300,\n                    SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n                    NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n                },\n                NotebookTask = new Databricks.Inputs.JobTaskNotebookTaskArgs\n                {\n                    NotebookPath = \"/Production/MakeFeatures\",\n                },\n            },\n        },\n    });\n\n    var jobUsage = new Databricks.Permissions(\"job_usage\", new()\n    {\n        JobId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_VIEW\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_MANAGE_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                ServicePrincipalName = awsPrincipal.ApplicationId,\n                PermissionLevel = \"IS_OWNER\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tawsPrincipal, err := databricks.NewServicePrincipal(ctx, \"aws_principal\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"main\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tName:              pulumi.String(\"Featurization\"),\n\t\t\tMaxConcurrentRuns: pulumi.Int(1),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"task1\"),\n\t\t\t\t\tNewCluster: \u0026databricks.JobTaskNewClusterArgs{\n\t\t\t\t\t\tNumWorkers:   pulumi.Int(300),\n\t\t\t\t\t\tSparkVersion: pulumi.String(latest.Id),\n\t\t\t\t\t\tNodeTypeId:   pulumi.String(smallest.Id),\n\t\t\t\t\t},\n\t\t\t\t\tNotebookTask: \u0026databricks.JobTaskNotebookTaskArgs{\n\t\t\t\t\t\tNotebookPath: pulumi.String(\"/Production/MakeFeatures\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"job_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tJobId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tServicePrincipalName: awsPrincipal.ApplicationId,\n\t\t\t\t\tPermissionLevel:      pulumi.String(\"IS_OWNER\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskNewClusterArgs;\nimport com.pulumi.databricks.inputs.JobTaskNotebookTaskArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var awsPrincipal = new ServicePrincipal(\"awsPrincipal\", ServicePrincipalArgs.builder()\n            .displayName(\"main\")\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Job(\"this\", JobArgs.builder()\n            .name(\"Featurization\")\n            .maxConcurrentRuns(1)\n            .tasks(JobTaskArgs.builder()\n                .taskKey(\"task1\")\n                .newCluster(JobTaskNewClusterArgs.builder()\n                    .numWorkers(300)\n                    .sparkVersion(latest.id())\n                    .nodeTypeId(smallest.id())\n                    .build())\n                .notebookTask(JobTaskNotebookTaskArgs.builder()\n                    .notebookPath(\"/Production/MakeFeatures\")\n                    .build())\n                .build())\n            .build());\n\n        var jobUsage = new Permissions(\"jobUsage\", PermissionsArgs.builder()\n            .jobId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_VIEW\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_MANAGE_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .servicePrincipalName(awsPrincipal.applicationId())\n                    .permissionLevel(\"IS_OWNER\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  awsPrincipal:\n    type: databricks:ServicePrincipal\n    name: aws_principal\n    properties:\n      displayName: main\n  this:\n    type: databricks:Job\n    properties:\n      name: Featurization\n      maxConcurrentRuns: 1\n      tasks:\n        - taskKey: task1\n          newCluster:\n            numWorkers: 300\n            sparkVersion: ${latest.id}\n            nodeTypeId: ${smallest.id}\n          notebookTask:\n            notebookPath: /Production/MakeFeatures\n  jobUsage:\n    type: databricks:Permissions\n    name: job_usage\n    properties:\n      jobId: ${this.id}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_MANAGE_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n        - servicePrincipalName: ${awsPrincipal.applicationId}\n          permissionLevel: IS_OWNER\nvariables:\n  latest:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments: {}\n  smallest:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Delta Live Tables usage\n\nThere are four assignable [permission levels](https://docs.databricks.com/security/access-control/dlt-acl.html#delta-live-tables-permissions) for databricks_pipeline: `CAN_VIEW`, `CAN_RUN`, `CAN_MANAGE`, and `IS_OWNER`. Admins are granted the `CAN_MANAGE` permission by default, and they can assign that permission to non-admin users, and service principals.\n\n- The creator of a DLT Pipeline has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a pipeline would revert ownership to the creator.\n- A DLT pipeline must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the pipeline. Nothing would change, per se, if the pipeline was created through Pulumi.\n- A DLT pipeline cannot have a group as an owner.\n- DLT Pipelines triggered through _Start_ assume the permissions of the pipeline owner and not the user, and service principal who issued Run Now.\n- Read [main documentation](https://docs.databricks.com/security/access-control/dlt-acl.html) for additional detail.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst me = databricks.getCurrentUser({});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst dltDemo = new databricks.Notebook(\"dlt_demo\", {\n    contentBase64: std.base64encode({\n        input: `import dlt\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n@dlt.table(\n   comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n)\ndef clickstream_raw():\n    return (spark.read.format(\"json\").load(json_path))\n`,\n    }).then(invoke =\u003e invoke.result),\n    language: \"PYTHON\",\n    path: me.then(me =\u003e `${me.home}/DLT_Demo`),\n});\nconst _this = new databricks.Pipeline(\"this\", {\n    name: me.then(me =\u003e `DLT Demo Pipeline (${me.alphanumeric})`),\n    storage: \"/test/tf-pipeline\",\n    configuration: {\n        key1: \"value1\",\n        key2: \"value2\",\n    },\n    libraries: [{\n        notebook: {\n            path: dltDemo.id,\n        },\n    }],\n    continuous: false,\n    filters: {\n        includes: [\"com.databricks.include\"],\n        excludes: [\"com.databricks.exclude\"],\n    },\n});\nconst dltUsage = new databricks.Permissions(\"dlt_usage\", {\n    pipelineId: _this.id,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_VIEW\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\nme = databricks.get_current_user()\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\ndlt_demo = databricks.Notebook(\"dlt_demo\",\n    content_base64=std.base64encode(input=\"\"\"import dlt\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n@dlt.table(\n   comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n)\ndef clickstream_raw():\n    return (spark.read.format(\"json\").load(json_path))\n\"\"\").result,\n    language=\"PYTHON\",\n    path=f\"{me.home}/DLT_Demo\")\nthis = databricks.Pipeline(\"this\",\n    name=f\"DLT Demo Pipeline ({me.alphanumeric})\",\n    storage=\"/test/tf-pipeline\",\n    configuration={\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n    },\n    libraries=[{\n        \"notebook\": {\n            \"path\": dlt_demo.id,\n        },\n    }],\n    continuous=False,\n    filters={\n        \"includes\": [\"com.databricks.include\"],\n        \"excludes\": [\"com.databricks.exclude\"],\n    })\ndlt_usage = databricks.Permissions(\"dlt_usage\",\n    pipeline_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_VIEW\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var dltDemo = new Databricks.Notebook(\"dlt_demo\", new()\n    {\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = @\"import dlt\njson_path = \"\"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\"\n@dlt.table(\n   comment=\"\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\"\n)\ndef clickstream_raw():\n    return (spark.read.format(\"\"json\"\").load(json_path))\n\",\n        }).Apply(invoke =\u003e invoke.Result),\n        Language = \"PYTHON\",\n        Path = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/DLT_Demo\",\n    });\n\n    var @this = new Databricks.Pipeline(\"this\", new()\n    {\n        Name = $\"DLT Demo Pipeline ({me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)})\",\n        Storage = \"/test/tf-pipeline\",\n        Configuration = \n        {\n            { \"key1\", \"value1\" },\n            { \"key2\", \"value2\" },\n        },\n        Libraries = new[]\n        {\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs\n                {\n                    Path = dltDemo.Id,\n                },\n            },\n        },\n        Continuous = false,\n        Filters = new Databricks.Inputs.PipelineFiltersArgs\n        {\n            Includes = new[]\n            {\n                \"com.databricks.include\",\n            },\n            Excludes = new[]\n            {\n                \"com.databricks.exclude\",\n            },\n        },\n    });\n\n    var dltUsage = new Databricks.Permissions(\"dlt_usage\", new()\n    {\n        PipelineId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_VIEW\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: `import dlt\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n@dlt.table(\n   comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n)\ndef clickstream_raw():\n    return (spark.read.format(\"json\").load(json_path))\n`,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdltDemo, err := databricks.NewNotebook(ctx, \"dlt_demo\", \u0026databricks.NotebookArgs{\n\t\t\tContentBase64: pulumi.String(invokeBase64encode.Result),\n\t\t\tLanguage:      pulumi.String(\"PYTHON\"),\n\t\t\tPath:          pulumi.Sprintf(\"%v/DLT_Demo\", me.Home),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewPipeline(ctx, \"this\", \u0026databricks.PipelineArgs{\n\t\t\tName:    pulumi.Sprintf(\"DLT Demo Pipeline (%v)\", me.Alphanumeric),\n\t\t\tStorage: pulumi.String(\"/test/tf-pipeline\"),\n\t\t\tConfiguration: pulumi.StringMap{\n\t\t\t\t\"key1\": pulumi.String(\"value1\"),\n\t\t\t\t\"key2\": pulumi.String(\"value2\"),\n\t\t\t},\n\t\t\tLibraries: databricks.PipelineLibraryArray{\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tNotebook: \u0026databricks.PipelineLibraryNotebookArgs{\n\t\t\t\t\t\tPath: dltDemo.ID(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContinuous: pulumi.Bool(false),\n\t\t\tFilters: \u0026databricks.PipelineFiltersArgs{\n\t\t\t\tIncludes: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"com.databricks.include\"),\n\t\t\t\t},\n\t\t\t\tExcludes: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"com.databricks.exclude\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"dlt_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tPipelineId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.NotebookArgs;\nimport com.pulumi.std.StdFunctions;\nimport com.pulumi.std.inputs.Base64encodeArgs;\nimport com.pulumi.databricks.Pipeline;\nimport com.pulumi.databricks.PipelineArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;\nimport com.pulumi.databricks.inputs.PipelineFiltersArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var dltDemo = new Notebook(\"dltDemo\", NotebookArgs.builder()\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"\"\"\nimport dlt\njson_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n@dlt.table(\n   comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n)\ndef clickstream_raw():\n    return (spark.read.format(\"json\").load(json_path))\n                \"\"\")\n                .build()).result())\n            .language(\"PYTHON\")\n            .path(String.format(\"%s/DLT_Demo\", me.home()))\n            .build());\n\n        var this_ = new Pipeline(\"this\", PipelineArgs.builder()\n            .name(String.format(\"DLT Demo Pipeline (%s)\", me.alphanumeric()))\n            .storage(\"/test/tf-pipeline\")\n            .configuration(Map.ofEntries(\n                Map.entry(\"key1\", \"value1\"),\n                Map.entry(\"key2\", \"value2\")\n            ))\n            .libraries(PipelineLibraryArgs.builder()\n                .notebook(PipelineLibraryNotebookArgs.builder()\n                    .path(dltDemo.id())\n                    .build())\n                .build())\n            .continuous(false)\n            .filters(PipelineFiltersArgs.builder()\n                .includes(\"com.databricks.include\")\n                .excludes(\"com.databricks.exclude\")\n                .build())\n            .build());\n\n        var dltUsage = new Permissions(\"dltUsage\", PermissionsArgs.builder()\n            .pipelineId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_VIEW\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  dltDemo:\n    type: databricks:Notebook\n    name: dlt_demo\n    properties:\n      contentBase64:\n        fn::invoke:\n          function: std:base64encode\n          arguments:\n            input: |\n              import dlt\n              json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"\n              @dlt.table(\n                 comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"\n              )\n              def clickstream_raw():\n                  return (spark.read.format(\"json\").load(json_path))\n          return: result\n      language: PYTHON\n      path: ${me.home}/DLT_Demo\n  this:\n    type: databricks:Pipeline\n    properties:\n      name: DLT Demo Pipeline (${me.alphanumeric})\n      storage: /test/tf-pipeline\n      configuration:\n        key1: value1\n        key2: value2\n      libraries:\n        - notebook:\n            path: ${dltDemo.id}\n      continuous: false\n      filters:\n        includes:\n          - com.databricks.include\n        excludes:\n          - com.databricks.exclude\n  dltUsage:\n    type: databricks:Permissions\n    name: dlt_usage\n    properties:\n      pipelineId: ${this.id}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Notebook usage\n\nValid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#notebook-permissions) for databricks.Notebook are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.\n\nA notebook could be specified by using either `notebook_path` or `notebook_id` attribute.  The value for the `notebook_id` is the object ID of the resource in the Databricks Workspace that is exposed as `object_id` attribute of the `databricks.Notebook` resource as shown below.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst _this = new databricks.Notebook(\"this\", {\n    contentBase64: std.base64encode({\n        input: \"# Welcome to your Python notebook\",\n    }).then(invoke =\u003e invoke.result),\n    path: \"/Production/ETL/Features\",\n    language: \"PYTHON\",\n});\nconst notebookUsageByPath = new databricks.Permissions(\"notebook_usage_by_path\", {\n    notebookPath: _this.path,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\nconst notebookUsageById = new databricks.Permissions(\"notebook_usage_by_id\", {\n    notebookId: _this.objectId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nthis = databricks.Notebook(\"this\",\n    content_base64=std.base64encode(input=\"# Welcome to your Python notebook\").result,\n    path=\"/Production/ETL/Features\",\n    language=\"PYTHON\")\nnotebook_usage_by_path = databricks.Permissions(\"notebook_usage_by_path\",\n    notebook_path=this.path,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\nnotebook_usage_by_id = databricks.Permissions(\"notebook_usage_by_id\",\n    notebook_id=this.object_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var @this = new Databricks.Notebook(\"this\", new()\n    {\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = \"# Welcome to your Python notebook\",\n        }).Apply(invoke =\u003e invoke.Result),\n        Path = \"/Production/ETL/Features\",\n        Language = \"PYTHON\",\n    });\n\n    var notebookUsageByPath = new Databricks.Permissions(\"notebook_usage_by_path\", new()\n    {\n        NotebookPath = @this.Path,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n    var notebookUsageById = new Databricks.Permissions(\"notebook_usage_by_id\", new()\n    {\n        NotebookId = @this.ObjectId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: \"# Welcome to your Python notebook\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewNotebook(ctx, \"this\", \u0026databricks.NotebookArgs{\n\t\t\tContentBase64: pulumi.String(invokeBase64encode.Result),\n\t\t\tPath:          pulumi.String(\"/Production/ETL/Features\"),\n\t\t\tLanguage:      pulumi.String(\"PYTHON\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"notebook_usage_by_path\", \u0026databricks.PermissionsArgs{\n\t\t\tNotebookPath: this.Path,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"notebook_usage_by_id\", \u0026databricks.PermissionsArgs{\n\t\t\tNotebookId: this.ObjectId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.NotebookArgs;\nimport com.pulumi.std.StdFunctions;\nimport com.pulumi.std.inputs.Base64encodeArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var this_ = new Notebook(\"this\", NotebookArgs.builder()\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"# Welcome to your Python notebook\")\n                .build()).result())\n            .path(\"/Production/ETL/Features\")\n            .language(\"PYTHON\")\n            .build());\n\n        var notebookUsageByPath = new Permissions(\"notebookUsageByPath\", PermissionsArgs.builder()\n            .notebookPath(this_.path())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n        var notebookUsageById = new Permissions(\"notebookUsageById\", PermissionsArgs.builder()\n            .notebookId(this_.objectId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:Notebook\n    properties:\n      contentBase64:\n        fn::invoke:\n          function: std:base64encode\n          arguments:\n            input: '# Welcome to your Python notebook'\n          return: result\n      path: /Production/ETL/Features\n      language: PYTHON\n  notebookUsageByPath:\n    type: databricks:Permissions\n    name: notebook_usage_by_path\n    properties:\n      notebookPath: ${this.path}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n  notebookUsageById:\n    type: databricks:Permissions\n    name: notebook_usage_by_id\n    properties:\n      notebookId: ${this.objectId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\u003e when importing a permissions resource, only the `notebook_id` is filled!\n\n## Workspace file usage\n\nValid permission levels for databricks.WorkspaceFile are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.\n\nA workspace file could be specified by using either `workspace_file_path` or `workspace_file_id` attribute.  The value for the `workspace_file_id` is the object ID of the resource in the Databricks Workspace that is exposed as `object_id` attribute of the `databricks.WorkspaceFile` resource as shown below.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst _this = new databricks.WorkspaceFile(\"this\", {\n    contentBase64: std.base64encode({\n        input: \"print('Hello World')\",\n    }).then(invoke =\u003e invoke.result),\n    path: \"/Production/ETL/Features.py\",\n});\nconst workspaceFileUsageByPath = new databricks.Permissions(\"workspace_file_usage_by_path\", {\n    workspaceFilePath: _this.path,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\nconst workspaceFileUsageById = new databricks.Permissions(\"workspace_file_usage_by_id\", {\n    workspaceFileId: _this.objectId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nthis = databricks.WorkspaceFile(\"this\",\n    content_base64=std.base64encode(input=\"print('Hello World')\").result,\n    path=\"/Production/ETL/Features.py\")\nworkspace_file_usage_by_path = databricks.Permissions(\"workspace_file_usage_by_path\",\n    workspace_file_path=this.path,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\nworkspace_file_usage_by_id = databricks.Permissions(\"workspace_file_usage_by_id\",\n    workspace_file_id=this.object_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var @this = new Databricks.WorkspaceFile(\"this\", new()\n    {\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = \"print('Hello World')\",\n        }).Apply(invoke =\u003e invoke.Result),\n        Path = \"/Production/ETL/Features.py\",\n    });\n\n    var workspaceFileUsageByPath = new Databricks.Permissions(\"workspace_file_usage_by_path\", new()\n    {\n        WorkspaceFilePath = @this.Path,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n    var workspaceFileUsageById = new Databricks.Permissions(\"workspace_file_usage_by_id\", new()\n    {\n        WorkspaceFileId = @this.ObjectId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: \"print('Hello World')\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewWorkspaceFile(ctx, \"this\", \u0026databricks.WorkspaceFileArgs{\n\t\t\tContentBase64: pulumi.String(invokeBase64encode.Result),\n\t\t\tPath:          pulumi.String(\"/Production/ETL/Features.py\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"workspace_file_usage_by_path\", \u0026databricks.PermissionsArgs{\n\t\t\tWorkspaceFilePath: this.Path,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"workspace_file_usage_by_id\", \u0026databricks.PermissionsArgs{\n\t\t\tWorkspaceFileId: this.ObjectId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.WorkspaceFile;\nimport com.pulumi.databricks.WorkspaceFileArgs;\nimport com.pulumi.std.StdFunctions;\nimport com.pulumi.std.inputs.Base64encodeArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var this_ = new WorkspaceFile(\"this\", WorkspaceFileArgs.builder()\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"print('Hello World')\")\n                .build()).result())\n            .path(\"/Production/ETL/Features.py\")\n            .build());\n\n        var workspaceFileUsageByPath = new Permissions(\"workspaceFileUsageByPath\", PermissionsArgs.builder()\n            .workspaceFilePath(this_.path())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n        var workspaceFileUsageById = new Permissions(\"workspaceFileUsageById\", PermissionsArgs.builder()\n            .workspaceFileId(this_.objectId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:WorkspaceFile\n    properties:\n      contentBase64:\n        fn::invoke:\n          function: std:base64encode\n          arguments:\n            input: print('Hello World')\n          return: result\n      path: /Production/ETL/Features.py\n  workspaceFileUsageByPath:\n    type: databricks:Permissions\n    name: workspace_file_usage_by_path\n    properties:\n      workspaceFilePath: ${this.path}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n  workspaceFileUsageById:\n    type: databricks:Permissions\n    name: workspace_file_usage_by_id\n    properties:\n      workspaceFileId: ${this.objectId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\u003e when importing a permissions resource, only the `workspace_file_id` is filled!\n\n## Folder usage\n\nValid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#folder-permissions) for folders of databricks.Directory are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`. Notebooks and experiments in a folder inherit all permissions settings of that folder. For example, a user (or service principal) that has `CAN_RUN` permission on a folder has `CAN_RUN` permission on the notebooks in that folder.\n\n- All users can list items in the folder without any permissions.\n- All users (or service principals) have `CAN_MANAGE` permission for items in the Workspace \u003e Shared Icon Shared folder. You can grant `CAN_MANAGE` permission to notebooks and folders by moving them to the Shared Icon Shared folder.\n- All users (or service principals) have `CAN_MANAGE` permission for objects the user creates.\n- User home directory - The user (or service principal) has `CAN_MANAGE` permission. All other users (or service principals) can list their directories.\n\nA folder could be specified by using either `directory_path` or `directory_id` attribute.  The value for the `directory_id` is the object ID of the resource in the Databricks Workspace that is exposed as `object_id` attribute of the `databricks.Directory` resource as shown below.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst _this = new databricks.Directory(\"this\", {path: \"/Production/ETL\"});\nconst folderUsageByPath = new databricks.Permissions(\"folder_usage_by_path\", {\n    directoryPath: _this.path,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\nconst folderUsageById = new databricks.Permissions(\"folder_usage_by_id\", {\n    directoryId: _this.objectId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nthis = databricks.Directory(\"this\", path=\"/Production/ETL\")\nfolder_usage_by_path = databricks.Permissions(\"folder_usage_by_path\",\n    directory_path=this.path,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\nfolder_usage_by_id = databricks.Permissions(\"folder_usage_by_id\",\n    directory_id=this.object_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var @this = new Databricks.Directory(\"this\", new()\n    {\n        Path = \"/Production/ETL\",\n    });\n\n    var folderUsageByPath = new Databricks.Permissions(\"folder_usage_by_path\", new()\n    {\n        DirectoryPath = @this.Path,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n    var folderUsageById = new Databricks.Permissions(\"folder_usage_by_id\", new()\n    {\n        DirectoryId = @this.ObjectId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewDirectory(ctx, \"this\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Production/ETL\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"folder_usage_by_path\", \u0026databricks.PermissionsArgs{\n\t\t\tDirectoryPath: this.Path,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"folder_usage_by_id\", \u0026databricks.PermissionsArgs{\n\t\t\tDirectoryId: this.ObjectId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var this_ = new Directory(\"this\", DirectoryArgs.builder()\n            .path(\"/Production/ETL\")\n            .build());\n\n        var folderUsageByPath = new Permissions(\"folderUsageByPath\", PermissionsArgs.builder()\n            .directoryPath(this_.path())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n        var folderUsageById = new Permissions(\"folderUsageById\", PermissionsArgs.builder()\n            .directoryId(this_.objectId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:Directory\n    properties:\n      path: /Production/ETL\n  folderUsageByPath:\n    type: databricks:Permissions\n    name: folder_usage_by_path\n    properties:\n      directoryPath: ${this.path}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n  folderUsageById:\n    type: databricks:Permissions\n    name: folder_usage_by_id\n    properties:\n      directoryId: ${this.objectId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\u003e when importing a permissions resource, only the `directory_id` is filled!\n\n## Repos usage\n\nValid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html) for databricks.Repo are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst _this = new databricks.Repo(\"this\", {url: \"https://github.com/user/demo.git\"});\nconst repoUsage = new databricks.Permissions(\"repo_usage\", {\n    repoId: _this.id,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nthis = databricks.Repo(\"this\", url=\"https://github.com/user/demo.git\")\nrepo_usage = databricks.Permissions(\"repo_usage\",\n    repo_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var @this = new Databricks.Repo(\"this\", new()\n    {\n        Url = \"https://github.com/user/demo.git\",\n    });\n\n    var repoUsage = new Databricks.Permissions(\"repo_usage\", new()\n    {\n        RepoId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewRepo(ctx, \"this\", \u0026databricks.RepoArgs{\n\t\t\tUrl: pulumi.String(\"https://github.com/user/demo.git\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"repo_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tRepoId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Repo;\nimport com.pulumi.databricks.RepoArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var this_ = new Repo(\"this\", RepoArgs.builder()\n            .url(\"https://github.com/user/demo.git\")\n            .build());\n\n        var repoUsage = new Permissions(\"repoUsage\", PermissionsArgs.builder()\n            .repoId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:Repo\n    properties:\n      url: https://github.com/user/demo.git\n  repoUsage:\n    type: databricks:Permissions\n    name: repo_usage\n    properties:\n      repoId: ${this.id}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## MLflow Experiment usage\n\nValid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#mlflow-experiment-permissions-1) for databricks.MlflowExperiment are: `CAN_READ`, `CAN_EDIT`, and `CAN_MANAGE`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.MlflowExperiment(\"this\", {\n    name: me.then(me =\u003e `${me.home}/Sample`),\n    artifactLocation: \"dbfs:/tmp/my-experiment\",\n    description: \"My MLflow experiment description\",\n});\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst experimentUsage = new databricks.Permissions(\"experiment_usage\", {\n    experimentId: _this.id,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.MlflowExperiment(\"this\",\n    name=f\"{me.home}/Sample\",\n    artifact_location=\"dbfs:/tmp/my-experiment\",\n    description=\"My MLflow experiment description\")\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nexperiment_usage = databricks.Permissions(\"experiment_usage\",\n    experiment_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.MlflowExperiment(\"this\", new()\n    {\n        Name = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/Sample\",\n        ArtifactLocation = \"dbfs:/tmp/my-experiment\",\n        Description = \"My MLflow experiment description\",\n    });\n\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var experimentUsage = new Databricks.Permissions(\"experiment_usage\", new()\n    {\n        ExperimentId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewMlflowExperiment(ctx, \"this\", \u0026databricks.MlflowExperimentArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v/Sample\", me.Home),\n\t\t\tArtifactLocation: pulumi.String(\"dbfs:/tmp/my-experiment\"),\n\t\t\tDescription:      pulumi.String(\"My MLflow experiment description\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"experiment_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tExperimentId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.MlflowExperiment;\nimport com.pulumi.databricks.MlflowExperimentArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var this_ = new MlflowExperiment(\"this\", MlflowExperimentArgs.builder()\n            .name(String.format(\"%s/Sample\", me.home()))\n            .artifactLocation(\"dbfs:/tmp/my-experiment\")\n            .description(\"My MLflow experiment description\")\n            .build());\n\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var experimentUsage = new Permissions(\"experimentUsage\", PermissionsArgs.builder()\n            .experimentId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowExperiment\n    properties:\n      name: ${me.home}/Sample\n      artifactLocation: dbfs:/tmp/my-experiment\n      description: My MLflow experiment description\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  experimentUsage:\n    type: databricks:Permissions\n    name: experiment_usage\n    properties:\n      experimentId: ${this.id}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_MANAGE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_EDIT\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## MLflow Model usage\n\nValid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#mlflow-model-permissions-1) for databricks.MlflowModel are: `CAN_READ`, `CAN_EDIT`, `CAN_MANAGE_STAGING_VERSIONS`, `CAN_MANAGE_PRODUCTION_VERSIONS`, and `CAN_MANAGE`. You can also manage permissions for all MLflow models by `registered_model_id = \"root\"`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MlflowModel(\"this\", {name: \"SomePredictions\"});\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst modelUsage = new databricks.Permissions(\"model_usage\", {\n    registeredModelId: _this.registeredModelId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_READ\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_MANAGE_PRODUCTION_VERSIONS\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE_STAGING_VERSIONS\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MlflowModel(\"this\", name=\"SomePredictions\")\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nmodel_usage = databricks.Permissions(\"model_usage\",\n    registered_model_id=this.registered_model_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_READ\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_MANAGE_PRODUCTION_VERSIONS\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE_STAGING_VERSIONS\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MlflowModel(\"this\", new()\n    {\n        Name = \"SomePredictions\",\n    });\n\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var modelUsage = new Databricks.Permissions(\"model_usage\", new()\n    {\n        RegisteredModelId = @this.RegisteredModelId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_READ\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_MANAGE_PRODUCTION_VERSIONS\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE_STAGING_VERSIONS\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMlflowModel(ctx, \"this\", \u0026databricks.MlflowModelArgs{\n\t\t\tName: pulumi.String(\"SomePredictions\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"model_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tRegisteredModelId: this.RegisteredModelId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_READ\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE_PRODUCTION_VERSIONS\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE_STAGING_VERSIONS\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MlflowModel(\"this\", MlflowModelArgs.builder()\n            .name(\"SomePredictions\")\n            .build());\n\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var modelUsage = new Permissions(\"modelUsage\", PermissionsArgs.builder()\n            .registeredModelId(this_.registeredModelId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_READ\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_MANAGE_PRODUCTION_VERSIONS\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE_STAGING_VERSIONS\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowModel\n    properties:\n      name: SomePredictions\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  modelUsage:\n    type: databricks:Permissions\n    name: model_usage\n    properties:\n      registeredModelId: ${this.registeredModelId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_READ\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_MANAGE_PRODUCTION_VERSIONS\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE_STAGING_VERSIONS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Model serving usage\n\nValid permission levels for databricks.ModelServing are: `CAN_VIEW`, `CAN_QUERY`, and `CAN_MANAGE`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ModelServing(\"this\", {\n    name: \"tf-test\",\n    config: {\n        servedModels: [{\n            name: \"prod_model\",\n            modelName: \"test\",\n            modelVersion: \"1\",\n            workloadSize: \"Small\",\n            scaleToZeroEnabled: true,\n        }],\n    },\n});\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst mlServingUsage = new databricks.Permissions(\"ml_serving_usage\", {\n    servingEndpointId: _this.servingEndpointId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_VIEW\",\n        },\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_QUERY\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.ModelServing(\"this\",\n    name=\"tf-test\",\n    config={\n        \"served_models\": [{\n            \"name\": \"prod_model\",\n            \"model_name\": \"test\",\n            \"model_version\": \"1\",\n            \"workload_size\": \"Small\",\n            \"scale_to_zero_enabled\": True,\n        }],\n    })\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nml_serving_usage = databricks.Permissions(\"ml_serving_usage\",\n    serving_endpoint_id=this.serving_endpoint_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_VIEW\",\n        },\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_QUERY\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ModelServing(\"this\", new()\n    {\n        Name = \"tf-test\",\n        Config = new Databricks.Inputs.ModelServingConfigArgs\n        {\n            ServedModels = new[]\n            {\n                new Databricks.Inputs.ModelServingConfigServedModelArgs\n                {\n                    Name = \"prod_model\",\n                    ModelName = \"test\",\n                    ModelVersion = \"1\",\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = true,\n                },\n            },\n        },\n    });\n\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var mlServingUsage = new Databricks.Permissions(\"ml_serving_usage\", new()\n    {\n        ServingEndpointId = @this.ServingEndpointId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_VIEW\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_QUERY\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewModelServing(ctx, \"this\", \u0026databricks.ModelServingArgs{\n\t\t\tName: pulumi.String(\"tf-test\"),\n\t\t\tConfig: \u0026databricks.ModelServingConfigArgs{\n\t\t\t\tServedModels: databricks.ModelServingConfigServedModelArray{\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedModelArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"prod_model\"),\n\t\t\t\t\t\tModelName:          pulumi.String(\"test\"),\n\t\t\t\t\t\tModelVersion:       pulumi.String(\"1\"),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(true),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"ml_serving_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tServingEndpointId: this.ServingEndpointId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_QUERY\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ModelServing;\nimport com.pulumi.databricks.ModelServingArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ModelServing(\"this\", ModelServingArgs.builder()\n            .name(\"tf-test\")\n            .config(ModelServingConfigArgs.builder()\n                .servedModels(ModelServingConfigServedModelArgs.builder()\n                    .name(\"prod_model\")\n                    .modelName(\"test\")\n                    .modelVersion(\"1\")\n                    .workloadSize(\"Small\")\n                    .scaleToZeroEnabled(true)\n                    .build())\n                .build())\n            .build());\n\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var mlServingUsage = new Permissions(\"mlServingUsage\", PermissionsArgs.builder()\n            .servingEndpointId(this_.servingEndpointId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_VIEW\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_QUERY\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ModelServing\n    properties:\n      name: tf-test\n      config:\n        servedModels:\n          - name: prod_model\n            modelName: test\n            modelVersion: '1'\n            workloadSize: Small\n            scaleToZeroEnabled: true\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  mlServingUsage:\n    type: databricks:Permissions\n    name: ml_serving_usage\n    properties:\n      servingEndpointId: ${this.servingEndpointId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_MANAGE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_QUERY\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Mosaic AI Vector Search usage\n\nValid permission levels for databricks.VectorSearchEndpoint are: `CAN_USE` and `CAN_MANAGE`.\n\n\u003e You need to use the `endpoint_id` attribute of `databricks.VectorSearchEndpoint` as value for `vector_search_endpoint_id`, not the `id`!\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.VectorSearchEndpoint(\"this\", {\n    name: \"vector-search-test\",\n    endpointType: \"STANDARD\",\n});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst vectorSearchEndpointUsage = new databricks.Permissions(\"vector_search_endpoint_usage\", {\n    vectorSearchEndpointId: _this.endpointId,\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_USE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.VectorSearchEndpoint(\"this\",\n    name=\"vector-search-test\",\n    endpoint_type=\"STANDARD\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nvector_search_endpoint_usage = databricks.Permissions(\"vector_search_endpoint_usage\",\n    vector_search_endpoint_id=this.endpoint_id,\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_USE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.VectorSearchEndpoint(\"this\", new()\n    {\n        Name = \"vector-search-test\",\n        EndpointType = \"STANDARD\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var vectorSearchEndpointUsage = new Databricks.Permissions(\"vector_search_endpoint_usage\", new()\n    {\n        VectorSearchEndpointId = @this.EndpointId,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_USE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewVectorSearchEndpoint(ctx, \"this\", \u0026databricks.VectorSearchEndpointArgs{\n\t\t\tName:         pulumi.String(\"vector-search-test\"),\n\t\t\tEndpointType: pulumi.String(\"STANDARD\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"vector_search_endpoint_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tVectorSearchEndpointId: this.EndpointId,\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.VectorSearchEndpoint;\nimport com.pulumi.databricks.VectorSearchEndpointArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new VectorSearchEndpoint(\"this\", VectorSearchEndpointArgs.builder()\n            .name(\"vector-search-test\")\n            .endpointType(\"STANDARD\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var vectorSearchEndpointUsage = new Permissions(\"vectorSearchEndpointUsage\", PermissionsArgs.builder()\n            .vectorSearchEndpointId(this_.endpointId())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_USE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:VectorSearchEndpoint\n    properties:\n      name: vector-search-test\n      endpointType: STANDARD\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  vectorSearchEndpointUsage:\n    type: databricks:Permissions\n    name: vector_search_endpoint_usage\n    properties:\n      vectorSearchEndpointId: ${this.endpointId}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_USE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Passwords usage\n\nBy default on AWS deployments, all admin users can sign in to Databricks using either SSO or their username and password, and all API users can authenticate to the Databricks REST APIs using their username and password. As an admin, you [can limit](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#optional-configure-password-access-control) admin users’ and API users’ ability to authenticate with their username and password by configuring `CAN_USE` permissions using password access control.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst guests = new databricks.Group(\"guests\", {displayName: \"Guest Users\"});\nconst passwordUsage = new databricks.Permissions(\"password_usage\", {\n    authorization: \"passwords\",\n    accessControls: [{\n        groupName: guests.displayName,\n        permissionLevel: \"CAN_USE\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nguests = databricks.Group(\"guests\", display_name=\"Guest Users\")\npassword_usage = databricks.Permissions(\"password_usage\",\n    authorization=\"passwords\",\n    access_controls=[{\n        \"group_name\": guests.display_name,\n        \"permission_level\": \"CAN_USE\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var guests = new Databricks.Group(\"guests\", new()\n    {\n        DisplayName = \"Guest Users\",\n    });\n\n    var passwordUsage = new Databricks.Permissions(\"password_usage\", new()\n    {\n        Authorization = \"passwords\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = guests.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tguests, err := databricks.NewGroup(ctx, \"guests\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Guest Users\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"password_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tAuthorization: pulumi.String(\"passwords\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       guests.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var guests = new Group(\"guests\", GroupArgs.builder()\n            .displayName(\"Guest Users\")\n            .build());\n\n        var passwordUsage = new Permissions(\"passwordUsage\", PermissionsArgs.builder()\n            .authorization(\"passwords\")\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .groupName(guests.displayName())\n                .permissionLevel(\"CAN_USE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  guests:\n    type: databricks:Group\n    properties:\n      displayName: Guest Users\n  passwordUsage:\n    type: databricks:Permissions\n    name: password_usage\n    properties:\n      authorization: passwords\n      accessControls:\n        - groupName: ${guests.displayName}\n          permissionLevel: CAN_USE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Token usage\n\nIt is required to have at least 1 personal access token in the workspace before you can manage tokens permissions.\n\n!\u003e **Warning** There can be only one `authorization = \"tokens\"` permissions resource per workspace, otherwise there'll be a permanent configuration drift. After applying changes, users who previously had either `CAN_USE` or `CAN_MANAGE` permission but no longer have either permission have their access to token-based authentication revoked. Their active tokens are immediately deleted (revoked).\n\nOnly [possible permission](https://docs.databricks.com/administration-guide/access-control/tokens.html) to assign to non-admin group is `CAN_USE`, where _admins_ `CAN_MANAGE` all tokens:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst tokenUsage = new databricks.Permissions(\"token_usage\", {\n    authorization: \"tokens\",\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_USE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_USE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\ntoken_usage = databricks.Permissions(\"token_usage\",\n    authorization=\"tokens\",\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_USE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_USE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var tokenUsage = new Databricks.Permissions(\"token_usage\", new()\n    {\n        Authorization = \"tokens\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"token_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tAuthorization: pulumi.String(\"tokens\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var tokenUsage = new Permissions(\"tokenUsage\", PermissionsArgs.builder()\n            .authorization(\"tokens\")\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_USE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_USE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  tokenUsage:\n    type: databricks:Permissions\n    name: token_usage\n    properties:\n      authorization: tokens\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_USE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_USE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## SQL warehouse usage\n\n[SQL warehouses](https://docs.databricks.com/sql/user/security/access-control/sql-endpoint-acl.html) have five possible permissions: `CAN_USE`, `CAN_MONITOR`, `CAN_MANAGE`, `CAN_VIEW` and `IS_OWNER`:\n\n- The creator of a warehouse has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a warehouse would revert ownership to the creator.\n- A warehouse must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the warehouse. Nothing would change, per se, if the warehouse was created through Pulumi.\n- A warehouse cannot have a group as an owner.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst _this = new databricks.SqlEndpoint(\"this\", {\n    name: me.then(me =\u003e `Endpoint of ${me.alphanumeric}`),\n    clusterSize: \"Small\",\n    maxNumClusters: 1,\n    tags: {\n        customTags: [{\n            key: \"City\",\n            value: \"Amsterdam\",\n        }],\n    },\n});\nconst endpointUsage = new databricks.Permissions(\"endpoint_usage\", {\n    sqlEndpointId: _this.id,\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_USE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nthis = databricks.SqlEndpoint(\"this\",\n    name=f\"Endpoint of {me.alphanumeric}\",\n    cluster_size=\"Small\",\n    max_num_clusters=1,\n    tags={\n        \"custom_tags\": [{\n            \"key\": \"City\",\n            \"value\": \"Amsterdam\",\n        }],\n    })\nendpoint_usage = databricks.Permissions(\"endpoint_usage\",\n    sql_endpoint_id=this.id,\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_USE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var @this = new Databricks.SqlEndpoint(\"this\", new()\n    {\n        Name = $\"Endpoint of {me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)}\",\n        ClusterSize = \"Small\",\n        MaxNumClusters = 1,\n        Tags = new Databricks.Inputs.SqlEndpointTagsArgs\n        {\n            CustomTags = new[]\n            {\n                new Databricks.Inputs.SqlEndpointTagsCustomTagArgs\n                {\n                    Key = \"City\",\n                    Value = \"Amsterdam\",\n                },\n            },\n        },\n    });\n\n    var endpointUsage = new Databricks.Permissions(\"endpoint_usage\", new()\n    {\n        SqlEndpointId = @this.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_USE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewSqlEndpoint(ctx, \"this\", \u0026databricks.SqlEndpointArgs{\n\t\t\tName:           pulumi.Sprintf(\"Endpoint of %v\", me.Alphanumeric),\n\t\t\tClusterSize:    pulumi.String(\"Small\"),\n\t\t\tMaxNumClusters: pulumi.Int(1),\n\t\t\tTags: \u0026databricks.SqlEndpointTagsArgs{\n\t\t\t\tCustomTags: databricks.SqlEndpointTagsCustomTagArray{\n\t\t\t\t\t\u0026databricks.SqlEndpointTagsCustomTagArgs{\n\t\t\t\t\t\tKey:   pulumi.String(\"City\"),\n\t\t\t\t\t\tValue: pulumi.String(\"Amsterdam\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"endpoint_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlEndpointId: this.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.SqlEndpoint;\nimport com.pulumi.databricks.SqlEndpointArgs;\nimport com.pulumi.databricks.inputs.SqlEndpointTagsArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var this_ = new SqlEndpoint(\"this\", SqlEndpointArgs.builder()\n            .name(String.format(\"Endpoint of %s\", me.alphanumeric()))\n            .clusterSize(\"Small\")\n            .maxNumClusters(1)\n            .tags(SqlEndpointTagsArgs.builder()\n                .customTags(SqlEndpointTagsCustomTagArgs.builder()\n                    .key(\"City\")\n                    .value(\"Amsterdam\")\n                    .build())\n                .build())\n            .build());\n\n        var endpointUsage = new Permissions(\"endpointUsage\", PermissionsArgs.builder()\n            .sqlEndpointId(this_.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_USE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  this:\n    type: databricks:SqlEndpoint\n    properties:\n      name: Endpoint of ${me.alphanumeric}\n      clusterSize: Small\n      maxNumClusters: 1\n      tags:\n        customTags:\n          - key: City\n            value: Amsterdam\n  endpointUsage:\n    type: databricks:Permissions\n    name: endpoint_usage\n    properties:\n      sqlEndpointId: ${this.id}\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_USE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Dashboard usage\n\n[Dashboards](https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html) have four possible permissions: `CAN_READ`, `CAN_RUN`, `CAN_EDIT` and `CAN_MANAGE`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst dashboard = new databricks.Dashboard(\"dashboard\", {displayName: \"TF New Dashboard\"});\nconst dashboardUsage = new databricks.Permissions(\"dashboard_usage\", {\n    dashboardId: dashboard.id,\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\ndashboard = databricks.Dashboard(\"dashboard\", display_name=\"TF New Dashboard\")\ndashboard_usage = databricks.Permissions(\"dashboard_usage\",\n    dashboard_id=dashboard.id,\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var dashboard = new Databricks.Dashboard(\"dashboard\", new()\n    {\n        DisplayName = \"TF New Dashboard\",\n    });\n\n    var dashboardUsage = new Databricks.Permissions(\"dashboard_usage\", new()\n    {\n        DashboardId = dashboard.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdashboard, err := databricks.NewDashboard(ctx, \"dashboard\", \u0026databricks.DashboardArgs{\n\t\t\tDisplayName: pulumi.String(\"TF New Dashboard\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"dashboard_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tDashboardId: dashboard.ID(),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Dashboard;\nimport com.pulumi.databricks.DashboardArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var dashboard = new Dashboard(\"dashboard\", DashboardArgs.builder()\n            .displayName(\"TF New Dashboard\")\n            .build());\n\n        var dashboardUsage = new Permissions(\"dashboardUsage\", PermissionsArgs.builder()\n            .dashboardId(dashboard.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  dashboard:\n    type: databricks:Dashboard\n    properties:\n      displayName: TF New Dashboard\n  dashboardUsage:\n    type: databricks:Permissions\n    name: dashboard_usage\n    properties:\n      dashboardId: ${dashboard.id}\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Legacy SQL Dashboard usage\n\n[Legacy SQL dashboards](https://docs.databricks.com/sql/user/security/access-control/dashboard-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst sqlDashboardUsage = new databricks.Permissions(\"sql_dashboard_usage\", {\n    sqlDashboardId: \"3244325\",\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nsql_dashboard_usage = databricks.Permissions(\"sql_dashboard_usage\",\n    sql_dashboard_id=\"3244325\",\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var sqlDashboardUsage = new Databricks.Permissions(\"sql_dashboard_usage\", new()\n    {\n        SqlDashboardId = \"3244325\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"sql_dashboard_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlDashboardId: pulumi.String(\"3244325\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var sqlDashboardUsage = new Permissions(\"sqlDashboardUsage\", PermissionsArgs.builder()\n            .sqlDashboardId(\"3244325\")\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  sqlDashboardUsage:\n    type: databricks:Permissions\n    name: sql_dashboard_usage\n    properties:\n      sqlDashboardId: '3244325'\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## SQL Query usage\n\n[SQL queries](https://docs.databricks.com/sql/user/security/access-control/query-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:\n\n\u003e If you do not define an `access_control` block granting `CAN_MANAGE` explictly for the user calling this provider, Databricks Pulumi Provider will add `CAN_MANAGE` permission for the caller. This is a failsafe to prevent situations where the caller is locked out from making changes to the targeted `databricks.SqlQuery` resource when backend API do not apply permission inheritance correctly.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst queryUsage = new databricks.Permissions(\"query_usage\", {\n    sqlQueryId: \"3244325\",\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nquery_usage = databricks.Permissions(\"query_usage\",\n    sql_query_id=\"3244325\",\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var queryUsage = new Databricks.Permissions(\"query_usage\", new()\n    {\n        SqlQueryId = \"3244325\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"query_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlQueryId: pulumi.String(\"3244325\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var queryUsage = new Permissions(\"queryUsage\", PermissionsArgs.builder()\n            .sqlQueryId(\"3244325\")\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  queryUsage:\n    type: databricks:Permissions\n    name: query_usage\n    properties:\n      sqlQueryId: '3244325'\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## SQL Alert usage\n\n[SQL alerts](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auto = new databricks.Group(\"auto\", {displayName: \"Automation\"});\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst alertUsage = new databricks.Permissions(\"alert_usage\", {\n    sqlAlertId: \"3244325\",\n    accessControls: [\n        {\n            groupName: auto.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nauto = databricks.Group(\"auto\", display_name=\"Automation\")\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\nalert_usage = databricks.Permissions(\"alert_usage\",\n    sql_alert_id=\"3244325\",\n    access_controls=[\n        {\n            \"group_name\": auto.display_name,\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auto = new Databricks.Group(\"auto\", new()\n    {\n        DisplayName = \"Automation\",\n    });\n\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var alertUsage = new Databricks.Permissions(\"alert_usage\", new()\n    {\n        SqlAlertId = \"3244325\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = auto.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tauto, err := databricks.NewGroup(ctx, \"auto\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"alert_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlAlertId: pulumi.String(\"3244325\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       auto.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auto = new Group(\"auto\", GroupArgs.builder()\n            .displayName(\"Automation\")\n            .build());\n\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var alertUsage = new Permissions(\"alertUsage\", PermissionsArgs.builder()\n            .sqlAlertId(\"3244325\")\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(auto.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auto:\n    type: databricks:Group\n    properties:\n      displayName: Automation\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  alertUsage:\n    type: databricks:Permissions\n    name: alert_usage\n    properties:\n      sqlAlertId: '3244325'\n      accessControls:\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Databricks Apps usage\n\n[Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) have two possible permissions: `CAN_USE` and `CAN_MANAGE`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst eng = new databricks.Group(\"eng\", {displayName: \"Engineering\"});\nconst appUsage = new databricks.Permissions(\"app_usage\", {\n    appName: \"myapp\",\n    accessControls: [\n        {\n            groupName: \"users\",\n            permissionLevel: \"CAN_USE\",\n        },\n        {\n            groupName: eng.displayName,\n            permissionLevel: \"CAN_MANAGE\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\neng = databricks.Group(\"eng\", display_name=\"Engineering\")\napp_usage = databricks.Permissions(\"app_usage\",\n    app_name=\"myapp\",\n    access_controls=[\n        {\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_USE\",\n        },\n        {\n            \"group_name\": eng.display_name,\n            \"permission_level\": \"CAN_MANAGE\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var eng = new Databricks.Group(\"eng\", new()\n    {\n        DisplayName = \"Engineering\",\n    });\n\n    var appUsage = new Databricks.Permissions(\"app_usage\", new()\n    {\n        AppName = \"myapp\",\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = \"users\",\n                PermissionLevel = \"CAN_USE\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = eng.DisplayName,\n                PermissionLevel = \"CAN_MANAGE\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\teng, err := databricks.NewGroup(ctx, \"eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissions(ctx, \"app_usage\", \u0026databricks.PermissionsArgs{\n\t\t\tAppName: pulumi.String(\"myapp\"),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_USE\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       eng.DisplayName,\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var eng = new Group(\"eng\", GroupArgs.builder()\n            .displayName(\"Engineering\")\n            .build());\n\n        var appUsage = new Permissions(\"appUsage\", PermissionsArgs.builder()\n            .appName(\"myapp\")\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(\"users\")\n                    .permissionLevel(\"CAN_USE\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(eng.displayName())\n                    .permissionLevel(\"CAN_MANAGE\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  eng:\n    type: databricks:Group\n    properties:\n      displayName: Engineering\n  appUsage:\n    type: databricks:Permissions\n    name: app_usage\n    properties:\n      appName: myapp\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_USE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_MANAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Instance Profiles\n\nInstance Profiles are not managed by General Permissions API and therefore databricks.GroupInstanceProfile and databricks.UserInstanceProfile should be used to allow usage of specific AWS EC2 IAM roles to users or groups.\n\n## Secrets\n\nOne can control access to databricks.Secret through `initial_manage_principal` argument on databricks.SecretScope or databricks_secret_acl, so that users (or service principals) can `READ`, `WRITE` or `MANAGE` entries within secret scope.\n\n## Tables, Views and Databases\n\nGeneral Permissions API does not apply to access control for tables and they have to be managed separately using the databricks.SqlPermissions resource, though you're encouraged to use Unity Catalog or migrate to it.\n\n## Data Access with Unity Catalog\n\nInitially in Unity Catalog all users have no access to data, which has to be later assigned through databricks.Grants or databricks.Grant resource.\n\n## Import\n\nThe resource permissions can be imported using the object id\n\n```sh\n$ pulumi import databricks:index/permissions:Permissions databricks_permissions \u003cobject type\u003e/\u003cobject id\u003e\n```\n\n",
            "properties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "appName": {
                    "type": "string"
                },
                "authorization": {
                    "type": "string"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterPolicyId": {
                    "type": "string"
                },
                "dashboardId": {
                    "type": "string"
                },
                "directoryId": {
                    "type": "string"
                },
                "directoryPath": {
                    "type": "string"
                },
                "experimentId": {
                    "type": "string"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "jobId": {
                    "type": "string"
                },
                "notebookId": {
                    "type": "string"
                },
                "notebookPath": {
                    "type": "string"
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "repoId": {
                    "type": "string"
                },
                "repoPath": {
                    "type": "string"
                },
                "servingEndpointId": {
                    "type": "string"
                },
                "sqlAlertId": {
                    "type": "string"
                },
                "sqlDashboardId": {
                    "type": "string"
                },
                "sqlEndpointId": {
                    "type": "string"
                },
                "sqlQueryId": {
                    "type": "string"
                },
                "vectorSearchEndpointId": {
                    "type": "string"
                },
                "workspaceFileId": {
                    "type": "string"
                },
                "workspaceFilePath": {
                    "type": "string"
                }
            },
            "required": [
                "accessControls",
                "objectType"
            ],
            "inputProperties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "appName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "authorization": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterPolicyId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "dashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directoryPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "experimentId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "jobId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "notebookId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "notebookPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repoId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repoPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "servingEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlAlertId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlDashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlQueryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "vectorSearchEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "workspaceFileId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "workspaceFilePath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accessControls"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Permissions resources.\n",
                "properties": {
                    "accessControls": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                        }
                    },
                    "appName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "authorization": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterPolicyId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "dashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directoryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directoryPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "experimentId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "jobId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "notebookId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "notebookPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "objectType": {
                        "type": "string",
                        "description": "type of permissions.\n"
                    },
                    "pipelineId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "repoId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "repoPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "servingEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlAlertId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlDashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlQueryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "vectorSearchEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "workspaceFileId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "workspaceFilePath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/pipeline:Pipeline": {
            "description": "Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dltDemo = new databricks.Notebook(\"dlt_demo\", {});\nconst dltDemoRepo = new databricks.Repo(\"dlt_demo\", {});\nconst _this = new databricks.Pipeline(\"this\", {\n    name: \"Pipeline Name\",\n    storage: \"/test/first-pipeline\",\n    configuration: {\n        key1: \"value1\",\n        key2: \"value2\",\n    },\n    clusters: [\n        {\n            label: \"default\",\n            numWorkers: 2,\n            customTags: {\n                cluster_type: \"default\",\n            },\n        },\n        {\n            label: \"maintenance\",\n            numWorkers: 1,\n            customTags: {\n                cluster_type: \"maintenance\",\n            },\n        },\n    ],\n    libraries: [\n        {\n            notebook: {\n                path: dltDemo.id,\n            },\n        },\n        {\n            file: {\n                path: pulumi.interpolate`${dltDemoRepo.path}/pipeline.sql`,\n            },\n        },\n    ],\n    continuous: false,\n    notifications: [{\n        emailRecipients: [\n            \"user@domain.com\",\n            \"user1@domain.com\",\n        ],\n        alerts: [\n            \"on-update-failure\",\n            \"on-update-fatal-failure\",\n            \"on-update-success\",\n            \"on-flow-failure\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndlt_demo = databricks.Notebook(\"dlt_demo\")\ndlt_demo_repo = databricks.Repo(\"dlt_demo\")\nthis = databricks.Pipeline(\"this\",\n    name=\"Pipeline Name\",\n    storage=\"/test/first-pipeline\",\n    configuration={\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n    },\n    clusters=[\n        {\n            \"label\": \"default\",\n            \"num_workers\": 2,\n            \"custom_tags\": {\n                \"cluster_type\": \"default\",\n            },\n        },\n        {\n            \"label\": \"maintenance\",\n            \"num_workers\": 1,\n            \"custom_tags\": {\n                \"cluster_type\": \"maintenance\",\n            },\n        },\n    ],\n    libraries=[\n        {\n            \"notebook\": {\n                \"path\": dlt_demo.id,\n            },\n        },\n        {\n            \"file\": {\n                \"path\": dlt_demo_repo.path.apply(lambda path: f\"{path}/pipeline.sql\"),\n            },\n        },\n    ],\n    continuous=False,\n    notifications=[{\n        \"email_recipients\": [\n            \"user@domain.com\",\n            \"user1@domain.com\",\n        ],\n        \"alerts\": [\n            \"on-update-failure\",\n            \"on-update-fatal-failure\",\n            \"on-update-success\",\n            \"on-flow-failure\",\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dltDemo = new Databricks.Notebook(\"dlt_demo\");\n\n    var dltDemoRepo = new Databricks.Repo(\"dlt_demo\");\n\n    var @this = new Databricks.Pipeline(\"this\", new()\n    {\n        Name = \"Pipeline Name\",\n        Storage = \"/test/first-pipeline\",\n        Configuration = \n        {\n            { \"key1\", \"value1\" },\n            { \"key2\", \"value2\" },\n        },\n        Clusters = new[]\n        {\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"default\",\n                NumWorkers = 2,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"default\" },\n                },\n            },\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"maintenance\",\n                NumWorkers = 1,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"maintenance\" },\n                },\n            },\n        },\n        Libraries = new[]\n        {\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs\n                {\n                    Path = dltDemo.Id,\n                },\n            },\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                File = new Databricks.Inputs.PipelineLibraryFileArgs\n                {\n                    Path = dltDemoRepo.Path.Apply(path =\u003e $\"{path}/pipeline.sql\"),\n                },\n            },\n        },\n        Continuous = false,\n        Notifications = new[]\n        {\n            new Databricks.Inputs.PipelineNotificationArgs\n            {\n                EmailRecipients = new[]\n                {\n                    \"user@domain.com\",\n                    \"user1@domain.com\",\n                },\n                Alerts = new[]\n                {\n                    \"on-update-failure\",\n                    \"on-update-fatal-failure\",\n                    \"on-update-success\",\n                    \"on-flow-failure\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdltDemo, err := databricks.NewNotebook(ctx, \"dlt_demo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdltDemoRepo, err := databricks.NewRepo(ctx, \"dlt_demo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPipeline(ctx, \"this\", \u0026databricks.PipelineArgs{\n\t\t\tName:    pulumi.String(\"Pipeline Name\"),\n\t\t\tStorage: pulumi.String(\"/test/first-pipeline\"),\n\t\t\tConfiguration: pulumi.StringMap{\n\t\t\t\t\"key1\": pulumi.String(\"value1\"),\n\t\t\t\t\"key2\": pulumi.String(\"value2\"),\n\t\t\t},\n\t\t\tClusters: databricks.PipelineClusterArray{\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"default\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(2),\n\t\t\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.String(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"maintenance\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(1),\n\t\t\t\t\tCustomTags: pulumi.StringMap{\n\t\t\t\t\t\t\"cluster_type\": pulumi.String(\"maintenance\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tLibraries: databricks.PipelineLibraryArray{\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tNotebook: \u0026databricks.PipelineLibraryNotebookArgs{\n\t\t\t\t\t\tPath: dltDemo.ID(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tFile: \u0026databricks.PipelineLibraryFileArgs{\n\t\t\t\t\t\tPath: dltDemoRepo.Path.ApplyT(func(path string) (string, error) {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"%v/pipeline.sql\", path), nil\n\t\t\t\t\t\t}).(pulumi.StringOutput),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContinuous: pulumi.Bool(false),\n\t\t\tNotifications: databricks.PipelineNotificationArray{\n\t\t\t\t\u0026databricks.PipelineNotificationArgs{\n\t\t\t\t\tEmailRecipients: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\tpulumi.String(\"user1@domain.com\"),\n\t\t\t\t\t},\n\t\t\t\t\tAlerts: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"on-update-failure\"),\n\t\t\t\t\t\tpulumi.String(\"on-update-fatal-failure\"),\n\t\t\t\t\t\tpulumi.String(\"on-update-success\"),\n\t\t\t\t\t\tpulumi.String(\"on-flow-failure\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.Repo;\nimport com.pulumi.databricks.Pipeline;\nimport com.pulumi.databricks.PipelineArgs;\nimport com.pulumi.databricks.inputs.PipelineClusterArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryFileArgs;\nimport com.pulumi.databricks.inputs.PipelineNotificationArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dltDemo = new Notebook(\"dltDemo\");\n\n        var dltDemoRepo = new Repo(\"dltDemoRepo\");\n\n        var this_ = new Pipeline(\"this\", PipelineArgs.builder()\n            .name(\"Pipeline Name\")\n            .storage(\"/test/first-pipeline\")\n            .configuration(Map.ofEntries(\n                Map.entry(\"key1\", \"value1\"),\n                Map.entry(\"key2\", \"value2\")\n            ))\n            .clusters(            \n                PipelineClusterArgs.builder()\n                    .label(\"default\")\n                    .numWorkers(2)\n                    .customTags(Map.of(\"cluster_type\", \"default\"))\n                    .build(),\n                PipelineClusterArgs.builder()\n                    .label(\"maintenance\")\n                    .numWorkers(1)\n                    .customTags(Map.of(\"cluster_type\", \"maintenance\"))\n                    .build())\n            .libraries(            \n                PipelineLibraryArgs.builder()\n                    .notebook(PipelineLibraryNotebookArgs.builder()\n                        .path(dltDemo.id())\n                        .build())\n                    .build(),\n                PipelineLibraryArgs.builder()\n                    .file(PipelineLibraryFileArgs.builder()\n                        .path(dltDemoRepo.path().applyValue(_path -\u003e String.format(\"%s/pipeline.sql\", _path)))\n                        .build())\n                    .build())\n            .continuous(false)\n            .notifications(PipelineNotificationArgs.builder()\n                .emailRecipients(                \n                    \"user@domain.com\",\n                    \"user1@domain.com\")\n                .alerts(                \n                    \"on-update-failure\",\n                    \"on-update-fatal-failure\",\n                    \"on-update-success\",\n                    \"on-flow-failure\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dltDemo:\n    type: databricks:Notebook\n    name: dlt_demo\n  dltDemoRepo:\n    type: databricks:Repo\n    name: dlt_demo\n  this:\n    type: databricks:Pipeline\n    properties:\n      name: Pipeline Name\n      storage: /test/first-pipeline\n      configuration:\n        key1: value1\n        key2: value2\n      clusters:\n        - label: default\n          numWorkers: 2\n          customTags:\n            cluster_type: default\n        - label: maintenance\n          numWorkers: 1\n          customTags:\n            cluster_type: maintenance\n      libraries:\n        - notebook:\n            path: ${dltDemo.id}\n        - file:\n            path: ${dltDemoRepo.path}/pipeline.sql\n      continuous: false\n      notifications:\n        - emailRecipients:\n            - user@domain.com\n            - user1@domain.com\n          alerts:\n            - on-update-failure\n            - on-update-fatal-failure\n            - on-update-success\n            - on-flow-failure\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getPipelines to retrieve [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipeline data.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n\n## Import\n\nThe resource job can be imported using the id of the pipeline\n\nbash\n\n```sh\n$ pulumi import databricks:index/pipeline:Pipeline this \u003cpipeline-id\u003e\n```\n\n",
            "properties": {
                "allowDuplicateNames": {
                    "type": "boolean",
                    "description": "Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.\n"
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "optional string specifying ID of the budget policy for this DLT pipeline.\n"
                },
                "catalog": {
                    "type": "string",
                    "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n"
                },
                "cause": {
                    "type": "string"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment",
                    "description": "Deployment type of this pipeline. Supports following attributes:\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.\n"
                },
                "eventLog": {
                    "$ref": "#/types/databricks:index/PipelineEventLog:PipelineEventLog",
                    "description": "an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:\n"
                },
                "expectedLastModified": {
                    "type": "integer"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters",
                    "description": "Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:\n"
                },
                "gatewayDefinition": {
                    "$ref": "#/types/databricks:index/PipelineGatewayDefinition:PipelineGatewayDefinition",
                    "description": "The definition of a gateway pipeline to support CDC. Consists of following attributes:\n"
                },
                "health": {
                    "type": "string"
                },
                "ingestionDefinition": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinition:PipelineIngestionDefinition"
                },
                "lastModified": {
                    "type": "integer"
                },
                "latestUpdates": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLatestUpdate:PipelineLatestUpdate"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "notifications": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                    }
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "restartWindow": {
                    "$ref": "#/types/databricks:index/PipelineRestartWindow:PipelineRestartWindow"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/PipelineRunAs:PipelineRunAs"
                },
                "runAsUserName": {
                    "type": "string"
                },
                "schema": {
                    "type": "string",
                    "description": "The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.\n"
                },
                "serverless": {
                    "type": "boolean",
                    "description": "An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.\n"
                },
                "state": {
                    "type": "string"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n"
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/PipelineTrigger:PipelineTrigger"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the DLT pipeline on the given workspace.\n"
                }
            },
            "required": [
                "cause",
                "clusterId",
                "creatorUserName",
                "health",
                "lastModified",
                "latestUpdates",
                "name",
                "runAs",
                "runAsUserName",
                "state",
                "url"
            ],
            "inputProperties": {
                "allowDuplicateNames": {
                    "type": "boolean",
                    "description": "Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.\n"
                },
                "budgetPolicyId": {
                    "type": "string",
                    "description": "optional string specifying ID of the budget policy for this DLT pipeline.\n"
                },
                "catalog": {
                    "type": "string",
                    "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n",
                    "willReplaceOnChanges": true
                },
                "cause": {
                    "type": "string"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment",
                    "description": "Deployment type of this pipeline. Supports following attributes:\n"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.\n"
                },
                "eventLog": {
                    "$ref": "#/types/databricks:index/PipelineEventLog:PipelineEventLog",
                    "description": "an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:\n"
                },
                "expectedLastModified": {
                    "type": "integer"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters",
                    "description": "Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:\n"
                },
                "gatewayDefinition": {
                    "$ref": "#/types/databricks:index/PipelineGatewayDefinition:PipelineGatewayDefinition",
                    "description": "The definition of a gateway pipeline to support CDC. Consists of following attributes:\n"
                },
                "health": {
                    "type": "string"
                },
                "ingestionDefinition": {
                    "$ref": "#/types/databricks:index/PipelineIngestionDefinition:PipelineIngestionDefinition"
                },
                "lastModified": {
                    "type": "integer"
                },
                "latestUpdates": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLatestUpdate:PipelineLatestUpdate"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "notifications": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                    }
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "restartWindow": {
                    "$ref": "#/types/databricks:index/PipelineRestartWindow:PipelineRestartWindow"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/PipelineRunAs:PipelineRunAs"
                },
                "runAsUserName": {
                    "type": "string"
                },
                "schema": {
                    "type": "string",
                    "description": "The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.\n"
                },
                "serverless": {
                    "type": "boolean",
                    "description": "An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.\n"
                },
                "state": {
                    "type": "string"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n",
                    "willReplaceOnChanges": true
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/PipelineTrigger:PipelineTrigger"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the DLT pipeline on the given workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Pipeline resources.\n",
                "properties": {
                    "allowDuplicateNames": {
                        "type": "boolean",
                        "description": "Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.\n"
                    },
                    "budgetPolicyId": {
                        "type": "string",
                        "description": "optional string specifying ID of the budget policy for this DLT pipeline.\n"
                    },
                    "catalog": {
                        "type": "string",
                        "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n",
                        "willReplaceOnChanges": true
                    },
                    "cause": {
                        "type": "string"
                    },
                    "channel": {
                        "type": "string",
                        "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                        },
                        "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                    },
                    "configuration": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                    },
                    "continuous": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                    },
                    "creatorUserName": {
                        "type": "string"
                    },
                    "deployment": {
                        "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment",
                        "description": "Deployment type of this pipeline. Supports following attributes:\n"
                    },
                    "development": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline in development mode. The default value is `false`.\n"
                    },
                    "edition": {
                        "type": "string",
                        "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.\n"
                    },
                    "eventLog": {
                        "$ref": "#/types/databricks:index/PipelineEventLog:PipelineEventLog",
                        "description": "an optional block specifying a table where DLT Event Log will be stored.  Consists of the following fields:\n"
                    },
                    "expectedLastModified": {
                        "type": "integer"
                    },
                    "filters": {
                        "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters",
                        "description": "Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:\n"
                    },
                    "gatewayDefinition": {
                        "$ref": "#/types/databricks:index/PipelineGatewayDefinition:PipelineGatewayDefinition",
                        "description": "The definition of a gateway pipeline to support CDC. Consists of following attributes:\n"
                    },
                    "health": {
                        "type": "string"
                    },
                    "ingestionDefinition": {
                        "$ref": "#/types/databricks:index/PipelineIngestionDefinition:PipelineIngestionDefinition"
                    },
                    "lastModified": {
                        "type": "integer"
                    },
                    "latestUpdates": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineLatestUpdate:PipelineLatestUpdate"
                        }
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                        },
                        "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                    },
                    "notifications": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                        }
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                    },
                    "restartWindow": {
                        "$ref": "#/types/databricks:index/PipelineRestartWindow:PipelineRestartWindow"
                    },
                    "runAs": {
                        "$ref": "#/types/databricks:index/PipelineRunAs:PipelineRunAs"
                    },
                    "runAsUserName": {
                        "type": "string"
                    },
                    "schema": {
                        "type": "string",
                        "description": "The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.\n"
                    },
                    "serverless": {
                        "type": "boolean",
                        "description": "An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.\n"
                    },
                    "state": {
                        "type": "string"
                    },
                    "storage": {
                        "type": "string",
                        "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n",
                        "willReplaceOnChanges": true
                    },
                    "target": {
                        "type": "string",
                        "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                    },
                    "trigger": {
                        "$ref": "#/types/databricks:index/PipelineTrigger:PipelineTrigger"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the DLT pipeline on the given workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/qualityMonitor:QualityMonitor": {
            "description": "This resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks. \n\nA `databricks.QualityMonitor` is attached to a databricks.SqlTable and can be of type timeseries, snapshot or inference.\n\n## Plugin Framework Migration\n\nThe quality monitor resource has been migrated from sdkv2 to plugin framework。 If you encounter any problem with this resource and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_RESOURCES=\"databricks.QualityMonitor\"`.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst myTestTable = new databricks.SqlTable(\"myTestTable\", {\n    catalogName: \"main\",\n    schemaName: things.name,\n    name: \"bar\",\n    tableType: \"MANAGED\",\n    dataSourceFormat: \"DELTA\",\n    columns: [{\n        name: \"timestamp\",\n        type: \"int\",\n    }],\n});\nconst testTimeseriesMonitor = new databricks.QualityMonitor(\"testTimeseriesMonitor\", {\n    tableName: pulumi.interpolate`${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: pulumi.interpolate`/Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}`,\n    outputSchemaName: pulumi.interpolate`${sandbox.name}.${things.name}`,\n    timeSeries: {\n        granularities: [\"1 hour\"],\n        timestampCol: \"timestamp\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    name=\"things\",\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nmy_test_table = databricks.SqlTable(\"myTestTable\",\n    catalog_name=\"main\",\n    schema_name=things.name,\n    name=\"bar\",\n    table_type=\"MANAGED\",\n    data_source_format=\"DELTA\",\n    columns=[{\n        \"name\": \"timestamp\",\n        \"type\": \"int\",\n    }])\ntest_timeseries_monitor = databricks.QualityMonitor(\"testTimeseriesMonitor\",\n    table_name=pulumi.Output.all(\n        sandboxName=sandbox.name,\n        thingsName=things.name,\n        myTestTableName=my_test_table.name\n).apply(lambda resolved_outputs: f\"{resolved_outputs['sandboxName']}.{resolved_outputs['thingsName']}.{resolved_outputs['myTestTableName']}\")\n,\n    assets_dir=my_test_table.name.apply(lambda name: f\"/Shared/provider-test/databricks_quality_monitoring/{name}\"),\n    output_schema_name=pulumi.Output.all(\n        sandboxName=sandbox.name,\n        thingsName=things.name\n).apply(lambda resolved_outputs: f\"{resolved_outputs['sandboxName']}.{resolved_outputs['thingsName']}\")\n,\n    time_series={\n        \"granularities\": [\"1 hour\"],\n        \"timestamp_col\": \"timestamp\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var myTestTable = new Databricks.SqlTable(\"myTestTable\", new()\n    {\n        CatalogName = \"main\",\n        SchemaName = things.Name,\n        Name = \"bar\",\n        TableType = \"MANAGED\",\n        DataSourceFormat = \"DELTA\",\n        Columns = new[]\n        {\n            new Databricks.Inputs.SqlTableColumnArgs\n            {\n                Name = \"timestamp\",\n                Type = \"int\",\n            },\n        },\n    });\n\n    var testTimeseriesMonitor = new Databricks.QualityMonitor(\"testTimeseriesMonitor\", new()\n    {\n        TableName = Output.Tuple(sandbox.Name, things.Name, myTestTable.Name).Apply(values =\u003e\n        {\n            var sandboxName = values.Item1;\n            var thingsName = values.Item2;\n            var myTestTableName = values.Item3;\n            return $\"{sandboxName}.{thingsName}.{myTestTableName}\";\n        }),\n        AssetsDir = myTestTable.Name.Apply(name =\u003e $\"/Shared/provider-test/databricks_quality_monitoring/{name}\"),\n        OutputSchemaName = Output.Tuple(sandbox.Name, things.Name).Apply(values =\u003e\n        {\n            var sandboxName = values.Item1;\n            var thingsName = values.Item2;\n            return $\"{sandboxName}.{thingsName}\";\n        }),\n        TimeSeries = new Databricks.Inputs.QualityMonitorTimeSeriesArgs\n        {\n            Granularities = new[]\n            {\n                \"1 hour\",\n            },\n            TimestampCol = \"timestamp\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyTestTable, err := databricks.NewSqlTable(ctx, \"myTestTable\", \u0026databricks.SqlTableArgs{\n\t\t\tCatalogName:      pulumi.String(\"main\"),\n\t\t\tSchemaName:       things.Name,\n\t\t\tName:             pulumi.String(\"bar\"),\n\t\t\tTableType:        pulumi.String(\"MANAGED\"),\n\t\t\tDataSourceFormat: pulumi.String(\"DELTA\"),\n\t\t\tColumns: databricks.SqlTableColumnArray{\n\t\t\t\t\u0026databricks.SqlTableColumnArgs{\n\t\t\t\t\tName: pulumi.String(\"timestamp\"),\n\t\t\t\t\tType: pulumi.String(\"int\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewQualityMonitor(ctx, \"testTimeseriesMonitor\", \u0026databricks.QualityMonitorArgs{\n\t\t\tTableName: pulumi.All(sandbox.Name, things.Name, myTestTable.Name).ApplyT(func(_args []interface{}) (string, error) {\n\t\t\t\tsandboxName := _args[0].(string)\n\t\t\t\tthingsName := _args[1].(string)\n\t\t\t\tmyTestTableName := _args[2].(string)\n\t\t\t\treturn fmt.Sprintf(\"%v.%v.%v\", sandboxName, thingsName, myTestTableName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tAssetsDir: myTestTable.Name.ApplyT(func(name string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"/Shared/provider-test/databricks_quality_monitoring/%v\", name), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tOutputSchemaName: pulumi.All(sandbox.Name, things.Name).ApplyT(func(_args []interface{}) (string, error) {\n\t\t\t\tsandboxName := _args[0].(string)\n\t\t\t\tthingsName := _args[1].(string)\n\t\t\t\treturn fmt.Sprintf(\"%v.%v\", sandboxName, thingsName), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tTimeSeries: \u0026databricks.QualityMonitorTimeSeriesArgs{\n\t\t\t\tGranularities: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"1 hour\"),\n\t\t\t\t},\n\t\t\t\tTimestampCol: pulumi.String(\"timestamp\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.SqlTable;\nimport com.pulumi.databricks.SqlTableArgs;\nimport com.pulumi.databricks.inputs.SqlTableColumnArgs;\nimport com.pulumi.databricks.QualityMonitor;\nimport com.pulumi.databricks.QualityMonitorArgs;\nimport com.pulumi.databricks.inputs.QualityMonitorTimeSeriesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var myTestTable = new SqlTable(\"myTestTable\", SqlTableArgs.builder()\n            .catalogName(\"main\")\n            .schemaName(things.name())\n            .name(\"bar\")\n            .tableType(\"MANAGED\")\n            .dataSourceFormat(\"DELTA\")\n            .columns(SqlTableColumnArgs.builder()\n                .name(\"timestamp\")\n                .type(\"int\")\n                .build())\n            .build());\n\n        var testTimeseriesMonitor = new QualityMonitor(\"testTimeseriesMonitor\", QualityMonitorArgs.builder()\n            .tableName(Output.tuple(sandbox.name(), things.name(), myTestTable.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                var myTestTableName = values.t3;\n                return String.format(\"%s.%s.%s\", sandboxName,thingsName,myTestTableName);\n            }))\n            .assetsDir(myTestTable.name().applyValue(_name -\u003e String.format(\"/Shared/provider-test/databricks_quality_monitoring/%s\", _name)))\n            .outputSchemaName(Output.tuple(sandbox.name(), things.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                return String.format(\"%s.%s\", sandboxName,thingsName);\n            }))\n            .timeSeries(QualityMonitorTimeSeriesArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n  myTestTable:\n    type: databricks:SqlTable\n    properties:\n      catalogName: main\n      schemaName: ${things.name}\n      name: bar\n      tableType: MANAGED\n      dataSourceFormat: DELTA\n      columns:\n        - name: timestamp\n          type: int\n  testTimeseriesMonitor:\n    type: databricks:QualityMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      timeSeries:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Inference Monitor\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.QualityMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    inferenceLog: {\n        granularities: [\"1 hour\"],\n        timestampCol: \"timestamp\",\n        predictionCol: \"prediction\",\n        modelIdCol: \"model_id\",\n        problemType: \"PROBLEM_TYPE_REGRESSION\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.QualityMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_quality_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    inference_log={\n        \"granularities\": [\"1 hour\"],\n        \"timestamp_col\": \"timestamp\",\n        \"prediction_col\": \"prediction\",\n        \"model_id_col\": \"model_id\",\n        \"problem_type\": \"PROBLEM_TYPE_REGRESSION\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.QualityMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_quality_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        InferenceLog = new Databricks.Inputs.QualityMonitorInferenceLogArgs\n        {\n            Granularities = new[]\n            {\n                \"1 hour\",\n            },\n            TimestampCol = \"timestamp\",\n            PredictionCol = \"prediction\",\n            ModelIdCol = \"model_id\",\n            ProblemType = \"PROBLEM_TYPE_REGRESSION\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewQualityMonitor(ctx, \"testMonitorInference\", \u0026databricks.QualityMonitorArgs{\n\t\t\tTableName:        pulumi.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name),\n\t\t\tAssetsDir:        pulumi.Sprintf(\"/Shared/provider-test/databricks_quality_monitoring/%v\", myTestTable.Name),\n\t\t\tOutputSchemaName: pulumi.Sprintf(\"%v.%v\", sandbox.Name, things.Name),\n\t\t\tInferenceLog: \u0026databricks.QualityMonitorInferenceLogArgs{\n\t\t\t\tGranularities: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"1 hour\"),\n\t\t\t\t},\n\t\t\t\tTimestampCol:  pulumi.String(\"timestamp\"),\n\t\t\t\tPredictionCol: pulumi.String(\"prediction\"),\n\t\t\t\tModelIdCol:    pulumi.String(\"model_id\"),\n\t\t\t\tProblemType:   pulumi.String(\"PROBLEM_TYPE_REGRESSION\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.QualityMonitor;\nimport com.pulumi.databricks.QualityMonitorArgs;\nimport com.pulumi.databricks.inputs.QualityMonitorInferenceLogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new QualityMonitor(\"testMonitorInference\", QualityMonitorArgs.builder()\n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_quality_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .inferenceLog(QualityMonitorInferenceLogArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .predictionCol(\"prediction\")\n                .modelIdCol(\"model_id\")\n                .problemType(\"PROBLEM_TYPE_REGRESSION\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:QualityMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      inferenceLog:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n        predictionCol: prediction\n        modelIdCol: model_id\n        problemType: PROBLEM_TYPE_REGRESSION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n### Snapshot Monitor\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.QualityMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    snapshot: {},\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.QualityMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_quality_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    snapshot={})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.QualityMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_quality_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        Snapshot = null,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewQualityMonitor(ctx, \"testMonitorInference\", \u0026databricks.QualityMonitorArgs{\n\t\t\tTableName:        pulumi.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name),\n\t\t\tAssetsDir:        pulumi.Sprintf(\"/Shared/provider-test/databricks_quality_monitoring/%v\", myTestTable.Name),\n\t\t\tOutputSchemaName: pulumi.Sprintf(\"%v.%v\", sandbox.Name, things.Name),\n\t\t\tSnapshot:         \u0026databricks.QualityMonitorSnapshotArgs{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.QualityMonitor;\nimport com.pulumi.databricks.QualityMonitorArgs;\nimport com.pulumi.databricks.inputs.QualityMonitorSnapshotArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new QualityMonitor(\"testMonitorInference\", QualityMonitorArgs.builder()\n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_quality_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .snapshot(QualityMonitorSnapshotArgs.builder()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:QualityMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_quality_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      snapshot: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Catalog\n* databricks.Schema\n* databricks.SqlTable\n",
            "properties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/QualityMonitorCustomMetric:QualityMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "The ID of the generated dashboard.\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/QualityMonitorDataClassificationConfig:QualityMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "driftMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/QualityMonitorInferenceLog:QualityMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "monitorId": {
                    "type": "string",
                    "description": "ID of this monitor is the same as the full table name of the format `{catalog}.{schema_name}.{table_name}`\n"
                },
                "monitorVersion": {
                    "type": "string",
                    "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/QualityMonitorNotifications:QualityMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "profileMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/QualityMonitorSchedule:QualityMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.  (Can't be updated after creation).\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/QualityMonitorSnapshot:QualityMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of the Monitor\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/QualityMonitorTimeSeries:QualityMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.  (Can't be updated after creation)\n"
                }
            },
            "required": [
                "assetsDir",
                "dashboardId",
                "driftMetricsTableName",
                "monitorId",
                "monitorVersion",
                "outputSchemaName",
                "profileMetricsTableName",
                "status",
                "tableName"
            ],
            "inputProperties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/QualityMonitorCustomMetric:QualityMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/QualityMonitorDataClassificationConfig:QualityMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/QualityMonitorInferenceLog:QualityMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "monitorId": {
                    "type": "string",
                    "description": "ID of this monitor is the same as the full table name of the format `{catalog}.{schema_name}.{table_name}`\n"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/QualityMonitorNotifications:QualityMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/QualityMonitorSchedule:QualityMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.  (Can't be updated after creation).\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/QualityMonitorSnapshot:QualityMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/QualityMonitorTimeSeries:QualityMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.  (Can't be updated after creation)\n"
                }
            },
            "requiredInputs": [
                "assetsDir",
                "outputSchemaName",
                "tableName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering QualityMonitor resources.\n",
                "properties": {
                    "assetsDir": {
                        "type": "string",
                        "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                    },
                    "baselineTableName": {
                        "type": "string",
                        "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                    },
                    "customMetrics": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/QualityMonitorCustomMetric:QualityMonitorCustomMetric"
                        },
                        "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                    },
                    "dashboardId": {
                        "type": "string",
                        "description": "The ID of the generated dashboard.\n"
                    },
                    "dataClassificationConfig": {
                        "$ref": "#/types/databricks:index/QualityMonitorDataClassificationConfig:QualityMonitorDataClassificationConfig",
                        "description": "The data classification config for the monitor\n"
                    },
                    "driftMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "inferenceLog": {
                        "$ref": "#/types/databricks:index/QualityMonitorInferenceLog:QualityMonitorInferenceLog",
                        "description": "Configuration for the inference log monitor\n"
                    },
                    "latestMonitorFailureMsg": {
                        "type": "string"
                    },
                    "monitorId": {
                        "type": "string",
                        "description": "ID of this monitor is the same as the full table name of the format `{catalog}.{schema_name}.{table_name}`\n"
                    },
                    "monitorVersion": {
                        "type": "string",
                        "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                    },
                    "notifications": {
                        "$ref": "#/types/databricks:index/QualityMonitorNotifications:QualityMonitorNotifications",
                        "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                    },
                    "outputSchemaName": {
                        "type": "string",
                        "description": "Schema where output metric tables are created\n"
                    },
                    "profileMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/QualityMonitorSchedule:QualityMonitorSchedule",
                        "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                    },
                    "skipBuiltinDashboard": {
                        "type": "boolean",
                        "description": "Whether to skip creating a default dashboard summarizing data quality metrics.  (Can't be updated after creation).\n"
                    },
                    "slicingExprs": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                    },
                    "snapshot": {
                        "$ref": "#/types/databricks:index/QualityMonitorSnapshot:QualityMonitorSnapshot",
                        "description": "Configuration for monitoring snapshot tables.\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of the Monitor\n"
                    },
                    "tableName": {
                        "type": "string",
                        "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                    },
                    "timeSeries": {
                        "$ref": "#/types/databricks:index/QualityMonitorTimeSeries:QualityMonitorTimeSeries",
                        "description": "Configuration for monitoring timeseries tables.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.  (Can't be updated after creation)\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/query:Query": {
            "description": "\n\n## Import\n\nThis resource can be imported using query ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/query:Query this \u003cquery-id\u003e\n```\n\n",
            "properties": {
                "applyAutoLimit": {
                    "type": "boolean",
                    "description": "Whether to apply a 1000 row limit to the query result.\n"
                },
                "catalog": {
                    "type": "string",
                    "description": "Name of the catalog where this query will be executed.\n"
                },
                "createTime": {
                    "type": "string",
                    "description": "The timestamp string indicating when the query was created.\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the query.\n"
                },
                "lastModifierUserName": {
                    "type": "string",
                    "description": "Username of the user who last saved changes to this query.\n"
                },
                "lifecycleState": {
                    "type": "string",
                    "description": "The workspace state of the query. Used for tracking trashed status. (Possible values are `ACTIVE` or `TRASHED`).\n"
                },
                "ownerUserName": {
                    "type": "string",
                    "description": "Query owner's username.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/QueryParameter:QueryParameter"
                    },
                    "description": "Query parameter definition.  Consists of following attributes (one of `*_value` is required):\n"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The path to a workspace folder containing the query. The default is the user's home folder.  If changed, the query will be recreated.\n"
                },
                "queryText": {
                    "type": "string",
                    "description": "Text of SQL query.\n"
                },
                "runAsMode": {
                    "type": "string",
                    "description": "Sets the \"Run as\" role for the object.  Should be one of `OWNER`, `VIEWER`.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "Name of the schema where this query will be executed.\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Tags that will be added to the query.\n"
                },
                "updateTime": {
                    "type": "string",
                    "description": "The timestamp string indicating when the query was updated.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of a SQL warehouse which will be used to execute this query.\n"
                }
            },
            "required": [
                "createTime",
                "displayName",
                "lastModifierUserName",
                "lifecycleState",
                "queryText",
                "updateTime",
                "warehouseId"
            ],
            "inputProperties": {
                "applyAutoLimit": {
                    "type": "boolean",
                    "description": "Whether to apply a 1000 row limit to the query result.\n"
                },
                "catalog": {
                    "type": "string",
                    "description": "Name of the catalog where this query will be executed.\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "Name of the query.\n"
                },
                "ownerUserName": {
                    "type": "string",
                    "description": "Query owner's username.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/QueryParameter:QueryParameter"
                    },
                    "description": "Query parameter definition.  Consists of following attributes (one of `*_value` is required):\n"
                },
                "parentPath": {
                    "type": "string",
                    "description": "The path to a workspace folder containing the query. The default is the user's home folder.  If changed, the query will be recreated.\n",
                    "willReplaceOnChanges": true
                },
                "queryText": {
                    "type": "string",
                    "description": "Text of SQL query.\n"
                },
                "runAsMode": {
                    "type": "string",
                    "description": "Sets the \"Run as\" role for the object.  Should be one of `OWNER`, `VIEWER`.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "Name of the schema where this query will be executed.\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Tags that will be added to the query.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of a SQL warehouse which will be used to execute this query.\n"
                }
            },
            "requiredInputs": [
                "displayName",
                "queryText",
                "warehouseId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Query resources.\n",
                "properties": {
                    "applyAutoLimit": {
                        "type": "boolean",
                        "description": "Whether to apply a 1000 row limit to the query result.\n"
                    },
                    "catalog": {
                        "type": "string",
                        "description": "Name of the catalog where this query will be executed.\n"
                    },
                    "createTime": {
                        "type": "string",
                        "description": "The timestamp string indicating when the query was created.\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "General description that conveys additional information about this query such as usage notes.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Name of the query.\n"
                    },
                    "lastModifierUserName": {
                        "type": "string",
                        "description": "Username of the user who last saved changes to this query.\n"
                    },
                    "lifecycleState": {
                        "type": "string",
                        "description": "The workspace state of the query. Used for tracking trashed status. (Possible values are `ACTIVE` or `TRASHED`).\n"
                    },
                    "ownerUserName": {
                        "type": "string",
                        "description": "Query owner's username.\n"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/QueryParameter:QueryParameter"
                        },
                        "description": "Query parameter definition.  Consists of following attributes (one of `*_value` is required):\n"
                    },
                    "parentPath": {
                        "type": "string",
                        "description": "The path to a workspace folder containing the query. The default is the user's home folder.  If changed, the query will be recreated.\n",
                        "willReplaceOnChanges": true
                    },
                    "queryText": {
                        "type": "string",
                        "description": "Text of SQL query.\n"
                    },
                    "runAsMode": {
                        "type": "string",
                        "description": "Sets the \"Run as\" role for the object.  Should be one of `OWNER`, `VIEWER`.\n"
                    },
                    "schema": {
                        "type": "string",
                        "description": "Name of the schema where this query will be executed.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Tags that will be added to the query.\n"
                    },
                    "updateTime": {
                        "type": "string",
                        "description": "The timestamp string indicating when the query was updated.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "ID of a SQL warehouse which will be used to execute this query.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/recipient:Recipient": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nIn Delta Sharing, a recipient is an entity that receives shares from a provider. In Unity Catalog, a share is a securable object that represents an organization and associates it with a credential or secure sharing identifier that allows that organization to access one or more shares.\n\nAs a data provider (sharer), you can define multiple recipients for any given Unity Catalog metastore, but if you want to share data from multiple metastores with a particular user or group of users, you must define the recipient separately for each metastore. A recipient can have access to multiple shares.\n\nA `databricks.Recipient` is contained within databricks.Metastore and can have permissions to `SELECT` from a list of shares.\n\n## Example Usage\n\n### Databricks Sharing with non databricks recipient\n\nSetting `authentication_type` type to `TOKEN` creates a temporary url to download a credentials file. This is used to\nauthenticate to the sharing server to access data. This is for when the recipient is not using Databricks.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as random from \"@pulumi/random\";\n\nconst db2opensharecode = new random.index.Password(\"db2opensharecode\", {\n    length: 16,\n    special: true,\n});\nconst current = databricks.getCurrentUser({});\nconst db2open = new databricks.Recipient(\"db2open\", {\n    name: current.then(current =\u003e `${current.alphanumeric}-recipient`),\n    comment: \"Made by Pulumi\",\n    authenticationType: \"TOKEN\",\n    sharingCode: db2opensharecode.result,\n    ipAccessList: {\n        allowedIpAddresses: [],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_random as random\n\ndb2opensharecode = random.index.Password(\"db2opensharecode\",\n    length=16,\n    special=True)\ncurrent = databricks.get_current_user()\ndb2open = databricks.Recipient(\"db2open\",\n    name=f\"{current.alphanumeric}-recipient\",\n    comment=\"Made by Pulumi\",\n    authentication_type=\"TOKEN\",\n    sharing_code=db2opensharecode[\"result\"],\n    ip_access_list={\n        \"allowed_ip_addresses\": [],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Random = Pulumi.Random;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var db2opensharecode = new Random.Index.Password(\"db2opensharecode\", new()\n    {\n        Length = 16,\n        Special = true,\n    });\n\n    var current = Databricks.GetCurrentUser.Invoke();\n\n    var db2open = new Databricks.Recipient(\"db2open\", new()\n    {\n        Name = $\"{current.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)}-recipient\",\n        Comment = \"Made by Pulumi\",\n        AuthenticationType = \"TOKEN\",\n        SharingCode = db2opensharecode.Result,\n        IpAccessList = new Databricks.Inputs.RecipientIpAccessListArgs\n        {\n            AllowedIpAddresses = new() { },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-random/sdk/v4/go/random\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdb2opensharecode, err := random.NewPassword(ctx, \"db2opensharecode\", \u0026random.PasswordArgs{\n\t\t\tLength:  16,\n\t\t\tSpecial: true,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcurrent, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewRecipient(ctx, \"db2open\", \u0026databricks.RecipientArgs{\n\t\t\tName:               pulumi.Sprintf(\"%v-recipient\", current.Alphanumeric),\n\t\t\tComment:            pulumi.String(\"Made by Pulumi\"),\n\t\t\tAuthenticationType: pulumi.String(\"TOKEN\"),\n\t\t\tSharingCode:        db2opensharecode.Result,\n\t\t\tIpAccessList: \u0026databricks.RecipientIpAccessListArgs{\n\t\t\t\tAllowedIpAddresses: pulumi.StringArray{},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.random.password;\nimport com.pulumi.random.passwordArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.Recipient;\nimport com.pulumi.databricks.RecipientArgs;\nimport com.pulumi.databricks.inputs.RecipientIpAccessListArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var db2opensharecode = new Password(\"db2opensharecode\", PasswordArgs.builder()\n            .length(16)\n            .special(true)\n            .build());\n\n        final var current = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var db2open = new Recipient(\"db2open\", RecipientArgs.builder()\n            .name(String.format(\"%s-recipient\", current.alphanumeric()))\n            .comment(\"Made by Pulumi\")\n            .authenticationType(\"TOKEN\")\n            .sharingCode(db2opensharecode.result())\n            .ipAccessList(RecipientIpAccessListArgs.builder()\n                .allowedIpAddresses()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  db2opensharecode:\n    type: random:password\n    properties:\n      length: 16\n      special: true\n  db2open:\n    type: databricks:Recipient\n    properties:\n      name: ${current.alphanumeric}-recipient\n      comment: Made by Pulumi\n      authenticationType: TOKEN\n      sharingCode: ${db2opensharecode.result}\n      ipAccessList:\n        allowedIpAddresses: []\nvariables:\n  current:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe recipient resource can be imported using the name of the recipient.\n\nbash\n\n```sh\n$ pulumi import databricks:index/recipient:Recipient this \u003crecipient_name\u003e\n```\n\n",
            "properties": {
                "activated": {
                    "type": "boolean"
                },
                "activationUrl": {
                    "type": "string",
                    "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                },
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n"
                },
                "cloud": {
                    "type": "string",
                    "description": "Cloud vendor of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of recipient creator.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when `authentication_type` is `DATABRICKS`.\n"
                },
                "expirationTime": {
                    "type": "integer",
                    "description": "Expiration timestamp of the token in epoch milliseconds.\n"
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "Recipient IP access list.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of recipient's Unity Catalog metastore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the recipient owner.\n"
                },
                "propertiesKvpairs": {
                    "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                    "description": "Recipient properties - object consisting of following fields:\n"
                },
                "region": {
                    "type": "string",
                    "description": "Cloud region of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of recipient Token updater.\n"
                }
            },
            "required": [
                "activated",
                "activationUrl",
                "authenticationType",
                "cloud",
                "createdAt",
                "createdBy",
                "metastoreId",
                "name",
                "region",
                "tokens",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when `authentication_type` is `DATABRICKS`.\n",
                    "willReplaceOnChanges": true
                },
                "expirationTime": {
                    "type": "integer",
                    "description": "Expiration timestamp of the token in epoch milliseconds.\n"
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "Recipient IP access list.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the recipient owner.\n"
                },
                "propertiesKvpairs": {
                    "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                    "description": "Recipient properties - object consisting of following fields:\n"
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                }
            },
            "requiredInputs": [
                "authenticationType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Recipient resources.\n",
                "properties": {
                    "activated": {
                        "type": "boolean"
                    },
                    "activationUrl": {
                        "type": "string",
                        "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                    },
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string",
                        "description": "Cloud vendor of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the recipient.\n"
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Username of recipient creator.\n"
                    },
                    "dataRecipientGlobalMetastoreId": {
                        "type": "string",
                        "description": "Required when `authentication_type` is `DATABRICKS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "expirationTime": {
                        "type": "integer",
                        "description": "Expiration timestamp of the token in epoch milliseconds.\n"
                    },
                    "ipAccessList": {
                        "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                        "description": "Recipient IP access list.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of recipient's Unity Catalog metastore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of recipient. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the recipient owner.\n"
                    },
                    "propertiesKvpairs": {
                        "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                        "description": "Recipient properties - object consisting of following fields:\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Cloud region of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "sharingCode": {
                        "type": "string",
                        "description": "The one-time sharing code provided by the data recipient.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "tokens": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                        },
                        "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                    },
                    "updatedAt": {
                        "type": "integer",
                        "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                    },
                    "updatedBy": {
                        "type": "string",
                        "description": "Username of recipient Token updater.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/registeredModel:RegisteredModel": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource allows you to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.RegisteredModel(\"this\", {\n    name: \"my_model\",\n    catalogName: \"main\",\n    schemaName: \"default\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.RegisteredModel(\"this\",\n    name=\"my_model\",\n    catalog_name=\"main\",\n    schema_name=\"default\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.RegisteredModel(\"this\", new()\n    {\n        Name = \"my_model\",\n        CatalogName = \"main\",\n        SchemaName = \"default\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewRegisteredModel(ctx, \"this\", \u0026databricks.RegisteredModelArgs{\n\t\t\tName:        pulumi.String(\"my_model\"),\n\t\t\tCatalogName: pulumi.String(\"main\"),\n\t\t\tSchemaName:  pulumi.String(\"default\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.RegisteredModel;\nimport com.pulumi.databricks.RegisteredModelArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegisteredModel(\"this\", RegisteredModelArgs.builder()\n            .name(\"my_model\")\n            .catalogName(\"main\")\n            .schemaName(\"default\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:RegisteredModel\n    properties:\n      name: my_model\n      catalogName: main\n      schemaName: default\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Grants can be used to grant principals `ALL_PRIVILEGES`, `APPLY_TAG`, and `EXECUTE` privileges.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n\n## Import\n\nThe registered model resource can be imported using the full (3-level) name of the model.\n\nbash\n\n```sh\n$ pulumi import databricks:index/registeredModel:RegisteredModel this \u003ccatalog_name.schema_name.model_name\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n"
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the registered model owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n"
                }
            },
            "required": [
                "catalogName",
                "name",
                "owner",
                "schemaName",
                "storageLocation"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the registered model owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering RegisteredModel resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "The comment attached to the registered model.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the registered model owner.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/repo:Repo": {
            "description": "This resource allows you to manage [Databricks Git folders](https://docs.databricks.com/en/repos/index.html) (formerly known as Databricks Repos).\n\n\u003e To create a Git folder from a private repository you need to configure Git token as described in the [documentation](https://docs.databricks.com/en/repos/index.html#configure-your-git-integration-with-databricks).  To set this token you can use databricks.GitCredential resource.\n\n## Example Usage\n\nYou can declare Pulumi-managed Git folder by specifying `url` attribute of Git repository. In addition to that you may need to specify `git_provider` attribute if Git provider doesn't belong to cloud Git providers (Github, GitLab, ...).  If `path` attribute isn't provided, then Git folder will be created in the default location:\n\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst nutterInHome = new databricks.Repo(\"nutter_in_home\", {url: \"https://github.com/user/demo.git\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nnutter_in_home = databricks.Repo(\"nutter_in_home\", url=\"https://github.com/user/demo.git\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var nutterInHome = new Databricks.Repo(\"nutter_in_home\", new()\n    {\n        Url = \"https://github.com/user/demo.git\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewRepo(ctx, \"nutter_in_home\", \u0026databricks.RepoArgs{\n\t\t\tUrl: pulumi.String(\"https://github.com/user/demo.git\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Repo;\nimport com.pulumi.databricks.RepoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var nutterInHome = new Repo(\"nutterInHome\", RepoArgs.builder()\n            .url(\"https://github.com/user/demo.git\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  nutterInHome:\n    type: databricks:Repo\n    name: nutter_in_home\n    properties:\n      url: https://github.com/user/demo.git\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can access repos.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GitCredential to manage Git credentials.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html). \n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n## Import\n\nThe resource can be imported using the Git folder ID (obtained via UI or using API)\n\nbash\n\n```sh\n$ pulumi import databricks:index/repo:Repo this repo_id\n```\n\n",
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created.\n"
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, Git folder is re-created.\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "branch",
                "commitHash",
                "gitProvider",
                "path",
                "url",
                "workspacePath"
            ],
            "inputProperties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created.\n",
                    "willReplaceOnChanges": true
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                    "willReplaceOnChanges": true
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, Git folder is re-created.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Repo resources.\n",
                "properties": {
                    "branch": {
                        "type": "string",
                        "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                    },
                    "commitHash": {
                        "type": "string",
                        "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "path to put the checked out Git folder. If not specified, , then the Git folder will be created in the default location.  If the value changes, Git folder is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "sparseCheckout": {
                        "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                        "willReplaceOnChanges": true
                    },
                    "tag": {
                        "type": "string",
                        "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "The URL of the Git Repository to clone from. If the value changes, Git folder is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/restrictWorkspaceAdminsSetting:RestrictWorkspaceAdminsSetting": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThe `databricks.RestrictWorkspaceAdminsSetting` resource lets you control the capabilities of workspace admins.\n\nWith the status set to `ALLOW_ALL`, workspace admins can:\n\n1. Create service principal personal access tokens on behalf of any service principal in their workspace.\n2. Change a job owner to any user in the workspace.\n3. Change the job run_as setting to any user in their workspace or a service principal on which they have the Service Principal User role.\n\nWith the status set to `RESTRICT_TOKENS_AND_JOB_RUN_AS`, workspace admins can:\n\n1. Only create personal access tokens on behalf of service principals on which they have the Service Principal User role.\n2. Only change a job owner to themselves.\n3. Only change the job run_as setting to themselves a service principal on which they have the Service Principal User role.\n\n\u003e Only account admins can update the setting. And the account admin must be part of the workspace to change the setting status.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.RestrictWorkspaceAdminsSetting(\"this\", {restrictWorkspaceAdmins: {\n    status: \"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.RestrictWorkspaceAdminsSetting(\"this\", restrict_workspace_admins={\n    \"status\": \"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.RestrictWorkspaceAdminsSetting(\"this\", new()\n    {\n        RestrictWorkspaceAdmins = new Databricks.Inputs.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs\n        {\n            Status = \"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewRestrictWorkspaceAdminsSetting(ctx, \"this\", \u0026databricks.RestrictWorkspaceAdminsSettingArgs{\n\t\t\tRestrictWorkspaceAdmins: \u0026databricks.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs{\n\t\t\t\tStatus: pulumi.String(\"RESTRICT_TOKENS_AND_JOB_RUN_AS\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.RestrictWorkspaceAdminsSetting;\nimport com.pulumi.databricks.RestrictWorkspaceAdminsSettingArgs;\nimport com.pulumi.databricks.inputs.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RestrictWorkspaceAdminsSetting(\"this\", RestrictWorkspaceAdminsSettingArgs.builder()\n            .restrictWorkspaceAdmins(RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs.builder()\n                .status(\"RESTRICT_TOKENS_AND_JOB_RUN_AS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:RestrictWorkspaceAdminsSetting\n    properties:\n      restrictWorkspaceAdmins:\n        status: RESTRICT_TOKENS_AND_JOB_RUN_AS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/restrictWorkspaceAdminsSetting:RestrictWorkspaceAdminsSetting this global\n```\n\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "restrictWorkspaceAdmins": {
                    "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "etag",
                "restrictWorkspaceAdmins",
                "settingName"
            ],
            "inputProperties": {
                "etag": {
                    "type": "string"
                },
                "restrictWorkspaceAdmins": {
                    "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "restrictWorkspaceAdmins"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering RestrictWorkspaceAdminsSetting resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "restrictWorkspaceAdmins": {
                        "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                        "description": "The configuration details.\n"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/schema:Schema": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nWithin a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, Databases (also called Schemas), and Tables / Views.\n\nA `databricks.Schema` is contained within databricks.Catalog and can contain tables \u0026 views.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    name=\"things\",\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n\n## Import\n\nThis resource can be imported by its full name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/schema:Schema this \u003ccatalog_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "catalogName",
                "enablePredictiveOptimization",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Schema resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "enablePredictiveOptimization": {
                        "type": "string",
                        "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete schema regardless of its contents.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the schema owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Extensible Schema properties.\n"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secret:Secret": {
            "description": "With this resource you can insert a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret’s value. The server encrypts the secret using the secret scope’s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope. The secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. The maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000. You can read a secret value only from within a command on a cluster (for example, through a notebook); there is no API to read a secret value outside of a cluster. The permission applied is based on who is invoking the command and you must have at least READ permission. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst app = new databricks.SecretScope(\"app\", {name: \"application-secret-scope\"});\nconst publishingApi = new databricks.Secret(\"publishing_api\", {\n    key: \"publishing_api\",\n    stringValue: example.value,\n    scope: app.id,\n});\nconst _this = new databricks.Cluster(\"this\", {sparkConf: {\n    \"fs.azure.account.oauth2.client.secret\": publishingApi.configReference,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp = databricks.SecretScope(\"app\", name=\"application-secret-scope\")\npublishing_api = databricks.Secret(\"publishing_api\",\n    key=\"publishing_api\",\n    string_value=example[\"value\"],\n    scope=app.id)\nthis = databricks.Cluster(\"this\", spark_conf={\n    \"fs.azure.account.oauth2.client.secret\": publishing_api.config_reference,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var app = new Databricks.SecretScope(\"app\", new()\n    {\n        Name = \"application-secret-scope\",\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishing_api\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = example.Value,\n        Scope = app.Id,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        SparkConf = \n        {\n            { \"fs.azure.account.oauth2.client.secret\", publishingApi.ConfigReference },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"application-secret-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpublishingApi, err := databricks.NewSecret(ctx, \"publishing_api\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(example.Value),\n\t\t\tScope:       app.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"fs.azure.account.oauth2.client.secret\": publishingApi.ConfigReference,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var app = new SecretScope(\"app\", SecretScopeArgs.builder()\n            .name(\"application-secret-scope\")\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()\n            .key(\"publishing_api\")\n            .stringValue(example.value())\n            .scope(app.id())\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()\n            .sparkConf(Map.of(\"fs.azure.account.oauth2.client.secret\", publishingApi.configReference()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  app:\n    type: databricks:SecretScope\n    properties:\n      name: application-secret-scope\n  publishingApi:\n    type: databricks:Secret\n    name: publishing_api\n    properties:\n      key: publishing_api\n      stringValue: ${example.value}\n      scope: ${app.id}\n  this:\n    type: databricks:Cluster\n    properties:\n      sparkConf:\n        fs.azure.account.oauth2.client.secret: ${publishingApi.configReference}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe resource secret can be imported using `scopeName|||secretKey` combination. **This may change in future versions.**\n\nbash\n\n```sh\n$ pulumi import databricks:index/secret:Secret app `scopeName|||secretKey`\n```\n\n",
            "properties": {
                "configReference": {
                    "type": "string",
                    "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                },
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "(Integer) time secret was updated\n"
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true
                }
            },
            "required": [
                "configReference",
                "key",
                "lastUpdatedTimestamp",
                "scope",
                "stringValue"
            ],
            "inputProperties": {
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "key",
                "scope",
                "stringValue"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Secret resources.\n",
                "properties": {
                    "configReference": {
                        "type": "string",
                        "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                    },
                    "key": {
                        "type": "string",
                        "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "(Integer) time secret was updated\n"
                    },
                    "scope": {
                        "type": "string",
                        "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "stringValue": {
                        "type": "string",
                        "description": "(String) super secret sensitive value.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretAcl:SecretAcl": {
            "description": "Create or overwrite the ACL associated with the given principal (user or group) on the specified databricks_secret_scope. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\nThis way, data scientists can read the Publishing API key that is synchronized from, for example, Azure Key Vault.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ds = new databricks.Group(\"ds\", {displayName: \"data-scientists\"});\nconst app = new databricks.SecretScope(\"app\", {name: \"app-secret-scope\"});\nconst mySecretAcl = new databricks.SecretAcl(\"my_secret_acl\", {\n    principal: ds.displayName,\n    permission: \"READ\",\n    scope: app.name,\n});\nconst publishingApi = new databricks.Secret(\"publishing_api\", {\n    key: \"publishing_api\",\n    stringValue: example.value,\n    scope: app.name,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nds = databricks.Group(\"ds\", display_name=\"data-scientists\")\napp = databricks.SecretScope(\"app\", name=\"app-secret-scope\")\nmy_secret_acl = databricks.SecretAcl(\"my_secret_acl\",\n    principal=ds.display_name,\n    permission=\"READ\",\n    scope=app.name)\npublishing_api = databricks.Secret(\"publishing_api\",\n    key=\"publishing_api\",\n    string_value=example[\"value\"],\n    scope=app.name)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"data-scientists\",\n    });\n\n    var app = new Databricks.SecretScope(\"app\", new()\n    {\n        Name = \"app-secret-scope\",\n    });\n\n    var mySecretAcl = new Databricks.SecretAcl(\"my_secret_acl\", new()\n    {\n        Principal = ds.DisplayName,\n        Permission = \"READ\",\n        Scope = app.Name,\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishing_api\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = example.Value,\n        Scope = app.Name,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"data-scientists\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"app-secret-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecretAcl(ctx, \"my_secret_acl\", \u0026databricks.SecretAclArgs{\n\t\t\tPrincipal:  ds.DisplayName,\n\t\t\tPermission: pulumi.String(\"READ\"),\n\t\t\tScope:      app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecret(ctx, \"publishing_api\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(example.Value),\n\t\t\tScope:       app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.SecretAcl;\nimport com.pulumi.databricks.SecretAclArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ds = new Group(\"ds\", GroupArgs.builder()\n            .displayName(\"data-scientists\")\n            .build());\n\n        var app = new SecretScope(\"app\", SecretScopeArgs.builder()\n            .name(\"app-secret-scope\")\n            .build());\n\n        var mySecretAcl = new SecretAcl(\"mySecretAcl\", SecretAclArgs.builder()\n            .principal(ds.displayName())\n            .permission(\"READ\")\n            .scope(app.name())\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()\n            .key(\"publishing_api\")\n            .stringValue(example.value())\n            .scope(app.name())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: data-scientists\n  app:\n    type: databricks:SecretScope\n    properties:\n      name: app-secret-scope\n  mySecretAcl:\n    type: databricks:SecretAcl\n    name: my_secret_acl\n    properties:\n      principal: ${ds.displayName}\n      permission: READ\n      scope: ${app.name}\n  publishingApi:\n    type: databricks:Secret\n    name: publishing_api\n    properties:\n      key: publishing_api\n      stringValue: ${example.value}\n      scope: ${app.name}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe resource secret acl can be imported using `scopeName|||principalName` combination.\n\nbash\n\n```sh\n$ pulumi import databricks:index/secretAcl:SecretAcl object `scopeName|||principalName`\n```\n\n",
            "properties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n"
                },
                "principal": {
                    "type": "string",
                    "description": "principal's identifier. It can be:\n"
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n"
                }
            },
            "required": [
                "permission",
                "principal",
                "scope"
            ],
            "inputProperties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "description": "principal's identifier. It can be:\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permission",
                "principal",
                "scope"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretAcl resources.\n",
                "properties": {
                    "permission": {
                        "type": "string",
                        "description": "`READ`, `WRITE` or `MANAGE`.\n",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "description": "principal's identifier. It can be:\n",
                        "willReplaceOnChanges": true
                    },
                    "scope": {
                        "type": "string",
                        "description": "name of the scope\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretScope:SecretScope": {
            "description": "Sometimes accessing data requires that you authenticate to external data sources through JDBC. Instead of directly entering your credentials into a notebook, use Databricks secrets to store your credentials and reference them in notebooks and jobs. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SecretScope(\"this\", {name: \"terraform-demo-scope\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SecretScope(\"this\", name=\"terraform-demo-scope\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SecretScope(\"this\", new()\n    {\n        Name = \"terraform-demo-scope\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSecretScope(ctx, \"this\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"terraform-demo-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SecretScope(\"this\", SecretScopeArgs.builder()\n            .name(\"terraform-demo-scope\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SecretScope\n    properties:\n      name: terraform-demo-scope\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe secret resource scope can be imported using the scope name. `initial_manage_principal` state won't be imported, because the underlying API doesn't include it in the response.\n\nbash\n\n```sh\n$ pulumi import databricks:index/secretScope:SecretScope object \u003cscopeName\u003e\n```\n\n",
            "properties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n"
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata"
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                }
            },
            "required": [
                "backendType",
                "name"
            ],
            "inputProperties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                    "willReplaceOnChanges": true
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretScope resources.\n",
                "properties": {
                    "backendType": {
                        "type": "string",
                        "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                    },
                    "initialManagePrincipal": {
                        "type": "string",
                        "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                        "willReplaceOnChanges": true
                    },
                    "keyvaultMetadata": {
                        "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipal:ServicePrincipal": {
            "description": "Directly manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) that could be added to databricks.Group in Databricks account or workspace.\n\nThere are different types of service principals:\n\n* Databricks-managed - exists only inside the Databricks platform (all clouds) and couldn't be used for accessing non-Databricks services.\n* Azure-managed - existing Azure service principal (enterprise application) is registered inside Databricks.  It could be used to work with other Azure services.\n\n\u003e To assign account level service principals to workspace use databricks_mws_permission_assignment.\n\n\u003e Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level service principals. Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level service principals.\n\nTo create service principals in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using the supported authentication method for account operations.\n\nThe default behavior when deleting a `databricks.ServicePrincipal` resource depends on whether the provider is configured at the workspace-level or account-level. When the provider is configured at the workspace-level, the service principal will be deleted from the workspace. When the provider is configured at the account-level, the service principal will be deactivated but not deleted. When the provider is configured at the account level, to delete the service principal from the account when the resource is deleted, set `disable_as_user_deletion = false`. Conversely, when the provider is configured at the account-level, to deactivate the service principal when the resource is deleted, set `disable_as_user_deletion = true`.\n\n## Example Usage\n\nCreating regular Databricks-managed service principal:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Admin SP\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Admin SP\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Admin SP\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Admin SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .displayName(\"Admin SP\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Admin SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating service principal with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Admin SP\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: sp.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Admin SP\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=sp.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Admin SP\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = sp.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Admin SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: sp.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .displayName(\"Admin SP\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(sp.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Admin SP\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${sp.id}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Azure-managed service principal with cluster create permissions:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {\n    applicationId: \"00000000-0000-0000-0000-000000000000\",\n    displayName: \"Example service principal\",\n    allowClusterCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\",\n    application_id=\"00000000-0000-0000-0000-000000000000\",\n    display_name=\"Example service principal\",\n    allow_cluster_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n        DisplayName = \"Example service principal\",\n        AllowClusterCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId:      pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tDisplayName:        pulumi.String(\"Example service principal\"),\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .displayName(\"Example service principal\")\n            .allowClusterCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n      displayName: Example service principal\n      allowClusterCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Databricks-managed service principal in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .displayName(\"Automation-only SP\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Azure-managed service principal in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {applicationId: \"00000000-0000-0000-0000-000000000000\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", application_id=\"00000000-0000-0000-0000-000000000000\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()\n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n- End to end workspace management guide.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and more to manage secrets for the service principal (only for AWS deployments)\n\n## Import\n\nThe resource scim service principal can be imported using its id, for example `2345678901234567`. To get the service principal ID, call [Get service principals](https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html#get-service-principals).\n\nbash\n\n```sh\n$ pulumi import databricks:index/servicePrincipal:ServicePrincipal me \u003cservice-principal-id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create service principal: Service principal with application ID X already exists` errors and implicitly import the specified service principal into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "aclPrincipalId",
                "applicationId",
                "displayName",
                "home",
                "repos"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n",
                    "willReplaceOnChanges": true
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create service principal: Service principal with application ID X already exists` errors and implicitly import the specified service principal into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipal resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                    },
                    "disableAsUserDeletion": {
                        "type": "boolean",
                        "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean",
                        "description": "Ignore `cannot create service principal: Service principal with application ID X already exists` errors and implicitly import the specified service principal into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                    },
                    "forceDeleteHomeDir": {
                        "type": "boolean",
                        "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "forceDeleteRepos": {
                        "type": "boolean",
                        "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalRole:ServicePrincipalRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to a databricks_service_principal.\n\n## Example Usage\n\nGranting a service principal access to an instance profile\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"My Service Principal\"});\nconst myServicePrincipalInstanceProfile = new databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\", {\n    servicePrincipalId: _this.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nthis = databricks.ServicePrincipal(\"this\", display_name=\"My Service Principal\")\nmy_service_principal_instance_profile = databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\",\n    service_principal_id=this.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"My Service Principal\",\n    });\n\n    var myServicePrincipalInstanceProfile = new Databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\", new()\n    {\n        ServicePrincipalId = @this.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"My Service Principal\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipalRole(ctx, \"my_service_principal_instance_profile\", \u0026databricks.ServicePrincipalRoleArgs{\n\t\t\tServicePrincipalId: this.ID(),\n\t\t\tRole:               instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.ServicePrincipalRole;\nimport com.pulumi.databricks.ServicePrincipalRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()\n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()\n            .displayName(\"My Service Principal\")\n            .build());\n\n        var myServicePrincipalInstanceProfile = new ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", ServicePrincipalRoleArgs.builder()\n            .servicePrincipalId(this_.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: My Service Principal\n  myServicePrincipalInstanceProfile:\n    type: databricks:ServicePrincipalRole\n    name: my_service_principal_instance_profile\n    properties:\n      servicePrincipalId: ${this.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.UserRole to attach role or databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n"
                }
            },
            "required": [
                "role",
                "servicePrincipalId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "This is the id of the role or instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "This is the id of the service principal resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalSecret:ServicePrincipalSecret": {
            "description": "\u003e This resource can only be used with an account-level provider.\n\nWith this resource you can create a secret for a given [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html).\n\nThis secret can be used to configure the Databricks Pulumi Provider to authenticate with the service principal. See Authenticating with service principal.\n\nAdditionally, the secret can be used to request OAuth tokens for the service principal, which can be used to authenticate to Databricks REST APIs. See [Authentication using OAuth tokens for service principals](https://docs.databricks.com/dev-tools/authentication-oauth.html).\n\n## Example Usage\n\nCreate service principal secret\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst terraformSp = new databricks.ServicePrincipalSecret(\"terraform_sp\", {servicePrincipalId: _this.id});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nterraform_sp = databricks.ServicePrincipalSecret(\"terraform_sp\", service_principal_id=this[\"id\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var terraformSp = new Databricks.ServicePrincipalSecret(\"terraform_sp\", new()\n    {\n        ServicePrincipalId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipalSecret(ctx, \"terraform_sp\", \u0026databricks.ServicePrincipalSecretArgs{\n\t\t\tServicePrincipalId: pulumi.Any(this.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipalSecret;\nimport com.pulumi.databricks.ServicePrincipalSecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var terraformSp = new ServicePrincipalSecret(\"terraformSp\", ServicePrincipalSecretArgs.builder()\n            .servicePrincipalId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  terraformSp:\n    type: databricks:ServicePrincipalSecret\n    name: terraform_sp\n    properties:\n      servicePrincipalId: ${this.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.ServicePrincipal to manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) in Databricks\n",
            "properties": {
                "secret": {
                    "type": "string",
                    "description": "Generated secret for the service principal\n",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "ID of the databricks.ServicePrincipal (not application ID).\n"
                },
                "status": {
                    "type": "string"
                }
            },
            "required": [
                "secret",
                "servicePrincipalId",
                "status"
            ],
            "inputProperties": {
                "secret": {
                    "type": "string",
                    "description": "Generated secret for the service principal\n",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "ID of the databricks.ServicePrincipal (not application ID).\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalSecret resources.\n",
                "properties": {
                    "secret": {
                        "type": "string",
                        "description": "Generated secret for the service principal\n",
                        "secret": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "ID of the databricks.ServicePrincipal (not application ID).\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/share:Share": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nIn Delta Sharing, a share is a read-only collection of tables and table partitions that a provider wants to share with one or more recipients. If your recipient uses a Unity Catalog-enabled Databricks workspace, you can also include notebook files, views (including dynamic views that restrict access at the row and column level), Unity Catalog volumes, and Unity Catalog models in a share.\n\nIn a Unity Catalog-enabled Databricks workspace, a share is a securable object registered in Unity Catalog. A `databricks.Share` is contained within a databricks_metastore. If you remove a share from your Unity Catalog metastore, all recipients of that share lose the ability to access it.\n\n## Example Usage\n\n\u003e In Pulumi configuration, it is recommended to define objects in alphabetical order of their `name` arguments, so that you get consistent and readable diff. Whenever objects are added or removed, or `name` is renamed, you'll observe a change in the majority of tasks. It's related to the fact that the current version of the provider treats `object` blocks as an ordered list. Alternatively, `object` block could have been an unordered set, though end-users would see the entire block replaced upon a change in single property of the task.\n\nCreating a Delta Sharing share and add some existing tables to it\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst things = databricks.getTables({\n    catalogName: \"sandbox\",\n    schemaName: \"things\",\n});\nconst some = new databricks.Share(\"some\", {\n    objects: .map(entry =\u003e ({\n        name: entry.value,\n        dataObjectType: \"TABLE\",\n    })),\n    name: \"my_share\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_tables(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nsome = databricks.Share(\"some\",\n    objects=[{\n        \"name\": entry[\"value\"],\n        \"data_object_type\": \"TABLE\",\n    } for entry in [{\"key\": k, \"value\": v} for k, v in things.ids]],\n    name=\"my_share\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var things = Databricks.GetTables.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var some = new Databricks.Share(\"some\", new()\n    {\n        Objects = ,\n        Name = \"my_share\",\n    });\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating a Delta Sharing share and add a schema to it(including all current and future tables).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst schemaShare = new databricks.Share(\"schema_share\", {\n    name: \"schema_share\",\n    objects: [{\n        name: \"catalog_name.schema_name\",\n        dataObjectType: \"SCHEMA\",\n        historyDataSharingStatus: \"ENABLED\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nschema_share = databricks.Share(\"schema_share\",\n    name=\"schema_share\",\n    objects=[{\n        \"name\": \"catalog_name.schema_name\",\n        \"data_object_type\": \"SCHEMA\",\n        \"history_data_sharing_status\": \"ENABLED\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var schemaShare = new Databricks.Share(\"schema_share\", new()\n    {\n        Name = \"schema_share\",\n        Objects = new[]\n        {\n            new Databricks.Inputs.ShareObjectArgs\n            {\n                Name = \"catalog_name.schema_name\",\n                DataObjectType = \"SCHEMA\",\n                HistoryDataSharingStatus = \"ENABLED\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewShare(ctx, \"schema_share\", \u0026databricks.ShareArgs{\n\t\t\tName: pulumi.String(\"schema_share\"),\n\t\t\tObjects: databricks.ShareObjectArray{\n\t\t\t\t\u0026databricks.ShareObjectArgs{\n\t\t\t\t\tName:                     pulumi.String(\"catalog_name.schema_name\"),\n\t\t\t\t\tDataObjectType:           pulumi.String(\"SCHEMA\"),\n\t\t\t\t\tHistoryDataSharingStatus: pulumi.String(\"ENABLED\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Share;\nimport com.pulumi.databricks.ShareArgs;\nimport com.pulumi.databricks.inputs.ShareObjectArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var schemaShare = new Share(\"schemaShare\", ShareArgs.builder()\n            .name(\"schema_share\")\n            .objects(ShareObjectArgs.builder()\n                .name(\"catalog_name.schema_name\")\n                .dataObjectType(\"SCHEMA\")\n                .historyDataSharingStatus(\"ENABLED\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  schemaShare:\n    type: databricks:Share\n    name: schema_share\n    properties:\n      name: schema_share\n      objects:\n        - name: catalog_name.schema_name\n          dataObjectType: SCHEMA\n          historyDataSharingStatus: ENABLED\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating a Delta Sharing share and share a table with partitions spec and history\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst some = new databricks.Share(\"some\", {\n    name: \"my_share\",\n    objects: [{\n        name: \"my_catalog.my_schema.my_table\",\n        dataObjectType: \"TABLE\",\n        historyDataSharingStatus: \"ENABLED\",\n        partitions: [\n            {\n                values: [\n                    {\n                        name: \"year\",\n                        op: \"EQUAL\",\n                        value: \"2009\",\n                    },\n                    {\n                        name: \"month\",\n                        op: \"EQUAL\",\n                        value: \"12\",\n                    },\n                ],\n            },\n            {\n                values: [{\n                    name: \"year\",\n                    op: \"EQUAL\",\n                    value: \"2010\",\n                }],\n            },\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsome = databricks.Share(\"some\",\n    name=\"my_share\",\n    objects=[{\n        \"name\": \"my_catalog.my_schema.my_table\",\n        \"data_object_type\": \"TABLE\",\n        \"history_data_sharing_status\": \"ENABLED\",\n        \"partitions\": [\n            {\n                \"values\": [\n                    {\n                        \"name\": \"year\",\n                        \"op\": \"EQUAL\",\n                        \"value\": \"2009\",\n                    },\n                    {\n                        \"name\": \"month\",\n                        \"op\": \"EQUAL\",\n                        \"value\": \"12\",\n                    },\n                ],\n            },\n            {\n                \"values\": [{\n                    \"name\": \"year\",\n                    \"op\": \"EQUAL\",\n                    \"value\": \"2010\",\n                }],\n            },\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var some = new Databricks.Share(\"some\", new()\n    {\n        Name = \"my_share\",\n        Objects = new[]\n        {\n            new Databricks.Inputs.ShareObjectArgs\n            {\n                Name = \"my_catalog.my_schema.my_table\",\n                DataObjectType = \"TABLE\",\n                HistoryDataSharingStatus = \"ENABLED\",\n                Partitions = new[]\n                {\n                    new Databricks.Inputs.ShareObjectPartitionArgs\n                    {\n                        Values = new[]\n                        {\n                            new Databricks.Inputs.ShareObjectPartitionValueArgs\n                            {\n                                Name = \"year\",\n                                Op = \"EQUAL\",\n                                Value = \"2009\",\n                            },\n                            new Databricks.Inputs.ShareObjectPartitionValueArgs\n                            {\n                                Name = \"month\",\n                                Op = \"EQUAL\",\n                                Value = \"12\",\n                            },\n                        },\n                    },\n                    new Databricks.Inputs.ShareObjectPartitionArgs\n                    {\n                        Values = new[]\n                        {\n                            new Databricks.Inputs.ShareObjectPartitionValueArgs\n                            {\n                                Name = \"year\",\n                                Op = \"EQUAL\",\n                                Value = \"2010\",\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewShare(ctx, \"some\", \u0026databricks.ShareArgs{\n\t\t\tName: pulumi.String(\"my_share\"),\n\t\t\tObjects: databricks.ShareObjectArray{\n\t\t\t\t\u0026databricks.ShareObjectArgs{\n\t\t\t\t\tName:                     pulumi.String(\"my_catalog.my_schema.my_table\"),\n\t\t\t\t\tDataObjectType:           pulumi.String(\"TABLE\"),\n\t\t\t\t\tHistoryDataSharingStatus: pulumi.String(\"ENABLED\"),\n\t\t\t\t\tPartitions: databricks.ShareObjectPartitionArray{\n\t\t\t\t\t\t\u0026databricks.ShareObjectPartitionArgs{\n\t\t\t\t\t\t\tValues: databricks.ShareObjectPartitionValueArray{\n\t\t\t\t\t\t\t\t\u0026databricks.ShareObjectPartitionValueArgs{\n\t\t\t\t\t\t\t\t\tName:  pulumi.String(\"year\"),\n\t\t\t\t\t\t\t\t\tOp:    pulumi.String(\"EQUAL\"),\n\t\t\t\t\t\t\t\t\tValue: pulumi.String(\"2009\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t\t\u0026databricks.ShareObjectPartitionValueArgs{\n\t\t\t\t\t\t\t\t\tName:  pulumi.String(\"month\"),\n\t\t\t\t\t\t\t\t\tOp:    pulumi.String(\"EQUAL\"),\n\t\t\t\t\t\t\t\t\tValue: pulumi.String(\"12\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\u0026databricks.ShareObjectPartitionArgs{\n\t\t\t\t\t\t\tValues: databricks.ShareObjectPartitionValueArray{\n\t\t\t\t\t\t\t\t\u0026databricks.ShareObjectPartitionValueArgs{\n\t\t\t\t\t\t\t\t\tName:  pulumi.String(\"year\"),\n\t\t\t\t\t\t\t\t\tOp:    pulumi.String(\"EQUAL\"),\n\t\t\t\t\t\t\t\t\tValue: pulumi.String(\"2010\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Share;\nimport com.pulumi.databricks.ShareArgs;\nimport com.pulumi.databricks.inputs.ShareObjectArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var some = new Share(\"some\", ShareArgs.builder()\n            .name(\"my_share\")\n            .objects(ShareObjectArgs.builder()\n                .name(\"my_catalog.my_schema.my_table\")\n                .dataObjectType(\"TABLE\")\n                .historyDataSharingStatus(\"ENABLED\")\n                .partitions(                \n                    ShareObjectPartitionArgs.builder()\n                        .values(                        \n                            ShareObjectPartitionValueArgs.builder()\n                                .name(\"year\")\n                                .op(\"EQUAL\")\n                                .value(\"2009\")\n                                .build(),\n                            ShareObjectPartitionValueArgs.builder()\n                                .name(\"month\")\n                                .op(\"EQUAL\")\n                                .value(\"12\")\n                                .build())\n                        .build(),\n                    ShareObjectPartitionArgs.builder()\n                        .values(ShareObjectPartitionValueArgs.builder()\n                            .name(\"year\")\n                            .op(\"EQUAL\")\n                            .value(\"2010\")\n                            .build())\n                        .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  some:\n    type: databricks:Share\n    properties:\n      name: my_share\n      objects:\n        - name: my_catalog.my_schema.my_table\n          dataObjectType: TABLE\n          historyDataSharingStatus: ENABLED\n          partitions:\n            - values:\n                - name: year\n                  op: EQUAL\n                  value: '2009'\n                - name: month\n                  op: EQUAL\n                  value: '12'\n            - values:\n                - name: year\n                  op: EQUAL\n                  value: '2010'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n* databricks.getShares to read existing Delta Sharing shares.\n\n## Import\n\nThe share resource can be imported using the name of the share.\n\nbash\n\n```sh\n$ pulumi import databricks:index/share:Share this \u003cshare_name\u003e\n```\n\n",
            "properties": {
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n"
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the share owner.\n"
                },
                "storageLocation": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "createdBy",
                "name",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the share owner.\n"
                },
                "storageLocation": {
                    "type": "string"
                },
                "storageRoot": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Share resources.\n",
                "properties": {
                    "comment": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of share. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                        }
                    },
                    "owner": {
                        "type": "string",
                        "description": "User name/group name/sp application_id of the share owner.\n"
                    },
                    "storageLocation": {
                        "type": "string"
                    },
                    "storageRoot": {
                        "type": "string"
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlAlert:SqlAlert": {
            "description": "This resource allows you to manage [Databricks SQL Alerts](https://docs.databricks.com/sql/user/queries/index.html).\n\n\u003e To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Queries\"});\nconst _this = new databricks.SqlQuery(\"this\", {\n    dataSourceId: example.dataSourceId,\n    name: \"My Query Name\",\n    query: \"SELECT 1 AS p1, 2 as p2\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n});\nconst alert = new databricks.SqlAlert(\"alert\", {\n    queryId: _this.id,\n    name: \"My Alert\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    rearm: 1,\n    options: {\n        column: \"p1\",\n        op: \"==\",\n        value: \"2\",\n        muted: false,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Queries\")\nthis = databricks.SqlQuery(\"this\",\n    data_source_id=example[\"dataSourceId\"],\n    name=\"My Query Name\",\n    query=\"SELECT 1 AS p1, 2 as p2\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"))\nalert = databricks.SqlAlert(\"alert\",\n    query_id=this.id,\n    name=\"My Alert\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    rearm=1,\n    options={\n        \"column\": \"p1\",\n        \"op\": \"==\",\n        \"value\": \"2\",\n        \"muted\": False,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Queries\",\n    });\n\n    var @this = new Databricks.SqlQuery(\"this\", new()\n    {\n        DataSourceId = example.DataSourceId,\n        Name = \"My Query Name\",\n        Query = \"SELECT 1 AS p1, 2 as p2\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n    });\n\n    var alert = new Databricks.SqlAlert(\"alert\", new()\n    {\n        QueryId = @this.Id,\n        Name = \"My Alert\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        Rearm = 1,\n        Options = new Databricks.Inputs.SqlAlertOptionsArgs\n        {\n            Column = \"p1\",\n            Op = \"==\",\n            Value = \"2\",\n            Muted = false,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Queries\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewSqlQuery(ctx, \"this\", \u0026databricks.SqlQueryArgs{\n\t\t\tDataSourceId: pulumi.Any(example.DataSourceId),\n\t\t\tName:         pulumi.String(\"My Query Name\"),\n\t\t\tQuery:        pulumi.String(\"SELECT 1 AS p1, 2 as p2\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlAlert(ctx, \"alert\", \u0026databricks.SqlAlertArgs{\n\t\t\tQueryId: this.ID(),\n\t\t\tName:    pulumi.String(\"My Alert\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tRearm: pulumi.Int(1),\n\t\t\tOptions: \u0026databricks.SqlAlertOptionsArgs{\n\t\t\t\tColumn: pulumi.String(\"p1\"),\n\t\t\t\tOp:     pulumi.String(\"==\"),\n\t\t\t\tValue:  pulumi.String(\"2\"),\n\t\t\t\tMuted:  pulumi.Bool(false),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlQuery;\nimport com.pulumi.databricks.SqlQueryArgs;\nimport com.pulumi.databricks.SqlAlert;\nimport com.pulumi.databricks.SqlAlertArgs;\nimport com.pulumi.databricks.inputs.SqlAlertOptionsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()\n            .path(\"/Shared/Queries\")\n            .build());\n\n        var this_ = new SqlQuery(\"this\", SqlQueryArgs.builder()\n            .dataSourceId(example.dataSourceId())\n            .name(\"My Query Name\")\n            .query(\"SELECT 1 AS p1, 2 as p2\")\n            .parent(sharedDir.objectId().applyValue(_objectId -\u003e String.format(\"folders/%s\", _objectId)))\n            .build());\n\n        var alert = new SqlAlert(\"alert\", SqlAlertArgs.builder()\n            .queryId(this_.id())\n            .name(\"My Alert\")\n            .parent(sharedDir.objectId().applyValue(_objectId -\u003e String.format(\"folders/%s\", _objectId)))\n            .rearm(1)\n            .options(SqlAlertOptionsArgs.builder()\n                .column(\"p1\")\n                .op(\"==\")\n                .value(\"2\")\n                .muted(false)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Queries\n  this:\n    type: databricks:SqlQuery\n    properties:\n      dataSourceId: ${example.dataSourceId}\n      name: My Query Name\n      query: SELECT 1 AS p1, 2 as p2\n      parent: folders/${sharedDir.objectId}\n  alert:\n    type: databricks:SqlAlert\n    properties:\n      queryId: ${this.id}\n      name: My Alert\n      parent: folders/${sharedDir.objectId}\n      rearm: 1\n      options:\n        column: p1\n        op: ==\n        value: '2'\n        muted: false\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\ndatabricks.Permissions can control which groups or individual users can *Manage*, *Edit*, *Run* or *View* individual alerts.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlQuery to manage Databricks SQL [Queries](https://docs.databricks.com/sql/user/queries/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n\n## Import\n\nThis resource can be imported using alert ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlAlert:SqlAlert this \u003calert-id\u003e\n```\n\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "options": {
                    "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                    "description": "Alert configuration options.\n"
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n"
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "rearm": {
                    "type": "integer",
                    "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "name",
                "options",
                "queryId",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "options": {
                    "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                    "description": "Alert configuration options.\n"
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n",
                    "willReplaceOnChanges": true
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "rearm": {
                    "type": "integer",
                    "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "options",
                "queryId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlAlert resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the alert.\n"
                    },
                    "options": {
                        "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                        "description": "Alert configuration options.\n"
                    },
                    "parent": {
                        "type": "string",
                        "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n",
                        "willReplaceOnChanges": true
                    },
                    "queryId": {
                        "type": "string",
                        "description": "ID of the query evaluated by the alert.\n"
                    },
                    "rearm": {
                        "type": "integer",
                        "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlDashboard:SqlDashboard": {
            "description": "\u003e Please switch to databricks.Dashboard to author new AI/BI dashboards using the latest tooling.\n\nThis resource is used to manage [Legacy dashboards](https://docs.databricks.com/sql/user/dashboards/index.html). To manage [SQL resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n\n\u003e documentation for this resource is a work in progress.\n\nA dashboard may have one or more widgets.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Dashboards\"});\nconst d1 = new databricks.SqlDashboard(\"d1\", {\n    name: \"My Dashboard Name\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    tags: [\n        \"some-tag\",\n        \"another-tag\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Dashboards\")\nd1 = databricks.SqlDashboard(\"d1\",\n    name=\"My Dashboard Name\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    tags=[\n        \"some-tag\",\n        \"another-tag\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Dashboards\",\n    });\n\n    var d1 = new Databricks.SqlDashboard(\"d1\", new()\n    {\n        Name = \"My Dashboard Name\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        Tags = new[]\n        {\n            \"some-tag\",\n            \"another-tag\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Dashboards\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlDashboard(ctx, \"d1\", \u0026databricks.SqlDashboardArgs{\n\t\t\tName: pulumi.String(\"My Dashboard Name\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"some-tag\"),\n\t\t\t\tpulumi.String(\"another-tag\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlDashboard;\nimport com.pulumi.databricks.SqlDashboardArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()\n            .path(\"/Shared/Dashboards\")\n            .build());\n\n        var d1 = new SqlDashboard(\"d1\", SqlDashboardArgs.builder()\n            .name(\"My Dashboard Name\")\n            .parent(sharedDir.objectId().applyValue(_objectId -\u003e String.format(\"folders/%s\", _objectId)))\n            .tags(            \n                \"some-tag\",\n                \"another-tag\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Dashboards\n  d1:\n    type: databricks:SqlDashboard\n    properties:\n      name: My Dashboard Name\n      parent: folders/${sharedDir.objectId}\n      tags:\n        - some-tag\n        - another-tag\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nExample permission to share dashboard with all users:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1 = new databricks.Permissions(\"d1\", {\n    sqlDashboardId: d1DatabricksSqlDashboard.id,\n    accessControls: [{\n        groupName: users.displayName,\n        permissionLevel: \"CAN_RUN\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1 = databricks.Permissions(\"d1\",\n    sql_dashboard_id=d1_databricks_sql_dashboard[\"id\"],\n    access_controls=[{\n        \"group_name\": users[\"displayName\"],\n        \"permission_level\": \"CAN_RUN\",\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1 = new Databricks.Permissions(\"d1\", new()\n    {\n        SqlDashboardId = d1DatabricksSqlDashboard.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = users.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"d1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlDashboardId: pulumi.Any(d1DatabricksSqlDashboard.Id),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(users.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1 = new Permissions(\"d1\", PermissionsArgs.builder()\n            .sqlDashboardId(d1DatabricksSqlDashboard.id())\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .groupName(users.displayName())\n                .permissionLevel(\"CAN_RUN\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1:\n    type: databricks:Permissions\n    properties:\n      sqlDashboardId: ${d1DatabricksSqlDashboard.id}\n      accessControls:\n        - groupName: ${users.displayName}\n          permissionLevel: CAN_RUN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_dashboard` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlDashboard:SqlDashboard this \u003cdashboard-id\u003e\n```\n\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "dashboardFiltersEnabled": {
                    "type": "boolean"
                },
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "name",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "dashboardFiltersEnabled": {
                    "type": "boolean"
                },
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "runAsRole": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlDashboard resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "dashboardFiltersEnabled": {
                        "type": "boolean"
                    },
                    "name": {
                        "type": "string"
                    },
                    "parent": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "runAsRole": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlEndpoint:SqlEndpoint": {
            "description": "This resource is used to manage [Databricks SQL warehouses](https://docs.databricks.com/sql/admin/sql-endpoints.html). To create [SQL warehouses](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.SqlEndpoint(\"this\", {\n    name: me.then(me =\u003e `Endpoint of ${me.alphanumeric}`),\n    clusterSize: \"Small\",\n    maxNumClusters: 1,\n    tags: {\n        customTags: [{\n            key: \"City\",\n            value: \"Amsterdam\",\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.SqlEndpoint(\"this\",\n    name=f\"Endpoint of {me.alphanumeric}\",\n    cluster_size=\"Small\",\n    max_num_clusters=1,\n    tags={\n        \"custom_tags\": [{\n            \"key\": \"City\",\n            \"value\": \"Amsterdam\",\n        }],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.SqlEndpoint(\"this\", new()\n    {\n        Name = $\"Endpoint of {me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)}\",\n        ClusterSize = \"Small\",\n        MaxNumClusters = 1,\n        Tags = new Databricks.Inputs.SqlEndpointTagsArgs\n        {\n            CustomTags = new[]\n            {\n                new Databricks.Inputs.SqlEndpointTagsCustomTagArgs\n                {\n                    Key = \"City\",\n                    Value = \"Amsterdam\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlEndpoint(ctx, \"this\", \u0026databricks.SqlEndpointArgs{\n\t\t\tName:           pulumi.Sprintf(\"Endpoint of %v\", me.Alphanumeric),\n\t\t\tClusterSize:    pulumi.String(\"Small\"),\n\t\t\tMaxNumClusters: pulumi.Int(1),\n\t\t\tTags: \u0026databricks.SqlEndpointTagsArgs{\n\t\t\t\tCustomTags: databricks.SqlEndpointTagsCustomTagArray{\n\t\t\t\t\t\u0026databricks.SqlEndpointTagsCustomTagArgs{\n\t\t\t\t\t\tKey:   pulumi.String(\"City\"),\n\t\t\t\t\t\tValue: pulumi.String(\"Amsterdam\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.SqlEndpoint;\nimport com.pulumi.databricks.SqlEndpointArgs;\nimport com.pulumi.databricks.inputs.SqlEndpointTagsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var this_ = new SqlEndpoint(\"this\", SqlEndpointArgs.builder()\n            .name(String.format(\"Endpoint of %s\", me.alphanumeric()))\n            .clusterSize(\"Small\")\n            .maxNumClusters(1)\n            .tags(SqlEndpointTagsArgs.builder()\n                .customTags(SqlEndpointTagsCustomTagArgs.builder()\n                    .key(\"City\")\n                    .value(\"Amsterdam\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlEndpoint\n    properties:\n      name: Endpoint of ${me.alphanumeric}\n      clusterSize: Small\n      maxNumClusters: 1\n      tags:\n        customTags:\n          - key: City\n            value: Amsterdam\nvariables:\n  me:\n    fn::invoke:\n      function: databricks:getCurrentUser\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access control\n\n* databricks.Permissions can control which groups or individual users can *Can Use* or *Can Manage* SQL warehouses.\n* `databricks_sql_access` on databricks.Group or databricks_user.\n\n## Related resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_endpoint` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlEndpoint:SqlEndpoint this \u003cendpoint-id\u003e\n```\n\n",
            "properties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "creatorName": {
                    "type": "string",
                    "description": "The username of the user who created the endpoint.\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                },
                "healths": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointHealth:SqlEndpointHealth"
                    },
                    "description": "Health status of the endpoint.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse. Must be unique.\n"
                },
                "numActiveSessions": {
                    "type": "integer",
                    "description": "The current number of clusters used by the endpoint.\n"
                },
                "numClusters": {
                    "type": "integer",
                    "description": "The current number of clusters used by the endpoint.\n"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of the endpoint.\n"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                }
            },
            "required": [
                "clusterSize",
                "creatorName",
                "dataSourceId",
                "enableServerlessCompute",
                "healths",
                "jdbcUrl",
                "name",
                "numActiveSessions",
                "numClusters",
                "odbcParams",
                "state"
            ],
            "inputProperties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse. Must be unique.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                }
            },
            "requiredInputs": [
                "clusterSize"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlEndpoint resources.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "creatorName": {
                        "type": "string",
                        "description": "The username of the user who created the endpoint.\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                    },
                    "healths": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlEndpointHealth:SqlEndpointHealth"
                        },
                        "description": "Health status of the endpoint.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the SQL warehouse. Must be unique.\n"
                    },
                    "numActiveSessions": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "numClusters": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of the endpoint.\n"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                        "description": "Databricks tags all endpoint resources with these tags.\n"
                    },
                    "warehouseType": {
                        "type": "string",
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlGlobalConfig:SqlGlobalConfig": {
            "description": "This resource configures the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace. *Please note that changing parameters of this resource will restart all running databricks_sql_endpoint.*  To use this resource you need to be an administrator.\n\n## Example Usage\n\n### AWS example\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    instanceProfileArn: \"arn:....\",\n    dataAccessConfig: {\n        \"spark.sql.session.timeZone\": \"UTC\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    instance_profile_arn=\"arn:....\",\n    data_access_config={\n        \"spark.sql.session.timeZone\": \"UTC\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        InstanceProfileArn = \"arn:....\",\n        DataAccessConfig = \n        {\n            { \"spark.sql.session.timeZone\", \"UTC\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy:     pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tInstanceProfileArn: pulumi.String(\"arn:....\"),\n\t\t\tDataAccessConfig: pulumi.StringMap{\n\t\t\t\t\"spark.sql.session.timeZone\": pulumi.String(\"UTC\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()\n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .instanceProfileArn(\"arn:....\")\n            .dataAccessConfig(Map.of(\"spark.sql.session.timeZone\", \"UTC\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      instanceProfileArn: arn:....\n      dataAccessConfig:\n        spark.sql.session.timeZone: UTC\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Azure example\n\nFor Azure you should use the `data_access_config` to provide the service principal configuration. You can use the Databricks SQL Admin Console UI to help you generate the right configuration values.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    dataAccessConfig: {\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": applicationId,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": `{{secrets/${secretScope}/${secretKey}}}`,\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": `https://login.microsoftonline.com/${tenantId}/oauth2/token`,\n    },\n    sqlConfigParams: {\n        ANSI_MODE: \"true\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    data_access_config={\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": application_id,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": f\"{{{{secrets/{secret_scope}/{secret_key}}}}}\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\",\n    },\n    sql_config_params={\n        \"ANSI_MODE\": \"true\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        DataAccessConfig = \n        {\n            { \"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\" },\n            { \"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.id\", applicationId },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.secret\", $\"{{{{secrets/{secretScope}/{secretKey}}}}}\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", $\"https://login.microsoftonline.com/{tenantId}/oauth2/token\" },\n        },\n        SqlConfigParams = \n        {\n            { \"ANSI_MODE\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy: pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tDataAccessConfig: pulumi.StringMap{\n\t\t\t\t\"spark.hadoop.fs.azure.account.auth.type\":              pulumi.String(\"OAuth\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth.provider.type\":    pulumi.String(\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.id\":       pulumi.Any(applicationId),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.secret\":   pulumi.Sprintf(\"{{secrets/%v/%v}}\", secretScope, secretKey),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": pulumi.Sprintf(\"https://login.microsoftonline.com/%v/oauth2/token\", tenantId),\n\t\t\t},\n\t\t\tSqlConfigParams: pulumi.StringMap{\n\t\t\t\t\"ANSI_MODE\": pulumi.String(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()\n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .dataAccessConfig(Map.ofEntries(\n                Map.entry(\"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.id\", applicationId),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.secret\", String.format(\"{{{{secrets/%s/%s}}}}\", secretScope,secretKey)),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", String.format(\"https://login.microsoftonline.com/%s/oauth2/token\", tenantId))\n            ))\n            .sqlConfigParams(Map.of(\"ANSI_MODE\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      dataAccessConfig:\n        spark.hadoop.fs.azure.account.auth.type: OAuth\n        spark.hadoop.fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n        spark.hadoop.fs.azure.account.oauth2.client.id: ${applicationId}\n        spark.hadoop.fs.azure.account.oauth2.client.secret: '{{secrets/${secretScope}/${secretKey}}}'\n        spark.hadoop.fs.azure.account.oauth2.client.endpoint: https://login.microsoftonline.com/${tenantId}/oauth2/token\n      sqlConfigParams:\n        ANSI_MODE: 'true'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_global_config` resource with command like the following (you need to use `global` as ID):\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlGlobalConfig:SqlGlobalConfig this global\n```\n\n",
            "properties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "required": [
                "enableServerlessCompute"
            ],
            "inputProperties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlGlobalConfig resources.\n",
                "properties": {
                    "dataAccessConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                    },
                    "googleServiceAccount": {
                        "type": "string",
                        "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                    },
                    "securityPolicy": {
                        "type": "string",
                        "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                    },
                    "sqlConfigParams": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlPermissions:SqlPermissions": {
            "description": "\u003e Please switch to databricks.Grants with Unity Catalog to manage data access, which provides a better and faster way for managing data security. `databricks.Grants` resource *doesn't require a technical cluster to perform operations*. On workspaces with Unity Catalog enabled, you may run into errors such as `Error: cannot create sql permissions: cannot read current grants: For unity catalog, please specify the catalog name explicitly. E.g. SHOW GRANT ``your.address@email.com`` ON CATALOG main`. This happens if your `default_catalog_name` was set to a UC catalog instead of `hive_metastore`. The workaround is to re-assign the metastore again with the default catalog set to be `hive_metastore`. See databricks_metastore_assignment.\n\nThis resource manages data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html). In order to enable Table Access control, you have to login to the workspace as administrator, go to `Admin Console`, pick `Access Control` tab, click on `Enable` button in `Table Access Control` section, and click `Confirm`. The security guarantees of table access control **will only be effective if cluster access control is also turned on**. Please make sure that no users can create clusters in your workspace and all databricks.Cluster have approximately the following configuration:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {sparkConf: {\n    \"spark.databricks.acl.dfAclsEnabled\": \"true\",\n    \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\", spark_conf={\n    \"spark.databricks.acl.dfAclsEnabled\": \"true\",\n    \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        SparkConf = \n        {\n            { \"spark.databricks.acl.dfAclsEnabled\", \"true\" },\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tSparkConf: pulumi.StringMap{\n\t\t\t\t\"spark.databricks.acl.dfAclsEnabled\":     pulumi.String(\"true\"),\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.String(\"python,sql\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.acl.dfAclsEnabled\", \"true\"),\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      sparkConf:\n        spark.databricks.acl.dfAclsEnabled: 'true'\n        spark.databricks.repl.allowedLanguages: python,sql\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIt is required to define all permissions for a securable in a single resource, otherwise Pulumi cannot guarantee config drift prevention.\n\n## Example Usage\n\nThe following resource definition will enforce access control on a table by executing the following SQL queries on a special auto-terminating cluster it would create for this operation:\n\n* ```SHOW GRANT ON TABLE `default`.`foo` ```\n* ```REVOKE ALL PRIVILEGES ON TABLE `default`.`foo` FROM ... every group and user that has access to it ...```\n* ```GRANT MODIFY, SELECT ON TABLE `default`.`foo` TO `serge@example.com` ```\n* ```GRANT SELECT ON TABLE `default`.`foo` TO `special group` ```\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fooTable = new databricks.SqlPermissions(\"foo_table\", {\n    table: \"foo\",\n    privilegeAssignments: [\n        {\n            principal: \"serge@example.com\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            principal: \"special group\",\n            privileges: [\"SELECT\"],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfoo_table = databricks.SqlPermissions(\"foo_table\",\n    table=\"foo\",\n    privilege_assignments=[\n        {\n            \"principal\": \"serge@example.com\",\n            \"privileges\": [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            \"principal\": \"special group\",\n            \"privileges\": [\"SELECT\"],\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fooTable = new Databricks.SqlPermissions(\"foo_table\", new()\n    {\n        Table = \"foo\",\n        PrivilegeAssignments = new[]\n        {\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"serge@example.com\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                    \"MODIFY\",\n                },\n            },\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"special group\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlPermissions(ctx, \"foo_table\", \u0026databricks.SqlPermissionsArgs{\n\t\t\tTable: pulumi.String(\"foo\"),\n\t\t\tPrivilegeAssignments: databricks.SqlPermissionsPrivilegeAssignmentArray{\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"serge@example.com\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"special group\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlPermissions;\nimport com.pulumi.databricks.SqlPermissionsArgs;\nimport com.pulumi.databricks.inputs.SqlPermissionsPrivilegeAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fooTable = new SqlPermissions(\"fooTable\", SqlPermissionsArgs.builder()\n            .table(\"foo\")\n            .privilegeAssignments(            \n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"serge@example.com\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build(),\n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"special group\")\n                    .privileges(\"SELECT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fooTable:\n    type: databricks:SqlPermissions\n    name: foo_table\n    properties:\n      table: foo\n      privilegeAssignments:\n        - principal: serge@example.com\n          privileges:\n            - SELECT\n            - MODIFY\n        - principal: special group\n          privileges:\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Grants to manage data access in Unity Catalog.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are:\n\n* `table/default.foo` - table `foo` in a `default` database. Database is always mandatory.\n\n* `view/bar.foo` - view `foo` in `bar` database.\n\n* `database/bar` - `bar` database.\n\n* `catalog/` - entire catalog. `/` suffix is mandatory.\n\n* `any file/` - direct access to any file. `/` suffix is mandatory.\n\n* `anonymous function/` - anonymous function. `/` suffix is mandatory.\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlPermissions:SqlPermissions foo /\u003cobject-type\u003e/\u003cobject-name\u003e\n```\n\n",
            "properties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n"
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading/writing any file. Defaults to `false`.\n"
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n"
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n"
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading/writing any file. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n",
                    "willReplaceOnChanges": true
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlPermissions resources.\n",
                "properties": {
                    "anonymousFunction": {
                        "type": "boolean",
                        "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "anyFile": {
                        "type": "boolean",
                        "description": "If this access control for reading/writing any file. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "catalog": {
                        "type": "boolean",
                        "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "database": {
                        "type": "string",
                        "description": "Name of the database. Has default value of `default`.\n",
                        "willReplaceOnChanges": true
                    },
                    "privilegeAssignments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                        }
                    },
                    "table": {
                        "type": "string",
                        "description": "Name of the table. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "description": "Name of the view. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlQuery:SqlQuery": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n\u003e documentation for this resource is a work in progress.\n\nA query may have one or more visualizations.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Queries\"});\nconst q1 = new databricks.SqlQuery(\"q1\", {\n    dataSourceId: example.dataSourceId,\n    name: \"My Query Name\",\n    query: `                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n`,\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    runAsRole: \"viewer\",\n    parameters: [\n        {\n            name: \"p1\",\n            title: \"Title for p1\",\n            text: {\n                value: \"default\",\n            },\n        },\n        {\n            name: \"p2\",\n            title: \"Title for p2\",\n            \"enum\": {\n                options: [\n                    \"default\",\n                    \"foo\",\n                    \"bar\",\n                ],\n                value: \"default\",\n                multiple: {\n                    prefix: \"\\\"\",\n                    suffix: \"\\\"\",\n                    separator: \",\",\n                },\n            },\n        },\n        {\n            name: \"p3\",\n            title: \"Title for p3\",\n            date: {\n                value: \"2022-01-01\",\n            },\n        },\n    ],\n    tags: [\n        \"t1\",\n        \"t2\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Queries\")\nq1 = databricks.SqlQuery(\"q1\",\n    data_source_id=example[\"dataSourceId\"],\n    name=\"My Query Name\",\n    query=\"\"\"                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n\"\"\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    run_as_role=\"viewer\",\n    parameters=[\n        {\n            \"name\": \"p1\",\n            \"title\": \"Title for p1\",\n            \"text\": {\n                \"value\": \"default\",\n            },\n        },\n        {\n            \"name\": \"p2\",\n            \"title\": \"Title for p2\",\n            \"enum\": {\n                \"options\": [\n                    \"default\",\n                    \"foo\",\n                    \"bar\",\n                ],\n                \"value\": \"default\",\n                \"multiple\": {\n                    \"prefix\": \"\\\"\",\n                    \"suffix\": \"\\\"\",\n                    \"separator\": \",\",\n                },\n            },\n        },\n        {\n            \"name\": \"p3\",\n            \"title\": \"Title for p3\",\n            \"date\": {\n                \"value\": \"2022-01-01\",\n            },\n        },\n    ],\n    tags=[\n        \"t1\",\n        \"t2\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Queries\",\n    });\n\n    var q1 = new Databricks.SqlQuery(\"q1\", new()\n    {\n        DataSourceId = example.DataSourceId,\n        Name = \"My Query Name\",\n        Query = @\"                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        RunAsRole = \"viewer\",\n        Parameters = new[]\n        {\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p1\",\n                Title = \"Title for p1\",\n                Text = new Databricks.Inputs.SqlQueryParameterTextArgs\n                {\n                    Value = \"default\",\n                },\n            },\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p2\",\n                Title = \"Title for p2\",\n                Enum = new Databricks.Inputs.SqlQueryParameterEnumArgs\n                {\n                    Options = new[]\n                    {\n                        \"default\",\n                        \"foo\",\n                        \"bar\",\n                    },\n                    Value = \"default\",\n                    Multiple = new Databricks.Inputs.SqlQueryParameterEnumMultipleArgs\n                    {\n                        Prefix = \"\\\"\",\n                        Suffix = \"\\\"\",\n                        Separator = \",\",\n                    },\n                },\n            },\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p3\",\n                Title = \"Title for p3\",\n                Date = new Databricks.Inputs.SqlQueryParameterDateArgs\n                {\n                    Value = \"2022-01-01\",\n                },\n            },\n        },\n        Tags = new[]\n        {\n            \"t1\",\n            \"t2\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Queries\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlQuery(ctx, \"q1\", \u0026databricks.SqlQueryArgs{\n\t\t\tDataSourceId: pulumi.Any(example.DataSourceId),\n\t\t\tName:         pulumi.String(\"My Query Name\"),\n\t\t\tQuery:        pulumi.String(\"                        SELECT {{ p1 }} AS p1\\n                        WHERE 1=1\\n                        AND p2 in ({{ p2 }})\\n                        AND event_date \u003e date '{{ p3 }}'\\n\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tRunAsRole: pulumi.String(\"viewer\"),\n\t\t\tParameters: databricks.SqlQueryParameterArray{\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p1\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p1\"),\n\t\t\t\t\tText: \u0026databricks.SqlQueryParameterTextArgs{\n\t\t\t\t\t\tValue: pulumi.String(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p2\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p2\"),\n\t\t\t\t\tEnum: \u0026databricks.SqlQueryParameterEnumArgs{\n\t\t\t\t\t\tOptions: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"default\"),\n\t\t\t\t\t\t\tpulumi.String(\"foo\"),\n\t\t\t\t\t\t\tpulumi.String(\"bar\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t\tValue: pulumi.String(\"default\"),\n\t\t\t\t\t\tMultiple: \u0026databricks.SqlQueryParameterEnumMultipleArgs{\n\t\t\t\t\t\t\tPrefix:    pulumi.String(\"\\\"\"),\n\t\t\t\t\t\t\tSuffix:    pulumi.String(\"\\\"\"),\n\t\t\t\t\t\t\tSeparator: pulumi.String(\",\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p3\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p3\"),\n\t\t\t\t\tDate: \u0026databricks.SqlQueryParameterDateArgs{\n\t\t\t\t\t\tValue: pulumi.String(\"2022-01-01\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"t1\"),\n\t\t\t\tpulumi.String(\"t2\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlQuery;\nimport com.pulumi.databricks.SqlQueryArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterTextArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterEnumArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterEnumMultipleArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterDateArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()\n            .path(\"/Shared/Queries\")\n            .build());\n\n        var q1 = new SqlQuery(\"q1\", SqlQueryArgs.builder()\n            .dataSourceId(example.dataSourceId())\n            .name(\"My Query Name\")\n            .query(\"\"\"\n                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n            \"\"\")\n            .parent(sharedDir.objectId().applyValue(_objectId -\u003e String.format(\"folders/%s\", _objectId)))\n            .runAsRole(\"viewer\")\n            .parameters(            \n                SqlQueryParameterArgs.builder()\n                    .name(\"p1\")\n                    .title(\"Title for p1\")\n                    .text(SqlQueryParameterTextArgs.builder()\n                        .value(\"default\")\n                        .build())\n                    .build(),\n                SqlQueryParameterArgs.builder()\n                    .name(\"p2\")\n                    .title(\"Title for p2\")\n                    .enum_(SqlQueryParameterEnumArgs.builder()\n                        .options(                        \n                            \"default\",\n                            \"foo\",\n                            \"bar\")\n                        .value(\"default\")\n                        .multiple(SqlQueryParameterEnumMultipleArgs.builder()\n                            .prefix(\"\\\"\")\n                            .suffix(\"\\\"\")\n                            .separator(\",\")\n                            .build())\n                        .build())\n                    .build(),\n                SqlQueryParameterArgs.builder()\n                    .name(\"p3\")\n                    .title(\"Title for p3\")\n                    .date(SqlQueryParameterDateArgs.builder()\n                        .value(\"2022-01-01\")\n                        .build())\n                    .build())\n            .tags(            \n                \"t1\",\n                \"t2\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Queries\n  q1:\n    type: databricks:SqlQuery\n    properties:\n      dataSourceId: ${example.dataSourceId}\n      name: My Query Name\n      query: |2\n                                SELECT {{ p1 }} AS p1\n                                WHERE 1=1\n                                AND p2 in ({{ p2 }})\n                                AND event_date \u003e date '{{ p3 }}'\n      parent: folders/${sharedDir.objectId}\n      runAsRole: viewer\n      parameters:\n        - name: p1\n          title: Title for p1\n          text:\n            value: default\n        - name: p2\n          title: Title for p2\n          enum:\n            options:\n              - default\n              - foo\n              - bar\n            value: default\n            multiple:\n              prefix: '\"'\n              suffix: '\"'\n              separator: ','\n        - name: p3\n          title: Title for p3\n          date:\n            value: 2022-01-01\n      tags:\n        - t1\n        - t2\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nExample permission to share query with all users:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst q1 = new databricks.Permissions(\"q1\", {\n    sqlQueryId: q1DatabricksSqlQuery.id,\n    accessControls: [\n        {\n            groupName: users.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: team.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nq1 = databricks.Permissions(\"q1\",\n    sql_query_id=q1_databricks_sql_query[\"id\"],\n    access_controls=[\n        {\n            \"group_name\": users[\"displayName\"],\n            \"permission_level\": \"CAN_RUN\",\n        },\n        {\n            \"group_name\": team[\"displayName\"],\n            \"permission_level\": \"CAN_EDIT\",\n        },\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var q1 = new Databricks.Permissions(\"q1\", new()\n    {\n        SqlQueryId = q1DatabricksSqlQuery.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = users.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = team.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"q1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlQueryId: pulumi.Any(q1DatabricksSqlQuery.Id),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(users.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(team.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var q1 = new Permissions(\"q1\", PermissionsArgs.builder()\n            .sqlQueryId(q1DatabricksSqlQuery.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(users.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(team.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  q1:\n    type: databricks:Permissions\n    properties:\n      sqlQueryId: ${q1DatabricksSqlQuery.id}\n      accessControls:\n        - groupName: ${users.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${team.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Troubleshooting\n\nIn case you see `Error: cannot create sql query: Internal Server Error` during `pulumi up`; double check that you are using the correct `data_source_id`\n\nOperations on `databricks.SqlQuery` schedules are ⛔️ deprecated. You can create, update or delete a schedule for SQLA and other Databricks resources using the databricks.Job resource.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n* databricks.Job to schedule Databricks SQL queries (as well as dashboards and alerts) using Databricks Jobs.\n\n## Import\n\nYou can import a `databricks_sql_query` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlQuery:SqlQuery this \u003cquery-id\u003e\n```\n\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "Data source ID of a SQL warehouse\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the object.\n"
                },
                "query": {
                    "type": "string",
                    "description": "The text of the query to be run.\n"
                },
                "runAsRole": {
                    "type": "string",
                    "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                    "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "dataSourceId",
                "name",
                "query",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "Data source ID of a SQL warehouse\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the object.\n",
                    "willReplaceOnChanges": true
                },
                "query": {
                    "type": "string",
                    "description": "The text of the query to be run.\n"
                },
                "runAsRole": {
                    "type": "string",
                    "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                    "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "dataSourceId",
                "query"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlQuery resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "Data source ID of a SQL warehouse\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "General description that conveys additional information about this query such as usage notes.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                        }
                    },
                    "parent": {
                        "type": "string",
                        "description": "The identifier of the workspace folder containing the object.\n",
                        "willReplaceOnChanges": true
                    },
                    "query": {
                        "type": "string",
                        "description": "The text of the query to be run.\n"
                    },
                    "runAsRole": {
                        "type": "string",
                        "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                        "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlTable:SqlTable": {
            "description": "\n\n## Import\n\nThis resource can be imported by its full name.\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlTable:SqlTable this \u003ccatalog_name\u003e.\u003cschema_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces the creation of a new resource.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to liquid cluster the table by. For automatic clustering, set `cluster_keys` to `[\"AUTO\"]`. To turn off clustering, set it to `[\"NONE\"]`. Conflicts with `partitions`.\n"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text. Changing the comment is not currently supported on the `VIEW` table type.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, and `TEXT`. Change forces the creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n"
                },
                "effectiveProperties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "name": {
                    "type": "string",
                    "description": "Name of table relative to parent catalog and schema. Change forces the creation of a new resource.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map of user defined table options. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the table owner.\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to partition the table by. Change forces the creation of a new resource. Conflicts with `cluster_keys`.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "A map of table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces the creation of a new resource.\n"
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces the creation of a new resource.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL`, or `VIEW`. Change forces the creation of a new resource.\n"
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                }
            },
            "required": [
                "catalogName",
                "clusterId",
                "columns",
                "effectiveProperties",
                "name",
                "owner",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces the creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to liquid cluster the table by. For automatic clustering, set `cluster_keys` to `[\"AUTO\"]`. To turn off clustering, set it to `[\"NONE\"]`. Conflicts with `partitions`.\n"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text. Changing the comment is not currently supported on the `VIEW` table type.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, and `TEXT`. Change forces the creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of table relative to parent catalog and schema. Change forces the creation of a new resource.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Map of user defined table options. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the table owner.\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to partition the table by. Change forces the creation of a new resource. Conflicts with `cluster_keys`.\n",
                    "willReplaceOnChanges": true
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "A map of table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces the creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces the creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL`, or `VIEW`. Change forces the creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlTable resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog. Change forces the creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a subset of columns to liquid cluster the table by. For automatic clustering, set `cluster_keys` to `[\"AUTO\"]`. To turn off clustering, set it to `[\"NONE\"]`. Conflicts with `partitions`.\n"
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                        }
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text. Changing the comment is not currently supported on the `VIEW` table type.\n"
                    },
                    "dataSourceFormat": {
                        "type": "string",
                        "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, and `TEXT`. Change forces the creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n",
                        "willReplaceOnChanges": true
                    },
                    "effectiveProperties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        }
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of table relative to parent catalog and schema. Change forces the creation of a new resource.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Map of user defined table options. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "User name/group name/sp application_id of the table owner.\n"
                    },
                    "partitions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a subset of columns to partition the table by. Change forces the creation of a new resource. Conflicts with `cluster_keys`.\n",
                        "willReplaceOnChanges": true
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "A map of table properties.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of parent Schema relative to parent Catalog. Change forces the creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces the creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                    },
                    "tableType": {
                        "type": "string",
                        "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL`, or `VIEW`. Change forces the creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string",
                        "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlVisualization:SqlVisualization": {
            "description": "\n\n## Import\n\nYou can import a `databricks_sql_visualization` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlVisualization:SqlVisualization this \u003cquery-id\u003e/\u003cvisualization-id\u003e\n```\n\n",
            "properties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string"
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "options",
                "queryId",
                "type",
                "visualizationId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "options",
                "queryId",
                "type"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlVisualization resources.\n",
                "properties": {
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "options": {
                        "type": "string"
                    },
                    "queryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "queryPlan": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlWidget:SqlWidget": {
            "description": "\u003e Please switch to databricks.Dashboard to author new AI/BI dashboards using the latest tooling\n\nTo manage [SQL resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n\u003e documentation for this resource is a work in progress.\n\nA widget is always tied to a Legacy dashboard. Every dashboard may have one or more widgets.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1w1 = new databricks.SqlWidget(\"d1w1\", {\n    dashboardId: d1.id,\n    text: \"Hello! I'm a **text widget**!\",\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 0,\n        posY: 0,\n    },\n});\nconst d1w2 = new databricks.SqlWidget(\"d1w2\", {\n    dashboardId: d1.id,\n    visualizationId: q1v1.id,\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 3,\n        posY: 0,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1w1 = databricks.SqlWidget(\"d1w1\",\n    dashboard_id=d1[\"id\"],\n    text=\"Hello! I'm a **text widget**!\",\n    position={\n        \"size_x\": 3,\n        \"size_y\": 4,\n        \"pos_x\": 0,\n        \"pos_y\": 0,\n    })\nd1w2 = databricks.SqlWidget(\"d1w2\",\n    dashboard_id=d1[\"id\"],\n    visualization_id=q1v1[\"id\"],\n    position={\n        \"size_x\": 3,\n        \"size_y\": 4,\n        \"pos_x\": 3,\n        \"pos_y\": 0,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1w1 = new Databricks.SqlWidget(\"d1w1\", new()\n    {\n        DashboardId = d1.Id,\n        Text = \"Hello! I'm a **text widget**!\",\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 0,\n            PosY = 0,\n        },\n    });\n\n    var d1w2 = new Databricks.SqlWidget(\"d1w2\", new()\n    {\n        DashboardId = d1.Id,\n        VisualizationId = q1v1.Id,\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 3,\n            PosY = 0,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlWidget(ctx, \"d1w1\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId: pulumi.Any(d1.Id),\n\t\t\tText:        pulumi.String(\"Hello! I'm a **text widget**!\"),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(0),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlWidget(ctx, \"d1w2\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId:     pulumi.Any(d1.Id),\n\t\t\tVisualizationId: pulumi.Any(q1v1.Id),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(3),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlWidget;\nimport com.pulumi.databricks.SqlWidgetArgs;\nimport com.pulumi.databricks.inputs.SqlWidgetPositionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1w1 = new SqlWidget(\"d1w1\", SqlWidgetArgs.builder()\n            .dashboardId(d1.id())\n            .text(\"Hello! I'm a **text widget**!\")\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(0)\n                .posY(0)\n                .build())\n            .build());\n\n        var d1w2 = new SqlWidget(\"d1w2\", SqlWidgetArgs.builder()\n            .dashboardId(d1.id())\n            .visualizationId(q1v1.id())\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(3)\n                .posY(0)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1w1:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${d1.id}\n      text: Hello! I'm a **text widget**!\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 0\n        posY: 0\n  d1w2:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${d1.id}\n      visualizationId: ${q1v1.id}\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 3\n        posY: 0\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_widget` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlWidget:SqlWidget this \u003cdashboard-id\u003e/\u003cwidget-id\u003e\n```\n\n",
            "properties": {
                "dashboardId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                },
                "widgetId": {
                    "type": "string"
                }
            },
            "required": [
                "dashboardId",
                "widgetId"
            ],
            "inputProperties": {
                "dashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "widgetId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "dashboardId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlWidget resources.\n",
                "properties": {
                    "dashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "description": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                        }
                    },
                    "position": {
                        "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                    },
                    "text": {
                        "type": "string"
                    },
                    "title": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "widgetId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/storageCredential:StorageCredential": {
            "description": "\u003e This resource can be used with an account or workspace-level provider.\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- `databricks.StorageCredential` represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- databricks.ExternalLocation are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\nOn AWS, the IAM role for a storage credential requires a trust policy. See [documentation](https://docs.databricks.com/en/connect/unity-catalog/cloud-storage/storage-credentials.html#step-1-create-an-iam-role) for more details. The data source databricks.getAwsUnityCatalogAssumeRolePolicy can be used to create the necessary AWS Unity Catalog assume role policy.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    },\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"CREATE_EXTERNAL_TABLE\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst externalMi = new databricks.StorageCredential(\"external_mi\", {\n    name: \"mi_credential\",\n    azureManagedIdentity: {\n        accessConnectorId: example.id,\n    },\n    comment: \"Managed identity credential managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: externalMi.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal_mi = databricks.StorageCredential(\"external_mi\",\n    name=\"mi_credential\",\n    azure_managed_identity={\n        \"access_connector_id\": example[\"id\"],\n    },\n    comment=\"Managed identity credential managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external_mi.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"CREATE_EXTERNAL_TABLE\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var externalMi = new Databricks.StorageCredential(\"external_mi\", new()\n    {\n        Name = \"mi_credential\",\n        AzureManagedIdentity = new Databricks.Inputs.StorageCredentialAzureManagedIdentityArgs\n        {\n            AccessConnectorId = example.Id,\n        },\n        Comment = \"Managed identity credential managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = externalMi.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternalMi, err := databricks.NewStorageCredential(ctx, \"external_mi\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.String(\"mi_credential\"),\n\t\t\tAzureManagedIdentity: \u0026databricks.StorageCredentialAzureManagedIdentityArgs{\n\t\t\t\tAccessConnectorId: pulumi.Any(example.Id),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed identity credential managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: externalMi.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAzureManagedIdentityArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var externalMi = new StorageCredential(\"externalMi\", StorageCredentialArgs.builder()\n            .name(\"mi_credential\")\n            .azureManagedIdentity(StorageCredentialAzureManagedIdentityArgs.builder()\n                .accessConnectorId(example.id())\n                .build())\n            .comment(\"Managed identity credential managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .storageCredential(externalMi.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  externalMi:\n    type: databricks:StorageCredential\n    name: external_mi\n    properties:\n      name: mi_credential\n      azureManagedIdentity:\n        accessConnectorId: ${example.id}\n      comment: Managed identity credential managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${externalMi.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: \"the-creds\",\n    databricksGcpServiceAccount: {},\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=\"the-creds\",\n    databricks_gcp_service_account={})\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external.id,\n    grants=[{\n        \"principal\": \"Data Engineers\",\n        \"privileges\": [\"CREATE_EXTERNAL_TABLE\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = \"the-creds\",\n        DatabricksGcpServiceAccount = null,\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName:                        pulumi.String(\"the-creds\"),\n\t\t\tDatabricksGcpServiceAccount: \u0026databricks.StorageCredentialDatabricksGcpServiceAccountArgs{},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialDatabricksGcpServiceAccountArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(\"the-creds\")\n            .databricksGcpServiceAccount(StorageCredentialDatabricksGcpServiceAccountArgs.builder()\n                .build())\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()\n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: the-creds\n      databricksGcpServiceAccount: {}\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/storageCredential:StorageCredential this \u003cname\u003e\n```\n\n",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/StorageCredentialCloudflareApiToken:StorageCredentialCloudflareApiToken"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete storage credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update storage credential regardless of its dependents.\n"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                },
                "storageCredentialId": {
                    "type": "string",
                    "description": "Unique ID of storage credential.\n"
                }
            },
            "required": [
                "databricksGcpServiceAccount",
                "isolationMode",
                "metastoreId",
                "name",
                "owner",
                "storageCredentialId"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/StorageCredentialCloudflareApiToken:StorageCredentialCloudflareApiToken"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete storage credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update storage credential regardless of its dependents.\n"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering StorageCredential resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                    },
                    "cloudflareApiToken": {
                        "$ref": "#/types/databricks:index/StorageCredentialCloudflareApiToken:StorageCredentialCloudflareApiToken"
                    },
                    "comment": {
                        "type": "string"
                    },
                    "databricksGcpServiceAccount": {
                        "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete storage credential regardless of its dependencies.\n"
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "description": "Update storage credential regardless of its dependents.\n"
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                    },
                    "isolationMode": {
                        "type": "string",
                        "description": "Whether the storage credential is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATION_MODE_ISOLATED` or `ISOLATION_MODE_OPEN`. Setting the credential to `ISOLATION_MODE_ISOLATED` will automatically allow access from the current workspace.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "description": "Indicates whether the storage credential is only usable for read operations.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                    },
                    "storageCredentialId": {
                        "type": "string",
                        "description": "Unique ID of storage credential.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/systemSchema:SystemSchema": {
            "description": "\u003e This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n\u003e This resource can only be used with a workspace-level provider!\n\n\u003e Certain system schemas (such as `billing`) may be auto-enabled once GA and should not be manually declared in Pulumi configurations.\n\nManages system tables enablement. System tables are a Databricks-hosted analytical store of your account’s operational data. System tables can be used for historical observability across your account. System tables must be enabled by an account admin.\n\n## Example Usage\n\nEnable the system schema `access`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SystemSchema(\"this\", {schema: \"access\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SystemSchema(\"this\", schema=\"access\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SystemSchema(\"this\", new()\n    {\n        Schema = \"access\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSystemSchema(ctx, \"this\", \u0026databricks.SystemSchemaArgs{\n\t\t\tSchema: pulumi.String(\"access\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SystemSchema;\nimport com.pulumi.databricks.SystemSchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SystemSchema(\"this\", SystemSchemaArgs.builder()\n            .schema(\"access\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SystemSchema\n    properties:\n      schema: access\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by the metastore id and schema name\n\nbash\n\n```sh\n$ pulumi import databricks:index/systemSchema:SystemSchema this '\u003cmetastore_id\u003e|\u003cschema_name\u003e'\n```\n\n",
            "properties": {
                "autoEnabled": {
                    "type": "boolean"
                },
                "fullName": {
                    "type": "string",
                    "description": "the full name of the system schema, in form of `system.\u003cschema\u003e`.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "schema": {
                    "type": "string",
                    "description": "name of the system schema.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of enablement for the system schema.\n"
                }
            },
            "required": [
                "autoEnabled",
                "fullName",
                "metastoreId",
                "state"
            ],
            "inputProperties": {
                "schema": {
                    "type": "string",
                    "description": "name of the system schema.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of enablement for the system schema.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SystemSchema resources.\n",
                "properties": {
                    "autoEnabled": {
                        "type": "boolean"
                    },
                    "fullName": {
                        "type": "string",
                        "description": "the full name of the system schema, in form of `system.\u003cschema\u003e`.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "schema": {
                        "type": "string",
                        "description": "name of the system schema.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of enablement for the system schema.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/table:Table": {
            "properties": {
                "catalogName": {
                    "type": "string"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "schemaName": {
                    "type": "string"
                },
                "storageCredentialName": {
                    "type": "string"
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string"
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "required": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "name",
                "owner",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    }
                },
                "schemaName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Table resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                        }
                    },
                    "comment": {
                        "type": "string"
                    },
                    "dataSourceFormat": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "owner": {
                        "type": "string"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        }
                    },
                    "schemaName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string"
                    },
                    "tableType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/token:Token": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource creates [Personal Access Tokens](https://docs.databricks.com/sql/user/security/personal-access-tokens.html) for the same user that is authenticated with the provider. Most likely you should use databricks.OboToken to create [On-Behalf-Of tokens](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-personal-access-tokens-for-a-service-principal) for a databricks.ServicePrincipal in Databricks workspaces on AWS. Databricks workspaces on other clouds use their own native OAuth token flows.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// create PAT token to provision entities within workspace\nconst pat = new databricks.Token(\"pat\", {\n    comment: \"Pulumi Provisioning\",\n    lifetimeSeconds: 8640000,\n});\nexport const databricksToken = pat.tokenValue;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# create PAT token to provision entities within workspace\npat = databricks.Token(\"pat\",\n    comment=\"Pulumi Provisioning\",\n    lifetime_seconds=8640000)\npulumi.export(\"databricksToken\", pat.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // create PAT token to provision entities within workspace\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = \"Pulumi Provisioning\",\n        LifetimeSeconds = 8640000,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = pat.TokenValue,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// create PAT token to provision entities within workspace\n\t\tpat, err := databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"Pulumi Provisioning\"),\n\t\t\tLifetimeSeconds: pulumi.Int(8640000),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", pat.TokenValue)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // create PAT token to provision entities within workspace\n        var pat = new Token(\"pat\", TokenArgs.builder()\n            .comment(\"Pulumi Provisioning\")\n            .lifetimeSeconds(8640000)\n            .build());\n\n        ctx.export(\"databricksToken\", pat.tokenValue());\n    }\n}\n```\n```yaml\nresources:\n  # create PAT token to provision entities within workspace\n  pat:\n    type: databricks:Token\n    properties:\n      comment: Pulumi Provisioning\n      lifetimeSeconds: 8.64e+06\noutputs:\n  # output token for other modules\n  databricksToken: ${pat.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nA token can be automatically rotated by taking a dependency on the `time_rotating` resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as time from \"@pulumiverse/time\";\n\nconst _this = new time.Rotating(\"this\", {rotationDays: 30});\nconst pat = new databricks.Token(\"pat\", {\n    comment: pulumi.interpolate`Pulumi (created: ${_this.rfc3339})`,\n    lifetimeSeconds: 60 * 24 * 60 * 60,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumiverse_time as time\n\nthis = time.Rotating(\"this\", rotation_days=30)\npat = databricks.Token(\"pat\",\n    comment=this.rfc3339.apply(lambda rfc3339: f\"Pulumi (created: {rfc3339})\"),\n    lifetime_seconds=60 * 24 * 60 * 60)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Time = Pulumiverse.Time;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Time.Rotating(\"this\", new()\n    {\n        RotationDays = 30,\n    });\n\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = @this.Rfc3339.Apply(rfc3339 =\u003e $\"Pulumi (created: {rfc3339})\"),\n        LifetimeSeconds = 60 * 24 * 60 * 60,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-time/sdk/go/time\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := time.NewRotating(ctx, \"this\", \u0026time.RotatingArgs{\n\t\t\tRotationDays: pulumi.Int(30),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment: this.Rfc3339.ApplyT(func(rfc3339 string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"Pulumi (created: %v)\", rfc3339), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tLifetimeSeconds: int(60 * 24 * 60 * 60),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumiverse.time.Rotating;\nimport com.pulumiverse.time.RotatingArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Rotating(\"this\", RotatingArgs.builder()\n            .rotationDays(30)\n            .build());\n\n        var pat = new Token(\"pat\", TokenArgs.builder()\n            .comment(this_.rfc3339().applyValue(_rfc3339 -\u003e String.format(\"Pulumi (created: %s)\", _rfc3339)))\n            .lifetimeSeconds(60 * 24 * 60 * 60)\n            .build());\n\n    }\n}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, then expire time will be set to maximum allowed by the workspace configuration or platform.\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "creationTime",
                "expiryTime",
                "tokenId",
                "tokenValue"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, then expire time will be set to maximum allowed by the workspace configuration or platform.\n",
                    "willReplaceOnChanges": true
                },
                "tokenId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Token resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "expiryTime": {
                        "type": "integer"
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, then expire time will be set to maximum allowed by the workspace configuration or platform.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenId": {
                        "type": "string"
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/user:User": {
            "description": "This resource allows you to manage [users in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/users.html), [Databricks Account Console](https://accounts.cloud.databricks.com/) or [Azure Databricks Account Console](https://accounts.azuredatabricks.net). You can also associate Databricks users to databricks_group. Upon user creation the user will receive a welcome email. You can also get information about caller identity using databricks.getCurrentUser data source.\n\n\u003e To assign account level users to workspace use databricks_mws_permission_assignment.\n\n\u003e Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level users.  Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level users.\n\nTo create users in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments.\n\nThe default behavior when deleting a `databricks.User` resource depends on whether the provider is configured at the workspace-level or account-level. When the provider is configured at the workspace-level, the user will be deleted from the workspace. When the provider is configured at the account-level, the user will be deactivated but not deleted. When the provider is configured at the account level, to delete the user from the account when the resource is deleted, set `disable_as_user_deletion = false`. Conversely, when the provider is configured at the account-level, to deactivate the user when the resource is deleted, set `disable_as_user_deletion = true`.\n\n## Example Usage\n\nCreating regular user:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user with cluster create permissions:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n    allowClusterCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\",\n    allow_cluster_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n        AllowClusterCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName:           pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName:        pulumi.String(\"Example user\"),\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .allowClusterCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n      displayName: Example user\n      allowClusterCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountUser = new databricks.User(\"account_user\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_user = databricks.User(\"account_user\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountUser = new Databricks.User(\"account_user\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"account_user\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var accountUser = new User(\"accountUser\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountUser:\n    type: databricks:User\n    name: account_user\n    properties:\n      userName: me@example.com\n      displayName: Example user\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountUser = new databricks.User(\"account_user\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_user = databricks.User(\"account_user\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountUser = new Databricks.User(\"account_user\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"account_user\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var accountUser = new User(\"accountUser\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountUser:\n    type: databricks:User\n    name: account_user\n    properties:\n      userName: me@example.com\n      displayName: Example user\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\nThe resource scim user can be imported using id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/user:User me \u003cuser-id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create user: User with username X already exists` errors and implicitly import the specific user into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n"
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "required": [
                "aclPrincipalId",
                "disableAsUserDeletion",
                "displayName",
                "home",
                "repos",
                "userName"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean",
                    "description": "Ignore `cannot create user: User with username X already exists` errors and implicitly import the specific user into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "requiredInputs": [
                "userName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering User resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "disableAsUserDeletion": {
                        "type": "boolean",
                        "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the username that can be the full name of the user.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean",
                        "description": "Ignore `cannot create user: User with username X already exists` errors and implicitly import the specific user into Pulumi state, enforcing entitlements defined in the instance of resource. _This functionality is experimental_ and is designed to simplify corner cases, like Azure Active Directory synchronisation.\n"
                    },
                    "forceDeleteHomeDir": {
                        "type": "boolean",
                        "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                    },
                    "forceDeleteRepos": {
                        "type": "boolean",
                        "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                    },
                    "userName": {
                        "type": "string",
                        "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userInstanceProfile:UserInstanceProfile": {
            "description": "\u003e **Deprecated** Please rewrite with databricks_user_role. This resource will be removed in v0.5.x\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserInstanceProfile = new databricks.UserInstanceProfile(\"my_user_instance_profile\", {\n    userId: myUser.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_instance_profile = databricks.UserInstanceProfile(\"my_user_instance_profile\",\n    user_id=my_user.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserInstanceProfile = new Databricks.UserInstanceProfile(\"my_user_instance_profile\", new()\n    {\n        UserId = myUser.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserInstanceProfile(ctx, \"my_user_instance_profile\", \u0026databricks.UserInstanceProfileArgs{\n\t\t\tUserId:            myUser.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserInstanceProfile;\nimport com.pulumi.databricks.UserInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()\n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserInstanceProfile = new UserInstanceProfile(\"myUserInstanceProfile\", UserInstanceProfileArgs.builder()\n            .userId(myUser.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserInstanceProfile:\n    type: databricks:UserInstanceProfile\n    name: my_user_instance_profile\n    properties:\n      userId: ${myUser.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "instanceProfileId",
                "userId"
            ],
            "inputProperties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "instanceProfileId",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserInstanceProfile resources.\n",
                "properties": {
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userRole:UserRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to databricks_user.\n\n## Example Usage\n\nAdding AWS instance profile to a user\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserRole = new databricks.UserRole(\"my_user_role\", {\n    userId: myUser.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_role = databricks.UserRole(\"my_user_role\",\n    user_id=my_user.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserRole = new Databricks.UserRole(\"my_user_role\", new()\n    {\n        UserId = myUser.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"my_user_role\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()\n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserRole = new UserRole(\"myUserRole\", UserRoleArgs.builder()\n            .userId(myUser.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserRole:\n    type: databricks:UserRole\n    name: my_user_role\n    properties:\n      userId: ${myUser.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAdding user as administrator to Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserAccountAdmin = new databricks.UserRole(\"my_user_account_admin\", {\n    userId: myUser.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_account_admin = databricks.UserRole(\"my_user_account_admin\",\n    user_id=my_user.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserAccountAdmin = new Databricks.UserRole(\"my_user_account_admin\", new()\n    {\n        UserId = myUser.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"my_user_account_admin\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myUser = new User(\"myUser\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserAccountAdmin = new UserRole(\"myUserAccountAdmin\", UserRoleArgs.builder()\n            .userId(myUser.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserAccountAdmin:\n    type: databricks:UserRole\n    name: my_user_account_admin\n    properties:\n      userId: ${myUser.id}\n      role: account_admin\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "role",
                "userId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/vectorSearchEndpoint:VectorSearchEndpoint": {
            "description": "\u003e This resource can only be used on a Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Mosaic AI Vector Search Endpoint](https://docs.databricks.com/en/generative-ai/vector-search.html) in Databricks.  Mosaic AI Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database.  The Mosaic AI Vector Search Endpoint is used to create and access vector search indexes.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.VectorSearchEndpoint(\"this\", {\n    name: \"vector-search-test\",\n    endpointType: \"STANDARD\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.VectorSearchEndpoint(\"this\",\n    name=\"vector-search-test\",\n    endpoint_type=\"STANDARD\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.VectorSearchEndpoint(\"this\", new()\n    {\n        Name = \"vector-search-test\",\n        EndpointType = \"STANDARD\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewVectorSearchEndpoint(ctx, \"this\", \u0026databricks.VectorSearchEndpointArgs{\n\t\t\tName:         pulumi.String(\"vector-search-test\"),\n\t\t\tEndpointType: pulumi.String(\"STANDARD\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.VectorSearchEndpoint;\nimport com.pulumi.databricks.VectorSearchEndpointArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new VectorSearchEndpoint(\"this\", VectorSearchEndpointArgs.builder()\n            .name(\"vector-search-test\")\n            .endpointType(\"STANDARD\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:VectorSearchEndpoint\n    properties:\n      name: vector-search-test\n      endpointType: STANDARD\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Mosaic AI Vector Search Endpoint\n\nbash\n\n```sh\n$ pulumi import databricks:index/vectorSearchEndpoint:VectorSearchEndpoint this \u003cendpoint-name\u003e\n```\n\n",
            "properties": {
                "creationTimestamp": {
                    "type": "integer",
                    "description": "Timestamp of endpoint creation (milliseconds).\n"
                },
                "creator": {
                    "type": "string",
                    "description": "Creator of the endpoint.\n"
                },
                "endpointId": {
                    "type": "string",
                    "description": "Unique internal identifier of the endpoint (UUID).\n"
                },
                "endpointStatuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus"
                    },
                    "description": "Object describing the current status of the endpoint consisting of the following fields:\n"
                },
                "endpointType": {
                    "type": "string",
                    "description": "Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "Timestamp of the last update to the endpoint (milliseconds).\n"
                },
                "lastUpdatedUser": {
                    "type": "string",
                    "description": "User who last updated the endpoint.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Mosaic AI Vector Search Endpoint to create.\n"
                },
                "numIndexes": {
                    "type": "integer",
                    "description": "Number of indexes on the endpoint.\n"
                }
            },
            "required": [
                "creationTimestamp",
                "creator",
                "endpointId",
                "endpointStatuses",
                "endpointType",
                "lastUpdatedTimestamp",
                "lastUpdatedUser",
                "name",
                "numIndexes"
            ],
            "inputProperties": {
                "endpointType": {
                    "type": "string",
                    "description": "Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Mosaic AI Vector Search Endpoint to create.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "endpointType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering VectorSearchEndpoint resources.\n",
                "properties": {
                    "creationTimestamp": {
                        "type": "integer",
                        "description": "Timestamp of endpoint creation (milliseconds).\n"
                    },
                    "creator": {
                        "type": "string",
                        "description": "Creator of the endpoint.\n"
                    },
                    "endpointId": {
                        "type": "string",
                        "description": "Unique internal identifier of the endpoint (UUID).\n"
                    },
                    "endpointStatuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus"
                        },
                        "description": "Object describing the current status of the endpoint consisting of the following fields:\n"
                    },
                    "endpointType": {
                        "type": "string",
                        "description": "Type of Mosaic AI Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "Timestamp of the last update to the endpoint (milliseconds).\n"
                    },
                    "lastUpdatedUser": {
                        "type": "string",
                        "description": "User who last updated the endpoint.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Mosaic AI Vector Search Endpoint to create.\n",
                        "willReplaceOnChanges": true
                    },
                    "numIndexes": {
                        "type": "integer",
                        "description": "Number of indexes on the endpoint.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/vectorSearchIndex:VectorSearchIndex": {
            "description": "\u003e This resource can only be used on a Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Mosaic AI Vector Search Index](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html) in Databricks.  Mosaic AI Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database.  The Mosaic AI Vector Search Index provides the ability to search data in the linked Delta Table.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sync = new databricks.VectorSearchIndex(\"sync\", {\n    name: \"main.default.vector_search_index\",\n    endpointName: thisDatabricksVectorSearchEndpoint.name,\n    primaryKey: \"id\",\n    indexType: \"DELTA_SYNC\",\n    deltaSyncIndexSpec: {\n        sourceTable: \"main.default.source_table\",\n        pipelineType: \"TRIGGERED\",\n        embeddingSourceColumns: [{\n            name: \"text\",\n            embeddingModelEndpointName: _this.name,\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsync = databricks.VectorSearchIndex(\"sync\",\n    name=\"main.default.vector_search_index\",\n    endpoint_name=this_databricks_vector_search_endpoint[\"name\"],\n    primary_key=\"id\",\n    index_type=\"DELTA_SYNC\",\n    delta_sync_index_spec={\n        \"source_table\": \"main.default.source_table\",\n        \"pipeline_type\": \"TRIGGERED\",\n        \"embedding_source_columns\": [{\n            \"name\": \"text\",\n            \"embedding_model_endpoint_name\": this[\"name\"],\n        }],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sync = new Databricks.VectorSearchIndex(\"sync\", new()\n    {\n        Name = \"main.default.vector_search_index\",\n        EndpointName = thisDatabricksVectorSearchEndpoint.Name,\n        PrimaryKey = \"id\",\n        IndexType = \"DELTA_SYNC\",\n        DeltaSyncIndexSpec = new Databricks.Inputs.VectorSearchIndexDeltaSyncIndexSpecArgs\n        {\n            SourceTable = \"main.default.source_table\",\n            PipelineType = \"TRIGGERED\",\n            EmbeddingSourceColumns = new[]\n            {\n                new Databricks.Inputs.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs\n                {\n                    Name = \"text\",\n                    EmbeddingModelEndpointName = @this.Name,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewVectorSearchIndex(ctx, \"sync\", \u0026databricks.VectorSearchIndexArgs{\n\t\t\tName:         pulumi.String(\"main.default.vector_search_index\"),\n\t\t\tEndpointName: pulumi.Any(thisDatabricksVectorSearchEndpoint.Name),\n\t\t\tPrimaryKey:   pulumi.String(\"id\"),\n\t\t\tIndexType:    pulumi.String(\"DELTA_SYNC\"),\n\t\t\tDeltaSyncIndexSpec: \u0026databricks.VectorSearchIndexDeltaSyncIndexSpecArgs{\n\t\t\t\tSourceTable:  pulumi.String(\"main.default.source_table\"),\n\t\t\t\tPipelineType: pulumi.String(\"TRIGGERED\"),\n\t\t\t\tEmbeddingSourceColumns: databricks.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArray{\n\t\t\t\t\t\u0026databricks.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs{\n\t\t\t\t\t\tName:                       pulumi.String(\"text\"),\n\t\t\t\t\t\tEmbeddingModelEndpointName: pulumi.Any(this.Name),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.VectorSearchIndex;\nimport com.pulumi.databricks.VectorSearchIndexArgs;\nimport com.pulumi.databricks.inputs.VectorSearchIndexDeltaSyncIndexSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sync = new VectorSearchIndex(\"sync\", VectorSearchIndexArgs.builder()\n            .name(\"main.default.vector_search_index\")\n            .endpointName(thisDatabricksVectorSearchEndpoint.name())\n            .primaryKey(\"id\")\n            .indexType(\"DELTA_SYNC\")\n            .deltaSyncIndexSpec(VectorSearchIndexDeltaSyncIndexSpecArgs.builder()\n                .sourceTable(\"main.default.source_table\")\n                .pipelineType(\"TRIGGERED\")\n                .embeddingSourceColumns(VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs.builder()\n                    .name(\"text\")\n                    .embeddingModelEndpointName(this_.name())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sync:\n    type: databricks:VectorSearchIndex\n    properties:\n      name: main.default.vector_search_index\n      endpointName: ${thisDatabricksVectorSearchEndpoint.name}\n      primaryKey: id\n      indexType: DELTA_SYNC\n      deltaSyncIndexSpec:\n        sourceTable: main.default.source_table\n        pipelineType: TRIGGERED\n        embeddingSourceColumns:\n          - name: text\n            embeddingModelEndpointName: ${this.name}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Mosaic AI Vector Search Index\n\nbash\n\n```sh\n$ pulumi import databricks:index/vectorSearchIndex:VectorSearchIndex this \u003cindex-name\u003e\n```\n\n",
            "properties": {
                "creator": {
                    "type": "string",
                    "description": "Creator of the endpoint.\n"
                },
                "deltaSyncIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                    "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`. This field is a block and is documented below.\n"
                },
                "directAccessIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                    "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`. This field is a block and is documented below.\n"
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data.\n"
                },
                "indexType": {
                    "type": "string",
                    "description": "Mosaic AI Vector Search index type. Currently supported values are:\n* `DELTA_SYNC`: An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes.\n* `DIRECT_ACCESS`: An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Three-level name of the Mosaic AI Vector Search Index to create (`catalog.schema.index_name`).\n"
                },
                "primaryKey": {
                    "type": "string",
                    "description": "The column name that will be used as a primary key.\n"
                },
                "statuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus"
                    },
                    "description": "Object describing the current status of the index consisting of the following fields:\n"
                }
            },
            "required": [
                "creator",
                "endpointName",
                "indexType",
                "name",
                "primaryKey",
                "statuses"
            ],
            "inputProperties": {
                "deltaSyncIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                    "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`. This field is a block and is documented below.\n",
                    "willReplaceOnChanges": true
                },
                "directAccessIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                    "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`. This field is a block and is documented below.\n",
                    "willReplaceOnChanges": true
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data.\n",
                    "willReplaceOnChanges": true
                },
                "indexType": {
                    "type": "string",
                    "description": "Mosaic AI Vector Search index type. Currently supported values are:\n* `DELTA_SYNC`: An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes.\n* `DIRECT_ACCESS`: An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Three-level name of the Mosaic AI Vector Search Index to create (`catalog.schema.index_name`).\n",
                    "willReplaceOnChanges": true
                },
                "primaryKey": {
                    "type": "string",
                    "description": "The column name that will be used as a primary key.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "endpointName",
                "indexType",
                "primaryKey"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering VectorSearchIndex resources.\n",
                "properties": {
                    "creator": {
                        "type": "string",
                        "description": "Creator of the endpoint.\n"
                    },
                    "deltaSyncIndexSpec": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                        "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`. This field is a block and is documented below.\n",
                        "willReplaceOnChanges": true
                    },
                    "directAccessIndexSpec": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                        "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`. This field is a block and is documented below.\n",
                        "willReplaceOnChanges": true
                    },
                    "endpointName": {
                        "type": "string",
                        "description": "The name of the Mosaic AI Vector Search Endpoint that will be used for indexing the data.\n",
                        "willReplaceOnChanges": true
                    },
                    "indexType": {
                        "type": "string",
                        "description": "Mosaic AI Vector Search index type. Currently supported values are:\n* `DELTA_SYNC`: An index that automatically syncs with a source Delta Table, automatically and incrementally updating the index as the underlying data in the Delta Table changes.\n* `DIRECT_ACCESS`: An index that supports the direct read and write of vectors and metadata through our REST and SDK APIs. With this model, the user manages index updates.\n",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Three-level name of the Mosaic AI Vector Search Index to create (`catalog.schema.index_name`).\n",
                        "willReplaceOnChanges": true
                    },
                    "primaryKey": {
                        "type": "string",
                        "description": "The column name that will be used as a primary key.\n",
                        "willReplaceOnChanges": true
                    },
                    "statuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus"
                        },
                        "description": "Object describing the current status of the index consisting of the following fields:\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/volume:Volume": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nVolumes are Unity Catalog objects representing a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files. While tables provide governance over tabular datasets, volumes add governance over non-tabular datasets. You can use volumes to store and access files in any format, including structured, semi-structured, and unstructured data.\n\nA volume resides in the third layer of Unity Catalog’s three-level namespace. Volumes are siblings to tables, views, and other objects organized under a schema in Unity Catalog.\n\nA volume can be **managed** or **external**.\n\nA **managed volume** is a Unity Catalog-governed storage volume created within the default storage location of the containing schema. Managed volumes allow the creation of governed storage for working with files without the overhead of external locations and storage credentials. You do not need to specify a location when creating a managed volume, and all file access for data in managed volumes is through paths managed by Unity Catalog.\n\nAn **external volume** is a Unity Catalog-governed storage volume registered against a directory within an external location.\n\nA volume can be referenced using its identifier: ```\u003ccatalogName\u003e.\u003cschemaName\u003e.\u003cvolumeName\u003e```, where:\n\n* ```\u003ccatalogName\u003e```: The name of the catalog containing the Volume.\n* ```\u003cschemaName\u003e```: The name of the schema containing the Volume.\n* ```\u003cvolumeName\u003e```: The name of the Volume. It identifies the volume object.\n\nThe path to access files in volumes uses the following format:\n\n```/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/\u003cpath\u003e/\u003cfile_name\u003e```\n\nDatabricks also supports an optional ```dbfs:/``` scheme, so the following path also works:\n\n```dbfs:/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/\u003cpath\u003e/\u003cfile_name\u003e```\n\nThis resource manages Volumes in Unity Catalog.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.name,\n    name: \"things\",\n    comment: \"this schema is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst external = new databricks.StorageCredential(\"external\", {\n    name: \"creds\",\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n});\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external_location\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.name,\n});\nconst _this = new databricks.Volume(\"this\", {\n    name: \"quickstart_volume\",\n    catalogName: sandbox.name,\n    schemaName: things.name,\n    volumeType: \"EXTERNAL\",\n    storageLocation: some.url,\n    comment: \"this volume is managed by terraform\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.name,\n    name=\"things\",\n    comment=\"this schema is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nexternal = databricks.StorageCredential(\"external\",\n    name=\"creds\",\n    aws_iam_role={\n        \"role_arn\": external_data_access[\"arn\"],\n    })\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external_location\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external.name)\nthis = databricks.Volume(\"this\",\n    name=\"quickstart_volume\",\n    catalog_name=sandbox.name,\n    schema_name=things.name,\n    volume_type=\"EXTERNAL\",\n    storage_location=some.url,\n    comment=\"this volume is managed by terraform\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Name,\n        Name = \"things\",\n        Comment = \"this schema is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = \"creds\",\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n    });\n\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external_location\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Name,\n    });\n\n    var @this = new Databricks.Volume(\"this\", new()\n    {\n        Name = \"quickstart_volume\",\n        CatalogName = sandbox.Name,\n        SchemaName = things.Name,\n        VolumeType = \"EXTERNAL\",\n        StorageLocation = some.Url,\n        Comment = \"this volume is managed by terraform\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"purpose\": pulumi.String(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.Name,\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this schema is managed by terraform\"),\n\t\t\tProperties: pulumi.StringMap{\n\t\t\t\t\"kind\": pulumi.String(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.String(\"creds\"),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external_location\"),\n\t\t\tUrl:            pulumi.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id),\n\t\t\tCredentialName: external.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewVolume(ctx, \"this\", \u0026databricks.VolumeArgs{\n\t\t\tName:            pulumi.String(\"quickstart_volume\"),\n\t\t\tCatalogName:     sandbox.Name,\n\t\t\tSchemaName:      things.Name,\n\t\t\tVolumeType:      pulumi.String(\"EXTERNAL\"),\n\t\t\tStorageLocation: some.Url,\n\t\t\tComment:         pulumi.String(\"this volume is managed by terraform\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Volume;\nimport com.pulumi.databricks.VolumeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()\n            .catalogName(sandbox.name())\n            .name(\"things\")\n            .comment(\"this schema is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()\n            .name(\"creds\")\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .build());\n\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()\n            .name(\"external_location\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.name())\n            .build());\n\n        var this_ = new Volume(\"this\", VolumeArgs.builder()\n            .name(\"quickstart_volume\")\n            .catalogName(sandbox.name())\n            .schemaName(things.name())\n            .volumeType(\"EXTERNAL\")\n            .storageLocation(some.url())\n            .comment(\"this volume is managed by terraform\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.name}\n      name: things\n      comment: this schema is managed by terraform\n      properties:\n        kind: various\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: creds\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external_location\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.name}\n  this:\n    type: databricks:Volume\n    properties:\n      name: quickstart_volume\n      catalogName: ${sandbox.name}\n      schemaName: ${things.name}\n      volumeType: EXTERNAL\n      storageLocation: ${some.url}\n      comment: this volume is managed by terraform\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by `full_name` which is the 3-level Volume identifier: `\u003ccatalog\u003e.\u003cschema\u003e.\u003cname\u003e`\n\nbash\n\n```sh\n$ pulumi import databricks:index/volume:Volume this \u003ccatalog_name\u003e.\u003cschema_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent Catalog. Change forces creation of a new resource.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Volume\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the volume owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n"
                },
                "volumePath": {
                    "type": "string",
                    "description": "base file path for this Unity Catalog Volume in form of `/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cname\u003e`.\n"
                },
                "volumeType": {
                    "type": "string",
                    "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "catalogName",
                "name",
                "owner",
                "schemaName",
                "volumePath",
                "volumeType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent Catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Volume\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the volume owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "volumeType": {
                    "type": "string",
                    "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName",
                "volumeType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Volume resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent Catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Free-form text.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Volume\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the volume owner.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "volumePath": {
                        "type": "string",
                        "description": "base file path for this Unity Catalog Volume in form of `/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cname\u003e`.\n"
                    },
                    "volumeType": {
                        "type": "string",
                        "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceBinding:WorkspaceBinding": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nIf you use workspaces to isolate user data access, you may want to limit access to catalog, external locations or storage credentials from specific workspaces in your account, also known as workspace binding\n\nBy default, Databricks assigns the securable to all workspaces attached to the current metastore. By using `databricks.WorkspaceBinding`, the securable will be unassigned from all workspaces and only assigned explicitly using this resource.\n\n\u003e To use this resource the securable must have its isolation mode set to `ISOLATED` (for databricks_catalog) or `ISOLATION_MODE_ISOLATED` (for  (for databricks_external_location, databricks.StorageCredential or databricks_credential) for the `isolation_mode` attribute. Alternatively, the isolation mode can be set using the UI or API by following [this guide](https://docs.databricks.com/data-governance/unity-catalog/create-catalogs.html#configuration), [this guide](https://docs.databricks.com/en/connect/unity-catalog/external-locations.html#workspace-binding) or [this guide](https://docs.databricks.com/en/connect/unity-catalog/storage-credentials.html#optional-assign-a-storage-credential-to-specific-workspaces).\n\n\u003e If the securable's isolation mode was set to `ISOLATED` using Pulumi then the securable will have been automatically bound to the workspace it was created from.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    isolationMode: \"ISOLATED\",\n});\nconst sandboxWorkspaceBinding = new databricks.WorkspaceBinding(\"sandbox\", {\n    securableName: sandbox.name,\n    workspaceId: other.workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    isolation_mode=\"ISOLATED\")\nsandbox_workspace_binding = databricks.WorkspaceBinding(\"sandbox\",\n    securable_name=sandbox.name,\n    workspace_id=other[\"workspaceId\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        IsolationMode = \"ISOLATED\",\n    });\n\n    var sandboxWorkspaceBinding = new Databricks.WorkspaceBinding(\"sandbox\", new()\n    {\n        SecurableName = sandbox.Name,\n        WorkspaceId = other.WorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:          pulumi.String(\"sandbox\"),\n\t\t\tIsolationMode: pulumi.String(\"ISOLATED\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewWorkspaceBinding(ctx, \"sandbox\", \u0026databricks.WorkspaceBindingArgs{\n\t\t\tSecurableName: sandbox.Name,\n\t\t\tWorkspaceId:   pulumi.Any(other.WorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.WorkspaceBinding;\nimport com.pulumi.databricks.WorkspaceBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()\n            .name(\"sandbox\")\n            .isolationMode(\"ISOLATED\")\n            .build());\n\n        var sandboxWorkspaceBinding = new WorkspaceBinding(\"sandboxWorkspaceBinding\", WorkspaceBindingArgs.builder()\n            .securableName(sandbox.name())\n            .workspaceId(other.workspaceId())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      isolationMode: ISOLATED\n  sandboxWorkspaceBinding:\n    type: databricks:WorkspaceBinding\n    name: sandbox\n    properties:\n      securableName: ${sandbox.name}\n      workspaceId: ${other.workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by using combination of workspace ID, securable type and name:\n\n```sh\n$ pulumi import databricks:index/workspaceBinding:WorkspaceBinding this \"\u003cworkspace_id\u003e|\u003csecurable_type\u003e|\u003csecurable_name\u003e\"\n```\n\n",
            "properties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`.\n"
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead."
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n"
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Can be `catalog`, `external_location`, `storage_credential` or `credential`. Default to `catalog`. Change forces creation of a new resource.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "securableName"
            ],
            "inputProperties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`.\n",
                    "willReplaceOnChanges": true
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                    "willReplaceOnChanges": true
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n"
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Can be `catalog`, `external_location`, `storage_credential` or `credential`. Default to `catalog`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceBinding resources.\n",
                "properties": {
                    "bindingType": {
                        "type": "string",
                        "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`.\n",
                        "willReplaceOnChanges": true
                    },
                    "catalogName": {
                        "type": "string",
                        "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                        "willReplaceOnChanges": true
                    },
                    "securableName": {
                        "type": "string",
                        "description": "Name of securable. Change forces creation of a new resource.\n"
                    },
                    "securableType": {
                        "type": "string",
                        "description": "Type of securable. Can be `catalog`, `external_location`, `storage_credential` or `credential`. Default to `catalog`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "ID of the workspace. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceConf:WorkspaceConf": {
            "description": "\u003e This resource has an evolving API, which may change in future versions of the provider.\n\nManages workspace configuration for expert usage. Currently, more than one instance of resource can exist in Pulumi state, though there's no deterministic behavior, when they manage the same property. We strongly recommend to use a single `databricks.WorkspaceConf` per workspace.\n\n\u003e Deleting `databricks.WorkspaceConf` resources may fail depending on the configuration properties set, including but not limited to `enableIpAccessLists`, `enableGp3`, and `maxTokenLifetimeDays`. The provider will print a warning if this occurs. You can verify the workspace configuration by reviewing [the workspace settings in the UI](https://docs.databricks.com/en/admin/workspace-settings/index.html).\n\n## Example Usage\n\nAllows specification of custom configuration properties for expert usage:\n\n- `enableIpAccessLists` - enables the use of databricks.IpAccessList resources\n- `maxTokenLifetimeDays` - (string) Maximum token lifetime of new tokens in days, as an integer. This value can range from 1 day to 730 days (2 years). If not specified, the maximum lifetime of new tokens is 730 days. **WARNING:** This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set.\n- `enableTokensConfig` - (boolean) Enable or disable personal access tokens for this workspace.\n- `enableDeprecatedClusterNamedInitScripts` - (boolean) Enable or disable [legacy cluster-named init scripts](https://docs.databricks.com/clusters/init-scripts.html#disable-legacy-cluster-named-init-scripts-for-a-workspace) for this workspace.\n- `enableDeprecatedGlobalInitScripts` - (boolean) Enable or disable [legacy global init scripts](https://docs.databricks.com/clusters/init-scripts.html#migrate-legacy-scripts) for this workspace.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: \"true\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": \"true\",\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.StringMap{\n\t\t\t\t\"enableIpAccessLists\": pulumi.String(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()\n            .customConfig(Map.of(\"enableIpAccessLists\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\n!\u003e Importing this resource is not currently supported.\n\n",
            "properties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "inputProperties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "type": "string"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceConf resources.\n",
                "properties": {
                    "customConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceFile:WorkspaceFile": {
            "description": "This resource allows you to manage [Databricks Workspace Files](https://docs.databricks.com/files/workspace.html).\n\n## Import\n\nThe workspace file resource can be imported using workspace file path\n\nbash\n\n```sh\n$ pulumi import databricks:index/workspaceFile:WorkspaceFile this /path/to/file\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded file content. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a workspace file\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the workspace file\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "objectId",
                "path",
                "url",
                "workspacePath"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "The base64-encoded file content. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a workspace file\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "The base64-encoded file content. Conflicts with `source`. Use of `content_base64` is discouraged, as it's increasing memory footprint of Pulumi state and should only be used in exceptional circumstances, like creating a workspace file with configuration properties for a data pipeline.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a workspace file\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the workspace file\n"
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        }
    },
    "functions": {
        "databricks:index/getApp:getApp": {
            "description": "\u003e This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n[Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.\n\nThis data source allows you to fetch information about a Databricks App.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getApp({\n    name: \"my-custom-app\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_app(name=\"my-custom-app\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetApp.Invoke(new()\n    {\n        Name = \"my-custom-app\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupApp(ctx, \u0026databricks.LookupAppArgs{\n\t\t\tName: \"my-custom-app\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAppArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getApp(GetAppArgs.builder()\n            .name(\"my-custom-app\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getApp\n      arguments:\n        name: my-custom-app\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getApp.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "The name of the app.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getApp.\n",
                "properties": {
                    "app": {
                        "$ref": "#/types/databricks:index/getAppApp:getAppApp",
                        "description": "attribute\n"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Name of the serving endpoint to grant permission on.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "app",
                    "name",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getApps:getApps": {
            "description": "\u003e This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n[Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) run directly on a customer’s Databricks instance, integrate with their data, use and extend Databricks services, and enable users to interact through single sign-on. This resource creates the application but does not handle app deployment, which should be handled separately as part of your CI/CD pipeline.\n\nThis data source allows you to fetch information about all Databricks Apps within a workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allApps = databricks.getApps({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_apps = databricks.get_apps()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allApps = Databricks.GetApps.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetApps(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allApps = DatabricksFunctions.getApps(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n    }\n}\n```\n```yaml\nvariables:\n  allApps:\n    fn::invoke:\n      function: databricks:getApps\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.App to manage [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code.\n",
            "outputs": {
                "description": "A collection of values returned by getApps.\n",
                "properties": {
                    "apps": {
                        "items": {
                            "$ref": "#/types/databricks:index/getAppsApp:getAppsApp"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "apps",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy": {
            "description": "This data source constructs necessary AWS STS assume role policy for you.\n\n## Example Usage\n\nEnd-to-end example of provisioning Cross-account IAM role with databricks.MwsCredentials and aws_iam_role:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject\u003cany\u003e(\"databricksAccountId\");\nconst _this = databricks.getAwsCrossAccountPolicy({});\nconst crossAccountPolicy = new aws.iam.Policy(\"cross_account_policy\", {\n    name: `${prefix}-crossaccount-iam-policy`,\n    policy: _this.then(_this =\u003e _this.json),\n});\nconst thisGetAwsAssumeRolePolicy = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccount = new aws.iam.Role(\"cross_account\", {\n    name: `${prefix}-crossaccount-iam-role`,\n    assumeRolePolicy: thisGetAwsAssumeRolePolicy.then(thisGetAwsAssumeRolePolicy =\u003e thisGetAwsAssumeRolePolicy.json),\n    description: \"Grants Databricks full access to VPC resources\",\n});\nconst crossAccountRolePolicyAttachment = new aws.iam.RolePolicyAttachment(\"cross_account\", {\n    policyArn: crossAccountPolicy.arn,\n    role: crossAccount.name,\n});\n// required only in case of multi-workspace setup\nconst thisMwsCredentials = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossAccount.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nthis = databricks.get_aws_cross_account_policy()\ncross_account_policy = aws.iam.Policy(\"cross_account_policy\",\n    name=f\"{prefix}-crossaccount-iam-policy\",\n    policy=this.json)\nthis_get_aws_assume_role_policy = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account = aws.iam.Role(\"cross_account\",\n    name=f\"{prefix}-crossaccount-iam-role\",\n    assume_role_policy=this_get_aws_assume_role_policy.json,\n    description=\"Grants Databricks full access to VPC resources\")\ncross_account_role_policy_attachment = aws.iam.RolePolicyAttachment(\"cross_account\",\n    policy_arn=cross_account_policy.arn,\n    role=cross_account.name)\n# required only in case of multi-workspace setup\nthis_mws_credentials = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=cross_account.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var crossAccountPolicy = new Aws.Iam.Policy(\"cross_account_policy\", new()\n    {\n        Name = $\"{prefix}-crossaccount-iam-policy\",\n        PolicyDocument = @this.Apply(@this =\u003e @this.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json)),\n    });\n\n    var thisGetAwsAssumeRolePolicy = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccount = new Aws.Iam.Role(\"cross_account\", new()\n    {\n        Name = $\"{prefix}-crossaccount-iam-role\",\n        AssumeRolePolicy = thisGetAwsAssumeRolePolicy.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json),\n        Description = \"Grants Databricks full access to VPC resources\",\n    });\n\n    var crossAccountRolePolicyAttachment = new Aws.Iam.RolePolicyAttachment(\"cross_account\", new()\n    {\n        PolicyArn = crossAccountPolicy.Arn,\n        Role = crossAccount.Name,\n    });\n\n    // required only in case of multi-workspace setup\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossAccount.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tthis, err := databricks.GetAwsCrossAccountPolicy(ctx, \u0026databricks.GetAwsCrossAccountPolicyArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountPolicy, err := iam.NewPolicy(ctx, \"cross_account_policy\", \u0026iam.PolicyArgs{\n\t\t\tName:   pulumi.Sprintf(\"%v-crossaccount-iam-policy\", prefix),\n\t\t\tPolicy: pulumi.String(this.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsAssumeRolePolicy, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccount, err := iam.NewRole(ctx, \"cross_account\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v-crossaccount-iam-role\", prefix),\n\t\t\tAssumeRolePolicy: pulumi.String(thisGetAwsAssumeRolePolicy.Json),\n\t\t\tDescription:      pulumi.String(\"Grants Databricks full access to VPC resources\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"cross_account\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: crossAccountPolicy.Arn,\n\t\t\tRole:      crossAccount.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// required only in case of multi-workspace setup\n\t\t_, err = databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.Sprintf(\"%v-creds\", prefix),\n\t\t\tRoleArn:         crossAccount.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()\n            .build());\n\n        var crossAccountPolicy = new Policy(\"crossAccountPolicy\", PolicyArgs.builder()\n            .name(String.format(\"%s-crossaccount-iam-policy\", prefix))\n            .policy(this_.json())\n            .build());\n\n        final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccount = new Role(\"crossAccount\", RoleArgs.builder()\n            .name(String.format(\"%s-crossaccount-iam-role\", prefix))\n            .assumeRolePolicy(thisGetAwsAssumeRolePolicy.json())\n            .description(\"Grants Databricks full access to VPC resources\")\n            .build());\n\n        var crossAccountRolePolicyAttachment = new RolePolicyAttachment(\"crossAccountRolePolicyAttachment\", RolePolicyAttachmentArgs.builder()\n            .policyArn(crossAccountPolicy.arn())\n            .role(crossAccount.name())\n            .build());\n\n        // required only in case of multi-workspace setup\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()\n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossAccount.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  crossAccountPolicy:\n    type: aws:iam:Policy\n    name: cross_account_policy\n    properties:\n      name: ${prefix}-crossaccount-iam-policy\n      policy: ${this.json}\n  crossAccount:\n    type: aws:iam:Role\n    name: cross_account\n    properties:\n      name: ${prefix}-crossaccount-iam-role\n      assumeRolePolicy: ${thisGetAwsAssumeRolePolicy.json}\n      description: Grants Databricks full access to VPC resources\n  crossAccountRolePolicyAttachment:\n    type: aws:iam:RolePolicyAttachment\n    name: cross_account\n    properties:\n      policyArn: ${crossAccountPolicy.arn}\n      role: ${crossAccount.name}\n  # required only in case of multi-workspace setup\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossAccount.arn}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsCrossAccountPolicy\n      arguments: {}\n  thisGetAwsAssumeRolePolicy:\n    fn::invoke:\n      function: databricks:getAwsAssumeRolePolicy\n      arguments:\n        externalId: ${databricksAccountId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsAssumeRolePolicy.\n",
                "properties": {
                    "awsPartition": {
                        "type": "string",
                        "description": "AWS partition. The options are `aws`, `aws-us-gov`, or `aws-us-gov-dod`. Defaults to `aws`\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksAccountId": {
                        "type": "string",
                        "deprecationMessage": "databricks_account_id will be will be removed in the next major release.",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "forLogDelivery": {
                        "type": "boolean",
                        "description": "Either or not this assume role policy should be created for usage log delivery. Defaults to false.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "externalId"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsAssumeRolePolicy.\n",
                "properties": {
                    "awsPartition": {
                        "type": "string"
                    },
                    "databricksAccountId": {
                        "deprecationMessage": "databricks_account_id will be will be removed in the next major release.",
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "forLogDelivery": {
                        "type": "boolean"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    }
                },
                "required": [
                    "externalId",
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsBucketPolicy:getAwsBucketPolicy": {
            "description": "This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisBucketV2 = new aws.s3.BucketV2(\"this\", {\n    bucket: \"\u003cunique_bucket_name\u003e\",\n    forceDestroy: true,\n});\nconst _this = databricks.getAwsBucketPolicyOutput({\n    bucket: thisBucketV2.bucket,\n});\nconst thisBucketPolicy = new aws.s3.BucketPolicy(\"this\", {\n    bucket: thisBucketV2.id,\n    policy: _this.apply(_this =\u003e _this.json),\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nthis_bucket_v2 = aws.s3.BucketV2(\"this\",\n    bucket=\"\u003cunique_bucket_name\u003e\",\n    force_destroy=True)\nthis = databricks.get_aws_bucket_policy_output(bucket=this_bucket_v2.bucket)\nthis_bucket_policy = aws.s3.BucketPolicy(\"this\",\n    bucket=this_bucket_v2.id,\n    policy=this.json)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisBucketV2 = new Aws.S3.BucketV2(\"this\", new()\n    {\n        Bucket = \"\u003cunique_bucket_name\u003e\",\n        ForceDestroy = true,\n    });\n\n    var @this = Databricks.GetAwsBucketPolicy.Invoke(new()\n    {\n        Bucket = thisBucketV2.Bucket,\n    });\n\n    var thisBucketPolicy = new Aws.S3.BucketPolicy(\"this\", new()\n    {\n        Bucket = thisBucketV2.Id,\n        Policy = @this.Apply(@this =\u003e @this.Apply(getAwsBucketPolicyResult =\u003e getAwsBucketPolicyResult.Json)),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisBucketV2, err := s3.NewBucketV2(ctx, \"this\", \u0026s3.BucketV2Args{\n\t\t\tBucket:       pulumi.String(\"\u003cunique_bucket_name\u003e\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis := databricks.GetAwsBucketPolicyOutput(ctx, databricks.GetAwsBucketPolicyOutputArgs{\n\t\t\tBucket: thisBucketV2.Bucket,\n\t\t}, nil)\n\t\t_, err = s3.NewBucketPolicy(ctx, \"this\", \u0026s3.BucketPolicyArgs{\n\t\t\tBucket: thisBucketV2.ID(),\n\t\t\tPolicy: pulumi.String(this.ApplyT(func(this databricks.GetAwsBucketPolicyResult) (*string, error) {\n\t\t\t\treturn \u0026this.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput)),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;\nimport com.pulumi.aws.s3.BucketPolicy;\nimport com.pulumi.aws.s3.BucketPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisBucketV2 = new BucketV2(\"thisBucketV2\", BucketV2Args.builder()\n            .bucket(\"\u003cunique_bucket_name\u003e\")\n            .forceDestroy(true)\n            .build());\n\n        final var this = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()\n            .bucket(thisBucketV2.bucket())\n            .build());\n\n        var thisBucketPolicy = new BucketPolicy(\"thisBucketPolicy\", BucketPolicyArgs.builder()\n            .bucket(thisBucketV2.id())\n            .policy(this_.applyValue(_this_ -\u003e _this_.json()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisBucketV2:\n    type: aws:s3:BucketV2\n    name: this\n    properties:\n      bucket: \u003cunique_bucket_name\u003e\n      forceDestroy: true\n  thisBucketPolicy:\n    type: aws:s3:BucketPolicy\n    name: this\n    properties:\n      bucket: ${thisBucketV2.id}\n      policy: ${this.json}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsBucketPolicy\n      arguments:\n        bucket: ${thisBucketV2.bucket}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nBucket policy with full access:\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsBucketPolicy.\n",
                "properties": {
                    "awsPartition": {
                        "type": "string",
                        "description": "AWS partition. The options are `aws`, `aws-us-gov`, or `aws-us-gov-dod`. Defaults to `aws`\n",
                        "willReplaceOnChanges": true
                    },
                    "bucket": {
                        "type": "string",
                        "description": "AWS S3 Bucket name for which to generate the policy document.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksAccountId": {
                        "type": "string",
                        "deprecationMessage": "databricks_account_id will be will be removed in the next major release.",
                        "willReplaceOnChanges": true
                    },
                    "databricksE2AccountId": {
                        "type": "string",
                        "description": "Your Databricks account ID. Used to generate  restrictive IAM policies that will increase the security of your root bucket\n",
                        "willReplaceOnChanges": true
                    },
                    "fullAccessRole": {
                        "type": "string",
                        "description": "Data access role that can have full access for this bucket\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "bucket"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsBucketPolicy.\n",
                "properties": {
                    "awsPartition": {
                        "type": "string"
                    },
                    "bucket": {
                        "type": "string"
                    },
                    "databricksAccountId": {
                        "deprecationMessage": "databricks_account_id will be will be removed in the next major release.",
                        "type": "string"
                    },
                    "databricksE2AccountId": {
                        "type": "string"
                    },
                    "fullAccessRole": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "(Read-only) AWS IAM Policy JSON document to grant Databricks full access to bucket.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "bucket",
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy": {
            "description": "\u003e **Note** This data source can only be used with an account-level provider!\n\nThis data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n\n## Example Usage\n\nFor more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getAwsCrossAccountPolicy({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_cross_account_policy()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetAwsCrossAccountPolicy(ctx, \u0026databricks.GetAwsCrossAccountPolicyArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.builder()\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsCrossAccountPolicy\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks workspaces with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsCrossAccountPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string",
                        "description": "— Your AWS account ID, which is a number.\n",
                        "willReplaceOnChanges": true
                    },
                    "awsPartition": {
                        "type": "string",
                        "description": "AWS partition. The options are `aws`, `aws-us-gov`, or `aws-us-gov-dod`. Defaults to `aws`\n",
                        "willReplaceOnChanges": true
                    },
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of Data IAM role ARNs that are explicitly granted `iam:PassRole` action.\nThe below arguments are only valid for `restricted` policy type\n",
                        "willReplaceOnChanges": true
                    },
                    "policyType": {
                        "type": "string",
                        "description": "The type of cross account policy to generated: `managed` for Databricks-managed VPC and `customer` for customer-managed VPC, `restricted` for customer-managed VPC with policy restrictions\n",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "— AWS Region name for your VPC deployment, for example `us-west-2`.\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupId": {
                        "type": "string",
                        "description": "— ID of your AWS security group. When you add a security group restriction, you cannot reuse the cross-account IAM role or reference a credentials ID (`credentials_id`) for any other workspaces. For those other workspaces, you must create separate roles, policies, and credentials objects.\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "description": "— ID of the AWS VPC where you want to launch workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getAwsCrossAccountPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsPartition": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    },
                    "passRoles": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "policyType": {
                        "type": "string"
                    },
                    "region": {
                        "type": "string"
                    },
                    "securityGroupId": {
                        "type": "string"
                    },
                    "vpcId": {
                        "type": "string"
                    }
                },
                "required": [
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.\n\nThis data source constructs the necessary AWS Unity Catalog assume role policy for you.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getAwsUnityCatalogPolicy({\n    awsAccountId: awsAccountId,\n    bucketName: \"databricks-bucket\",\n    roleName: `${prefix}-uc-access`,\n    kmsName: \"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\",\n});\nconst thisGetAwsUnityCatalogAssumeRolePolicy = databricks.getAwsUnityCatalogAssumeRolePolicy({\n    awsAccountId: awsAccountId,\n    roleName: `${prefix}-uc-access`,\n    externalId: \"12345\",\n});\nconst unityMetastore = new aws.iam.Policy(\"unity_metastore\", {\n    name: `${prefix}-unity-catalog-metastore-access-iam-policy`,\n    policy: _this.then(_this =\u003e _this.json),\n});\nconst metastoreDataAccess = new aws.iam.Role(\"metastore_data_access\", {\n    name: `${prefix}-uc-access`,\n    assumeRolePolicy: thisGetAwsUnityCatalogAssumeRolePolicy.then(thisGetAwsUnityCatalogAssumeRolePolicy =\u003e thisGetAwsUnityCatalogAssumeRolePolicy.json),\n    managedPolicyArns: [unityMetastore.arn],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_unity_catalog_policy(aws_account_id=aws_account_id,\n    bucket_name=\"databricks-bucket\",\n    role_name=f\"{prefix}-uc-access\",\n    kms_name=\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\")\nthis_get_aws_unity_catalog_assume_role_policy = databricks.get_aws_unity_catalog_assume_role_policy(aws_account_id=aws_account_id,\n    role_name=f\"{prefix}-uc-access\",\n    external_id=\"12345\")\nunity_metastore = aws.iam.Policy(\"unity_metastore\",\n    name=f\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n    policy=this.json)\nmetastore_data_access = aws.iam.Role(\"metastore_data_access\",\n    name=f\"{prefix}-uc-access\",\n    assume_role_policy=this_get_aws_unity_catalog_assume_role_policy.json,\n    managed_policy_arns=[unity_metastore.arn])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsUnityCatalogPolicy.Invoke(new()\n    {\n        AwsAccountId = awsAccountId,\n        BucketName = \"databricks-bucket\",\n        RoleName = $\"{prefix}-uc-access\",\n        KmsName = \"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\",\n    });\n\n    var thisGetAwsUnityCatalogAssumeRolePolicy = Databricks.GetAwsUnityCatalogAssumeRolePolicy.Invoke(new()\n    {\n        AwsAccountId = awsAccountId,\n        RoleName = $\"{prefix}-uc-access\",\n        ExternalId = \"12345\",\n    });\n\n    var unityMetastore = new Aws.Iam.Policy(\"unity_metastore\", new()\n    {\n        Name = $\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n        PolicyDocument = @this.Apply(@this =\u003e @this.Apply(getAwsUnityCatalogPolicyResult =\u003e getAwsUnityCatalogPolicyResult.Json)),\n    });\n\n    var metastoreDataAccess = new Aws.Iam.Role(\"metastore_data_access\", new()\n    {\n        Name = $\"{prefix}-uc-access\",\n        AssumeRolePolicy = thisGetAwsUnityCatalogAssumeRolePolicy.Apply(getAwsUnityCatalogAssumeRolePolicyResult =\u003e getAwsUnityCatalogAssumeRolePolicyResult.Json),\n        ManagedPolicyArns = new[]\n        {\n            unityMetastore.Arn,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetAwsUnityCatalogPolicy(ctx, \u0026databricks.GetAwsUnityCatalogPolicyArgs{\n\t\t\tAwsAccountId: awsAccountId,\n\t\t\tBucketName:   \"databricks-bucket\",\n\t\t\tRoleName:     fmt.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tKmsName:      pulumi.StringRef(\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsUnityCatalogAssumeRolePolicy, err := databricks.GetAwsUnityCatalogAssumeRolePolicy(ctx, \u0026databricks.GetAwsUnityCatalogAssumeRolePolicyArgs{\n\t\t\tAwsAccountId: awsAccountId,\n\t\t\tRoleName:     fmt.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tExternalId:   \"12345\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tunityMetastore, err := iam.NewPolicy(ctx, \"unity_metastore\", \u0026iam.PolicyArgs{\n\t\t\tName:   pulumi.Sprintf(\"%v-unity-catalog-metastore-access-iam-policy\", prefix),\n\t\t\tPolicy: pulumi.String(this.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRole(ctx, \"metastore_data_access\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tAssumeRolePolicy: pulumi.String(thisGetAwsUnityCatalogAssumeRolePolicy.Json),\n\t\t\tManagedPolicyArns: pulumi.StringArray{\n\t\t\t\tunityMetastore.Arn,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;\nimport com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()\n            .awsAccountId(awsAccountId)\n            .bucketName(\"databricks-bucket\")\n            .roleName(String.format(\"%s-uc-access\", prefix))\n            .kmsName(\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\")\n            .build());\n\n        final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()\n            .awsAccountId(awsAccountId)\n            .roleName(String.format(\"%s-uc-access\", prefix))\n            .externalId(\"12345\")\n            .build());\n\n        var unityMetastore = new Policy(\"unityMetastore\", PolicyArgs.builder()\n            .name(String.format(\"%s-unity-catalog-metastore-access-iam-policy\", prefix))\n            .policy(this_.json())\n            .build());\n\n        var metastoreDataAccess = new Role(\"metastoreDataAccess\", RoleArgs.builder()\n            .name(String.format(\"%s-uc-access\", prefix))\n            .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())\n            .managedPolicyArns(unityMetastore.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  unityMetastore:\n    type: aws:iam:Policy\n    name: unity_metastore\n    properties:\n      name: ${prefix}-unity-catalog-metastore-access-iam-policy\n      policy: ${this.json}\n  metastoreDataAccess:\n    type: aws:iam:Role\n    name: metastore_data_access\n    properties:\n      name: ${prefix}-uc-access\n      assumeRolePolicy: ${thisGetAwsUnityCatalogAssumeRolePolicy.json}\n      managedPolicyArns:\n        - ${unityMetastore.arn}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsUnityCatalogPolicy\n      arguments:\n        awsAccountId: ${awsAccountId}\n        bucketName: databricks-bucket\n        roleName: ${prefix}-uc-access\n        kmsName: arn:aws:kms:us-west-2:111122223333:key/databricks-kms\n  thisGetAwsUnityCatalogAssumeRolePolicy:\n    fn::invoke:\n      function: databricks:getAwsUnityCatalogAssumeRolePolicy\n      arguments:\n        awsAccountId: ${awsAccountId}\n        roleName: ${prefix}-uc-access\n        externalId: '12345'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsUnityCatalogAssumeRolePolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string",
                        "description": "The Account ID of the current AWS account (not your Databricks account).\n",
                        "willReplaceOnChanges": true
                    },
                    "awsPartition": {
                        "type": "string",
                        "description": "AWS partition. The options are `aws`,`aws-us-gov` or `aws-us-gov-dod`. Defaults to `aws`\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "The storage credential external id.\n",
                        "willReplaceOnChanges": true
                    },
                    "roleName": {
                        "type": "string",
                        "description": "The name of the AWS IAM role to be created for Unity Catalog.\n",
                        "willReplaceOnChanges": true
                    },
                    "unityCatalogIamArn": {
                        "type": "string",
                        "description": "The Databricks Unity Catalog IAM Role ARN. Defaults to `arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL` on standard AWS partition selection, `arn:aws-us-gov:iam::044793339203:role/unity-catalog-prod-UCMasterRole-1QRFA8SGY15OJ` on GovCloud partition selection, and `arn:aws-us-gov:iam::170661010020:role/unity-catalog-prod-UCMasterRole-1DI6DL6ZP26AS` on GovCloud DoD partition selection\n"
                    }
                },
                "type": "object",
                "required": [
                    "awsAccountId",
                    "externalId",
                    "roleName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsUnityCatalogAssumeRolePolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsPartition": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "id": {
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document for assume role\n",
                        "type": "string"
                    },
                    "roleName": {
                        "type": "string"
                    },
                    "unityCatalogIamArn": {
                        "type": "string"
                    }
                },
                "required": [
                    "awsAccountId",
                    "externalId",
                    "id",
                    "json",
                    "roleName",
                    "unityCatalogIamArn"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.\n\nThis data source constructs the necessary AWS Unity Catalog policy for you.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getAwsUnityCatalogPolicy({\n    awsAccountId: awsAccountId,\n    bucketName: \"databricks-bucket\",\n    roleName: `${prefix}-uc-access`,\n    kmsName: \"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\",\n});\nconst thisGetAwsUnityCatalogAssumeRolePolicy = databricks.getAwsUnityCatalogAssumeRolePolicy({\n    awsAccountId: awsAccountId,\n    roleName: `${prefix}-uc-access`,\n    externalId: \"12345\",\n});\nconst unityMetastore = new aws.iam.Policy(\"unity_metastore\", {\n    name: `${prefix}-unity-catalog-metastore-access-iam-policy`,\n    policy: _this.then(_this =\u003e _this.json),\n});\nconst metastoreDataAccess = new aws.iam.Role(\"metastore_data_access\", {\n    name: `${prefix}-uc-access`,\n    assumeRolePolicy: thisGetAwsUnityCatalogAssumeRolePolicy.then(thisGetAwsUnityCatalogAssumeRolePolicy =\u003e thisGetAwsUnityCatalogAssumeRolePolicy.json),\n    managedPolicyArns: [unityMetastore.arn],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_unity_catalog_policy(aws_account_id=aws_account_id,\n    bucket_name=\"databricks-bucket\",\n    role_name=f\"{prefix}-uc-access\",\n    kms_name=\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\")\nthis_get_aws_unity_catalog_assume_role_policy = databricks.get_aws_unity_catalog_assume_role_policy(aws_account_id=aws_account_id,\n    role_name=f\"{prefix}-uc-access\",\n    external_id=\"12345\")\nunity_metastore = aws.iam.Policy(\"unity_metastore\",\n    name=f\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n    policy=this.json)\nmetastore_data_access = aws.iam.Role(\"metastore_data_access\",\n    name=f\"{prefix}-uc-access\",\n    assume_role_policy=this_get_aws_unity_catalog_assume_role_policy.json,\n    managed_policy_arns=[unity_metastore.arn])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsUnityCatalogPolicy.Invoke(new()\n    {\n        AwsAccountId = awsAccountId,\n        BucketName = \"databricks-bucket\",\n        RoleName = $\"{prefix}-uc-access\",\n        KmsName = \"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\",\n    });\n\n    var thisGetAwsUnityCatalogAssumeRolePolicy = Databricks.GetAwsUnityCatalogAssumeRolePolicy.Invoke(new()\n    {\n        AwsAccountId = awsAccountId,\n        RoleName = $\"{prefix}-uc-access\",\n        ExternalId = \"12345\",\n    });\n\n    var unityMetastore = new Aws.Iam.Policy(\"unity_metastore\", new()\n    {\n        Name = $\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n        PolicyDocument = @this.Apply(@this =\u003e @this.Apply(getAwsUnityCatalogPolicyResult =\u003e getAwsUnityCatalogPolicyResult.Json)),\n    });\n\n    var metastoreDataAccess = new Aws.Iam.Role(\"metastore_data_access\", new()\n    {\n        Name = $\"{prefix}-uc-access\",\n        AssumeRolePolicy = thisGetAwsUnityCatalogAssumeRolePolicy.Apply(getAwsUnityCatalogAssumeRolePolicyResult =\u003e getAwsUnityCatalogAssumeRolePolicyResult.Json),\n        ManagedPolicyArns = new[]\n        {\n            unityMetastore.Arn,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetAwsUnityCatalogPolicy(ctx, \u0026databricks.GetAwsUnityCatalogPolicyArgs{\n\t\t\tAwsAccountId: awsAccountId,\n\t\t\tBucketName:   \"databricks-bucket\",\n\t\t\tRoleName:     fmt.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tKmsName:      pulumi.StringRef(\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsUnityCatalogAssumeRolePolicy, err := databricks.GetAwsUnityCatalogAssumeRolePolicy(ctx, \u0026databricks.GetAwsUnityCatalogAssumeRolePolicyArgs{\n\t\t\tAwsAccountId: awsAccountId,\n\t\t\tRoleName:     fmt.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tExternalId:   \"12345\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tunityMetastore, err := iam.NewPolicy(ctx, \"unity_metastore\", \u0026iam.PolicyArgs{\n\t\t\tName:   pulumi.Sprintf(\"%v-unity-catalog-metastore-access-iam-policy\", prefix),\n\t\t\tPolicy: pulumi.String(this.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRole(ctx, \"metastore_data_access\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.Sprintf(\"%v-uc-access\", prefix),\n\t\t\tAssumeRolePolicy: pulumi.String(thisGetAwsUnityCatalogAssumeRolePolicy.Json),\n\t\t\tManagedPolicyArns: pulumi.StringArray{\n\t\t\t\tunityMetastore.Arn,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;\nimport com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()\n            .awsAccountId(awsAccountId)\n            .bucketName(\"databricks-bucket\")\n            .roleName(String.format(\"%s-uc-access\", prefix))\n            .kmsName(\"arn:aws:kms:us-west-2:111122223333:key/databricks-kms\")\n            .build());\n\n        final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()\n            .awsAccountId(awsAccountId)\n            .roleName(String.format(\"%s-uc-access\", prefix))\n            .externalId(\"12345\")\n            .build());\n\n        var unityMetastore = new Policy(\"unityMetastore\", PolicyArgs.builder()\n            .name(String.format(\"%s-unity-catalog-metastore-access-iam-policy\", prefix))\n            .policy(this_.json())\n            .build());\n\n        var metastoreDataAccess = new Role(\"metastoreDataAccess\", RoleArgs.builder()\n            .name(String.format(\"%s-uc-access\", prefix))\n            .assumeRolePolicy(thisGetAwsUnityCatalogAssumeRolePolicy.json())\n            .managedPolicyArns(unityMetastore.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  unityMetastore:\n    type: aws:iam:Policy\n    name: unity_metastore\n    properties:\n      name: ${prefix}-unity-catalog-metastore-access-iam-policy\n      policy: ${this.json}\n  metastoreDataAccess:\n    type: aws:iam:Role\n    name: metastore_data_access\n    properties:\n      name: ${prefix}-uc-access\n      assumeRolePolicy: ${thisGetAwsUnityCatalogAssumeRolePolicy.json}\n      managedPolicyArns:\n        - ${unityMetastore.arn}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getAwsUnityCatalogPolicy\n      arguments:\n        awsAccountId: ${awsAccountId}\n        bucketName: databricks-bucket\n        roleName: ${prefix}-uc-access\n        kmsName: arn:aws:kms:us-west-2:111122223333:key/databricks-kms\n  thisGetAwsUnityCatalogAssumeRolePolicy:\n    fn::invoke:\n      function: databricks:getAwsUnityCatalogAssumeRolePolicy\n      arguments:\n        awsAccountId: ${awsAccountId}\n        roleName: ${prefix}-uc-access\n        externalId: '12345'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsUnityCatalogPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string",
                        "description": "The Account ID of the current AWS account (not your Databricks account).\n",
                        "willReplaceOnChanges": true
                    },
                    "awsPartition": {
                        "type": "string",
                        "description": "AWS partition. The options are `aws`, `aws-us-gov`, or `aws-us-gov-dod`. Defaults to `aws`\n",
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "description": "The name of the S3 bucket used as root storage location for [managed tables](https://docs.databricks.com/data-governance/unity-catalog/index.html#managed-table) in Unity Catalog.\n",
                        "willReplaceOnChanges": true
                    },
                    "kmsName": {
                        "type": "string",
                        "description": "If encryption is enabled, provide the ARN of the KMS key that encrypts the S3 bucket contents. If encryption is disabled, do not provide this argument.\n",
                        "willReplaceOnChanges": true
                    },
                    "roleName": {
                        "type": "string",
                        "description": "The name of the AWS IAM role that you created in the previous step in the [official documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws).\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "awsAccountId",
                    "bucketName",
                    "roleName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsUnityCatalogPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsPartition": {
                        "type": "string"
                    },
                    "bucketName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    },
                    "kmsName": {
                        "type": "string"
                    },
                    "roleName": {
                        "type": "string"
                    }
                },
                "required": [
                    "awsAccountId",
                    "bucketName",
                    "json",
                    "roleName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getBudgetPolicies:getBudgetPolicies": {
            "description": "This data source can be used to fetch the list of budget policies. \n\n\u003e **Note** This data source can only be used with an account-level provider!\n\n\n## Example Usage\n\nGetting a list of all budget policies:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getBudgetPolicies({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_budget_policies()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetBudgetPolicies.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetBudgetPolicies(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getBudgetPolicies(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getBudgetPolicies\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "outputs": {
                "description": "A collection of values returned by getBudgetPolicies.\n",
                "properties": {
                    "budgetPolicies": {
                        "description": "The list of budget policy.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getBudgetPoliciesBudgetPolicy:getBudgetPoliciesBudgetPolicy"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "budgetPolicies",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getBudgetPolicy:getBudgetPolicy": {
            "description": "This data source can be used to get a single budget policy. \n\n\u003e **Note** This data source can only be used with an account-level provider!\n\n## Example Usage\n\nReferring to a budget policy by id:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getBudgetPolicy\n      arguments:\n        policyId: test\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getBudgetPolicy.\n",
                "properties": {
                    "customTags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getBudgetPolicyCustomTag:getBudgetPolicyCustomTag"
                        }
                    },
                    "policyName": {
                        "type": "string",
                        "description": "The name of the budget policy.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getBudgetPolicy.\n",
                "properties": {
                    "customTags": {
                        "items": {
                            "$ref": "#/types/databricks:index/getBudgetPolicyCustomTag:getBudgetPolicyCustomTag"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "policyId": {
                        "description": "The id of the budget policy.\n",
                        "type": "string"
                    },
                    "policyName": {
                        "description": "The name of the budget policy.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "policyId",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCatalog:getCatalog": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog\n\n## Example Usage\n\nRead  on a specific catalog `test`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst test = databricks.getCatalog({\n    name: \"test\",\n});\nconst things = new databricks.Grants(\"things\", {\n    catalog: test.then(test =\u003e test.name),\n    grants: [{\n        principal: \"sensitive\",\n        privileges: [\"USE_CATALOG\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest = databricks.get_catalog(name=\"test\")\nthings = databricks.Grants(\"things\",\n    catalog=test.name,\n    grants=[{\n        \"principal\": \"sensitive\",\n        \"privileges\": [\"USE_CATALOG\"],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var test = Databricks.GetCatalog.Invoke(new()\n    {\n        Name = \"test\",\n    });\n\n    var things = new Databricks.Grants(\"things\", new()\n    {\n        Catalog = test.Apply(getCatalogResult =\u003e getCatalogResult.Name),\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"sensitive\",\n                Privileges = new[]\n                {\n                    \"USE_CATALOG\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttest, err := databricks.LookupCatalog(ctx, \u0026databricks.LookupCatalogArgs{\n\t\t\tName: \"test\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"things\", \u0026databricks.GrantsArgs{\n\t\t\tCatalog: pulumi.String(test.Name),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"USE_CATALOG\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCatalogArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()\n            .name(\"test\")\n            .build());\n\n        var things = new Grants(\"things\", GrantsArgs.builder()\n            .catalog(test.name())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"sensitive\")\n                .privileges(\"USE_CATALOG\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  things:\n    type: databricks:Grants\n    properties:\n      catalog: ${test.name}\n      grants:\n        - principal: sensitive\n          privileges:\n            - USE_CATALOG\nvariables:\n  test:\n    fn::invoke:\n      function: databricks:getCatalog\n      arguments:\n        name: test\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Grant to manage grants within Unity Catalog.\n* databricks.getCatalogs to list all catalogs within Unity Catalog metastore.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCatalog.\n",
                "properties": {
                    "catalogInfo": {
                        "$ref": "#/types/databricks:index/getCatalogCatalogInfo:getCatalogCatalogInfo",
                        "description": "the [CatalogInfo](https://pkg.go.dev/github.com/databricks/databricks-sdk-go/service/catalog#CatalogInfo) object for a Unity Catalog catalog. This contains the following attributes (see ):\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "same as the `name`\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "name of the catalog\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getCatalog.\n",
                "properties": {
                    "catalogInfo": {
                        "$ref": "#/types/databricks:index/getCatalogCatalogInfo:getCatalogCatalogInfo",
                        "description": "the [CatalogInfo](https://pkg.go.dev/github.com/databricks/databricks-sdk-go/service/catalog#CatalogInfo) object for a Unity Catalog catalog. This contains the following attributes (see ):\n"
                    },
                    "id": {
                        "description": "same as the `name`\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Name of the catalog\n",
                        "type": "string"
                    }
                },
                "required": [
                    "catalogInfo",
                    "id",
                    "name"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCatalogs:getCatalogs": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.\n\n## Example Usage\n\nListing all catalogs:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getCatalogs({});\nexport const allCatalogs = all;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_catalogs()\npulumi.export(\"allCatalogs\", all)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetCatalogs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allCatalogs\"] = all,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetCatalogs(ctx, \u0026databricks.GetCatalogsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allCatalogs\", all)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCatalogsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getCatalogs(GetCatalogsArgs.builder()\n            .build());\n\n        ctx.export(\"allCatalogs\", all);\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getCatalogs\n      arguments: {}\noutputs:\n  allCatalogs: ${all}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCatalogs.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCatalogs.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Catalog names\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCluster:getCluster": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.\n\n## Example Usage\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\nconst allGetCluster = all.then(all =\u003e .reduce((__obj, [__key, __value]) =\u003e ({ ...__obj, [__key]: databricks.getCluster({\n    clusterId: __value,\n}) })));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\nall_get_cluster = {__key: databricks.get_cluster(cluster_id=__value) for __key, __value in all.ids}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n    var allGetCluster = ;\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "description": "The id of the cluster.\n"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "The exact name of the cluster to search. Can only be specified if there is exactly one cluster with the provided name.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "cluster ID\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "description": "Cluster name, which doesn’t have to be unique.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "cluster ID\n",
                        "type": "string"
                    }
                },
                "required": [
                    "clusterId",
                    "clusterInfo",
                    "clusterName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getClusterPolicy:getClusterPolicy": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_cluster_policy.\n\n## Example Usage\n\nReferring to a cluster policy by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst personal = databricks.getClusterPolicy({\n    name: \"Personal Compute\",\n});\nconst myCluster = new databricks.Cluster(\"my_cluster\", {policyId: personal.then(personal =\u003e personal.id)});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npersonal = databricks.get_cluster_policy(name=\"Personal Compute\")\nmy_cluster = databricks.Cluster(\"my_cluster\", policy_id=personal.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var personal = Databricks.GetClusterPolicy.Invoke(new()\n    {\n        Name = \"Personal Compute\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"my_cluster\", new()\n    {\n        PolicyId = personal.Apply(getClusterPolicyResult =\u003e getClusterPolicyResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpersonal, err := databricks.LookupClusterPolicy(ctx, \u0026databricks.LookupClusterPolicyArgs{\n\t\t\tName: pulumi.StringRef(\"Personal Compute\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"my_cluster\", \u0026databricks.ClusterArgs{\n\t\t\tPolicyId: pulumi.String(personal.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClusterPolicyArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()\n            .name(\"Personal Compute\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()\n            .policyId(personal.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    name: my_cluster\n    properties:\n      policyId: ${personal.id}\nvariables:\n  personal:\n    fn::invoke:\n      function: databricks:getClusterPolicy\n      arguments:\n        name: Personal Compute\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusterPolicy.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "Additional human-readable description of the cluster policy.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the cluster policy.\n"
                    },
                    "isDefault": {
                        "type": "boolean",
                        "description": "If true, policy is a default policy created and managed by Databricks.\n"
                    },
                    "maxClustersPerUser": {
                        "type": "integer",
                        "description": "Max number of clusters per user that can be active using this policy.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the cluster policy. The cluster policy must exist before this resource can be planned.\n"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "type": "string",
                        "description": "Policy definition JSON document expressed in Databricks [Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definitions).\n"
                    },
                    "policyFamilyId": {
                        "type": "string",
                        "description": "ID of the policy family.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusterPolicy.\n",
                "properties": {
                    "definition": {
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n",
                        "type": "string"
                    },
                    "description": {
                        "description": "Additional human-readable description of the cluster policy.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The id of the cluster policy.\n",
                        "type": "string"
                    },
                    "isDefault": {
                        "description": "If true, policy is a default policy created and managed by Databricks.\n",
                        "type": "boolean"
                    },
                    "maxClustersPerUser": {
                        "description": "Max number of clusters per user that can be active using this policy.\n",
                        "type": "integer"
                    },
                    "name": {
                        "type": "string"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "description": "Policy definition JSON document expressed in Databricks [Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definitions).\n",
                        "type": "string"
                    },
                    "policyFamilyId": {
                        "description": "ID of the policy family.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "definition",
                    "description",
                    "id",
                    "isDefault",
                    "maxClustersPerUser",
                    "name",
                    "policyFamilyDefinitionOverrides",
                    "policyFamilyId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getClusters:getClusters": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.\n\n## Example Usage\n\nRetrieve cluster IDs for all clusters:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nRetrieve cluster IDs for all clusters having \"Shared\" in the cluster name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getClusters({\n    clusterNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_clusters(cluster_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetClusters.Invoke(new()\n    {\n        ClusterNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tClusterNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .clusterNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments:\n        clusterNameContains: shared\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Filtering clusters\n\nListing clusters can be slow for workspaces containing many clusters. Use filters to limit the number of clusters returned for better performance. You can filter clusters by state, source, policy, or pinned status:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allRunningClusters = databricks.getClusters({\n    filterBy: {\n        clusterStates: [\"RUNNING\"],\n    },\n});\nconst allClustersWithPolicy = databricks.getClusters({\n    filterBy: {\n        policyId: \"1234-5678-9012\",\n    },\n});\nconst allApiClusters = databricks.getClusters({\n    filterBy: {\n        clusterSources: [\"API\"],\n    },\n});\nconst allPinnedClusters = databricks.getClusters({\n    filterBy: {\n        isPinned: true,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_running_clusters = databricks.get_clusters(filter_by={\n    \"cluster_states\": [\"RUNNING\"],\n})\nall_clusters_with_policy = databricks.get_clusters(filter_by={\n    \"policy_id\": \"1234-5678-9012\",\n})\nall_api_clusters = databricks.get_clusters(filter_by={\n    \"cluster_sources\": [\"API\"],\n})\nall_pinned_clusters = databricks.get_clusters(filter_by={\n    \"is_pinned\": True,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allRunningClusters = Databricks.GetClusters.Invoke(new()\n    {\n        FilterBy = new Databricks.Inputs.GetClustersFilterByInputArgs\n        {\n            ClusterStates = new[]\n            {\n                \"RUNNING\",\n            },\n        },\n    });\n\n    var allClustersWithPolicy = Databricks.GetClusters.Invoke(new()\n    {\n        FilterBy = new Databricks.Inputs.GetClustersFilterByInputArgs\n        {\n            PolicyId = \"1234-5678-9012\",\n        },\n    });\n\n    var allApiClusters = Databricks.GetClusters.Invoke(new()\n    {\n        FilterBy = new Databricks.Inputs.GetClustersFilterByInputArgs\n        {\n            ClusterSources = new[]\n            {\n                \"API\",\n            },\n        },\n    });\n\n    var allPinnedClusters = Databricks.GetClusters.Invoke(new()\n    {\n        FilterBy = new Databricks.Inputs.GetClustersFilterByInputArgs\n        {\n            IsPinned = true,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tFilterBy: databricks.GetClustersFilterBy{\n\t\t\t\tClusterStates: []string{\n\t\t\t\t\t\"RUNNING\",\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tFilterBy: databricks.GetClustersFilterBy{\n\t\t\t\tPolicyId: pulumi.StringRef(\"1234-5678-9012\"),\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tFilterBy: databricks.GetClustersFilterBy{\n\t\t\t\tClusterSources: []string{\n\t\t\t\t\t\"API\",\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tFilterBy: databricks.GetClustersFilterBy{\n\t\t\t\tIsPinned: pulumi.BoolRef(true),\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport com.pulumi.databricks.inputs.GetClustersFilterByArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allRunningClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .filterBy(GetClustersFilterByArgs.builder()\n                .clusterStates(\"RUNNING\")\n                .build())\n            .build());\n\n        final var allClustersWithPolicy = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .filterBy(GetClustersFilterByArgs.builder()\n                .policyId(\"1234-5678-9012\")\n                .build())\n            .build());\n\n        final var allApiClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .filterBy(GetClustersFilterByArgs.builder()\n                .clusterSources(\"API\")\n                .build())\n            .build());\n\n        final var allPinnedClusters = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .filterBy(GetClustersFilterByArgs.builder()\n                .isPinned(true)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allRunningClusters:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments:\n        filterBy:\n          clusterStates:\n            - RUNNING\n  allClustersWithPolicy:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments:\n        filterBy:\n          policyId: 1234-5678-9012\n  allApiClusters:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments:\n        filterBy:\n          clusterSources:\n            - API\n  allPinnedClusters:\n    fn::invoke:\n      function: databricks:getClusters\n      arguments:\n        filterBy:\n          isPinned: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string",
                        "description": "Only return databricks.Cluster ids that match the given name string.\n",
                        "willReplaceOnChanges": true
                    },
                    "filterBy": {
                        "$ref": "#/types/databricks:index/getClustersFilterBy:getClustersFilterBy",
                        "description": "Filters to apply to the listed clusters. See filter_by Configuration Block below for details.\n",
                        "willReplaceOnChanges": true
                    },
                    "id": {
                        "type": "string"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Cluster ids\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string"
                    },
                    "filterBy": {
                        "$ref": "#/types/databricks:index/getClustersFilterBy:getClustersFilterBy"
                    },
                    "id": {
                        "type": "string"
                    },
                    "ids": {
                        "description": "list of databricks.Cluster ids\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "id",
                    "ids"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentConfig:getCurrentConfig": {
            "description": "Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.\n\n## Example Usage\n\nCreate cloud-specific databricks_storage_credential:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nfunction singleOrNone\u003cT\u003e(elements: pulumi.Input\u003cT\u003e[]): pulumi.Input\u003cT\u003e {\n    if (elements.length != 1) {\n        throw new Error(\"singleOrNone expected input list to have a single element\");\n    }\n    return elements[0];\n}\n\nconst _this = databricks.getCurrentConfig({});\nconst external = new databricks.StorageCredential(\"external\", {\n    awsIamRole: singleOrNone(.map(entry =\u003e ({\n        roleArn: cloudCredentialId,\n    }))),\n    azureManagedIdentity: singleOrNone(.map(entry =\u003e ({\n        accessConnectorId: cloudCredentialId,\n    }))),\n    databricksGcpServiceAccount: singleOrNone(.map(entry =\u003e ({}))),\n    name: \"storage_cred\",\n    comment: \"Managed by TF\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndef single_or_none(elements):\n    if len(elements) != 1:\n        raise Exception(\"single_or_none expected input list to have a single element\")\n    return elements[0]\n\n\nthis = databricks.get_current_config()\nexternal = databricks.StorageCredential(\"external\",\n    aws_iam_role=single_or_none([{\n        \"roleArn\": cloud_credential_id,\n    } for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"aws\" else {\n        \"aws\": True,\n    }]]),\n    azure_managed_identity=single_or_none([{\n        \"accessConnectorId\": cloud_credential_id,\n    } for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"azure\" else {\n        \"azure\": True,\n    }]]),\n    databricks_gcp_service_account=single_or_none([{} for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"gcp\" else {\n        \"gcp\": True,\n    }]]),\n    name=\"storage_cred\",\n    comment=\"Managed by TF\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetCurrentConfig.Invoke();\n\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        AwsIamRole = Enumerable.Single(),\n        AzureManagedIdentity = Enumerable.Single(),\n        DatabricksGcpServiceAccount = Enumerable.Single(),\n        Name = \"storage_cred\",\n        Comment = \"Managed by TF\",\n    });\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Exported attributes\n\nData source exposes the following attributes:\n\n* `is_account` - Whether the provider is configured at account-level\n* `account_id` - Account Id if provider is configured at account-level\n* `host` - Host of the Databricks workspace or account console\n* `cloud_type` - Cloud type specified in the provider\n* `auth_type` - Auth type used by the provider\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCurrentConfig.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "authType": {
                        "type": "string"
                    },
                    "cloudType": {
                        "type": "string"
                    },
                    "host": {
                        "type": "string"
                    },
                    "isAccount": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCurrentConfig.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "authType": {
                        "type": "string"
                    },
                    "cloudType": {
                        "type": "string"
                    },
                    "host": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "isAccount": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "accountId",
                    "authType",
                    "cloudType",
                    "host",
                    "isAccount",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentMetastore:getCurrentMetastore": {
            "description": "Retrieves information about metastore attached to a given workspace.\n\n\u003e **Note** This is the workspace-level data source.\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.\n\n## Example Usage\n\nMetastoreSummary response for a metastore attached to the current workspace.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getCurrentMetastore({});\nexport const someMetastore = _this.then(_this =\u003e _this.metastoreInfo);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_current_metastore()\npulumi.export(\"someMetastore\", this.metastore_info)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetCurrentMetastore.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"someMetastore\"] = @this.Apply(@this =\u003e @this.Apply(getCurrentMetastoreResult =\u003e getCurrentMetastoreResult.MetastoreInfo)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetCurrentMetastore(ctx, \u0026databricks.GetCurrentMetastoreArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"someMetastore\", this.MetastoreInfo)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getCurrentMetastore(GetCurrentMetastoreArgs.builder()\n            .build());\n\n        ctx.export(\"someMetastore\", this_.metastoreInfo());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getCurrentMetastore\n      arguments: {}\noutputs:\n  someMetastore: ${this.metastoreInfo}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Metastore to get information for a metastore with a given ID.\n* databricks.getMetastores to get a mapping of name to id of all metastores.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCurrentMetastore.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "metastore ID. Will be `no_metastore` if there is no metastore assigned for the current workspace\n"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo",
                        "description": "summary about a metastore attached to the current workspace returned by [Get a metastore summary API](https://docs.databricks.com/api/workspace/metastores/summary). This contains the following attributes (check the API page for up-to-date details):\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCurrentMetastore.\n",
                "properties": {
                    "id": {
                        "description": "metastore ID. Will be `no_metastore` if there is no metastore assigned for the current workspace\n",
                        "type": "string"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo",
                        "description": "summary about a metastore attached to the current workspace returned by [Get a metastore summary API](https://docs.databricks.com/api/workspace/metastores/summary). This contains the following attributes (check the API page for up-to-date details):\n"
                    }
                },
                "required": [
                    "id",
                    "metastoreInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentUser:getCurrentUser": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.\n\n",
            "outputs": {
                "description": "A collection of values returned by getCurrentUser.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string"
                    },
                    "alphanumeric": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "home": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "repos": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string"
                    },
                    "workspaceUrl": {
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "alphanumeric",
                    "externalId",
                    "home",
                    "repos",
                    "userName",
                    "workspaceUrl",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDashboards:getDashboards": {
            "description": "This data source allows you to retrieve information about Databricks [Dashboards](https://docs.databricks.com/en/dashboards/index.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```yaml\nresources:\n  dashboardsPermissions:\n    type: databricks:Permissions\n    name: dashboards_permissions\n    properties:\n      depends:\n        - ${all}\n      dashboardId: ${range.value}\n      accessControls:\n        - groupName: Example Group\n          permissionLevel: CAN_MANAGE\n    options: {}\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getDashboards\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDashboards.\n",
                "properties": {
                    "dashboardNameContains": {
                        "type": "string",
                        "description": "A **case-insensitive** substring to filter Dashboards by their name.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getDashboards.\n",
                "properties": {
                    "dashboardNameContains": {
                        "type": "string"
                    },
                    "dashboards": {
                        "description": "A list of dashboards matching the specified criteria. Each element contains the following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getDashboardsDashboard:getDashboardsDashboard"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "dashboards",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDbfsFile:getDbfsFile": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst report = databricks.getDbfsFile({\n    path: \"dbfs:/reports/some.csv\",\n    limitFileSize: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nreport = databricks.get_dbfs_file(path=\"dbfs:/reports/some.csv\",\n    limit_file_size=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var report = Databricks.GetDbfsFile.Invoke(new()\n    {\n        Path = \"dbfs:/reports/some.csv\",\n        LimitFileSize = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDbfsFile(ctx, \u0026databricks.LookupDbfsFileArgs{\n\t\t\tPath:          \"dbfs:/reports/some.csv\",\n\t\t\tLimitFileSize: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()\n            .path(\"dbfs:/reports/some.csv\")\n            .limitFileSize(true)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  report:\n    fn::invoke:\n      function: databricks:getDbfsFile\n      arguments:\n        path: dbfs:/reports/some.csv\n        limitFileSize: 'true'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFile.\n",
                "properties": {
                    "limitFileSize": {
                        "type": "boolean",
                        "description": "Do not load content for files larger than 4MB.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file from which to get content.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "limitFileSize",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFile.\n",
                "properties": {
                    "content": {
                        "description": "base64-encoded file contents\n",
                        "type": "string"
                    },
                    "fileSize": {
                        "description": "size of the file in bytes\n",
                        "type": "integer"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "limitFileSize": {
                        "type": "boolean"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "required": [
                    "content",
                    "fileSize",
                    "limitFileSize",
                    "path",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDbfsFilePaths:getDbfsFilePaths": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst partitions = databricks.getDbfsFilePaths({\n    path: \"dbfs:/user/hive/default.db/table\",\n    recursive: false,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npartitions = databricks.get_dbfs_file_paths(path=\"dbfs:/user/hive/default.db/table\",\n    recursive=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var partitions = Databricks.GetDbfsFilePaths.Invoke(new()\n    {\n        Path = \"dbfs:/user/hive/default.db/table\",\n        Recursive = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetDbfsFilePaths(ctx, \u0026databricks.GetDbfsFilePathsArgs{\n\t\t\tPath:      \"dbfs:/user/hive/default.db/table\",\n\t\t\tRecursive: false,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()\n            .path(\"dbfs:/user/hive/default.db/table\")\n            .recursive(false)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  partitions:\n    fn::invoke:\n      function: databricks:getDbfsFilePaths\n      arguments:\n        path: dbfs:/user/hive/default.db/table\n        recursive: false\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFilePaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to perform listing\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or not recursively list all files\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFilePaths.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "path": {
                        "type": "string"
                    },
                    "pathLists": {
                        "description": "returns list of objects with `path` and `file_size` attributes in each\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList"
                        },
                        "type": "array"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "path",
                    "pathLists",
                    "recursive",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDirectory:getDirectory": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get information about a directory in a Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getDirectory({\n    path: \"/Production\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_directory(path=\"/Production\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetDirectory.Invoke(new()\n    {\n        Path = \"/Production\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDirectory(ctx, \u0026databricks.LookupDirectoryArgs{\n\t\t\tPath: \"/Production\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDirectoryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()\n            .path(\"/Production\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      function: databricks:getDirectory\n      arguments:\n        path: /Production\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDirectory.\n",
                "properties": {
                    "id": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "directory object ID\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Path to a directory in the workspace\n",
                        "willReplaceOnChanges": true
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object",
                "required": [
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDirectory.\n",
                "properties": {
                    "id": {
                        "type": "string"
                    },
                    "objectId": {
                        "description": "directory object ID\n",
                        "type": "integer"
                    },
                    "path": {
                        "type": "string"
                    },
                    "workspacePath": {
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n",
                        "type": "string"
                    }
                },
                "required": [
                    "id",
                    "objectId",
                    "path",
                    "workspacePath"
                ],
                "type": "object"
            }
        },
        "databricks:index/getExternalLocation:getExternalLocation": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\nRetrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.\n\n## Example Usage\n\nGetting details of an existing external location in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getExternalLocation({\n    name: \"this\",\n});\nexport const createdBy = _this.then(_this =\u003e _this.externalLocationInfo?.createdBy);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_external_location(name=\"this\")\npulumi.export(\"createdBy\", this.external_location_info.created_by)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetExternalLocation.Invoke(new()\n    {\n        Name = \"this\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"createdBy\"] = @this.Apply(@this =\u003e @this.Apply(getExternalLocationResult =\u003e getExternalLocationResult.ExternalLocationInfo?.CreatedBy)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupExternalLocation(ctx, \u0026databricks.LookupExternalLocationArgs{\n\t\t\tName: \"this\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"createdBy\", this.ExternalLocationInfo.CreatedBy)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetExternalLocationArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()\n            .name(\"this\")\n            .build());\n\n        ctx.export(\"createdBy\", this_.externalLocationInfo().createdBy());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getExternalLocation\n      arguments:\n        name: this\noutputs:\n  createdBy: ${this.externalLocationInfo.createdBy}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getExternalLocations to get names of all external locations\n* databricks.ExternalLocation to manage external locations within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getExternalLocation.\n",
                "properties": {
                    "externalLocationInfo": {
                        "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo",
                        "description": "array of objects with information about external location:\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "external location ID - same as name.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the external location\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getExternalLocation.\n",
                "properties": {
                    "externalLocationInfo": {
                        "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo",
                        "description": "array of objects with information about external location:\n"
                    },
                    "id": {
                        "description": "external location ID - same as name.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    }
                },
                "required": [
                    "externalLocationInfo",
                    "id",
                    "name"
                ],
                "type": "object"
            }
        },
        "databricks:index/getExternalLocations:getExternalLocations": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\nRetrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.\n\n## Example Usage\n\nList all external locations in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getExternalLocations({});\nexport const allExternalLocations = all.then(all =\u003e all.names);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_external_locations()\npulumi.export(\"allExternalLocations\", all.names)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetExternalLocations.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allExternalLocations\"] = all.Apply(getExternalLocationsResult =\u003e getExternalLocationsResult.Names),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetExternalLocations(ctx, \u0026databricks.GetExternalLocationsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allExternalLocations\", all.Names)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetExternalLocationsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getExternalLocations(GetExternalLocationsArgs.builder()\n            .build());\n\n        ctx.export(\"allExternalLocations\", all.names());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getExternalLocations\n      arguments: {}\noutputs:\n  allExternalLocations: ${all.names}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.ExternalLocation to get information about a single external location\n* databricks.ExternalLocation to manage external locations within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getExternalLocations.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks.ExternalLocation in the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getExternalLocations.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks.ExternalLocation in the metastore\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getFunctions:getFunctions": {
            "description": "\u003e This data source can only be used with a workspace-level provider!\n\nRetrieves a list of [User-Defined Functions (UDFs) registered in the Unity Catalog](https://docs.databricks.com/en/udf/unity-catalog.html).\n\n## Example Usage\n\nList all functions defined in a specific schema (`main.default` in this example):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getFunctions({\n    catalogName: \"main\",\n    schemaName: \"default\",\n});\nexport const allExternalLocations = all.then(all =\u003e all.functions);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_functions(catalog_name=\"main\",\n    schema_name=\"default\")\npulumi.export(\"allExternalLocations\", all.functions)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetFunctions.Invoke(new()\n    {\n        CatalogName = \"main\",\n        SchemaName = \"default\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allExternalLocations\"] = all.Apply(getFunctionsResult =\u003e getFunctionsResult.Functions),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetFunctions(ctx, \u0026databricks.GetFunctionsArgs{\n\t\t\tCatalogName: \"main\",\n\t\t\tSchemaName:  \"default\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allExternalLocations\", all.Functions)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetFunctionsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getFunctions(GetFunctionsArgs.builder()\n            .catalogName(\"main\")\n            .schemaName(\"default\")\n            .build());\n\n        ctx.export(\"allExternalLocations\", all.functions());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getFunctions\n      arguments:\n        catalogName: main\n        schemaName: default\noutputs:\n  allExternalLocations: ${all.functions}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to get information about a single schema\n",
            "inputs": {
                "description": "A collection of arguments for invoking getFunctions.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog.\n"
                    },
                    "functions": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getFunctionsFunction:getFunctionsFunction"
                        },
                        "description": "list of objects describing individual UDF. Each object consists of the following attributes (refer to [REST API documentation](https://docs.databricks.com/api/workspace/functions/list#functions) for up-to-date list of attributes. Default type is String):\n"
                    },
                    "includeBrowse": {
                        "type": "boolean",
                        "description": "flag to specify if include UDFs in the response for which the principal can only access selective metadata for.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema.\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getFunctions.\n",
                "properties": {
                    "catalogName": {
                        "description": "Name of parent catalog.\n",
                        "type": "string"
                    },
                    "functions": {
                        "description": "list of objects describing individual UDF. Each object consists of the following attributes (refer to [REST API documentation](https://docs.databricks.com/api/workspace/functions/list#functions) for up-to-date list of attributes. Default type is String):\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getFunctionsFunction:getFunctionsFunction"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "includeBrowse": {
                        "type": "boolean"
                    },
                    "schemaName": {
                        "description": "Name of parent schema relative to its parent catalog.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "functions",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getGroup:getGroup": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks.Group members, entitlements and instance profiles.\n\n## Example Usage\n\nAdding user to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getGroup.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n",
                        "willReplaceOnChanges": true
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n",
                        "willReplaceOnChanges": true
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the group. The group must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Collect information for all nested groups. *Defaults to true.*\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "displayName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getGroup.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n",
                        "type": "string"
                    },
                    "allowClusterCreate": {
                        "description": "True if group members can create clusters\n",
                        "type": "boolean"
                    },
                    "allowInstancePoolCreate": {
                        "description": "True if group members can create instance pools\n",
                        "type": "boolean"
                    },
                    "childGroups": {
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the group in an external identity provider.\n",
                        "type": "string"
                    },
                    "groups": {
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "instanceProfiles": {
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "members": {
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "recursive": {
                        "type": "boolean"
                    },
                    "servicePrincipals": {
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "users": {
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "childGroups",
                    "displayName",
                    "externalId",
                    "groups",
                    "instanceProfiles",
                    "members",
                    "servicePrincipals",
                    "users",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getInstancePool:getInstancePool": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_instance_pool.\n\n## Example Usage\n\nReferring to an instance pool by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst pool = databricks.getInstancePool({\n    name: \"All spot\",\n});\nconst myCluster = new databricks.Cluster(\"my_cluster\", {instancePoolId: pool.then(pool =\u003e pool.id)});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npool = databricks.get_instance_pool(name=\"All spot\")\nmy_cluster = databricks.Cluster(\"my_cluster\", instance_pool_id=pool.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var pool = Databricks.GetInstancePool.Invoke(new()\n    {\n        Name = \"All spot\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"my_cluster\", new()\n    {\n        InstancePoolId = pool.Apply(getInstancePoolResult =\u003e getInstancePoolResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpool, err := databricks.LookupInstancePool(ctx, \u0026databricks.LookupInstancePoolArgs{\n\t\t\tName: \"All spot\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"my_cluster\", \u0026databricks.ClusterArgs{\n\t\t\tInstancePoolId: pulumi.String(pool.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetInstancePoolArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()\n            .name(\"All spot\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()\n            .instancePoolId(pool.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    name: my_cluster\n    properties:\n      instancePoolId: ${pool.id}\nvariables:\n  pool:\n    fn::invoke:\n      function: databricks:getInstancePool\n      arguments:\n        name: All spot\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getInstancePool.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the instance pool. The instance pool must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getInstancePool.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "required": [
                    "name",
                    "poolInfo",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getInstanceProfiles:getInstanceProfiles": {
            "description": "Lists all available databricks_instance_profiles.\n\n## Example Usage\n\nGet all instance profiles:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getInstanceProfiles({});\nexport const allInstanceProfiles = all.then(all =\u003e all.instanceProfiles);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_instance_profiles()\npulumi.export(\"allInstanceProfiles\", all.instance_profiles)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetInstanceProfiles.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allInstanceProfiles\"] = all.Apply(getInstanceProfilesResult =\u003e getInstanceProfilesResult.InstanceProfiles),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetInstanceProfiles(ctx, \u0026databricks.GetInstanceProfilesArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allInstanceProfiles\", all.InstanceProfiles)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetInstanceProfilesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getInstanceProfiles(GetInstanceProfilesArgs.builder()\n            .build());\n\n        ctx.export(\"allInstanceProfiles\", all.instanceProfiles());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getInstanceProfiles\n      arguments: {}\noutputs:\n  allInstanceProfiles: ${all.instanceProfiles}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getInstanceProfiles.\n",
                "properties": {
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile"
                        },
                        "description": "Set of objects for a databricks_instance_profile. This contains the following attributes:\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getInstanceProfiles.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "instanceProfiles": {
                        "description": "Set of objects for a databricks_instance_profile. This contains the following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "instanceProfiles",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getJob:getJob": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.\n\n## Example Usage\n\nGetting the existing cluster id of specific databricks.Job by name or by id:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getJob({\n    jobName: \"My job\",\n});\nexport const jobNumWorkers = _this.then(_this =\u003e _this.jobSettings?.settings?.newCluster?.numWorkers);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_job(job_name=\"My job\")\npulumi.export(\"jobNumWorkers\", this.job_settings.settings.new_cluster.num_workers)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetJob.Invoke(new()\n    {\n        JobName = \"My job\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"jobNumWorkers\"] = @this.Apply(@this =\u003e @this.Apply(getJobResult =\u003e getJobResult.JobSettings?.Settings?.NewCluster?.NumWorkers)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupJob(ctx, \u0026databricks.LookupJobArgs{\n\t\t\tJobName: pulumi.StringRef(\"My job\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"jobNumWorkers\", this.JobSettings.Settings.NewCluster.NumWorkers)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJob(GetJobArgs.builder()\n            .jobName(\"My job\")\n            .build());\n\n        ctx.export(\"jobNumWorkers\", this_.jobSettings().settings().newCluster().numWorkers());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getJob\n      arguments:\n        jobName: My job\noutputs:\n  jobNumWorkers: ${this.jobSettings.settings.newCluster.numWorkers}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getJobs data to get all jobs and their names from a workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJob.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "the id of databricks.Job if the resource was matched by name.\n"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "the job name of databricks.Job if the resource was matched by id.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJob.\n",
                "properties": {
                    "id": {
                        "description": "the id of databricks.Job if the resource was matched by name.\n",
                        "type": "string"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "description": "the job name of databricks.Job if the resource was matched by id.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "id",
                    "jobId",
                    "jobName",
                    "jobSettings",
                    "name"
                ],
                "type": "object"
            }
        },
        "databricks:index/getJobs:getJobs": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.\n\n\u003e **Note** Data resource will error in case of jobs with duplicate names.\n\n## Example Usage\n\nGranting view databricks.Permissions to all databricks.Job within the workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const _this = await databricks.getJobs({});\n    const tests = await databricks.getJobs({\n        jobNameContains: \"test\",\n    });\n    const everyoneCanViewAllJobs: databricks.Permissions[] = [];\n    for (const range of Object.entries(_this.ids).map(([k, v]) =\u003e ({key: k, value: v}))) {\n        everyoneCanViewAllJobs.push(new databricks.Permissions(`everyone_can_view_all_jobs-${range.key}`, {\n            jobId: range.value,\n            accessControls: [{\n                groupName: \"users\",\n                permissionLevel: \"CAN_VIEW\",\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_jobs()\ntests = databricks.get_jobs(job_name_contains=\"test\")\neveryone_can_view_all_jobs = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(this.ids)]:\n    everyone_can_view_all_jobs.append(databricks.Permissions(f\"everyone_can_view_all_jobs-{range['key']}\",\n        job_id=range[\"value\"],\n        access_controls=[{\n            \"group_name\": \"users\",\n            \"permission_level\": \"CAN_VIEW\",\n        }]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var @this = await Databricks.GetJobs.InvokeAsync();\n\n    var tests = await Databricks.GetJobs.InvokeAsync(new()\n    {\n        JobNameContains = \"test\",\n    });\n\n    var everyoneCanViewAllJobs = new List\u003cDatabricks.Permissions\u003e();\n    foreach (var range in )\n    {\n        everyoneCanViewAllJobs.Add(new Databricks.Permissions($\"everyone_can_view_all_jobs-{range.Key}\", new()\n        {\n            JobId = range.Value,\n            AccessControls = new[]\n            {\n                new Databricks.Inputs.PermissionsAccessControlArgs\n                {\n                    GroupName = \"users\",\n                    PermissionLevel = \"CAN_VIEW\",\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetJobs(ctx, \u0026databricks.GetJobsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.GetJobs(ctx, \u0026databricks.GetJobsArgs{\n\t\t\tJobNameContains: pulumi.StringRef(\"test\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar everyoneCanViewAllJobs []*databricks.Permissions\n\t\tfor key0, val0 := range this.Ids {\n\t\t\t__res, err := databricks.NewPermissions(ctx, fmt.Sprintf(\"everyone_can_view_all_jobs-%v\", key0), \u0026databricks.PermissionsArgs{\n\t\t\t\tJobId: pulumi.String(val0),\n\t\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\teveryoneCanViewAllJobs = append(everyoneCanViewAllJobs, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobsArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()\n            .build());\n\n        final var tests = DatabricksFunctions.getJobs(GetJobsArgs.builder()\n            .jobNameContains(\"test\")\n            .build());\n\n        final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -\u003e {\n            final var resources = new ArrayList\u003cPermissions\u003e();\n            for (var range : KeyedValue.of(getJobsResult.ids())) {\n                var resource = new Permissions(\"everyoneCanViewAllJobs-\" + range.key(), PermissionsArgs.builder()\n                    .jobId(range.value())\n                    .accessControls(PermissionsAccessControlArgs.builder()\n                        .groupName(\"users\")\n                        .permissionLevel(\"CAN_VIEW\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  everyoneCanViewAllJobs:\n    type: databricks:Permissions\n    name: everyone_can_view_all_jobs\n    properties:\n      jobId: ${range.value}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n    options: {}\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getJobs\n      arguments: {}\n  tests:\n    fn::invoke:\n      function: databricks:getJobs\n      arguments:\n        jobNameContains: test\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nGetting ID of specific databricks.Job by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getJobs({});\nexport const x = _this.then(_this =\u003e `ID of `x` job is ${_this.ids?.x}`);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_jobs()\npulumi.export(\"x\", f\"ID of `x` job is {this.ids['x']}\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetJobs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"x\"] = @this.Apply(@this =\u003e $\"ID of `x` job is {@this.Apply(getJobsResult =\u003e getJobsResult.Ids?.X)}\"),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetJobs(ctx, \u0026databricks.GetJobsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"x\", pulumi.Sprintf(\"ID of `x` job is %v\", this.Ids.X))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJobs(GetJobsArgs.builder()\n            .build());\n\n        ctx.export(\"x\", String.format(\"ID of `x` job is %s\", this_.ids().x()));\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getJobs\n      arguments: {}\noutputs:\n  x: ID of `x` job is ${this.ids.x}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJobs.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    },
                    "jobNameContains": {
                        "type": "string",
                        "description": "Only return databricks.Job ids that match the given name string (case-insensitive).\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJobs.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "map of databricks.Job names to ids\n",
                        "type": "object"
                    },
                    "jobNameContains": {
                        "type": "string"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMetastore:getMetastore": {
            "description": "\u003e **Note** This data source can only be used with an account-level provider!\n\nRetrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.\n\n## Example Usage\n\nMetastoreInfo response for a given metastore id\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst metastore = new aws.s3.BucketV2(\"metastore\", {\n    bucket: `${prefix}-metastore`,\n    forceDestroy: true,\n});\nconst thisMetastore = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: pulumi.interpolate`s3://${metastore.id}/metastore`,\n    owner: unityAdminGroup,\n    forceDestroy: true,\n});\nconst _this = databricks.getMetastoreOutput({\n    metastoreId: thisMetastore.id,\n});\nexport const someMetastore = _this.apply(_this =\u003e _this.metastoreInfo);\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nmetastore = aws.s3.BucketV2(\"metastore\",\n    bucket=f\"{prefix}-metastore\",\n    force_destroy=True)\nthis_metastore = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=metastore.id.apply(lambda id: f\"s3://{id}/metastore\"),\n    owner=unity_admin_group,\n    force_destroy=True)\nthis = databricks.get_metastore_output(metastore_id=this_metastore.id)\npulumi.export(\"someMetastore\", this.metastore_info)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var metastore = new Aws.S3.BucketV2(\"metastore\", new()\n    {\n        Bucket = $\"{prefix}-metastore\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastore = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = metastore.Id.Apply(id =\u003e $\"s3://{id}/metastore\"),\n        Owner = unityAdminGroup,\n        ForceDestroy = true,\n    });\n\n    var @this = Databricks.GetMetastore.Invoke(new()\n    {\n        MetastoreId = thisMetastore.Id,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"someMetastore\"] = @this.Apply(@this =\u003e @this.Apply(getMetastoreResult =\u003e getMetastoreResult.MetastoreInfo)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmetastore, err := s3.NewBucketV2(ctx, \"metastore\", \u0026s3.BucketV2Args{\n\t\t\tBucket:       pulumi.Sprintf(\"%v-metastore\", prefix),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMetastore, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName: pulumi.String(\"primary\"),\n\t\t\tStorageRoot: metastore.ID().ApplyT(func(id string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"s3://%v/metastore\", id), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tOwner:        pulumi.Any(unityAdminGroup),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis := databricks.LookupMetastoreOutput(ctx, databricks.GetMetastoreOutputArgs{\n\t\t\tMetastoreId: thisMetastore.ID(),\n\t\t}, nil)\n\t\tctx.Export(\"someMetastore\", this.ApplyT(func(this databricks.GetMetastoreResult) (databricks.GetMetastoreMetastoreInfo, error) {\n\t\t\treturn this.MetastoreInfo, nil\n\t\t}).(databricks.GetMetastoreMetastoreInfoOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMetastoreArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var metastore = new BucketV2(\"metastore\", BucketV2Args.builder()\n            .bucket(String.format(\"%s-metastore\", prefix))\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastore = new Metastore(\"thisMetastore\", MetastoreArgs.builder()\n            .name(\"primary\")\n            .storageRoot(metastore.id().applyValue(_id -\u003e String.format(\"s3://%s/metastore\", _id)))\n            .owner(unityAdminGroup)\n            .forceDestroy(true)\n            .build());\n\n        final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()\n            .metastoreId(thisMetastore.id())\n            .build());\n\n        ctx.export(\"someMetastore\", this_.applyValue(_this_ -\u003e _this_.metastoreInfo()));\n    }\n}\n```\n```yaml\nresources:\n  metastore:\n    type: aws:s3:BucketV2\n    properties:\n      bucket: ${prefix}-metastore\n      forceDestroy: true\n  thisMetastore:\n    type: databricks:Metastore\n    name: this\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: ${unityAdminGroup}\n      forceDestroy: true\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMetastore\n      arguments:\n        metastoreId: ${thisMetastore.id}\noutputs:\n  someMetastore: ${this.metastoreInfo}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getMetastores to get mapping of name to id of all metastores.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMetastore.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "ID of the metastore\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "ID of the metastore\n"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo",
                        "description": "MetastoreInfo object for a databricks_metastore. This contains the following attributes:\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the metastore\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMetastore.\n",
                "properties": {
                    "id": {
                        "description": "ID of the metastore\n",
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo",
                        "description": "MetastoreInfo object for a databricks_metastore. This contains the following attributes:\n"
                    },
                    "name": {
                        "description": "Name of metastore.\n",
                        "type": "string"
                    },
                    "region": {
                        "type": "string"
                    }
                },
                "required": [
                    "id",
                    "metastoreId",
                    "metastoreInfo",
                    "name",
                    "region"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMetastores:getMetastores": {
            "description": "\u003e **Note** This data source can only be used with an account-level provider!\n\nRetrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users \u0026 service principals with account admin status\n\n## Example Usage\n\nMapping of name to id of all metastores:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMetastores({});\nexport const allMetastores = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_metastores()\npulumi.export(\"allMetastores\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMetastores.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMetastores\"] = all.Apply(getMetastoresResult =\u003e getMetastoresResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetMetastores(ctx, \u0026databricks.GetMetastoresArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMetastores\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMetastoresArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMetastores(GetMetastoresArgs.builder()\n            .build());\n\n        ctx.export(\"allMetastores\", all.ids());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getMetastores\n      arguments: {}\noutputs:\n  allMetastores: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Metastore to get information about a single metastore.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMetastores.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Mapping of name to id of databricks_metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMetastores.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "Mapping of name to id of databricks_metastore\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMlflowExperiment:getMlflowExperiment": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves the settings of databricks.MlflowExperiment by id or name.\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMlflowExperiment.\n",
                "properties": {
                    "artifactLocation": {
                        "type": "string",
                        "description": "Location where artifacts for the experiment are stored.\n"
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "Creation time in unix time stamp.\n"
                    },
                    "experimentId": {
                        "type": "string",
                        "description": "Unique identifier for the experiment.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "Unique identifier for the experiment. (same as `experiment_id`)\n"
                    },
                    "lastUpdateTime": {
                        "type": "integer",
                        "description": "Last update time in unix time stamp.\n"
                    },
                    "lifecycleStage": {
                        "type": "string",
                        "description": "Current life cycle stage of the experiment: `active` or `deleted`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Path to experiment.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowExperimentTag:getMlflowExperimentTag"
                        },
                        "description": "Additional metadata key-value pairs.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMlflowExperiment.\n",
                "properties": {
                    "artifactLocation": {
                        "description": "Location where artifacts for the experiment are stored.\n",
                        "type": "string"
                    },
                    "creationTime": {
                        "description": "Creation time in unix time stamp.\n",
                        "type": "integer"
                    },
                    "experimentId": {
                        "description": "Unique identifier for the experiment. (same as `id`)\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "Unique identifier for the experiment. (same as `experiment_id`)\n",
                        "type": "string"
                    },
                    "lastUpdateTime": {
                        "description": "Last update time in unix time stamp.\n",
                        "type": "integer"
                    },
                    "lifecycleStage": {
                        "description": "Current life cycle stage of the experiment: `active` or `deleted`.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Path to experiment.\n",
                        "type": "string"
                    },
                    "tags": {
                        "description": "Additional metadata key-value pairs.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowExperimentTag:getMlflowExperimentTag"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "artifactLocation",
                    "creationTime",
                    "experimentId",
                    "id",
                    "lastUpdateTime",
                    "lifecycleStage",
                    "name",
                    "tags"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMlflowModel:getMlflowModel": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves the settings of databricks.MlflowModel by name.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisMlflowModel = new databricks.MlflowModel(\"this\", {\n    name: \"My MLflow Model\",\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\nconst _this = databricks.getMlflowModel({\n    name: \"My MLflow Model\",\n});\nexport const model = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_mlflow_model = databricks.MlflowModel(\"this\",\n    name=\"My MLflow Model\",\n    description=\"My MLflow model description\",\n    tags=[\n        {\n            \"key\": \"key1\",\n            \"value\": \"value1\",\n        },\n        {\n            \"key\": \"key2\",\n            \"value\": \"value2\",\n        },\n    ])\nthis = databricks.get_mlflow_model(name=\"My MLflow Model\")\npulumi.export(\"model\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisMlflowModel = new Databricks.MlflowModel(\"this\", new()\n    {\n        Name = \"My MLflow Model\",\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n    var @this = Databricks.GetMlflowModel.Invoke(new()\n    {\n        Name = \"My MLflow Model\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"model\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"this\", \u0026databricks.MlflowModelArgs{\n\t\t\tName:        pulumi.String(\"My MLflow Model\"),\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: databricks.MlflowModelTagArray{\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.LookupMlflowModel(ctx, \u0026databricks.LookupMlflowModelArgs{\n\t\t\tName: \"My MLflow Model\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"model\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMlflowModelArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisMlflowModel = new MlflowModel(\"thisMlflowModel\", MlflowModelArgs.builder()\n            .name(\"My MLflow Model\")\n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n        final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()\n            .name(\"My MLflow Model\")\n            .build());\n\n        ctx.export(\"model\", this_);\n    }\n}\n```\n```yaml\nresources:\n  thisMlflowModel:\n    type: databricks:MlflowModel\n    name: this\n    properties:\n      name: My MLflow Model\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMlflowModel\n      arguments:\n        name: My MLflow Model\noutputs:\n  model: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getMlflowModel({\n    name: \"My MLflow Model with multiple versions\",\n});\nconst thisModelServing = new databricks.ModelServing(\"this\", {\n    name: \"model-serving-endpoint\",\n    config: {\n        servedModels: [{\n            name: \"model_serving_prod\",\n            modelName: _this.then(_this =\u003e _this.name),\n            modelVersion: _this.then(_this =\u003e _this.latestVersions?.[0]?.version),\n            workloadSize: \"Small\",\n            scaleToZeroEnabled: true,\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mlflow_model(name=\"My MLflow Model with multiple versions\")\nthis_model_serving = databricks.ModelServing(\"this\",\n    name=\"model-serving-endpoint\",\n    config={\n        \"served_models\": [{\n            \"name\": \"model_serving_prod\",\n            \"model_name\": this.name,\n            \"model_version\": this.latest_versions[0].version,\n            \"workload_size\": \"Small\",\n            \"scale_to_zero_enabled\": True,\n        }],\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMlflowModel.Invoke(new()\n    {\n        Name = \"My MLflow Model with multiple versions\",\n    });\n\n    var thisModelServing = new Databricks.ModelServing(\"this\", new()\n    {\n        Name = \"model-serving-endpoint\",\n        Config = new Databricks.Inputs.ModelServingConfigArgs\n        {\n            ServedModels = new[]\n            {\n                new Databricks.Inputs.ModelServingConfigServedModelArgs\n                {\n                    Name = \"model_serving_prod\",\n                    ModelName = @this.Apply(@this =\u003e @this.Apply(getMlflowModelResult =\u003e getMlflowModelResult.Name)),\n                    ModelVersion = @this.Apply(@this =\u003e @this.Apply(getMlflowModelResult =\u003e getMlflowModelResult.LatestVersions[0]?.Version)),\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = true,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupMlflowModel(ctx, \u0026databricks.LookupMlflowModelArgs{\n\t\t\tName: \"My MLflow Model with multiple versions\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewModelServing(ctx, \"this\", \u0026databricks.ModelServingArgs{\n\t\t\tName: pulumi.String(\"model-serving-endpoint\"),\n\t\t\tConfig: \u0026databricks.ModelServingConfigArgs{\n\t\t\t\tServedModels: databricks.ModelServingConfigServedModelArray{\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedModelArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"model_serving_prod\"),\n\t\t\t\t\t\tModelName:          pulumi.String(this.Name),\n\t\t\t\t\t\tModelVersion:       pulumi.String(this.LatestVersions[0].Version),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(true),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMlflowModelArgs;\nimport com.pulumi.databricks.ModelServing;\nimport com.pulumi.databricks.ModelServingArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()\n            .name(\"My MLflow Model with multiple versions\")\n            .build());\n\n        var thisModelServing = new ModelServing(\"thisModelServing\", ModelServingArgs.builder()\n            .name(\"model-serving-endpoint\")\n            .config(ModelServingConfigArgs.builder()\n                .servedModels(ModelServingConfigServedModelArgs.builder()\n                    .name(\"model_serving_prod\")\n                    .modelName(this_.name())\n                    .modelVersion(this_.latestVersions()[0].version())\n                    .workloadSize(\"Small\")\n                    .scaleToZeroEnabled(true)\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisModelServing:\n    type: databricks:ModelServing\n    name: this\n    properties:\n      name: model-serving-endpoint\n      config:\n        servedModels:\n          - name: model_serving_prod\n            modelName: ${this.name}\n            modelVersion: ${this.latestVersions[0].version}\n            workloadSize: Small\n            scaleToZeroEnabled: true\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMlflowModel\n      arguments:\n        name: My MLflow Model with multiple versions\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMlflowModel.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "User-specified description for the object.\n"
                    },
                    "latestVersions": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion"
                        },
                        "description": "Array of model versions, each the latest version for its stage.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the registered model.\n",
                        "willReplaceOnChanges": true
                    },
                    "permissionLevel": {
                        "type": "string",
                        "description": "Permission level of the requesting user on the object. For what is allowed at each level, see MLflow Model permissions.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelTag:getMlflowModelTag"
                        },
                        "description": "Array of tags associated with the model.\n"
                    },
                    "userId": {
                        "type": "string",
                        "description": "The username of the user that created the object.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getMlflowModel.\n",
                "properties": {
                    "description": {
                        "description": "User-specified description for the object.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "Unique identifier for the object.\n",
                        "type": "string"
                    },
                    "latestVersions": {
                        "description": "Array of model versions, each the latest version for its stage.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion"
                        },
                        "type": "array"
                    },
                    "name": {
                        "description": "Name of the model.\n",
                        "type": "string"
                    },
                    "permissionLevel": {
                        "description": "Permission level of the requesting user on the object. For what is allowed at each level, see MLflow Model permissions.\n",
                        "type": "string"
                    },
                    "tags": {
                        "description": "Array of tags associated with the model.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelTag:getMlflowModelTag"
                        },
                        "type": "array"
                    },
                    "userId": {
                        "description": "The username of the user that created the object.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "description",
                    "id",
                    "latestVersions",
                    "name",
                    "permissionLevel",
                    "tags",
                    "userId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMlflowModels:getMlflowModels": {
            "description": "\u003e **Note** This data source could be only used with workspace-level provider!\n\nRetrieves a list of databricks.MlflowModel objects, that were created by Pulumi or manually, so that special handling could be applied.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getMlflowModels({});\nexport const model = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mlflow_models()\npulumi.export(\"model\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMlflowModels.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"model\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetMlflowModels(ctx, \u0026databricks.GetMlflowModelsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"model\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMlflowModelsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMlflowModels(GetMlflowModelsArgs.builder()\n            .build());\n\n        ctx.export(\"model\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMlflowModels\n      arguments: {}\noutputs:\n  model: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMlflowModels.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks_mlflow_model\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMlflowModels.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks_mlflow_model\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsCredentials:getMwsCredentials": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nLists all databricks.MwsCredentials in Databricks Account.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work.\n\n## Example Usage\n\nListing all credentials in Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMwsCredentials({});\nexport const allMwsCredentials = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_mws_credentials()\npulumi.export(\"allMwsCredentials\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMwsCredentials.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMwsCredentials\"] = all.Apply(getMwsCredentialsResult =\u003e getMwsCredentialsResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.LookupMwsCredentials(ctx, \u0026databricks.LookupMwsCredentialsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMwsCredentials\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMwsCredentials(GetMwsCredentialsArgs.builder()\n            .build());\n\n        ctx.export(\"allMwsCredentials\", all.ids());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getMwsCredentials\n      arguments: {}\noutputs:\n  allMwsCredentials: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsCredentials.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsCredentials.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsNetworkConnectivityConfig:getMwsNetworkConnectivityConfig": {
            "description": "\u003e **Note** This data source can only be used with an account-level provider!\n\nRetrieves information about databricks.MwsNetworkConnectivityConfig in Databricks Account.\n\n## Example Usage\n\nFetching information about a network connectivity configuration in Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getMwsNetworkConnectivityConfig({\n    name: \"ncc\",\n});\nexport const config = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mws_network_connectivity_config(name=\"ncc\")\npulumi.export(\"config\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMwsNetworkConnectivityConfig.Invoke(new()\n    {\n        Name = \"ncc\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"config\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupMwsNetworkConnectivityConfig(ctx, \u0026databricks.LookupMwsNetworkConnectivityConfigArgs{\n\t\t\tName: \"ncc\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"config\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMwsNetworkConnectivityConfig(GetMwsNetworkConnectivityConfigArgs.builder()\n            .name(\"ncc\")\n            .build());\n\n        ctx.export(\"config\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMwsNetworkConnectivityConfig\n      arguments:\n        name: ncc\noutputs:\n  config: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getMwsNetworkConnectivityConfigs to get names of all network connectivity configurations.\n* databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsNetworkConnectivityConfig.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "The Databricks account ID associated with this network configuration.\n"
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was created.\n"
                    },
                    "egressConfig": {
                        "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfig:getMwsNetworkConnectivityConfigEgressConfig",
                        "description": "Array of egress configuration objects.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the network connectivity configuration.\n",
                        "willReplaceOnChanges": true
                    },
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "The Databricks network connectivity configuration ID.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "The region of the network connectivity configuration.\n"
                    },
                    "updatedTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when the network was updated.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getMwsNetworkConnectivityConfig.\n",
                "properties": {
                    "accountId": {
                        "description": "The Databricks account ID associated with this network configuration.\n",
                        "type": "string"
                    },
                    "creationTime": {
                        "description": "Time in epoch milliseconds when this object was created.\n",
                        "type": "integer"
                    },
                    "egressConfig": {
                        "$ref": "#/types/databricks:index/getMwsNetworkConnectivityConfigEgressConfig:getMwsNetworkConnectivityConfigEgressConfig",
                        "description": "Array of egress configuration objects.\n"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "The name of the network connectivity configuration.\n",
                        "type": "string"
                    },
                    "networkConnectivityConfigId": {
                        "description": "The Databricks network connectivity configuration ID.\n",
                        "type": "string"
                    },
                    "region": {
                        "description": "The region of the network connectivity configuration.\n",
                        "type": "string"
                    },
                    "updatedTime": {
                        "description": "Time in epoch milliseconds when the network was updated.\n",
                        "type": "integer"
                    }
                },
                "required": [
                    "accountId",
                    "creationTime",
                    "egressConfig",
                    "name",
                    "networkConnectivityConfigId",
                    "region",
                    "updatedTime",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsNetworkConnectivityConfigs:getMwsNetworkConnectivityConfigs": {
            "description": "\u003e **Note** This data source can only be used with an account-level provider!\n\nLists all databricks.MwsNetworkConnectivityConfig in Databricks Account.\n\n## Example Usage\n\nList all network connectivity configurations in Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getMwsNetworkConnectivityConfigs({});\nexport const all = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mws_network_connectivity_configs()\npulumi.export(\"all\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMwsNetworkConnectivityConfigs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"all\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetMwsNetworkConnectivityConfigs(ctx, \u0026databricks.GetMwsNetworkConnectivityConfigsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"all\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()\n            .build());\n\n        ctx.export(\"all\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMwsNetworkConnectivityConfigs\n      arguments: {}\noutputs:\n  all: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nList network connectivity configurations from a specific region in Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getMwsNetworkConnectivityConfigs({\n    region: \"us-east-1\",\n});\nexport const filtered = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mws_network_connectivity_configs(region=\"us-east-1\")\npulumi.export(\"filtered\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMwsNetworkConnectivityConfigs.Invoke(new()\n    {\n        Region = \"us-east-1\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"filtered\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetMwsNetworkConnectivityConfigs(ctx, \u0026databricks.GetMwsNetworkConnectivityConfigsArgs{\n\t\t\tRegion: pulumi.StringRef(\"us-east-1\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"filtered\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsNetworkConnectivityConfigsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMwsNetworkConnectivityConfigs(GetMwsNetworkConnectivityConfigsArgs.builder()\n            .region(\"us-east-1\")\n            .build());\n\n        ctx.export(\"filtered\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getMwsNetworkConnectivityConfigs\n      arguments:\n        region: us-east-1\noutputs:\n  filtered: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.MwsNetworkConnectivityConfig to get information about a single network connectivity configuration.\n* databricks.MwsNetworkConnectivityConfig to manage network connectivity configuration.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsNetworkConnectivityConfigs.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks_mws_network_connectivity_config\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Filter network connectivity configurations by region.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsNetworkConnectivityConfigs.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks_mws_network_connectivity_config\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "region": {
                        "type": "string"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsWorkspaces:getMwsWorkspaces": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nLists all databricks.MwsWorkspaces in Databricks Account.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work.\n\n## Example Usage\n\nListing all workspaces in\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMwsWorkspaces({});\nexport const allMwsWorkspaces = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_mws_workspaces()\npulumi.export(\"allMwsWorkspaces\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMwsWorkspaces.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMwsWorkspaces\"] = all.Apply(getMwsWorkspacesResult =\u003e getMwsWorkspacesResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.LookupMwsWorkspaces(ctx, map[string]interface{}{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMwsWorkspaces\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMwsWorkspaces(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        ctx.export(\"allMwsWorkspaces\", all.ids());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getMwsWorkspaces\n      arguments: {}\noutputs:\n  allMwsWorkspaces: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.\n* databricks.MetastoreAssignment to assign databricks.Metastore to databricks.MwsWorkspaces or azurerm_databricks_workspace\n",
            "outputs": {
                "description": "A collection of values returned by getMwsWorkspaces.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "type": "string"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNodeType:getNodeType": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nGets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.\n\n\u003e **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn't match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()\n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.id())\n            .nodeTypeId(withGpu.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        gpu: true\n        ml: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string",
                        "description": "Node category, which can be one of (depending on the cloud environment, could be checked with `databricks clusters list-node-types -o json|jq '.node_types[]|.category'|sort |uniq`):\n* `General Purpose` (all clouds)\n* `General Purpose (HDD)` (Azure)\n* `Compute Optimized` (all clouds)\n* `Memory Optimized` (all clouds)\n* `Memory Optimized (Remote HDD)` (Azure)\n* `Storage Optimized` (AWS, Azure)\n* `GPU Accelerated` (AWS, Azure)\n",
                        "willReplaceOnChanges": true
                    },
                    "fleet": {
                        "type": "boolean",
                        "description": "if we should limit the search only to [AWS fleet instance types](https://docs.databricks.com/compute/aws-fleet-instances.html). Default to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "gbPerCore": {
                        "type": "integer",
                        "description": "Number of gigabytes per core available on instance. Conflicts with `min_memory_gb`. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to nodes with AWS Graviton or Azure Cobalt CPUs. Default to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "id": {
                        "type": "string",
                        "description": "node type, that can be used for databricks_job, databricks_cluster, or databricks_instance_pool.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean",
                        "description": ". Pick only nodes that have IO Cache. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "localDisk": {
                        "type": "boolean",
                        "description": "Pick only nodes with local storage. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "localDiskMinSize": {
                        "type": "integer",
                        "description": "Pick only nodes that have size local storage greater or equal to given value. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minCores": {
                        "type": "integer",
                        "description": "Minimum number of CPU cores available on instance. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minGpus": {
                        "type": "integer",
                        "description": "Minimum number of GPU's attached to instance. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minMemoryGb": {
                        "type": "integer",
                        "description": "Minimum amount of memory per node in gigabytes. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "photonDriverCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon driver. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "photonWorkerCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon workers. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "supportPortForwarding": {
                        "type": "boolean",
                        "description": "Pick only nodes that support port forwarding. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string"
                    },
                    "fleet": {
                        "type": "boolean"
                    },
                    "gbPerCore": {
                        "type": "integer"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "description": "node type, that can be used for databricks_job, databricks_cluster, or databricks_instance_pool.\n",
                        "type": "string"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean"
                    },
                    "localDisk": {
                        "type": "boolean"
                    },
                    "localDiskMinSize": {
                        "type": "integer"
                    },
                    "minCores": {
                        "type": "integer"
                    },
                    "minGpus": {
                        "type": "integer"
                    },
                    "minMemoryGb": {
                        "type": "integer"
                    },
                    "photonDriverCapable": {
                        "type": "boolean"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean"
                    },
                    "supportPortForwarding": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNotebook:getNotebook": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to export a notebook from Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst features = databricks.getNotebook({\n    path: \"/Production/Features\",\n    format: \"SOURCE\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfeatures = databricks.get_notebook(path=\"/Production/Features\",\n    format=\"SOURCE\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var features = Databricks.GetNotebook.Invoke(new()\n    {\n        Path = \"/Production/Features\",\n        Format = \"SOURCE\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupNotebook(ctx, \u0026databricks.LookupNotebookArgs{\n\t\t\tPath:   \"/Production/Features\",\n\t\t\tFormat: \"SOURCE\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()\n            .path(\"/Production/Features\")\n            .format(\"SOURCE\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  features:\n    fn::invoke:\n      function: databricks:getNotebook\n      arguments:\n        path: /Production/Features\n        format: SOURCE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebook.\n",
                "properties": {
                    "format": {
                        "type": "string",
                        "description": "Notebook format to export. Either `SOURCE`, `HTML`, `JUPYTER`, or `DBC`.\n",
                        "willReplaceOnChanges": true
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Notebook path on the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "format",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebook.\n",
                "properties": {
                    "content": {
                        "description": "notebook content in selected format\n",
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "language": {
                        "description": "notebook language\n",
                        "type": "string"
                    },
                    "objectId": {
                        "description": "notebook object ID\n",
                        "type": "integer"
                    },
                    "objectType": {
                        "description": "notebook object type\n",
                        "type": "string"
                    },
                    "path": {
                        "type": "string"
                    },
                    "workspacePath": {
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n",
                        "type": "string"
                    }
                },
                "required": [
                    "content",
                    "format",
                    "language",
                    "objectId",
                    "objectType",
                    "path",
                    "workspacePath",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNotebookPaths:getNotebookPaths": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to list notebooks in the Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getNotebookPaths({\n    path: \"/Production\",\n    recursive: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_notebook_paths(path=\"/Production\",\n    recursive=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetNotebookPaths.Invoke(new()\n    {\n        Path = \"/Production\",\n        Recursive = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetNotebookPaths(ctx, \u0026databricks.GetNotebookPathsArgs{\n\t\t\tPath:      \"/Production\",\n\t\t\tRecursive: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookPathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()\n            .path(\"/Production\")\n            .recursive(true)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      function: databricks:getNotebookPaths\n      arguments:\n        path: /Production\n        recursive: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebookPaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path to workspace directory\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or recursively walk given path\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebookPaths.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "notebookPathLists": {
                        "description": "list of objects with `path` and `language` attributes\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList"
                        },
                        "type": "array"
                    },
                    "path": {
                        "type": "string"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "notebookPathLists",
                    "path",
                    "recursive",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNotificationDestinations:getNotificationDestinations": {
            "description": "This data source allows you to retrieve information about [Notification Destinations](https://docs.databricks.com/api/workspace/notificationdestinations). Notification Destinations are used to send notifications for query alerts and jobs to external systems such as email, Slack, Microsoft Teams, PagerDuty, or generic webhooks. \n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst email = new databricks.NotificationDestination(\"email\", {\n    displayName: \"Email Destination\",\n    config: {\n        email: {\n            addresses: [\"abc@gmail.com\"],\n        },\n    },\n});\nconst slack = new databricks.NotificationDestination(\"slack\", {\n    displayName: \"Slack Destination\",\n    config: {\n        slack: {\n            url: \"https://hooks.slack.com/services/...\",\n        },\n    },\n});\n// Lists all notification desitnations\nconst _this = databricks.getNotificationDestinations({});\n// List destinations of specific type and name\nconst filteredNotification = databricks.getNotificationDestinations({\n    displayNameContains: \"Destination\",\n    type: \"EMAIL\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nemail = databricks.NotificationDestination(\"email\",\n    display_name=\"Email Destination\",\n    config={\n        \"email\": {\n            \"addresses\": [\"abc@gmail.com\"],\n        },\n    })\nslack = databricks.NotificationDestination(\"slack\",\n    display_name=\"Slack Destination\",\n    config={\n        \"slack\": {\n            \"url\": \"https://hooks.slack.com/services/...\",\n        },\n    })\n# Lists all notification desitnations\nthis = databricks.get_notification_destinations()\n# List destinations of specific type and name\nfiltered_notification = databricks.get_notification_destinations(display_name_contains=\"Destination\",\n    type=\"EMAIL\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var email = new Databricks.NotificationDestination(\"email\", new()\n    {\n        DisplayName = \"Email Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            Email = new Databricks.Inputs.NotificationDestinationConfigEmailArgs\n            {\n                Addresses = new[]\n                {\n                    \"abc@gmail.com\",\n                },\n            },\n        },\n    });\n\n    var slack = new Databricks.NotificationDestination(\"slack\", new()\n    {\n        DisplayName = \"Slack Destination\",\n        Config = new Databricks.Inputs.NotificationDestinationConfigArgs\n        {\n            Slack = new Databricks.Inputs.NotificationDestinationConfigSlackArgs\n            {\n                Url = \"https://hooks.slack.com/services/...\",\n            },\n        },\n    });\n\n    // Lists all notification desitnations\n    var @this = Databricks.GetNotificationDestinations.Invoke();\n\n    // List destinations of specific type and name\n    var filteredNotification = Databricks.GetNotificationDestinations.Invoke(new()\n    {\n        DisplayNameContains = \"Destination\",\n        Type = \"EMAIL\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewNotificationDestination(ctx, \"email\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Email Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tEmail: \u0026databricks.NotificationDestinationConfigEmailArgs{\n\t\t\t\t\tAddresses: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"abc@gmail.com\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewNotificationDestination(ctx, \"slack\", \u0026databricks.NotificationDestinationArgs{\n\t\t\tDisplayName: pulumi.String(\"Slack Destination\"),\n\t\t\tConfig: \u0026databricks.NotificationDestinationConfigArgs{\n\t\t\t\tSlack: \u0026databricks.NotificationDestinationConfigSlackArgs{\n\t\t\t\t\tUrl: pulumi.String(\"https://hooks.slack.com/services/...\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Lists all notification desitnations\n\t\t_, err = databricks.GetNotificationDestinations(ctx, \u0026databricks.GetNotificationDestinationsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// List destinations of specific type and name\n\t\t_, err = databricks.GetNotificationDestinations(ctx, \u0026databricks.GetNotificationDestinationsArgs{\n\t\t\tDisplayNameContains: pulumi.StringRef(\"Destination\"),\n\t\t\tType:                pulumi.StringRef(\"EMAIL\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.NotificationDestination;\nimport com.pulumi.databricks.NotificationDestinationArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigEmailArgs;\nimport com.pulumi.databricks.inputs.NotificationDestinationConfigSlackArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotificationDestinationsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var email = new NotificationDestination(\"email\", NotificationDestinationArgs.builder()\n            .displayName(\"Email Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .email(NotificationDestinationConfigEmailArgs.builder()\n                    .addresses(\"abc@gmail.com\")\n                    .build())\n                .build())\n            .build());\n\n        var slack = new NotificationDestination(\"slack\", NotificationDestinationArgs.builder()\n            .displayName(\"Slack Destination\")\n            .config(NotificationDestinationConfigArgs.builder()\n                .slack(NotificationDestinationConfigSlackArgs.builder()\n                    .url(\"https://hooks.slack.com/services/...\")\n                    .build())\n                .build())\n            .build());\n\n        // Lists all notification desitnations\n        final var this = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()\n            .build());\n\n        // List destinations of specific type and name\n        final var filteredNotification = DatabricksFunctions.getNotificationDestinations(GetNotificationDestinationsArgs.builder()\n            .displayNameContains(\"Destination\")\n            .type(\"EMAIL\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  email:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Email Destination\n      config:\n        email:\n          addresses:\n            - abc@gmail.com\n  slack:\n    type: databricks:NotificationDestination\n    properties:\n      displayName: Slack Destination\n      config:\n        slack:\n          url: https://hooks.slack.com/services/...\nvariables:\n  # Lists all notification desitnations\n  this:\n    fn::invoke:\n      function: databricks:getNotificationDestinations\n      arguments: {}\n  # List destinations of specific type and name\n  filteredNotification:\n    fn::invoke:\n      function: databricks:getNotificationDestinations\n      arguments:\n        displayNameContains: Destination\n        type: EMAIL\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNotificationDestinations.\n",
                "properties": {
                    "displayNameContains": {
                        "type": "string",
                        "description": "A **case-insensitive** substring to filter Notification Destinations by their display name.\n"
                    },
                    "type": {
                        "type": "string",
                        "description": "The type of the Notification Destination to filter by. Valid values are: \n* `EMAIL` - Filters Notification Destinations of type Email.\n* `MICROSOFT_TEAMS` - Filters Notification Destinations of type Microsoft Teams.\n* `PAGERDUTY` - Filters Notification Destinations of type PagerDuty.\n* `SLACK` - Filters Notification Destinations of type Slack.\n* `WEBHOOK` - Filters Notification Destinations of type Webhook.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getNotificationDestinations.\n",
                "properties": {
                    "displayNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "notificationDestinations": {
                        "description": "A list of Notification Destinations matching the specified criteria. Each element contains the following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getNotificationDestinationsNotificationDestination:getNotificationDestinationsNotificationDestination"
                        },
                        "type": "array"
                    },
                    "type": {
                        "type": "string"
                    }
                },
                "required": [
                    "notificationDestinations",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getPipelines:getPipelines": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.\n\nRetrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.\n\n## Example Usage\n\nGet all Delta Live Tables pipelines:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getPipelines({});\nexport const allPipelines = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_pipelines()\npulumi.export(\"allPipelines\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetPipelines.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allPipelines\"] = all.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetPipelines(ctx, \u0026databricks.GetPipelinesArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allPipelines\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()\n            .build());\n\n        ctx.export(\"allPipelines\", all.ids());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getPipelines\n      arguments: {}\noutputs:\n  allPipelines: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFilter Delta Live Tables pipelines by name (exact match):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getPipelines({\n    pipelineName: \"my_pipeline\",\n});\nexport const myPipeline = _this.then(_this =\u003e _this.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_pipelines(pipeline_name=\"my_pipeline\")\npulumi.export(\"myPipeline\", this.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetPipelines.Invoke(new()\n    {\n        PipelineName = \"my_pipeline\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"myPipeline\"] = @this.Apply(@this =\u003e @this.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetPipelines(ctx, \u0026databricks.GetPipelinesArgs{\n\t\t\tPipelineName: pulumi.StringRef(\"my_pipeline\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"myPipeline\", this.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()\n            .pipelineName(\"my_pipeline\")\n            .build());\n\n        ctx.export(\"myPipeline\", this_.ids());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getPipelines\n      arguments:\n        pipelineName: my_pipeline\noutputs:\n  myPipeline: ${this.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFilter Delta Live Tables pipelines by name (wildcard search):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getPipelines({\n    pipelineName: \"%pipeline%\",\n});\nexport const wildcardPipelines = _this.then(_this =\u003e _this.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_pipelines(pipeline_name=\"%pipeline%\")\npulumi.export(\"wildcardPipelines\", this.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetPipelines.Invoke(new()\n    {\n        PipelineName = \"%pipeline%\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"wildcardPipelines\"] = @this.Apply(@this =\u003e @this.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetPipelines(ctx, \u0026databricks.GetPipelinesArgs{\n\t\t\tPipelineName: pulumi.StringRef(\"%pipeline%\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"wildcardPipelines\", this.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()\n            .pipelineName(\"%pipeline%\")\n            .build());\n\n        ctx.export(\"wildcardPipelines\", this_.ids());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getPipelines\n      arguments:\n        pipelineName: '%pipeline%'\noutputs:\n  wildcardPipelines: ${this.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getPipelines.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of ids for [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipelines matching the provided search criteria.\n"
                    },
                    "pipelineName": {
                        "type": "string",
                        "description": "Filter Delta Live Tables pipelines by name for a given search term. `%` is the supported wildcard operator.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getPipelines.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "List of ids for [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipelines matching the provided search criteria.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "pipelineName": {
                        "type": "string"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getRegisteredModel:getRegisteredModel": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource allows you to get information about [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getRegisteredModel({\n    fullName: \"main.default.my_model\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_registered_model(full_name=\"main.default.my_model\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetRegisteredModel.Invoke(new()\n    {\n        FullName = \"main.default.my_model\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupRegisteredModel(ctx, \u0026databricks.LookupRegisteredModelArgs{\n\t\t\tFullName: \"main.default.my_model\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetRegisteredModelArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getRegisteredModel(GetRegisteredModelArgs.builder()\n            .fullName(\"main.default.my_model\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getRegisteredModel\n      arguments:\n        fullName: main.default.my_model\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel resource to manage models within Unity Catalog.\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getRegisteredModel.\n",
                "properties": {
                    "fullName": {
                        "type": "string",
                        "description": "The fully-qualified name of the registered model (`catalog_name.schema_name.name`).\n"
                    },
                    "includeAliases": {
                        "type": "boolean",
                        "description": "flag to specify if list of aliases should be included into output.\n"
                    },
                    "includeBrowse": {
                        "type": "boolean",
                        "description": "flag to specify if include registered models in the response for which the principal can only access selective metadata for.\n"
                    },
                    "modelInfos": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getRegisteredModelModelInfo:getRegisteredModelModelInfo"
                        },
                        "description": "block with information about the model in Unity Catalog:\n"
                    }
                },
                "type": "object",
                "required": [
                    "fullName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getRegisteredModel.\n",
                "properties": {
                    "fullName": {
                        "description": "The fully-qualified name of the registered model (`catalog_name.schema_name.name`).\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "includeAliases": {
                        "type": "boolean"
                    },
                    "includeBrowse": {
                        "type": "boolean"
                    },
                    "modelInfos": {
                        "description": "block with information about the model in Unity Catalog:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getRegisteredModelModelInfo:getRegisteredModelModelInfo"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "fullName",
                    "modelInfos",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getRegisteredModelVersions:getRegisteredModelVersions": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource allows you to get information about versions of [Model in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getRegisteredModelVersions({\n    fullName: \"main.default.my_model\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_registered_model_versions(full_name=\"main.default.my_model\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetRegisteredModelVersions.Invoke(new()\n    {\n        FullName = \"main.default.my_model\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetRegisteredModelVersions(ctx, \u0026databricks.GetRegisteredModelVersionsArgs{\n\t\t\tFullName: \"main.default.my_model\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetRegisteredModelVersionsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getRegisteredModelVersions(GetRegisteredModelVersionsArgs.builder()\n            .fullName(\"main.default.my_model\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getRegisteredModelVersions\n      arguments:\n        fullName: main.default.my_model\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel data source to retrieve information about a model within Unity Catalog.\n* databricks.RegisteredModel resource to manage models within Unity Catalog.\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getRegisteredModelVersions.\n",
                "properties": {
                    "fullName": {
                        "type": "string",
                        "description": "The fully-qualified name of the registered model (`catalog_name.schema_name.name`).\n"
                    },
                    "modelVersions": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersion:getRegisteredModelVersionsModelVersion"
                        },
                        "description": "list of objects describing the model versions. Each object consists of following attributes:\n"
                    }
                },
                "type": "object",
                "required": [
                    "fullName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getRegisteredModelVersions.\n",
                "properties": {
                    "fullName": {
                        "description": "The fully-qualified name of the registered model (`catalog_name.schema_name.name`).\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "modelVersions": {
                        "description": "list of objects describing the model versions. Each object consists of following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getRegisteredModelVersionsModelVersion:getRegisteredModelVersionsModelVersion"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "fullName",
                    "modelVersions",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSchema:getSchema": {
            "description": "Retrieves details about databricks.Schema that was created by Pulumi or manually. \nA schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.\n\n## Example Usage\n\n* Retrieve details of all schemas in in a _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSchemas({\n    catalogName: \"sandbox\",\n});\nconst _this = all.then(all =\u003e .reduce((__obj, [__key, __value]) =\u003e ({ ...__obj, [__key]: databricks.getSchema({\n    name: __value,\n}) })));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_schemas(catalog_name=\"sandbox\")\nthis = {__key: databricks.get_schema(name=__value) for __key, __value in all.ids}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSchemas.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n    });\n\n    var @this = ;\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n* Search for a specific schema by its fully qualified name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getSchema({\n    name: \"catalog.schema\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_schema(name=\"catalog.schema\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetSchema.Invoke(new()\n    {\n        Name = \"catalog.schema\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupSchema(ctx, \u0026databricks.LookupSchemaArgs{\n\t\t\tName: \"catalog.schema\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()\n            .name(\"catalog.schema\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getSchema\n      arguments:\n        name: catalog.schema\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSchema.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "ID of this Unity Catalog Schema in form of `\u003ccatalog\u003e.\u003cschema\u003e`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "a fully qualified name of databricks_schema: *`catalog`.`schema`*\n",
                        "willReplaceOnChanges": true
                    },
                    "schemaInfo": {
                        "$ref": "#/types/databricks:index/getSchemaSchemaInfo:getSchemaSchemaInfo",
                        "description": "`SchemaInfo` object for a Unity Catalog schema. This contains the following attributes:\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSchema.\n",
                "properties": {
                    "id": {
                        "description": "ID of this Unity Catalog Schema in form of `\u003ccatalog\u003e.\u003cschema\u003e`.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Name of schema, relative to parent catalog.\n",
                        "type": "string"
                    },
                    "schemaInfo": {
                        "$ref": "#/types/databricks:index/getSchemaSchemaInfo:getSchemaSchemaInfo",
                        "description": "`SchemaInfo` object for a Unity Catalog schema. This contains the following attributes:\n"
                    }
                },
                "required": [
                    "id",
                    "name",
                    "schemaInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSchemas:getSchemas": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.\n\n## Example Usage\n\nListing all schemas in a _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = databricks.getSchemas({\n    catalogName: \"sandbox\",\n});\nexport const allSandboxSchemas = sandbox;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.get_schemas(catalog_name=\"sandbox\")\npulumi.export(\"allSandboxSchemas\", sandbox)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = Databricks.GetSchemas.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allSandboxSchemas\"] = sandbox,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.GetSchemas(ctx, \u0026databricks.GetSchemasArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allSandboxSchemas\", sandbox)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSchemasArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()\n            .catalogName(\"sandbox\")\n            .build());\n\n        ctx.export(\"allSandboxSchemas\", sandbox);\n    }\n}\n```\n```yaml\nvariables:\n  sandbox:\n    fn::invoke:\n      function: databricks:getSchemas\n      arguments:\n        catalogName: sandbox\noutputs:\n  allSandboxSchemas: ${sandbox}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getServicePrincipal:getServicePrincipal": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_service_principal.\n\n## Example Usage\n\nAdding service principal `11111111-2222-3333-4444-555666777888` to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst spn = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: spn.then(spn =\u003e spn.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nspn = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=spn.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var spn = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = spn.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tspn, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: pulumi.String(spn.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(spn.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${spn.id}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n  spn:\n    fn::invoke:\n      function: databricks:getServicePrincipal\n      arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n- End to end workspace management guide.\n- databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipal.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "ID of the service principal. The service principal must exist before this resource can be retrieved.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Exact display name of the service principal. The service principal must exist before this resource can be retrieved.  In case if there are several service principals with the same name, an error is thrown.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipal.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n",
                        "type": "string"
                    },
                    "active": {
                        "description": "Whether service principal is active or not.\n",
                        "type": "boolean"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n",
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the service principal in an external identity provider.\n",
                        "type": "string"
                    },
                    "home": {
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The id of the service principal.\n",
                        "type": "string"
                    },
                    "repos": {
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n",
                        "type": "string"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "active",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "id",
                    "repos",
                    "spId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getServicePrincipals:getServicePrincipals": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string",
                        "description": "Only return databricks.ServicePrincipal display name that match the given name string\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "displayNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "applicationIds",
                    "displayNameContains",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getServingEndpoints:getServingEndpoints": {
            "description": "\u003e This resource can only be used with a workspace-level provider!\n\nThis resource allows you to get information about [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getServingEndpoints({});\nconst mlServingUsage: databricks.Permissions[] = [];\nfor (const range = {value: 0}; range.value \u003c allDatabricksServingEndpoints.endpoints; range.value++) {\n    mlServingUsage.push(new databricks.Permissions(`ml_serving_usage-${range.value}`, {\n        servingEndpointId: range.value.id,\n        accessControls: [\n            {\n                groupName: \"users\",\n                permissionLevel: \"CAN_VIEW\",\n            },\n            {\n                groupName: auto.displayName,\n                permissionLevel: \"CAN_MANAGE\",\n            },\n            {\n                groupName: eng.displayName,\n                permissionLevel: \"CAN_QUERY\",\n            },\n        ],\n    }));\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_serving_endpoints()\nml_serving_usage = []\nfor range in [{\"value\": i} for i in range(0, all_databricks_serving_endpoints.endpoints)]:\n    ml_serving_usage.append(databricks.Permissions(f\"ml_serving_usage-{range['value']}\",\n        serving_endpoint_id=range[\"value\"][\"id\"],\n        access_controls=[\n            {\n                \"group_name\": \"users\",\n                \"permission_level\": \"CAN_VIEW\",\n            },\n            {\n                \"group_name\": auto[\"displayName\"],\n                \"permission_level\": \"CAN_MANAGE\",\n            },\n            {\n                \"group_name\": eng[\"displayName\"],\n                \"permission_level\": \"CAN_QUERY\",\n            },\n        ]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetServingEndpoints.Invoke();\n\n    var mlServingUsage = new List\u003cDatabricks.Permissions\u003e();\n    for (var rangeIndex = 0; rangeIndex \u003c allDatabricksServingEndpoints.Endpoints; rangeIndex++)\n    {\n        var range = new { Value = rangeIndex };\n        mlServingUsage.Add(new Databricks.Permissions($\"ml_serving_usage-{range.Value}\", new()\n        {\n            ServingEndpointId = range.Value.Id,\n            AccessControls = new[]\n            {\n                new Databricks.Inputs.PermissionsAccessControlArgs\n                {\n                    GroupName = \"users\",\n                    PermissionLevel = \"CAN_VIEW\",\n                },\n                new Databricks.Inputs.PermissionsAccessControlArgs\n                {\n                    GroupName = auto.DisplayName,\n                    PermissionLevel = \"CAN_MANAGE\",\n                },\n                new Databricks.Inputs.PermissionsAccessControlArgs\n                {\n                    GroupName = eng.DisplayName,\n                    PermissionLevel = \"CAN_QUERY\",\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetServingEndpoints(ctx, \u0026databricks.GetServingEndpointsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar mlServingUsage []*databricks.Permissions\n\t\tfor index := 0; index \u003c allDatabricksServingEndpoints.Endpoints; index++ {\n\t\t\tkey0 := index\n\t\t\tval0 := index\n\t\t\t__res, err := databricks.NewPermissions(ctx, fmt.Sprintf(\"ml_serving_usage-%v\", key0), \u0026databricks.PermissionsArgs{\n\t\t\t\tServingEndpointId: pulumi.Any(val0),\n\t\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t\t},\n\t\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\t\tGroupName:       pulumi.Any(auto.DisplayName),\n\t\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_MANAGE\"),\n\t\t\t\t\t},\n\t\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\t\tGroupName:       pulumi.Any(eng.DisplayName),\n\t\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_QUERY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tmlServingUsage = append(mlServingUsage, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServingEndpointsArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getServingEndpoints(GetServingEndpointsArgs.builder()\n            .build());\n\n        for (var i = 0; i \u003c allDatabricksServingEndpoints.endpoints(); i++) {\n            new Permissions(\"mlServingUsage-\" + i, PermissionsArgs.builder()\n                .servingEndpointId(range.value().id())\n                .accessControls(                \n                    PermissionsAccessControlArgs.builder()\n                        .groupName(\"users\")\n                        .permissionLevel(\"CAN_VIEW\")\n                        .build(),\n                    PermissionsAccessControlArgs.builder()\n                        .groupName(auto.displayName())\n                        .permissionLevel(\"CAN_MANAGE\")\n                        .build(),\n                    PermissionsAccessControlArgs.builder()\n                        .groupName(eng.displayName())\n                        .permissionLevel(\"CAN_QUERY\")\n                        .build())\n                .build());\n\n        \n}\n    }\n}\n```\n```yaml\nresources:\n  mlServingUsage:\n    type: databricks:Permissions\n    name: ml_serving_usage\n    properties:\n      servingEndpointId: ${range.value.id}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n        - groupName: ${auto.displayName}\n          permissionLevel: CAN_MANAGE\n        - groupName: ${eng.displayName}\n          permissionLevel: CAN_QUERY\n    options: {}\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getServingEndpoints\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServingEndpoints.\n",
                "properties": {
                    "endpoints": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getServingEndpointsEndpoint:getServingEndpointsEndpoint"
                        },
                        "description": "List of objects describing the serving endpoints. Each object consists of following attributes:\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServingEndpoints.\n",
                "properties": {
                    "endpoints": {
                        "description": "List of objects describing the serving endpoints. Each object consists of following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getServingEndpointsEndpoint:getServingEndpointsEndpoint"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "endpoints",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getShare:getShare": {
            "description": "Retrieves details about a databricks.Share that were created by Pulumi or manually.\n\n## Example Usage\n\nGetting details of an existing share in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getShare({\n    name: \"this\",\n});\nexport const createdBy = _this.then(_this =\u003e _this.createdBy);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_share(name=\"this\")\npulumi.export(\"createdBy\", this.created_by)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShare.Invoke(new()\n    {\n        Name = \"this\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"createdBy\"] = @this.Apply(@this =\u003e @this.Apply(getShareResult =\u003e getShareResult.CreatedBy)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupShare(ctx, \u0026databricks.LookupShareArgs{\n\t\t\tName: pulumi.StringRef(\"this\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"createdBy\", this.CreatedBy)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetShareArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShare(GetShareArgs.builder()\n            .name(\"this\")\n            .build());\n\n        ctx.export(\"createdBy\", this_.createdBy());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getShare\n      arguments:\n        name: this\noutputs:\n  createdBy: ${this.createdBy}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShare.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the share\n"
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "description": "arrays containing details of each object in the share.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShare.\n",
                "properties": {
                    "createdAt": {
                        "description": "Time when the share was created.\n",
                        "type": "integer"
                    },
                    "createdBy": {
                        "description": "The principal that created the share.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Full name of the object being shared.\n",
                        "type": "string"
                    },
                    "objects": {
                        "description": "arrays containing details of each object in the share.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "createdAt",
                    "createdBy",
                    "name",
                    "objects",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getShares:getShares": {
            "description": "Retrieves a list of databricks.Share name, that were created by Pulumi or manually.\n\n## Example Usage\n\nGetting all existing shares in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getShares({});\nexport const shareName = _this.then(_this =\u003e _this.shares);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_shares()\npulumi.export(\"shareName\", this.shares)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShares.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"shareName\"] = @this.Apply(@this =\u003e @this.Apply(getSharesResult =\u003e getSharesResult.Shares)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetShares(ctx, \u0026databricks.GetSharesArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"shareName\", this.Shares)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSharesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShares(GetSharesArgs.builder()\n            .build());\n\n        ctx.export(\"shareName\", this_.shares());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getShares\n      arguments: {}\noutputs:\n  shareName: ${this.shares}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShares.\n",
                "properties": {
                    "shares": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Share names.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShares.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "shares": {
                        "description": "list of databricks.Share names.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "shares",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSparkVersion:getSparkVersion": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nGets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.\n\n\u003e **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale={\n        \"min_workers\": 1,\n        \"max_workers\": 50,\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()\n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.id())\n            .nodeTypeId(withGpu.id())\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      function: databricks:getNodeType\n      arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      function: databricks:getSparkVersion\n      arguments:\n        gpu: true\n        ml: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that are in Beta stage. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "genomics": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "gpu": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that support GPUs. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`. _Deprecated with DBR 14.0 release. DBR version compiled for Graviton will be automatically installed when nodes with Graviton CPUs are specified in the cluster configuration._\n",
                        "deprecationMessage": "Not required anymore - it's automatically enabled on the Graviton-based node types",
                        "willReplaceOnChanges": true
                    },
                    "id": {
                        "type": "string",
                        "description": "Databricks Runtime version, that can be used as `spark_version` field in databricks_job, databricks_cluster, or databricks_instance_pool.\n"
                    },
                    "latest": {
                        "type": "boolean",
                        "description": "if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.\n",
                        "willReplaceOnChanges": true
                    },
                    "longTermSupport": {
                        "type": "boolean",
                        "description": "if we should limit the search only to LTS (long term support) \u0026 ESR (extended support) versions. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "ml": {
                        "type": "boolean",
                        "description": "if we should limit the search only to ML runtimes. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Photon runtimes. Default to `false`. *Deprecated with DBR 14.0 release. Specify `runtime_engine=\\\"PHOTON\\\"` in the cluster configuration instead!*\n",
                        "deprecationMessage": "Specify runtime_engine=\"PHOTON\" in the cluster configuration",
                        "willReplaceOnChanges": true
                    },
                    "scala": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.\n",
                        "willReplaceOnChanges": true
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean"
                    },
                    "genomics": {
                        "type": "boolean"
                    },
                    "gpu": {
                        "type": "boolean"
                    },
                    "graviton": {
                        "deprecationMessage": "Not required anymore - it's automatically enabled on the Graviton-based node types",
                        "type": "boolean"
                    },
                    "id": {
                        "description": "Databricks Runtime version, that can be used as `spark_version` field in databricks_job, databricks_cluster, or databricks_instance_pool.\n",
                        "type": "string"
                    },
                    "latest": {
                        "type": "boolean"
                    },
                    "longTermSupport": {
                        "type": "boolean"
                    },
                    "ml": {
                        "type": "boolean"
                    },
                    "photon": {
                        "deprecationMessage": "Specify runtime_engine=\"PHOTON\" in the cluster configuration",
                        "type": "boolean"
                    },
                    "scala": {
                        "type": "string"
                    },
                    "sparkVersion": {
                        "type": "string"
                    }
                },
                "required": [
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSqlWarehouse:getSqlWarehouse": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.\n\n## Example Usage\n\n* Retrieve attributes of each SQL warehouses in a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\nconst _this = all.then(all =\u003e .reduce((__obj, [__key, __value]) =\u003e ({ ...__obj, [__key]: databricks.getSqlWarehouse({\n    id: __value,\n}) })));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\nthis = {__key: databricks.get_sql_warehouse(id=__value) for __key, __value in all.ids}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n    var @this = ;\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n* Search for a specific SQL Warehouse by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouse({\n    name: \"Starter Warehouse\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouse(name=\"Starter Warehouse\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouse.Invoke(new()\n    {\n        Name = \"Starter Warehouse\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouse(ctx, \u0026databricks.GetSqlWarehouseArgs{\n\t\t\tName: pulumi.StringRef(\"Starter Warehouse\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehouseArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()\n            .name(\"Starter Warehouse\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getSqlWarehouse\n      arguments:\n        name: Starter Warehouse\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "creatorName": {
                        "type": "string",
                        "description": "The username of the user who created the endpoint.\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether [Photon](https://databricks.com/product/delta-engine) is enabled.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a serverless SQL warehouse.\n"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth",
                        "description": "Health status of the endpoint.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The ID of the SQL warehouse.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the SQL warehouse to search (case-sensitive).\n"
                    },
                    "numActiveSessions": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "numClusters": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of the endpoint.\n"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "tags used for SQL warehouse resources.\n"
                    },
                    "warehouseType": {
                        "type": "string",
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/index.html#warehouse-types) or [Azure](https://learn.microsoft.com/azure/databricks/sql/#warehouse-types).\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n",
                        "type": "integer"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n",
                        "type": "string"
                    },
                    "creatorName": {
                        "description": "The username of the user who created the endpoint.\n",
                        "type": "string"
                    },
                    "dataSourceId": {
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n",
                        "type": "string"
                    },
                    "enablePhoton": {
                        "description": "Whether [Photon](https://databricks.com/product/delta-engine) is enabled.\n",
                        "type": "boolean"
                    },
                    "enableServerlessCompute": {
                        "description": "Whether this SQL warehouse is a serverless SQL warehouse.\n",
                        "type": "boolean"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth",
                        "description": "Health status of the endpoint.\n"
                    },
                    "id": {
                        "description": "The ID of the SQL warehouse.\n",
                        "type": "string"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "description": "JDBC connection string.\n",
                        "type": "string"
                    },
                    "maxNumClusters": {
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n",
                        "type": "integer"
                    },
                    "minNumClusters": {
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n",
                        "type": "integer"
                    },
                    "name": {
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                        "type": "string"
                    },
                    "numActiveSessions": {
                        "description": "The current number of clusters used by the endpoint.\n",
                        "type": "integer"
                    },
                    "numClusters": {
                        "description": "The current number of clusters used by the endpoint.\n",
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n",
                        "type": "string"
                    },
                    "state": {
                        "description": "The current state of the endpoint.\n",
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "tags used for SQL warehouse resources.\n"
                    },
                    "warehouseType": {
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/index.html#warehouse-types) or [Azure](https://learn.microsoft.com/azure/databricks/sql/#warehouse-types).\n",
                        "type": "string"
                    }
                },
                "required": [
                    "autoStopMins",
                    "channel",
                    "clusterSize",
                    "creatorName",
                    "dataSourceId",
                    "enablePhoton",
                    "enableServerlessCompute",
                    "health",
                    "id",
                    "instanceProfileArn",
                    "jdbcUrl",
                    "maxNumClusters",
                    "minNumClusters",
                    "name",
                    "numActiveSessions",
                    "numClusters",
                    "odbcParams",
                    "spotInstancePolicy",
                    "state",
                    "tags",
                    "warehouseType"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSqlWarehouses:getSqlWarehouses": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.\n\n## Example Usage\n\nRetrieve IDs for all SQL warehouses:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, \u0026databricks.GetSqlWarehousesArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getSqlWarehouses\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nRetrieve IDs for all clusters having \"Shared\" in the warehouse name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getSqlWarehouses({\n    warehouseNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_sql_warehouses(warehouse_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetSqlWarehouses.Invoke(new()\n    {\n        WarehouseNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, \u0026databricks.GetSqlWarehousesArgs{\n\t\t\tWarehouseNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()\n            .warehouseNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      function: databricks:getSqlWarehouses\n      arguments:\n        warehouseNameContains: shared\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouses.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string",
                        "description": "Only return databricks.SqlEndpoint ids that match the given name string.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouses.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "list of databricks.SqlEndpoint ids\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "warehouseNameContains": {
                        "type": "string"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getStorageCredential:getStorageCredential": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\nRetrieves details about a databricks.StorageCredential that were created by Pulumi or manually.\n\n## Example Usage\n\nGetting details of an existing storage credential in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getStorageCredential({\n    name: \"this\",\n});\nexport const createdBy = _this.then(_this =\u003e _this.storageCredentialInfo?.createdBy);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_storage_credential(name=\"this\")\npulumi.export(\"createdBy\", this.storage_credential_info.created_by)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetStorageCredential.Invoke(new()\n    {\n        Name = \"this\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"createdBy\"] = @this.Apply(@this =\u003e @this.Apply(getStorageCredentialResult =\u003e getStorageCredentialResult.StorageCredentialInfo?.CreatedBy)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupStorageCredential(ctx, \u0026databricks.LookupStorageCredentialArgs{\n\t\t\tName: \"this\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"createdBy\", this.StorageCredentialInfo.CreatedBy)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetStorageCredentialArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()\n            .name(\"this\")\n            .build());\n\n        ctx.export(\"createdBy\", this_.storageCredentialInfo().createdBy());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getStorageCredential\n      arguments:\n        name: this\noutputs:\n  createdBy: ${this.storageCredentialInfo.createdBy}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getStorageCredentials to get names of all credentials\n* databricks.StorageCredential to manage Storage Credentials within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getStorageCredential.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "Unique ID of storage credential.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the storage credential\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialInfo": {
                        "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo",
                        "description": "array of objects with information about storage credential.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getStorageCredential.\n",
                "properties": {
                    "id": {
                        "description": "Unique ID of storage credential.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "storageCredentialInfo": {
                        "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo",
                        "description": "array of objects with information about storage credential.\n"
                    }
                },
                "required": [
                    "id",
                    "name",
                    "storageCredentialInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getStorageCredentials:getStorageCredentials": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\nRetrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.\n\n## Example Usage\n\nList all storage credentials in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getStorageCredentials({});\nexport const allStorageCredentials = all.then(all =\u003e all.names);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_storage_credentials()\npulumi.export(\"allStorageCredentials\", all.names)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetStorageCredentials.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allStorageCredentials\"] = all.Apply(getStorageCredentialsResult =\u003e getStorageCredentialsResult.Names),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetStorageCredentials(ctx, \u0026databricks.GetStorageCredentialsArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allStorageCredentials\", all.Names)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetStorageCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getStorageCredentials(GetStorageCredentialsArgs.builder()\n            .build());\n\n        ctx.export(\"allStorageCredentials\", all.names());\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      function: databricks:getStorageCredentials\n      arguments: {}\noutputs:\n  allStorageCredentials: ${all.names}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.StorageCredential to get information about a single credential\n* databricks.StorageCredential to manage Storage Credentials within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getStorageCredentials.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks.StorageCredential in the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getStorageCredentials.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks.StorageCredential in the metastore\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getTable:getTable": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog\n\n## Example Usage\n\nRead  on a specific table `main.certified.fct_transactions`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fctTransactions = databricks.getTable({\n    name: \"main.certified.fct_transactions\",\n});\nconst things = new databricks.Grants(\"things\", {\n    table: fctTransactions.then(fctTransactions =\u003e fctTransactions.name),\n    grants: [{\n        principal: \"sensitive\",\n        privileges: [\n            \"SELECT\",\n            \"MODIFY\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfct_transactions = databricks.get_table(name=\"main.certified.fct_transactions\")\nthings = databricks.Grants(\"things\",\n    table=fct_transactions.name,\n    grants=[{\n        \"principal\": \"sensitive\",\n        \"privileges\": [\n            \"SELECT\",\n            \"MODIFY\",\n        ],\n    }])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fctTransactions = Databricks.GetTable.Invoke(new()\n    {\n        Name = \"main.certified.fct_transactions\",\n    });\n\n    var things = new Databricks.Grants(\"things\", new()\n    {\n        Table = fctTransactions.Apply(getTableResult =\u003e getTableResult.Name),\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"sensitive\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                    \"MODIFY\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tfctTransactions, err := databricks.LookupTable(ctx, \u0026databricks.LookupTableArgs{\n\t\t\tName: \"main.certified.fct_transactions\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"things\", \u0026databricks.GrantsArgs{\n\t\t\tTable: pulumi.String(fctTransactions.Name),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTableArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()\n            .name(\"main.certified.fct_transactions\")\n            .build());\n\n        var things = new Grants(\"things\", GrantsArgs.builder()\n            .table(fctTransactions.name())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"sensitive\")\n                .privileges(                \n                    \"SELECT\",\n                    \"MODIFY\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  things:\n    type: databricks:Grants\n    properties:\n      table: ${fctTransactions.name}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\nvariables:\n  fctTransactions:\n    fn::invoke:\n      function: databricks:getTable\n      arguments:\n        name: main.certified.fct_transactions\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Grant to manage grants within Unity Catalog.\n* databricks.getTables to list all tables within a schema in Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getTable.\n",
                "properties": {
                    "id": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Full name of the databricks_table: _`catalog`.`schema`.`table`_\n",
                        "willReplaceOnChanges": true
                    },
                    "tableInfo": {
                        "$ref": "#/types/databricks:index/getTableTableInfo:getTableTableInfo",
                        "description": "TableInfo object for a Unity Catalog table. This contains the following attributes:\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getTable.\n",
                "properties": {
                    "id": {
                        "type": "string"
                    },
                    "name": {
                        "description": "Name of table, relative to parent schema.\n",
                        "type": "string"
                    },
                    "tableInfo": {
                        "$ref": "#/types/databricks:index/getTableTableInfo:getTableTableInfo",
                        "description": "TableInfo object for a Unity Catalog table. This contains the following attributes:\n"
                    }
                },
                "required": [
                    "id",
                    "name",
                    "tableInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getTables:getTables": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.\n\n## Example Usage\n\nGranting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const things = await databricks.getTables({\n        catalogName: \"sandbox\",\n        schemaName: \"things\",\n    });\n    const thingsGrants: databricks.Grants[] = [];\n    for (const range of things.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        thingsGrants.push(new databricks.Grants(`things-${range.key}`, {\n            table: range.value,\n            grants: [{\n                principal: \"sensitive\",\n                privileges: [\n                    \"SELECT\",\n                    \"MODIFY\",\n                ],\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_tables(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthings_grants = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(things.ids)]:\n    things_grants.append(databricks.Grants(f\"things-{range['key']}\",\n        table=range[\"value\"],\n        grants=[{\n            \"principal\": \"sensitive\",\n            \"privileges\": [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var things = await Databricks.GetTables.InvokeAsync(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var thingsGrants = new List\u003cDatabricks.Grants\u003e();\n    foreach (var range in )\n    {\n        thingsGrants.Add(new Databricks.Grants($\"things-{range.Key}\", new()\n        {\n            Table = range.Value,\n            GrantDetails = new[]\n            {\n                new Databricks.Inputs.GrantsGrantArgs\n                {\n                    Principal = \"sensitive\",\n                    Privileges = new[]\n                    {\n                        \"SELECT\",\n                        \"MODIFY\",\n                    },\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.GetTables(ctx, \u0026databricks.GetTablesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar thingsGrants []*databricks.Grants\n\t\tfor key0, val0 := range things.Ids {\n\t\t\t__res, err := databricks.NewGrants(ctx, fmt.Sprintf(\"things-%v\", key0), \u0026databricks.GrantsArgs{\n\t\t\t\tTable: pulumi.String(val0),\n\t\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tthingsGrants = append(thingsGrants, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTablesArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrants = things.applyValue(getTablesResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getTablesResult.ids())) {\n                var resource = new Grants(\"thingsGrants-\" + range.key(), GrantsArgs.builder()\n                    .table(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      table: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      function: databricks:getTables\n      arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getUser:getUser": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_user.\n\n## Example Usage\n\nAdding user to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.then(me =\u003e me.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.get_user(user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Apply(getUserResult =\u003e getUserResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: pulumi.String(me.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()\n            .groupId(admins.id())\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      function: databricks:getGroup\n      arguments:\n        displayName: admins\n  me:\n    fn::invoke:\n      function: databricks:getUser\n      arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n- End to end workspace management guide.\n- databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n- databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getUser.\n",
                "properties": {
                    "userId": {
                        "type": "string",
                        "description": "ID of the user.\n",
                        "willReplaceOnChanges": true
                    },
                    "userName": {
                        "type": "string",
                        "description": "User name of the user. The user must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getUser.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "active": {
                        "description": "Whether the user is active.\n",
                        "type": "boolean"
                    },
                    "alphanumeric": {
                        "description": "Alphanumeric representation of user local name. e.g. `mr_foo`.\n",
                        "type": "string"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "description": "Display name of the user, e.g. `Mr Foo`.\n",
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the user in an external identity provider.\n",
                        "type": "string"
                    },
                    "home": {
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "repos": {
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "userId": {
                        "type": "string"
                    },
                    "userName": {
                        "description": "Name of the user, e.g. `mr.foo@example.com`.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "active",
                    "alphanumeric",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "repos",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getViews:getViews": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.\n\n## Example Usage\n\nGranting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const things = await databricks.getViews({\n        catalogName: \"sandbox\",\n        schemaName: \"things\",\n    });\n    const thingsGrants: databricks.Grants[] = [];\n    for (const range of things.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        thingsGrants.push(new databricks.Grants(`things-${range.key}`, {\n            table: range.value,\n            grants: [{\n                principal: \"sensitive\",\n                privileges: [\n                    \"SELECT\",\n                    \"MODIFY\",\n                ],\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_views(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthings_grants = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(things.ids)]:\n    things_grants.append(databricks.Grants(f\"things-{range['key']}\",\n        table=range[\"value\"],\n        grants=[{\n            \"principal\": \"sensitive\",\n            \"privileges\": [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        }]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var things = await Databricks.GetViews.InvokeAsync(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var thingsGrants = new List\u003cDatabricks.Grants\u003e();\n    foreach (var range in )\n    {\n        thingsGrants.Add(new Databricks.Grants($\"things-{range.Key}\", new()\n        {\n            Table = range.Value,\n            GrantDetails = new[]\n            {\n                new Databricks.Inputs.GrantsGrantArgs\n                {\n                    Principal = \"sensitive\",\n                    Privileges = new[]\n                    {\n                        \"SELECT\",\n                        \"MODIFY\",\n                    },\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.GetViews(ctx, \u0026databricks.GetViewsArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar thingsGrants []*databricks.Grants\n\t\tfor key0, val0 := range things.Ids {\n\t\t\t__res, err := databricks.NewGrants(ctx, fmt.Sprintf(\"things-%v\", key0), \u0026databricks.GrantsArgs{\n\t\t\t\tTable: pulumi.String(val0),\n\t\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tthingsGrants = append(thingsGrants, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetViewsArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrants = things.applyValue(getViewsResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getViewsResult.ids())) {\n                var resource = new Grants(\"thingsGrants-\" + range.key(), GrantsArgs.builder()\n                    .table(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      table: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      function: databricks:getViews\n      arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getVolume:getVolume": {
            "description": "Retrieves details about databricks.Volume that was created by Pulumi or manually. \nA volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.\n\n## Example Usage\n\n* Retrieve details of all volumes in in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getVolumes({\n    catalogName: \"sandbox\",\n    schemaName: \"things\",\n});\nconst _this = all.then(all =\u003e .reduce((__obj, [__key, __value]) =\u003e ({ ...__obj, [__key]: databricks.getVolume({\n    name: __value,\n}) })));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_volumes(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthis = {__key: databricks.get_volume(name=__value) for __key, __value in all.ids}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetVolumes.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var @this = ;\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n* Search for a specific volume by its fully qualified name\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getVolume({\n    name: \"catalog.schema.volume\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_volume(name=\"catalog.schema.volume\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetVolume.Invoke(new()\n    {\n        Name = \"catalog.schema.volume\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupVolume(ctx, \u0026databricks.LookupVolumeArgs{\n\t\t\tName: \"catalog.schema.volume\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetVolumeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getVolume(GetVolumeArgs.builder()\n            .name(\"catalog.schema.volume\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getVolume\n      arguments:\n        name: catalog.schema.volume\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Volume to manage volumes within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getVolume.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "ID of this Unity Catalog Volume in form of `\u003ccatalog\u003e.\u003cschema\u003e.\u003cname\u003e`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "a fully qualified name of databricks_volume: *`catalog`.`schema`.`volume`*\n",
                        "willReplaceOnChanges": true
                    },
                    "volumeInfo": {
                        "$ref": "#/types/databricks:index/getVolumeVolumeInfo:getVolumeVolumeInfo",
                        "description": "`VolumeInfo` object for a Unity Catalog volume. This contains the following attributes:\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getVolume.\n",
                "properties": {
                    "id": {
                        "description": "ID of this Unity Catalog Volume in form of `\u003ccatalog\u003e.\u003cschema\u003e.\u003cname\u003e`.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "the name of the volume\n",
                        "type": "string"
                    },
                    "volumeInfo": {
                        "$ref": "#/types/databricks:index/getVolumeVolumeInfo:getVolumeVolumeInfo",
                        "description": "`VolumeInfo` object for a Unity Catalog volume. This contains the following attributes:\n"
                    }
                },
                "required": [
                    "id",
                    "name",
                    "volumeInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getVolumes:getVolumes": {
            "description": "\u003e **Note** This data source can only be used with a workspace-level provider!\n\nRetrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.\n\n## Plugin Framework Migration\n\nThe volumes data source has been migrated from sdkv2 to plugin framework in version 1.57。 If you encounter any problem with this data source and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_DATA_SOURCES=\"databricks.getVolumes\"`.\n\n## Example Usage\n\nListing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = databricks.getVolumes({\n    catalogName: \"sandbox\",\n    schemaName: \"things\",\n});\nexport const allVolumes = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_volumes(catalog_name=\"sandbox\",\n    schema_name=\"things\")\npulumi.export(\"allVolumes\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetVolumes.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allVolumes\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetVolumes(ctx, \u0026databricks.GetVolumesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allVolumes\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetVolumesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        ctx.export(\"allVolumes\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      function: databricks:getVolumes\n      arguments:\n        catalogName: sandbox\n        schemaName: things\noutputs:\n  allVolumes: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Volume to manage volumes within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getVolumes.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n"
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a list of databricks.Volume full names: *`catalog`.`schema`.`volume`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getVolumes.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "a list of databricks.Volume full names: *`catalog`.`schema`.`volume`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getZones:getZones": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks.MwsWorkspaces or azurerm_databricks_workspace, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows you to fetch all available AWS availability zones on your workspace on AWS.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst zones = databricks.getZones({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nzones = databricks.get_zones()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var zones = Databricks.GetZones.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetZones(ctx, \u0026databricks.GetZonesArgs{}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetZonesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var zones = DatabricksFunctions.getZones(GetZonesArgs.builder()\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  zones:\n    fn::invoke:\n      function: databricks:getZones\n      arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getZones.\n",
                "properties": {
                    "defaultZone": {
                        "type": "string",
                        "description": "This is the default zone that gets assigned to your workspace. This is the zone used by default for clusters and instance pools.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id for the zone object.\n"
                    },
                    "zones": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "This is a list of all the zones available for your subnets in your Databricks workspace.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getZones.\n",
                "properties": {
                    "defaultZone": {
                        "description": "This is the default zone that gets assigned to your workspace. This is the zone used by default for clusters and instance pools.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The id for the zone object.\n",
                        "type": "string"
                    },
                    "zones": {
                        "description": "This is a list of all the zones available for your subnets in your Databricks workspace.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "defaultZone",
                    "id",
                    "zones"
                ],
                "type": "object"
            }
        }
    }
}