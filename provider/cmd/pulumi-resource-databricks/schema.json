{
    "name": "databricks",
    "displayName": "Databricks",
    "description": "A Pulumi package for creating and managing databricks cloud resources.",
    "keywords": [
        "pulumi",
        "databricks",
        "category/infrastructure"
    ],
    "homepage": "https://www.pulumi.com",
    "license": "Apache-2.0",
    "attribution": "This Pulumi package is based on the [`databricks` Terraform Provider](https://github.com/databricks/terraform-provider-databricks).",
    "repository": "https://github.com/pulumi/pulumi-databricks",
    "publisher": "Pulumi",
    "meta": {
        "moduleFormat": "(.*)(?:/[^/]*)"
    },
    "language": {
        "csharp": {
            "packageReferences": {
                "Pulumi": "3.*"
            },
            "compatibility": "tfbridge20"
        },
        "go": {
            "importBasePath": "github.com/pulumi/pulumi-databricks/sdk/go/databricks",
            "generateResourceContainerTypes": true,
            "generateExtraInputTypes": true
        },
        "nodejs": {
            "packageDescription": "A Pulumi package for creating and managing databricks cloud resources.",
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "dependencies": {
                "@pulumi/pulumi": "^3.0.0"
            },
            "devDependencies": {
                "@types/mime": "^2.0.0",
                "@types/node": "^10.0.0"
            },
            "compatibility": "tfbridge20",
            "disableUnionOutputTypes": true
        },
        "python": {
            "requires": {
                "pulumi": "\u003e=3.0.0,\u003c4.0.0"
            },
            "readme": "\u003e This provider is a derived work of the [Terraform Provider](https://github.com/databricks/terraform-provider-databricks)\n\u003e distributed under [MPL 2.0](https://www.mozilla.org/en-US/MPL/2.0/). If you encounter a bug or missing feature,\n\u003e first check the [`pulumi-databricks` repo](https://github.com/pulumi/pulumi-databricks/issues); however, if that doesn't turn up anything,\n\u003e please consult the source [`terraform-provider-databricks` repo](https://github.com/databricks/terraform-provider-databricks/issues).",
            "compatibility": "tfbridge20",
            "pyproject": {
                "enabled": true
            }
        }
    },
    "config": {
        "variables": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        }
    },
    "types": {
        "databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule": {
            "properties": {
                "principals": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a list of principals who are granted a role. The following format is supported:\n* `users/{username}` (also exposed as `acl_principal_id` attribute of `databricks.User` resource).\n* `groups/{groupname}` (also exposed as `acl_principal_id` attribute of `databricks.Group` resource).\n* `servicePrincipals/{applicationId}` (also exposed as `acl_principal_id` attribute of `databricks.ServicePrincipal` resource).\n"
                },
                "role": {
                    "type": "string",
                    "description": "Role to be granted. The supported roles are listed below. For more information about these roles, refer to [service principal roles](https://docs.databricks.com/security/auth-authz/access-control/service-principal-acl.html#service-principal-roles), [group roles](https://docs.databricks.com/en/administration-guide/users-groups/groups.html#manage-roles-on-an-account-group-using-the-workspace-admin-settings-page) or [marketplace roles](https://docs.databricks.com/en/marketplace/get-started-provider.html#assign-the-marketplace-admin-role).\n* `roles/servicePrincipal.manager` - Manager of a service principal.\n* `roles/servicePrincipal.user` - User of a service principal.\n* `roles/group.manager` - Manager of a group.\n* `roles/marketplace.admin` - Admin of marketplace.\n"
                }
            },
            "type": "object",
            "required": [
                "role"
            ]
        },
        "databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher": {
            "properties": {
                "artifact": {
                    "type": "string",
                    "description": "The artifact path or maven coordinate.\n"
                },
                "matchType": {
                    "type": "string",
                    "description": "The pattern matching type of the artifact. Only `PREFIX_MATCH` is supported.\n"
                }
            },
            "type": "object",
            "required": [
                "artifact",
                "matchType"
            ]
        },
        "databricks:index/ClusterAutoscale:ClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer",
                    "description": "The maximum number of workers to which the cluster can scale up when overloaded. max_workers must be strictly greater than min_workers.\n\nWhen using a [Single Node cluster](https://docs.databricks.com/clusters/single-node.html), `num_workers` needs to be `0`. It can be set to `0` explicitly, or simply not specified, as it defaults to `0`.  When `num_workers` is `0`, provider checks for presence of the required Spark configurations:\n\n* `spark.master` must have prefix `local`, like `local[*]`\n* `spark.databricks.cluster.profile` must have value `singleNode`\n\nand also `custom_tag` entry:\n\n* `\"ResourceClass\" = \"SingleNode\"`\n\nThe following example demonstrates how to create an single node cluster:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst singleNode = new databricks.Cluster(\"single_node\", {\n    clusterName: \"Single Node\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.cluster.profile\": \"singleNode\",\n        \"spark.master\": \"local[*]\",\n    },\n    customTags: {\n        ResourceClass: \"SingleNode\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nsingle_node = databricks.Cluster(\"single_node\",\n    cluster_name=\"Single Node\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.cluster.profile\": \"singleNode\",\n        \"spark.master\": \"local[*]\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"SingleNode\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var singleNode = new Databricks.Cluster(\"single_node\", new()\n    {\n        ClusterName = \"Single Node\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.cluster.profile\", \"singleNode\" },\n            { \"spark.master\", \"local[*]\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"SingleNode\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"single_node\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Single Node\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.cluster.profile\": pulumi.Any(\"singleNode\"),\n\t\t\t\t\"spark.master\":                     pulumi.Any(\"local[*]\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"ResourceClass\": pulumi.Any(\"SingleNode\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var singleNode = new Cluster(\"singleNode\", ClusterArgs.builder()        \n            .clusterName(\"Single Node\")\n            .sparkVersion(latestLts.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.cluster.profile\", \"singleNode\"),\n                Map.entry(\"spark.master\", \"local[*]\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"SingleNode\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  singleNode:\n    type: databricks:Cluster\n    name: single_node\n    properties:\n      clusterName: Single Node\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.cluster.profile: singleNode\n        spark.master: local[*]\n      customTags:\n        ResourceClass: SingleNode\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "minWorkers": {
                    "type": "integer",
                    "description": "The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAwsAttributes:ClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Valid values are `SPOT`, `SPOT_WITH_FALLBACK` and `ON_DEMAND`. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster. Backend default value is `SPOT_WITH_FALLBACK` and could change in the future\n"
                },
                "ebsVolumeCount": {
                    "type": "integer",
                    "description": "The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.\n"
                },
                "ebsVolumeIops": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer",
                    "description": "The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).\n"
                },
                "ebsVolumeThroughput": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string",
                    "description": "The type of EBS volumes that will be launched with this cluster. Valid values are `GENERAL_PURPOSE_SSD` or `THROUGHPUT_OPTIMIZED_HDD`. Use this option only if you're not picking *Delta Optimized `i3.*`* node types.\n"
                },
                "firstOnDemand": {
                    "type": "integer",
                    "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. Backend default value is `1` and could change in the future\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new `i3.xlarge` spot instance, then the max price is half of the price of on-demand `i3.xlarge` instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand `i3.xlarge` instances. If not specified, the default value is `100`. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than `10000`.\n"
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-west-2a`. The provided availability zone must be in the same region as the Databricks deployment. For example, `us-west-2a` is not a valid zone ID if the Databricks deployment resides in the `us-east-1` region. Enable automatic availability zone selection (\"Auto-AZ\"), by setting the value `auto`. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributes:ClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all subsequent nodes past the `first_on_demand` ones. Valid values are `SPOT_AZURE`, `SPOT_WITH_FALLBACK_AZURE`, and `ON_DEMAND_AZURE`. Note: If `first_on_demand` is zero, this availability type will be used for the entire cluster.\n"
                },
                "firstOnDemand": {
                    "type": "integer",
                    "description": "The first `first_on_demand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `first_on_demand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.\n"
                },
                "logAnalyticsInfo": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributesLogAnalyticsInfo:ClusterAzureAttributesLogAnalyticsInfo"
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max price for Azure spot instances.  Use `-1` to specify the lowest price.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterAzureAttributesLogAnalyticsInfo:ClusterAzureAttributesLogAnalyticsInfo": {
            "properties": {
                "logAnalyticsPrimaryKey": {
                    "type": "string"
                },
                "logAnalyticsWorkspaceId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterCloneFrom:ClusterCloneFrom": {
            "properties": {
                "sourceClusterId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "sourceClusterId"
            ]
        },
        "databricks:index/ClusterClusterLogConf:ClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterClusterLogConfDbfs:ClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterLogConfS3:ClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string",
                    "description": "path inside the Spark container.\n\nFor example, you can mount Azure Data Lake Storage container using the following code:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst storageAccount = \"ewfw3ggwegwg\";\nconst storageContainer = \"test\";\nconst withNfs = new databricks.Cluster(\"with_nfs\", {clusterMountInfos: [{\n    networkFilesystemInfo: {\n        serverAddress: `${storageAccount}.blob.core.windows.net`,\n        mountOptions: \"sec=sys,vers=3,nolock,proto=tcp\",\n    },\n    remoteMountDirPath: `${storageAccount}/${storageContainer}`,\n    localMountDirPath: \"/mnt/nfs-test\",\n}]});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nstorage_account = \"ewfw3ggwegwg\"\nstorage_container = \"test\"\nwith_nfs = databricks.Cluster(\"with_nfs\", cluster_mount_infos=[databricks.ClusterClusterMountInfoArgs(\n    network_filesystem_info=databricks.ClusterClusterMountInfoNetworkFilesystemInfoArgs(\n        server_address=f\"{storage_account}.blob.core.windows.net\",\n        mount_options=\"sec=sys,vers=3,nolock,proto=tcp\",\n    ),\n    remote_mount_dir_path=f\"{storage_account}/{storage_container}\",\n    local_mount_dir_path=\"/mnt/nfs-test\",\n)])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var storageAccount = \"ewfw3ggwegwg\";\n\n    var storageContainer = \"test\";\n\n    var withNfs = new Databricks.Cluster(\"with_nfs\", new()\n    {\n        ClusterMountInfos = new[]\n        {\n            new Databricks.Inputs.ClusterClusterMountInfoArgs\n            {\n                NetworkFilesystemInfo = new Databricks.Inputs.ClusterClusterMountInfoNetworkFilesystemInfoArgs\n                {\n                    ServerAddress = $\"{storageAccount}.blob.core.windows.net\",\n                    MountOptions = \"sec=sys,vers=3,nolock,proto=tcp\",\n                },\n                RemoteMountDirPath = $\"{storageAccount}/{storageContainer}\",\n                LocalMountDirPath = \"/mnt/nfs-test\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tstorageAccount := \"ewfw3ggwegwg\"\n\t\tstorageContainer := \"test\"\n\t\t_, err := databricks.NewCluster(ctx, \"with_nfs\", \u0026databricks.ClusterArgs{\n\t\t\tClusterMountInfos: databricks.ClusterClusterMountInfoArray{\n\t\t\t\t\u0026databricks.ClusterClusterMountInfoArgs{\n\t\t\t\t\tNetworkFilesystemInfo: \u0026databricks.ClusterClusterMountInfoNetworkFilesystemInfoArgs{\n\t\t\t\t\t\tServerAddress: pulumi.String(fmt.Sprintf(\"%v.blob.core.windows.net\", storageAccount)),\n\t\t\t\t\t\tMountOptions:  pulumi.String(\"sec=sys,vers=3,nolock,proto=tcp\"),\n\t\t\t\t\t},\n\t\t\t\t\tRemoteMountDirPath: pulumi.String(fmt.Sprintf(\"%v/%v\", storageAccount, storageContainer)),\n\t\t\t\t\tLocalMountDirPath:  pulumi.String(\"/mnt/nfs-test\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterClusterMountInfoArgs;\nimport com.pulumi.databricks.inputs.ClusterClusterMountInfoNetworkFilesystemInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var storageAccount = \"ewfw3ggwegwg\";\n\n        final var storageContainer = \"test\";\n\n        var withNfs = new Cluster(\"withNfs\", ClusterArgs.builder()        \n            .clusterMountInfos(ClusterClusterMountInfoArgs.builder()\n                .networkFilesystemInfo(ClusterClusterMountInfoNetworkFilesystemInfoArgs.builder()\n                    .serverAddress(String.format(\"%s.blob.core.windows.net\", storageAccount))\n                    .mountOptions(\"sec=sys,vers=3,nolock,proto=tcp\")\n                    .build())\n                .remoteMountDirPath(String.format(\"%s/%s\", storageAccount,storageContainer))\n                .localMountDirPath(\"/mnt/nfs-test\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  withNfs:\n    type: databricks:Cluster\n    name: with_nfs\n    properties:\n      clusterMountInfos:\n        - networkFilesystemInfo:\n            serverAddress: ${storageAccount}.blob.core.windows.net\n            mountOptions: sec=sys,vers=3,nolock,proto=tcp\n          remoteMountDirPath: ${storageAccount}/${storageContainer}\n          localMountDirPath: /mnt/nfs-test\nvariables:\n  storageAccount: ewfw3ggwegwg\n  storageContainer: test\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo",
                    "description": "block specifying connection. It consists of:\n"
                },
                "remoteMountDirPath": {
                    "type": "string",
                    "description": "string specifying path to mount on the remote service.\n"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/ClusterClusterMountInfoNetworkFilesystemInfo:ClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string",
                    "description": "string that will be passed as options passed to the `mount` command.\n"
                },
                "serverAddress": {
                    "type": "string",
                    "description": "host name.\n"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/ClusterDockerImage:ClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth",
                    "description": "`basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.\n\nExample usage with azurerm_container_registry, that you can adapt to your specific use-case:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as docker from \"@pulumi/docker\";\n\nconst _this = new docker.index.RegistryImage(\"this\", {\n    build: [{}],\n    name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,\n});\nconst thisCluster = new databricks.Cluster(\"this\", {dockerImage: {\n    url: _this.name,\n    basicAuth: {\n        username: thisAzurermContainerRegistry.adminUsername,\n        password: thisAzurermContainerRegistry.adminPassword,\n    },\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_docker as docker\n\nthis = docker.index.RegistryImage(\"this\",\n    build=[{}],\n    name=f{this_azurerm_container_registry.login_server}/sample:latest)\nthis_cluster = databricks.Cluster(\"this\", docker_image=databricks.ClusterDockerImageArgs(\n    url=this[\"name\"],\n    basic_auth=databricks.ClusterDockerImageBasicAuthArgs(\n        username=this_azurerm_container_registry[\"adminUsername\"],\n        password=this_azurerm_container_registry[\"adminPassword\"],\n    ),\n))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Docker = Pulumi.Docker;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Docker.Index.RegistryImage(\"this\", new()\n    {\n        Build = new[]\n        {\n            null,\n        },\n        Name = $\"{thisAzurermContainerRegistry.LoginServer}/sample:latest\",\n    });\n\n    var thisCluster = new Databricks.Cluster(\"this\", new()\n    {\n        DockerImage = new Databricks.Inputs.ClusterDockerImageArgs\n        {\n            Url = @this.Name,\n            BasicAuth = new Databricks.Inputs.ClusterDockerImageBasicAuthArgs\n            {\n                Username = thisAzurermContainerRegistry.AdminUsername,\n                Password = thisAzurermContainerRegistry.AdminPassword,\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-docker/sdk/v4/go/docker\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := docker.NewRegistryImage(ctx, \"this\", \u0026docker.RegistryImageArgs{\n\t\t\tBuild: []map[string]interface{}{\n\t\t\t\tnil,\n\t\t\t},\n\t\t\tName: fmt.Sprintf(\"%v/sample:latest\", thisAzurermContainerRegistry.LoginServer),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tDockerImage: \u0026databricks.ClusterDockerImageArgs{\n\t\t\t\tUrl: this.Name,\n\t\t\t\tBasicAuth: \u0026databricks.ClusterDockerImageBasicAuthArgs{\n\t\t\t\t\tUsername: pulumi.Any(thisAzurermContainerRegistry.AdminUsername),\n\t\t\t\t\tPassword: pulumi.Any(thisAzurermContainerRegistry.AdminPassword),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.docker.registryImage;\nimport com.pulumi.docker.RegistryImageArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterDockerImageArgs;\nimport com.pulumi.databricks.inputs.ClusterDockerImageBasicAuthArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegistryImage(\"this\", RegistryImageArgs.builder()        \n            .build()\n            .name(String.format(\"%s/sample:latest\", thisAzurermContainerRegistry.loginServer()))\n            .build());\n\n        var thisCluster = new Cluster(\"thisCluster\", ClusterArgs.builder()        \n            .dockerImage(ClusterDockerImageArgs.builder()\n                .url(this_.name())\n                .basicAuth(ClusterDockerImageBasicAuthArgs.builder()\n                    .username(thisAzurermContainerRegistry.adminUsername())\n                    .password(thisAzurermContainerRegistry.adminPassword())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: docker:registryImage\n    properties:\n      build:\n        - {}\n      name: ${thisAzurermContainerRegistry.loginServer}/sample:latest\n  thisCluster:\n    type: databricks:Cluster\n    name: this\n    properties:\n      dockerImage:\n        url: ${this.name}\n        basicAuth:\n          username: ${thisAzurermContainerRegistry.adminUsername}\n          password: ${thisAzurermContainerRegistry.adminPassword}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL for the Docker image\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/ClusterDockerImageBasicAuth:ClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/ClusterGcpAttributes:ClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n"
                },
                "bootDiskSize": {
                    "type": "integer",
                    "description": "Boot disk size in GB\n"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.\n"
                },
                "localSsdCount": {
                    "type": "integer",
                    "description": "Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.\n"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean",
                    "description": "if we should use preemptible executors ([GCP documentation](https://cloud.google.com/compute/docs/instances/preemptible)). *Warning: this field is deprecated in favor of `availability`, and will be removed soon.*\n"
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone in which the cluster resides. This can be one of the following:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScript:ClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptFile:ClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptS3:ClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptVolumes:ClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/ClusterInitScriptWorkspace:ClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterInitScriptAbfss:ClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptDbfs:ClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptFile:ClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptGcs:ClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptS3:ClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptVolumes:ClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterInitScriptWorkspace:ClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/ClusterLibrary:ClusterLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterLibraryCran:ClusterLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterLibraryMaven:ClusterLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterLibraryPypi:ClusterLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterLibraryCran:ClusterLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterLibraryMaven:ClusterLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterLibraryPypi:ClusterLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryCran:ClusterPolicyLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryMaven:ClusterPolicyLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/ClusterPolicyLibraryPypi:ClusterPolicyLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/ClusterPolicyLibraryCran:ClusterPolicyLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterPolicyLibraryMaven:ClusterPolicyLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/ClusterPolicyLibraryPypi:ClusterPolicyLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/ClusterWorkloadType:ClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/ClusterWorkloadTypeClients:ClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean",
                    "description": "boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: `true`.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withNfs = new databricks.Cluster(\"with_nfs\", {workloadType: {\n    clients: {\n        jobs: false,\n        notebooks: true,\n    },\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_nfs = databricks.Cluster(\"with_nfs\", workload_type=databricks.ClusterWorkloadTypeArgs(\n    clients=databricks.ClusterWorkloadTypeClientsArgs(\n        jobs=False,\n        notebooks=True,\n    ),\n))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withNfs = new Databricks.Cluster(\"with_nfs\", new()\n    {\n        WorkloadType = new Databricks.Inputs.ClusterWorkloadTypeArgs\n        {\n            Clients = new Databricks.Inputs.ClusterWorkloadTypeClientsArgs\n            {\n                Jobs = false,\n                Notebooks = true,\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"with_nfs\", \u0026databricks.ClusterArgs{\n\t\t\tWorkloadType: \u0026databricks.ClusterWorkloadTypeArgs{\n\t\t\t\tClients: \u0026databricks.ClusterWorkloadTypeClientsArgs{\n\t\t\t\t\tJobs:      pulumi.Bool(false),\n\t\t\t\t\tNotebooks: pulumi.Bool(true),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterWorkloadTypeArgs;\nimport com.pulumi.databricks.inputs.ClusterWorkloadTypeClientsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var withNfs = new Cluster(\"withNfs\", ClusterArgs.builder()        \n            .workloadType(ClusterWorkloadTypeArgs.builder()\n                .clients(ClusterWorkloadTypeClientsArgs.builder()\n                    .jobs(false)\n                    .notebooks(true)\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  withNfs:\n    type: databricks:Cluster\n    name: with_nfs\n    properties:\n      workloadType:\n        clients:\n          jobs: false\n          notebooks: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "notebooks": {
                    "type": "boolean",
                    "description": "boolean flag defining if it's possible to run notebooks on this cluster. Default: `true`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace": {
            "properties": {
                "value": {
                    "type": "string",
                    "description": "The value for the setting.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetailsSseEncryptionDetails:ExternalLocationEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/ExternalLocationEncryptionDetailsSseEncryptionDetails:ExternalLocationEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/GrantsGrant:GrantsGrant": {
            "properties": {
                "principal": {
                    "type": "string"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "(String) Availability type used for all instances in the pool. Only `ON_DEMAND` and `SPOT` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "spotBidPricePercent": {
                    "type": "integer",
                    "description": "(Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*\n",
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "(String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `\"us-west-2a\"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `\"us-west-2a\"` is not a valid zone ID if the Databricks deployment resides in the `\"us-east-1\"` region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `SPOT_AZURE` and `ON_DEMAND_AZURE`.\n",
                    "willReplaceOnChanges": true
                },
                "spotBidMaxPrice": {
                    "type": "number",
                    "description": "The max price for Azure spot instances.  Use `-1` to specify the lowest price.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer",
                    "description": "(Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.\n"
                },
                "diskSize": {
                    "type": "integer",
                    "description": "(Integer) The size of each disk (in GiB) to attach.\n"
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType"
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolDiskSpecDiskType:InstancePoolDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "ebsVolumeType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string",
                    "description": "Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.\n",
                    "willReplaceOnChanges": true
                },
                "localSsdCount": {
                    "type": "integer",
                    "description": "Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.\n",
                    "willReplaceOnChanges": true
                },
                "zoneId": {
                    "type": "string",
                    "description": "Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-central1-a`. The provided availability zone must be in the same region as the Databricks workspace.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "localSsdCount",
                        "zoneId"
                    ]
                }
            }
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption",
                    "willReplaceOnChanges": true
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption",
                    "willReplaceOnChanges": true
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride"
                    },
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetOnDemandOption:InstancePoolInstancePoolFleetAttributesFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesFleetSpotOption:InstancePoolInstancePoolFleetAttributesFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride:InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instanceType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth",
                    "description": "`basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.\n\nExample usage with azurerm_container_registry, that you can adapt to your specific use-case:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as docker from \"@pulumi/docker\";\n\nconst _this = new docker.index.RegistryImage(\"this\", {\n    build: [{}],\n    name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,\n});\nconst thisInstancePool = new databricks.InstancePool(\"this\", {preloadedDockerImages: [{\n    url: _this.name,\n    basicAuth: {\n        username: thisAzurermContainerRegistry.adminUsername,\n        password: thisAzurermContainerRegistry.adminPassword,\n    },\n}]});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_docker as docker\n\nthis = docker.index.RegistryImage(\"this\",\n    build=[{}],\n    name=f{this_azurerm_container_registry.login_server}/sample:latest)\nthis_instance_pool = databricks.InstancePool(\"this\", preloaded_docker_images=[databricks.InstancePoolPreloadedDockerImageArgs(\n    url=this[\"name\"],\n    basic_auth=databricks.InstancePoolPreloadedDockerImageBasicAuthArgs(\n        username=this_azurerm_container_registry[\"adminUsername\"],\n        password=this_azurerm_container_registry[\"adminPassword\"],\n    ),\n)])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Docker = Pulumi.Docker;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Docker.Index.RegistryImage(\"this\", new()\n    {\n        Build = new[]\n        {\n            null,\n        },\n        Name = $\"{thisAzurermContainerRegistry.LoginServer}/sample:latest\",\n    });\n\n    var thisInstancePool = new Databricks.InstancePool(\"this\", new()\n    {\n        PreloadedDockerImages = new[]\n        {\n            new Databricks.Inputs.InstancePoolPreloadedDockerImageArgs\n            {\n                Url = @this.Name,\n                BasicAuth = new Databricks.Inputs.InstancePoolPreloadedDockerImageBasicAuthArgs\n                {\n                    Username = thisAzurermContainerRegistry.AdminUsername,\n                    Password = thisAzurermContainerRegistry.AdminPassword,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-docker/sdk/v4/go/docker\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := docker.NewRegistryImage(ctx, \"this\", \u0026docker.RegistryImageArgs{\n\t\t\tBuild: []map[string]interface{}{\n\t\t\t\tnil,\n\t\t\t},\n\t\t\tName: fmt.Sprintf(\"%v/sample:latest\", thisAzurermContainerRegistry.LoginServer),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstancePool(ctx, \"this\", \u0026databricks.InstancePoolArgs{\n\t\t\tPreloadedDockerImages: databricks.InstancePoolPreloadedDockerImageArray{\n\t\t\t\t\u0026databricks.InstancePoolPreloadedDockerImageArgs{\n\t\t\t\t\tUrl: this.Name,\n\t\t\t\t\tBasicAuth: \u0026databricks.InstancePoolPreloadedDockerImageBasicAuthArgs{\n\t\t\t\t\t\tUsername: pulumi.Any(thisAzurermContainerRegistry.AdminUsername),\n\t\t\t\t\t\tPassword: pulumi.Any(thisAzurermContainerRegistry.AdminPassword),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.docker.registryImage;\nimport com.pulumi.docker.RegistryImageArgs;\nimport com.pulumi.databricks.InstancePool;\nimport com.pulumi.databricks.InstancePoolArgs;\nimport com.pulumi.databricks.inputs.InstancePoolPreloadedDockerImageArgs;\nimport com.pulumi.databricks.inputs.InstancePoolPreloadedDockerImageBasicAuthArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegistryImage(\"this\", RegistryImageArgs.builder()        \n            .build()\n            .name(String.format(\"%s/sample:latest\", thisAzurermContainerRegistry.loginServer()))\n            .build());\n\n        var thisInstancePool = new InstancePool(\"thisInstancePool\", InstancePoolArgs.builder()        \n            .preloadedDockerImages(InstancePoolPreloadedDockerImageArgs.builder()\n                .url(this_.name())\n                .basicAuth(InstancePoolPreloadedDockerImageBasicAuthArgs.builder()\n                    .username(thisAzurermContainerRegistry.adminUsername())\n                    .password(thisAzurermContainerRegistry.adminPassword())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: docker:registryImage\n    properties:\n      build:\n        - {}\n      name: ${thisAzurermContainerRegistry.loginServer}/sample:latest\n  thisInstancePool:\n    type: databricks:InstancePool\n    name: this\n    properties:\n      preloadedDockerImages:\n        - url: ${this.name}\n          basicAuth:\n            username: ${thisAzurermContainerRegistry.adminUsername}\n            password: ${thisAzurermContainerRegistry.adminPassword}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
                    "willReplaceOnChanges": true
                },
                "url": {
                    "type": "string",
                    "description": "URL for the Docker image\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/InstancePoolPreloadedDockerImageBasicAuth:InstancePoolPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "username": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobContinuous:JobContinuous": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this continuous job is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobDbtTask:JobDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobDeployment:JobDeployment": {
            "properties": {
                "kind": {
                    "type": "string"
                },
                "metadataFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "kind"
            ]
        },
        "databricks:index/JobEmailNotifications:JobEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobEnvironment:JobEnvironment": {
            "properties": {
                "environmentKey": {
                    "type": "string"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/JobEnvironmentSpec:JobEnvironmentSpec"
                }
            },
            "type": "object",
            "required": [
                "environmentKey"
            ]
        },
        "databricks:index/JobEnvironmentSpec:JobEnvironmentSpec": {
            "properties": {
                "client": {
                    "type": "string"
                },
                "dependencies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "client"
            ]
        },
        "databricks:index/JobGitSource:JobGitSource": {
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `tag` and `commit`.\n"
                },
                "commit": {
                    "type": "string",
                    "description": "hash of Git commit to use. Conflicts with `branch` and `tag`.\n"
                },
                "jobSource": {
                    "$ref": "#/types/databricks:index/JobGitSourceJobSource:JobGitSourceJobSource"
                },
                "provider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.\n"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the Git branch to use. Conflicts with `branch` and `commit`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the Git repository to use.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobGitSourceJobSource:JobGitSourceJobSource": {
            "properties": {
                "dirtyState": {
                    "type": "string"
                },
                "importFromGitBranch": {
                    "type": "string"
                },
                "jobConfigPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "importFromGitBranch",
                "jobConfigPath"
            ]
        },
        "databricks:index/JobHealth:JobHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobHealthRule:JobHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobHealthRule:JobHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check.  The only supported metric is `RUN_DURATION_SECONDS` (check [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobCluster:JobJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string",
                    "description": "Identifier that can be referenced in `task` block, so that cluster is shared between tasks\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster",
                    "description": "Same set of parameters as for databricks.Cluster resource.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewCluster:JobJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobJobClusterNewClusterAutoscale:JobJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAwsAttributes:JobJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterAzureAttributes:JobJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConf:JobJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfDbfs:JobJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterLogConfS3:JobJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfo:JobJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImage:JobJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobJobClusterNewClusterDockerImageBasicAuth:JobJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobJobClusterNewClusterGcpAttributes:JobJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScript:JobJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptVolumes:JobJobClusterNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterInitScriptWorkspace:JobJobClusterNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobJobClusterNewClusterInitScriptAbfss:JobJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptDbfs:JobJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptFile:JobJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptGcs:JobJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptS3:JobJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptVolumes:JobJobClusterNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterInitScriptWorkspace:JobJobClusterNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadType:JobJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobJobClusterNewClusterWorkloadTypeClients:JobJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibrary:JobLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobLibraryCran:JobLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobLibraryMaven:JobLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobLibraryPypi:JobLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobLibraryCran:JobLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobLibraryMaven:JobLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobLibraryPypi:JobLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobNewCluster:JobNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobNewClusterInitScript:JobNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobNewClusterAutoscale:JobNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAwsAttributes:JobNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterAzureAttributes:JobNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConf:JobNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterClusterLogConfDbfs:JobNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterLogConfS3:JobNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfo:JobNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobNewClusterClusterMountInfoNetworkFilesystemInfo:JobNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobNewClusterDockerImage:JobNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobNewClusterDockerImageBasicAuth:JobNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobNewClusterGcpAttributes:JobNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScript:JobNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptVolumes:JobNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobNewClusterInitScriptWorkspace:JobNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNewClusterInitScriptAbfss:JobNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptDbfs:JobNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptFile:JobNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptGcs:JobNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptS3:JobNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptVolumes:JobNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterInitScriptWorkspace:JobNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobNewClusterWorkloadType:JobNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobNewClusterWorkloadTypeClients:JobNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobNotebookTask:JobNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobNotificationSettings:JobNotificationSettings": {
            "properties": {
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobParameter:JobParameter": {
            "properties": {
                "default": {
                    "type": "string",
                    "description": "Default value of the parameter.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the defined parameter. May only contain alphanumeric characters, `_`, `-`, and `.`.\n"
                }
            },
            "type": "object",
            "required": [
                "default",
                "name"
            ]
        },
        "databricks:index/JobPipelineTask:JobPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e **Note** The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobPythonWheelTask:JobPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobQueue:JobQueue": {
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "If true, enable queueing for the job.\n"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/JobRunAs:JobRunAs": {
            "properties": {
                "servicePrincipalName": {
                    "type": "string",
                    "description": "The application ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.\n\nExample:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Job(\"this\", {runAs: {\n    servicePrincipalName: \"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Job(\"this\", run_as=databricks.JobRunAsArgs(\n    service_principal_name=\"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Job(\"this\", new()\n    {\n        RunAs = new Databricks.Inputs.JobRunAsArgs\n        {\n            ServicePrincipalName = \"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tRunAs: \u0026databricks.JobRunAsArgs{\n\t\t\t\tServicePrincipalName: pulumi.String(\"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobRunAsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Job(\"this\", JobArgs.builder()        \n            .runAs(JobRunAsArgs.builder()\n                .servicePrincipalName(\"8d23ae77-912e-4a19-81e4-b9c3f5cc9349\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Job\n    properties:\n      runAs:\n        servicePrincipalName: 8d23ae77-912e-4a19-81e4-b9c3f5cc9349\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "userName": {
                    "type": "string",
                    "description": "The email of an active workspace user. Non-admin users can only set this field to their own email.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobRunJobTask:JobRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Job parameters for the task\n"
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobSchedule:JobSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this schedule is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted and a schedule is provided, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.\n"
                },
                "timezoneId": {
                    "type": "string",
                    "description": "A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.\n"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ]
        },
        "databricks:index/JobSparkJarTask:JobSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobSparkPythonTask:JobSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobSparkSubmitTask:JobSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTask:JobTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/JobTaskConditionTask:JobTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskDbtTask:JobTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskDependsOn:JobTaskDependsOn"
                    },
                    "description": "block specifying dependency(-ies) for a given task.\n"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "forEachTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTask:JobTaskForEachTask"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobTaskHealth:JobTaskHealth",
                    "description": "block described below that specifies health conditions for a given task.\n"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskLibrary:JobTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskNewCluster:JobTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskNotebookTask:JobTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobTaskNotificationSettings:JobTaskNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level (described below).\n"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskPipelineTask:JobTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "runIf": {
                    "type": "string",
                    "description": "An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobTaskRunJobTask:JobTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTask:JobTaskSqlTask"
                },
                "taskKey": {
                    "type": "string",
                    "description": "string specifying an unique key for a given task.\n* `*_task` - (Required) one of the specific task blocks described below:\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskWebhookNotifications:JobTaskWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout"
                    ]
                }
            }
        },
        "databricks:index/JobTaskConditionTask:JobTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string",
                    "description": "The left operand of the condition task. It could be a string value, job state, or a parameter reference.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n\nThis task does not require a cluster to execute and does not support retries or notifications.\n"
                },
                "right": {
                    "type": "string",
                    "description": "The right operand of the condition task. It could be a string value, job state, or parameter reference.\n"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/JobTaskDbtTask:JobTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskDependsOn:JobTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string",
                    "description": "Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `\"true\"` or `\"false\"`.\n"
                },
                "taskKey": {
                    "type": "string",
                    "description": "The name of the task this task depends on.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/JobTaskEmailNotifications:JobTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTask:JobTaskForEachTask": {
            "properties": {
                "concurrency": {
                    "type": "integer",
                    "description": "Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.\n"
                },
                "inputs": {
                    "type": "string",
                    "description": "(String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.\n"
                },
                "task": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTask:JobTaskForEachTaskTask",
                    "description": "Task to run against the `inputs` list.\n"
                }
            },
            "type": "object",
            "required": [
                "inputs",
                "task"
            ]
        },
        "databricks:index/JobTaskForEachTaskTask:JobTaskForEachTaskTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskConditionTask:JobTaskForEachTaskTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskDbtTask:JobTaskForEachTaskTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskDependsOn:JobTaskForEachTaskTaskDependsOn"
                    },
                    "description": "block specifying dependency(-ies) for a given task.\n"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskEmailNotifications:JobTaskForEachTaskTaskEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskHealth:JobTaskForEachTaskTaskHealth",
                    "description": "block described below that specifies health conditions for a given task.\n"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibrary:JobTaskForEachTaskTaskLibrary"
                    },
                    "description": "(Set) An optional list of libraries to be installed on the cluster that will execute the job.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.\n"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewCluster:JobTaskForEachTaskTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNotebookTask:JobTaskForEachTaskTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNotificationSettings:JobTaskForEachTaskTaskNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level (described below).\n"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskPipelineTask:JobTaskForEachTaskTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskPythonWheelTask:JobTaskForEachTaskTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "description": "(Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.\n"
                },
                "runIf": {
                    "type": "string",
                    "description": "An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.\n"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskRunJobTask:JobTaskForEachTaskTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkJarTask:JobTaskForEachTaskTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkPythonTask:JobTaskForEachTaskTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSparkSubmitTask:JobTaskForEachTaskTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTask:JobTaskForEachTaskTaskSqlTask"
                },
                "taskKey": {
                    "type": "string",
                    "description": "string specifying an unique key for a given task.\n* `*_task` - (Required) one of the specific task blocks described below:\n"
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotifications:JobTaskForEachTaskTaskWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "retryOnTimeout"
                    ]
                }
            }
        },
        "databricks:index/JobTaskForEachTaskTaskConditionTask:JobTaskForEachTaskTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string",
                    "description": "The left operand of the condition task. It could be a string value, job state, or a parameter reference.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n\nThis task does not require a cluster to execute and does not support retries or notifications.\n"
                },
                "right": {
                    "type": "string",
                    "description": "The right operand of the condition task. It could be a string value, job state, or parameter reference.\n"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskDbtTask:JobTaskForEachTaskTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string",
                    "description": "The name of the catalog to use inside Unity Catalog.\n"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(Array) Series of dbt commands to execute in sequence. Every command must start with \"dbt\".\n"
                },
                "profilesDirectory": {
                    "type": "string",
                    "description": "The relative path to the directory in the repository specified by `git_source` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.\n"
                },
                "projectDirectory": {
                    "type": "string",
                    "description": "The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.\n* If `source` is `GIT`: Relative path to the directory in the repository specified in the `git_source` block. Defaults to the repository's root directory when not specified.\n* If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.\n"
                },
                "schema": {
                    "type": "string",
                    "description": "The name of the schema dbt should run in. Defaults to `default`.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `git_source` block is present in the job definition.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "The ID of the SQL warehouse that dbt should execute against.\n\nYou also need to include a `git_source` block to configure the repository that contains the dbt project.\n"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskDependsOn:JobTaskForEachTaskTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string",
                    "description": "Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `\"true\"` or `\"false\"`.\n"
                },
                "taskKey": {
                    "type": "string",
                    "description": "The name of the task this task depends on.\n"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskEmailNotifications:JobTaskForEachTaskTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskHealth:JobTaskForEachTaskTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskHealthRule:JobTaskForEachTaskTaskHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskHealthRule:JobTaskForEachTaskTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check.  The only supported metric is `RUN_DURATION_SECONDS` (check [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskLibrary:JobTaskForEachTaskTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryCran:JobTaskForEachTaskTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryMaven:JobTaskForEachTaskTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskLibraryPypi:JobTaskForEachTaskTaskLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryCran:JobTaskForEachTaskTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryMaven:JobTaskForEachTaskTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskLibraryPypi:JobTaskForEachTaskTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewCluster:JobTaskForEachTaskTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAutoscale:JobTaskForEachTaskTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAwsAttributes:JobTaskForEachTaskTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributes:JobTaskForEachTaskTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConf:JobTaskForEachTaskTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterDockerImage:JobTaskForEachTaskTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterGcpAttributes:JobTaskForEachTaskTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScript:JobTaskForEachTaskTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadType:JobTaskForEachTaskTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAutoscale:JobTaskForEachTaskTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAwsAttributes:JobTaskForEachTaskTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterAzureAttributes:JobTaskForEachTaskTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConf:JobTaskForEachTaskTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs:JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfS3:JobTaskForEachTaskTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs:JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterLogConfS3:JobTaskForEachTaskTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterDockerImage:JobTaskForEachTaskTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth:JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth:JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterGcpAttributes:JobTaskForEachTaskTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScript:JobTaskForEachTaskTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptAbfss:JobTaskForEachTaskTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptDbfs:JobTaskForEachTaskTaskNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptFile:JobTaskForEachTaskTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptGcs:JobTaskForEachTaskTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptS3:JobTaskForEachTaskTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptVolumes:JobTaskForEachTaskTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptWorkspace:JobTaskForEachTaskTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptAbfss:JobTaskForEachTaskTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptDbfs:JobTaskForEachTaskTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptFile:JobTaskForEachTaskTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptGcs:JobTaskForEachTaskTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptS3:JobTaskForEachTaskTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptVolumes:JobTaskForEachTaskTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterInitScriptWorkspace:JobTaskForEachTaskTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadType:JobTaskForEachTaskTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadTypeClients:JobTaskForEachTaskTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNewClusterWorkloadTypeClients:JobTaskForEachTaskTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskNotebookTask:JobTaskForEachTaskTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskNotificationSettings:JobTaskForEachTaskTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "description": "(Bool) do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskPipelineTask:JobTaskForEachTaskTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e **Note** The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskPythonWheelTask:JobTaskForEachTaskTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskRunJobTask:JobTaskForEachTaskTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Job parameters for the task\n"
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSparkJarTask:JobTaskForEachTaskTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSparkPythonTask:JobTaskForEachTaskTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSparkSubmitTask:JobTaskForEachTaskTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTask:JobTaskForEachTaskTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskAlert:JobTaskForEachTaskTaskSqlTaskAlert",
                    "description": "block consisting of following fields:\n"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskDashboard:JobTaskForEachTaskTaskSqlTaskDashboard",
                    "description": "block consisting of following fields:\n"
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskFile:JobTaskForEachTaskTaskSqlTaskFile",
                    "description": "block consisting of single string fields:\n"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.\n"
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskQuery:JobTaskForEachTaskTaskSqlTaskQuery",
                    "description": "block consisting of single string field: `query_id` - identifier of the Databricks SQL Query (databricks_sql_query).\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless \u0026 Pro warehouses are supported right now.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskAlert:JobTaskForEachTaskTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Alert.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskAlertSubscription:JobTaskForEachTaskTaskSqlTaskAlertSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "alertId",
                "subscriptions"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskAlertSubscription:JobTaskForEachTaskTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskDashboard:JobTaskForEachTaskTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string",
                    "description": "string specifying a custom subject of email sent.\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskSqlTaskDashboardSubscription:JobTaskForEachTaskTaskSqlTaskDashboardSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskDashboardSubscription:JobTaskForEachTaskTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskFile:JobTaskForEachTaskTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string",
                    "description": "If `source` is `GIT`: Relative path to the file in the repository specified in the `git_source` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.\n\nExample\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlAggregationJob = new databricks.Job(\"sql_aggregation_job\", {\n    name: \"Example SQL Job\",\n    tasks: [\n        {\n            taskKey: \"run_agg_query\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                query: {\n                    queryId: aggQuery.id,\n                },\n            },\n        },\n        {\n            taskKey: \"run_dashboard\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                dashboard: {\n                    dashboardId: dash.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            taskKey: \"run_alert\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                alert: {\n                    alertId: alert.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsql_aggregation_job = databricks.Job(\"sql_aggregation_job\",\n    name=\"Example SQL Job\",\n    tasks=[\n        databricks.JobTaskArgs(\n            task_key=\"run_agg_query\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                query=databricks.JobTaskSqlTaskQueryArgs(\n                    query_id=agg_query[\"id\"],\n                ),\n            ),\n        ),\n        databricks.JobTaskArgs(\n            task_key=\"run_dashboard\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                dashboard=databricks.JobTaskSqlTaskDashboardArgs(\n                    dashboard_id=dash[\"id\"],\n                    subscriptions=[databricks.JobTaskSqlTaskDashboardSubscriptionArgs(\n                        user_name=\"user@domain.com\",\n                    )],\n                ),\n            ),\n        ),\n        databricks.JobTaskArgs(\n            task_key=\"run_alert\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                alert=databricks.JobTaskSqlTaskAlertArgs(\n                    alert_id=alert[\"id\"],\n                    subscriptions=[databricks.JobTaskSqlTaskAlertSubscriptionArgs(\n                        user_name=\"user@domain.com\",\n                    )],\n                ),\n            ),\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlAggregationJob = new Databricks.Job(\"sql_aggregation_job\", new()\n    {\n        Name = \"Example SQL Job\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_agg_query\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Query = new Databricks.Inputs.JobTaskSqlTaskQueryArgs\n                    {\n                        QueryId = aggQuery.Id,\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_dashboard\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Dashboard = new Databricks.Inputs.JobTaskSqlTaskDashboardArgs\n                    {\n                        DashboardId = dash.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskDashboardSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_alert\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Alert = new Databricks.Inputs.JobTaskSqlTaskAlertArgs\n                    {\n                        AlertId = alert.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskAlertSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"sql_aggregation_job\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.String(\"Example SQL Job\"),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_agg_query\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tQuery: \u0026databricks.JobTaskSqlTaskQueryArgs{\n\t\t\t\t\t\t\tQueryId: pulumi.Any(aggQuery.Id),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_dashboard\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tDashboard: \u0026databricks.JobTaskSqlTaskDashboardArgs{\n\t\t\t\t\t\t\tDashboardId: pulumi.Any(dash.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskDashboardSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskDashboardSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_alert\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tAlert: \u0026databricks.JobTaskSqlTaskAlertArgs{\n\t\t\t\t\t\t\tAlertId: pulumi.Any(alert.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskAlertSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskAlertSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskQueryArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskDashboardArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskAlertArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sqlAggregationJob = new Job(\"sqlAggregationJob\", JobArgs.builder()        \n            .name(\"Example SQL Job\")\n            .tasks(            \n                JobTaskArgs.builder()\n                    .taskKey(\"run_agg_query\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .query(JobTaskSqlTaskQueryArgs.builder()\n                            .queryId(aggQuery.id())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_dashboard\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .dashboard(JobTaskSqlTaskDashboardArgs.builder()\n                            .dashboardId(dash.id())\n                            .subscriptions(JobTaskSqlTaskDashboardSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_alert\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .alert(JobTaskSqlTaskAlertArgs.builder()\n                            .alertId(alert.id())\n                            .subscriptions(JobTaskSqlTaskAlertSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sqlAggregationJob:\n    type: databricks:Job\n    name: sql_aggregation_job\n    properties:\n      name: Example SQL Job\n      tasks:\n        - taskKey: run_agg_query\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            query:\n              queryId: ${aggQuery.id}\n        - taskKey: run_dashboard\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            dashboard:\n              dashboardId: ${dash.id}\n              subscriptions:\n                - userName: user@domain.com\n        - taskKey: run_alert\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            alert:\n              alertId: ${alert.id}\n              subscriptions:\n                - userName: user@domain.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.\n"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskSqlTaskQuery:JobTaskForEachTaskTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotifications:JobTaskForEachTaskTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnFailure:JobTaskForEachTaskTaskWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStart:JobTaskForEachTaskTaskWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnSuccess:JobTaskForEachTaskTaskWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnFailure:JobTaskForEachTaskTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnStart:JobTaskForEachTaskTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskForEachTaskTaskWebhookNotificationsOnSuccess:JobTaskForEachTaskTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskHealth:JobTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskHealthRule:JobTaskHealthRule"
                    },
                    "description": "list of rules that are represented as objects with the following attributes:\n"
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/JobTaskHealthRule:JobTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string",
                    "description": "string specifying the metric to check.  The only supported metric is `RUN_DURATION_SECONDS` (check [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).\n"
                },
                "op": {
                    "type": "string",
                    "description": "string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.\n"
                },
                "value": {
                    "type": "integer",
                    "description": "integer value used to compare to the given metric.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibrary:JobTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryCran:JobTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskLibraryCran:JobTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskLibraryMaven:JobTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/JobTaskLibraryPypi:JobTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/JobTaskNewCluster:JobTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverInstancePoolId",
                        "driverNodeTypeId",
                        "enableElasticDisk",
                        "enableLocalDiskEncryption",
                        "nodeTypeId",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/JobTaskNewClusterAutoscale:JobTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAwsAttributes:JobTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterAzureAttributes:JobTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConf:JobTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterClusterLogConfDbfs:JobTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterLogConfS3:JobTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfo:JobTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo:JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImage:JobTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTaskNewClusterDockerImageBasicAuth:JobTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/JobTaskNewClusterGcpAttributes:JobTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScript:JobTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptVolumes:JobTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterInitScriptWorkspace:JobTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNewClusterInitScriptAbfss:JobTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptDbfs:JobTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptFile:JobTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptGcs:JobTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptS3:JobTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptVolumes:JobTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterInitScriptWorkspace:JobTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadType:JobTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/JobTaskNewClusterWorkloadTypeClients:JobTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskNotebookTask:JobTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in base_parameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s base_parameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.\n"
                },
                "notebookPath": {
                    "type": "string",
                    "description": "The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `git_source`. If the value is empty, the task will use `GIT` if `git_source` is defined and `WORKSPACE` otherwise.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.\n"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/JobTaskNotificationSettings:JobTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean",
                    "description": "(Bool) do not send notifications to recipients specified in `on_start` for the retried runs and do not send notifications to recipients specified in `on_failure` until the last retry of the run.\n"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for cancelled runs.\n"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean",
                    "description": "(Bool) don't send alert for skipped runs.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskPipelineTask:JobTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean",
                    "description": "(Bool) Specifies if there should be full refresh of the pipeline.\n\n\u003e **Note** The following configuration blocks are only supported inside a `task` block\n"
                },
                "pipelineId": {
                    "type": "string",
                    "description": "The pipeline's unique ID.\n"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/JobTaskPythonWheelTask:JobTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string",
                    "description": "Python function as entry point for the task\n"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Named parameters for the task\n"
                },
                "packageName": {
                    "type": "string",
                    "description": "Name of Python package\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Parameters for the task\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskRunJobTask:JobTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer",
                    "description": "(String) ID of the job\n"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Job parameters for the task\n"
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/JobTaskSparkJarTask:JobTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string",
                    "description": "The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Parameters passed to the main method.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSparkPythonTask:JobTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command line parameters passed to the Python file.\n"
                },
                "pythonFile": {
                    "type": "string",
                    "description": "The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `git_source`.\n"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/JobTaskSparkSubmitTask:JobTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) Command-line parameters passed to spark submit.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTask:JobTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert",
                    "description": "block consisting of following fields:\n"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard",
                    "description": "block consisting of following fields:\n"
                },
                "file": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskFile:JobTaskSqlTaskFile",
                    "description": "block consisting of single string fields:\n"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.\n"
                },
                "query": {
                    "$ref": "#/types/databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery",
                    "description": "block consisting of single string field: `query_id` - identifier of the Databricks SQL Query (databricks_sql_query).\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless \u0026 Pro warehouses are supported right now.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskAlert:JobTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Alert.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskSqlTaskAlertSubscription:JobTaskSqlTaskAlertSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "alertId",
                "subscriptions"
            ]
        },
        "databricks:index/JobTaskSqlTaskAlertSubscription:JobTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskDashboard:JobTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string",
                    "description": "string specifying a custom subject of email sent.\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "(String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.\n"
                },
                "pauseSubscriptions": {
                    "type": "boolean",
                    "description": "flag that specifies if subscriptions are paused or not.\n"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskSqlTaskDashboardSubscription:JobTaskSqlTaskDashboardSubscription"
                    },
                    "description": "a list of subscription blocks consisting out of one of the required fields: `user_name` for user emails or `destination_id` - for Alert destination's identifier.\n"
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/JobTaskSqlTaskDashboardSubscription:JobTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskSqlTaskFile:JobTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string",
                    "description": "If `source` is `GIT`: Relative path to the file in the repository specified in the `git_source` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.\n\nExample\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlAggregationJob = new databricks.Job(\"sql_aggregation_job\", {\n    name: \"Example SQL Job\",\n    tasks: [\n        {\n            taskKey: \"run_agg_query\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                query: {\n                    queryId: aggQuery.id,\n                },\n            },\n        },\n        {\n            taskKey: \"run_dashboard\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                dashboard: {\n                    dashboardId: dash.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n        {\n            taskKey: \"run_alert\",\n            sqlTask: {\n                warehouseId: sqlJobWarehouse.id,\n                alert: {\n                    alertId: alert.id,\n                    subscriptions: [{\n                        userName: \"user@domain.com\",\n                    }],\n                },\n            },\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsql_aggregation_job = databricks.Job(\"sql_aggregation_job\",\n    name=\"Example SQL Job\",\n    tasks=[\n        databricks.JobTaskArgs(\n            task_key=\"run_agg_query\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                query=databricks.JobTaskSqlTaskQueryArgs(\n                    query_id=agg_query[\"id\"],\n                ),\n            ),\n        ),\n        databricks.JobTaskArgs(\n            task_key=\"run_dashboard\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                dashboard=databricks.JobTaskSqlTaskDashboardArgs(\n                    dashboard_id=dash[\"id\"],\n                    subscriptions=[databricks.JobTaskSqlTaskDashboardSubscriptionArgs(\n                        user_name=\"user@domain.com\",\n                    )],\n                ),\n            ),\n        ),\n        databricks.JobTaskArgs(\n            task_key=\"run_alert\",\n            sql_task=databricks.JobTaskSqlTaskArgs(\n                warehouse_id=sql_job_warehouse[\"id\"],\n                alert=databricks.JobTaskSqlTaskAlertArgs(\n                    alert_id=alert[\"id\"],\n                    subscriptions=[databricks.JobTaskSqlTaskAlertSubscriptionArgs(\n                        user_name=\"user@domain.com\",\n                    )],\n                ),\n            ),\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlAggregationJob = new Databricks.Job(\"sql_aggregation_job\", new()\n    {\n        Name = \"Example SQL Job\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_agg_query\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Query = new Databricks.Inputs.JobTaskSqlTaskQueryArgs\n                    {\n                        QueryId = aggQuery.Id,\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_dashboard\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Dashboard = new Databricks.Inputs.JobTaskSqlTaskDashboardArgs\n                    {\n                        DashboardId = dash.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskDashboardSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"run_alert\",\n                SqlTask = new Databricks.Inputs.JobTaskSqlTaskArgs\n                {\n                    WarehouseId = sqlJobWarehouse.Id,\n                    Alert = new Databricks.Inputs.JobTaskSqlTaskAlertArgs\n                    {\n                        AlertId = alert.Id,\n                        Subscriptions = new[]\n                        {\n                            new Databricks.Inputs.JobTaskSqlTaskAlertSubscriptionArgs\n                            {\n                                UserName = \"user@domain.com\",\n                            },\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewJob(ctx, \"sql_aggregation_job\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.String(\"Example SQL Job\"),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_agg_query\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tQuery: \u0026databricks.JobTaskSqlTaskQueryArgs{\n\t\t\t\t\t\t\tQueryId: pulumi.Any(aggQuery.Id),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_dashboard\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tDashboard: \u0026databricks.JobTaskSqlTaskDashboardArgs{\n\t\t\t\t\t\t\tDashboardId: pulumi.Any(dash.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskDashboardSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskDashboardSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"run_alert\"),\n\t\t\t\t\tSqlTask: \u0026databricks.JobTaskSqlTaskArgs{\n\t\t\t\t\t\tWarehouseId: pulumi.Any(sqlJobWarehouse.Id),\n\t\t\t\t\t\tAlert: \u0026databricks.JobTaskSqlTaskAlertArgs{\n\t\t\t\t\t\t\tAlertId: pulumi.Any(alert.Id),\n\t\t\t\t\t\t\tSubscriptions: databricks.JobTaskSqlTaskAlertSubscriptionArray{\n\t\t\t\t\t\t\t\t\u0026databricks.JobTaskSqlTaskAlertSubscriptionArgs{\n\t\t\t\t\t\t\t\t\tUserName: pulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskQueryArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskDashboardArgs;\nimport com.pulumi.databricks.inputs.JobTaskSqlTaskAlertArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sqlAggregationJob = new Job(\"sqlAggregationJob\", JobArgs.builder()        \n            .name(\"Example SQL Job\")\n            .tasks(            \n                JobTaskArgs.builder()\n                    .taskKey(\"run_agg_query\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .query(JobTaskSqlTaskQueryArgs.builder()\n                            .queryId(aggQuery.id())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_dashboard\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .dashboard(JobTaskSqlTaskDashboardArgs.builder()\n                            .dashboardId(dash.id())\n                            .subscriptions(JobTaskSqlTaskDashboardSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build(),\n                JobTaskArgs.builder()\n                    .taskKey(\"run_alert\")\n                    .sqlTask(JobTaskSqlTaskArgs.builder()\n                        .warehouseId(sqlJobWarehouse.id())\n                        .alert(JobTaskSqlTaskAlertArgs.builder()\n                            .alertId(alert.id())\n                            .subscriptions(JobTaskSqlTaskAlertSubscriptionArgs.builder()\n                                .userName(\"user@domain.com\")\n                                .build())\n                            .build())\n                        .build())\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sqlAggregationJob:\n    type: databricks:Job\n    name: sql_aggregation_job\n    properties:\n      name: Example SQL Job\n      tasks:\n        - taskKey: run_agg_query\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            query:\n              queryId: ${aggQuery.id}\n        - taskKey: run_dashboard\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            dashboard:\n              dashboardId: ${dash.id}\n              subscriptions:\n                - userName: user@domain.com\n        - taskKey: run_alert\n          sqlTask:\n            warehouseId: ${sqlJobWarehouse.id}\n            alert:\n              alertId: ${alert.id}\n              subscriptions:\n                - userName: user@domain.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "source": {
                    "type": "string",
                    "description": "The source of the project. Possible values are `WORKSPACE` and `GIT`.\n"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/JobTaskSqlTaskQuery:JobTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/JobTaskWebhookNotifications:JobTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnFailure:JobTaskWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnStart:JobTaskWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTaskWebhookNotificationsOnSuccess:JobTaskWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded:JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnFailure:JobTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnStart:JobTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTaskWebhookNotificationsOnSuccess:JobTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobTrigger:JobTrigger": {
            "properties": {
                "fileArrival": {
                    "$ref": "#/types/databricks:index/JobTriggerFileArrival:JobTriggerFileArrival",
                    "description": "configuration block to define a trigger for [File Arrival events](https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/file-arrival-triggers) consisting of following attributes:\n"
                },
                "pauseStatus": {
                    "type": "string",
                    "description": "Indicate whether this trigger is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pause_status` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pause_status`.\n"
                },
                "tableUpdate": {
                    "$ref": "#/types/databricks:index/JobTriggerTableUpdate:JobTriggerTableUpdate",
                    "description": "configuration block to define a trigger for Table Update events consisting of following attributes:\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobTriggerFileArrival:JobTriggerFileArrival": {
            "properties": {
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.\n"
                },
                "url": {
                    "type": "string",
                    "description": "string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (`/`).\n"
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/JobTriggerTableUpdate:JobTriggerTableUpdate": {
            "properties": {
                "condition": {
                    "type": "string",
                    "description": "The table(s) condition based on which to trigger a job run. Valid values are `ANY_UPDATED` or `ALL_UPDATED`.\n"
                },
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.\n"
                },
                "tableNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A list of Delta tables to monitor for changes. The table name must be in the format `catalog_name.schema_name.table_name`.\n"
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer",
                    "description": "If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.\n"
                }
            },
            "type": "object",
            "required": [
                "tableNames"
            ]
        },
        "databricks:index/JobWebhookNotifications:JobWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnDurationWarningThresholdExceeded:JobWebhookNotificationsOnDurationWarningThresholdExceeded"
                    },
                    "description": "(List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.\n\nNote that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://\u003cworkspace host\u003e/sql/destinations/\u003cnotification id\u003e?o=\u003cworkspace id\u003e`\n\nExample\n\n"
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure"
                    },
                    "description": "(List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.\n"
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart"
                    },
                    "description": "(List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.\n"
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess"
                    },
                    "description": "(List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/JobWebhookNotificationsOnDurationWarningThresholdExceeded:JobWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnFailure:JobWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnStart:JobWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/JobWebhookNotificationsOnSuccess:JobWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "ID of the job\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric": {
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "[create metric definition](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html#create-definition)\n"
                },
                "inputColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "Columns on the monitored table to apply the custom metrics to.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the custom metric.\n"
                },
                "outputDataType": {
                    "type": "string",
                    "description": "The output type of the custom metric.\n"
                },
                "type": {
                    "type": "string",
                    "description": "The type of the custom metric.\n"
                }
            },
            "type": "object",
            "required": [
                "definition",
                "inputColumns",
                "name",
                "outputDataType",
                "type"
            ]
        },
        "databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "labelCol": {
                    "type": "string",
                    "description": "Column of the model label\n"
                },
                "modelIdCol": {
                    "type": "string",
                    "description": "Column of the model id or version\n"
                },
                "predictionCol": {
                    "type": "string",
                    "description": "Column of the model prediction\n"
                },
                "predictionProbaCol": {
                    "type": "string",
                    "description": "Column of the model prediction probabilities\n"
                },
                "problemType": {
                    "type": "string",
                    "description": "Problem type the model aims to solve. Either `PROBLEM_TYPE_CLASSIFICATION` or `PROBLEM_TYPE_REGRESSION`\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "modelIdCol",
                "predictionCol",
                "problemType",
                "timestampCol"
            ]
        },
        "databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications": {
            "properties": {
                "onFailure": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotificationsOnFailure:LakehouseMonitorNotificationsOnFailure",
                    "description": "who to send notifications to on monitor failure.\n"
                },
                "onNewClassificationTagDetected": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotificationsOnNewClassificationTagDetected:LakehouseMonitorNotificationsOnNewClassificationTagDetected",
                    "description": "Who to send notifications to when new data classification tags are detected.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorNotificationsOnFailure:LakehouseMonitorNotificationsOnFailure": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorNotificationsOnNewClassificationTagDetected:LakehouseMonitorNotificationsOnNewClassificationTagDetected": {
            "properties": {
                "emailAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string",
                    "description": "optional string field that indicates whether a schedule is paused (`PAUSED`) or not (`UNPAUSED`).\n"
                },
                "quartzCronExpression": {
                    "type": "string",
                    "description": "string expression that determines when to run the monitor. See [Quartz documentation](https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for examples.\n"
                },
                "timezoneId": {
                    "type": "string",
                    "description": "string with timezone id (e.g., `PST`) in which to evaluate the Quartz expression.\n"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ]
        },
        "databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot": {
            "type": "object"
        },
        "databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries": {
            "properties": {
                "granularities": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of granularities to use when aggregating data into time windows based on their timestamp.\n"
                },
                "timestampCol": {
                    "type": "string",
                    "description": "Column of the timestamp of predictions\n"
                }
            },
            "type": "object",
            "required": [
                "granularities",
                "timestampCol"
            ]
        },
        "databricks:index/LibraryCran:LibraryCran": {
            "properties": {
                "package": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/LibraryMaven:LibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/LibraryPypi:LibraryPypi": {
            "properties": {
                "package": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repo": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "unityCatalogIamArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "externalId",
                        "roleArn",
                        "unityCatalogIamArn"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "accessConnectorId",
                        "credentialId"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecret": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "credentialId",
                        "email"
                    ]
                }
            }
        },
        "databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "privateKey": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "privateKeyId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/MlflowModelTag:MlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec": {
            "properties": {
                "authorization": {
                    "type": "string",
                    "description": "Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `\u003cauth type\u003e \u003ccredentials\u003e`, e.g. `Bearer \u003caccess_token\u003e`. If set to an empty string, no authorization header will be included in the request.\n"
                },
                "enableSslVerification": {
                    "type": "boolean",
                    "description": "Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.\n"
                },
                "secret": {
                    "type": "string",
                    "description": "Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as `X-Databricks-Signature: encoded_payload`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.\n"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec": {
            "properties": {
                "accessToken": {
                    "type": "string",
                    "description": "The personal access token used to authorize webhook's job runs.\n"
                },
                "jobId": {
                    "type": "string",
                    "description": "ID of the Databricks job that the webhook runs.\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.\n"
                }
            },
            "type": "object",
            "required": [
                "accessToken",
                "jobId"
            ]
        },
        "databricks:index/ModelServingConfig:ModelServingConfig": {
            "properties": {
                "autoCaptureConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigAutoCaptureConfig:ModelServingConfigAutoCaptureConfig",
                    "description": "Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.\n"
                },
                "servedEntities": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigServedEntity:ModelServingConfigServedEntity"
                    },
                    "description": "A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.\n"
                },
                "servedModels": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigServedModel:ModelServingConfigServedModel"
                    },
                    "description": "Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.\n",
                    "deprecationMessage": "Please use 'config.served_entities' instead of 'config.served_models'."
                },
                "trafficConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigTrafficConfig:ModelServingConfigTrafficConfig",
                    "description": "A single block represents the traffic split configuration amongst the served models.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "trafficConfig"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigAutoCaptureConfig:ModelServingConfigAutoCaptureConfig": {
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.\n",
                    "willReplaceOnChanges": true
                },
                "enabled": {
                    "type": "boolean",
                    "description": "If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable again.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.\n",
                    "willReplaceOnChanges": true
                },
                "tableNamePrefix": {
                    "type": "string",
                    "description": "The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigServedEntity:ModelServingConfigServedEntity": {
            "properties": {
                "entityName": {
                    "type": "string",
                    "description": "The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type `FEATURE_SPEC` in the UC. If it is a UC object, the full name of the object should be given in the form of `catalog_name.schema_name.model_name`.\n"
                },
                "entityVersion": {
                    "type": "string",
                    "description": "The version of the model in Databricks Model Registry to be served or empty if the entity is a `FEATURE_SPEC`.\n"
                },
                "environmentVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and subject to change. Example entity environment variables that refer to Databricks secrets: ```{\"OPENAI_API_KEY\": \"{{secrets/my_scope/my_key}}\", \"DATABRICKS_TOKEN\": \"{{secrets/my_scope2/my_key2}}\"}```\n"
                },
                "externalModel": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModel:ModelServingConfigServedEntityExternalModel",
                    "description": "The external model to be served. NOTE: Only one of `external_model` and (`entity_name`, `entity_version`, `workload_size`, `workload_type`, and `scale_to_zero_enabled`) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an `external_model` is present, the served entities list can only have one `served_entity` object. For an existing endpoint with `external_model`, it can not be updated to an endpoint without `external_model`. If the endpoint is created without `external_model`, users cannot update it to add `external_model` later.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "ARN of the instance profile that the served entity uses to access AWS resources.\n"
                },
                "maxProvisionedThroughput": {
                    "type": "integer",
                    "description": "The maximum tokens per second that the endpoint can scale up to.\n"
                },
                "minProvisionedThroughput": {
                    "type": "integer",
                    "description": "The minimum tokens per second that the endpoint can scale down to.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to `external_model.name`, with '.' and ':' replaced with '-', and if not specified for other entities, it defaults to -.\n"
                },
                "scaleToZeroEnabled": {
                    "type": "boolean",
                    "description": "Whether the compute resources for the served entity should scale down to zero.\n"
                },
                "workloadSize": {
                    "type": "string",
                    "description": "The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency). If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size is 0.\n"
                },
                "workloadType": {
                    "type": "string",
                    "description": "The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is `CPU`. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See the available [GPU types](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#gpu-workload-types).\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "name"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigServedEntityExternalModel:ModelServingConfigServedEntityExternalModel": {
            "properties": {
                "ai21labsConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAi21labsConfig:ModelServingConfigServedEntityExternalModelAi21labsConfig",
                    "description": "AI21Labs Config\n"
                },
                "amazonBedrockConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAmazonBedrockConfig:ModelServingConfigServedEntityExternalModelAmazonBedrockConfig",
                    "description": "Amazon Bedrock Config\n"
                },
                "anthropicConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelAnthropicConfig:ModelServingConfigServedEntityExternalModelAnthropicConfig",
                    "description": "Anthropic Config\n"
                },
                "cohereConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelCohereConfig:ModelServingConfigServedEntityExternalModelCohereConfig",
                    "description": "Cohere Config\n"
                },
                "databricksModelServingConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig:ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig",
                    "description": "Databricks Model Serving Config\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the external model.\n"
                },
                "openaiConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelOpenaiConfig:ModelServingConfigServedEntityExternalModelOpenaiConfig",
                    "description": "OpenAI Config\n"
                },
                "palmConfig": {
                    "$ref": "#/types/databricks:index/ModelServingConfigServedEntityExternalModelPalmConfig:ModelServingConfigServedEntityExternalModelPalmConfig",
                    "description": "PaLM Config\n"
                },
                "provider": {
                    "type": "string",
                    "description": "The name of the provider for the external model. Currently, the supported providers are `ai21labs`, `anthropic`, `amazon-bedrock`, `cohere`, `databricks-model-serving`, `openai`, and `palm`.\n"
                },
                "task": {
                    "type": "string",
                    "description": "The task type of the external model.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "provider",
                "task"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAi21labsConfig:ModelServingConfigServedEntityExternalModelAi21labsConfig": {
            "properties": {
                "ai21labsApiKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "ai21labsApiKey"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAmazonBedrockConfig:ModelServingConfigServedEntityExternalModelAmazonBedrockConfig": {
            "properties": {
                "awsAccessKeyId": {
                    "type": "string"
                },
                "awsRegion": {
                    "type": "string"
                },
                "awsSecretAccessKey": {
                    "type": "string"
                },
                "bedrockProvider": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "awsAccessKeyId",
                "awsRegion",
                "awsSecretAccessKey",
                "bedrockProvider"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelAnthropicConfig:ModelServingConfigServedEntityExternalModelAnthropicConfig": {
            "properties": {
                "anthropicApiKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "anthropicApiKey"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelCohereConfig:ModelServingConfigServedEntityExternalModelCohereConfig": {
            "properties": {
                "cohereApiKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "cohereApiKey"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig:ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig": {
            "properties": {
                "databricksApiToken": {
                    "type": "string"
                },
                "databricksWorkspaceUrl": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "databricksApiToken",
                "databricksWorkspaceUrl"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelOpenaiConfig:ModelServingConfigServedEntityExternalModelOpenaiConfig": {
            "properties": {
                "openaiApiBase": {
                    "type": "string"
                },
                "openaiApiKey": {
                    "type": "string"
                },
                "openaiApiType": {
                    "type": "string"
                },
                "openaiApiVersion": {
                    "type": "string"
                },
                "openaiDeploymentName": {
                    "type": "string"
                },
                "openaiOrganization": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "openaiApiKey"
            ]
        },
        "databricks:index/ModelServingConfigServedEntityExternalModelPalmConfig:ModelServingConfigServedEntityExternalModelPalmConfig": {
            "properties": {
                "palmApiKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "palmApiKey"
            ]
        },
        "databricks:index/ModelServingConfigServedModel:ModelServingConfigServedModel": {
            "properties": {
                "environmentVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "a map of environment variable name/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: `{{secrets/secret_scope/secret_key}}`.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "ARN of the instance profile that the served model will use to access AWS resources.\n"
                },
                "modelName": {
                    "type": "string",
                    "description": "The name of the model in Databricks Model Registry to be served.\n"
                },
                "modelVersion": {
                    "type": "string",
                    "description": "The version of the model in Databricks Model Registry to be served.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of a served model. It must be unique across an endpoint. If not specified, this field will default to `modelname-modelversion`. A served model name can consist of alphanumeric characters, dashes, and underscores.\n"
                },
                "scaleToZeroEnabled": {
                    "type": "boolean",
                    "description": "Whether the compute resources for the served model should scale down to zero. If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is `true`.\n"
                },
                "workloadSize": {
                    "type": "string",
                    "description": "The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency).\n"
                },
                "workloadType": {
                    "type": "string",
                    "description": "The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See documentation for all options. The default value is `CPU`.\n"
                }
            },
            "type": "object",
            "required": [
                "modelName",
                "modelVersion",
                "workloadSize"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "modelName",
                        "modelVersion",
                        "name",
                        "workloadSize"
                    ]
                }
            }
        },
        "databricks:index/ModelServingConfigTrafficConfig:ModelServingConfigTrafficConfig": {
            "properties": {
                "routes": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingConfigTrafficConfigRoute:ModelServingConfigTrafficConfigRoute"
                    },
                    "description": "Each block represents a route that defines traffic to each served entity. Each `served_entity` block needs to have a corresponding `routes` block.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/ModelServingConfigTrafficConfigRoute:ModelServingConfigTrafficConfigRoute": {
            "properties": {
                "servedModelName": {
                    "type": "string"
                },
                "trafficPercentage": {
                    "type": "integer",
                    "description": "The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.\n"
                }
            },
            "type": "object",
            "required": [
                "servedModelName",
                "trafficPercentage"
            ]
        },
        "databricks:index/ModelServingRateLimit:ModelServingRateLimit": {
            "properties": {
                "calls": {
                    "type": "integer",
                    "description": "Used to specify how many calls are allowed for a key within the renewal_period.\n"
                },
                "key": {
                    "type": "string",
                    "description": "Key field for a serving endpoint rate limit. Currently, only `user` and `endpoint` are supported, with `endpoint` being the default if not specified.\n"
                },
                "renewalPeriod": {
                    "type": "string",
                    "description": "Renewal period field for a serving endpoint rate limit. Currently, only `minute` is supported.\n"
                }
            },
            "type": "object",
            "required": [
                "calls",
                "renewalPeriod"
            ]
        },
        "databricks:index/ModelServingTag:ModelServingTag": {
            "properties": {
                "key": {
                    "type": "string",
                    "description": "The key field for a tag.\n"
                },
                "value": {
                    "type": "string",
                    "description": "The value field for a tag.\n"
                }
            },
            "type": "object",
            "required": [
                "key"
            ]
        },
        "databricks:index/MountAbfs:MountAbfs": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "initializeFileSystem": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope",
                "initializeFileSystem"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "containerName",
                        "initializeFileSystem",
                        "storageAccountName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountAdl:MountAdl": {
            "properties": {
                "clientId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clientSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sparkConfPrefix": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageResourceName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tenantId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "clientId",
                "clientSecretKey",
                "clientSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "clientId",
                        "clientSecretKey",
                        "clientSecretScope",
                        "storageResourceName",
                        "tenantId"
                    ]
                }
            }
        },
        "databricks:index/MountGs:MountGs": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "serviceAccount": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountS3:MountS3": {
            "properties": {
                "bucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instanceProfile": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "bucketName"
            ]
        },
        "databricks:index/MountWasb:MountWasb": {
            "properties": {
                "authType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "containerName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directory": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageAccountName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tokenSecretKey": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "tokenSecretScope": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "authType",
                "tokenSecretKey",
                "tokenSecretScope"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "authType",
                        "containerName",
                        "storageAccountName",
                        "tokenSecretKey",
                        "tokenSecretScope"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo": {
            "properties": {
                "keyAlias": {
                    "type": "string",
                    "description": "The AWS KMS key alias.\n",
                    "willReplaceOnChanges": true
                },
                "keyArn": {
                    "type": "string",
                    "description": "The AWS KMS key's Amazon Resource Name (ARN).\n",
                    "willReplaceOnChanges": true
                },
                "keyRegion": {
                    "type": "string",
                    "description": "(Computed) The AWS region in which KMS key is deployed to. This is not required.\n"
                }
            },
            "type": "object",
            "required": [
                "keyArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "keyArn",
                        "keyRegion"
                    ]
                }
            }
        },
        "databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo": {
            "properties": {
                "kmsKeyId": {
                    "type": "string",
                    "description": "The GCP KMS key's resource name.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "kmsKeyId"
            ]
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig": {
            "properties": {
                "defaultRules": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRules:MwsNetworkConnectivityConfigEgressConfigDefaultRules"
                },
                "targetRules": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRules:MwsNetworkConnectivityConfigEgressConfigTargetRules"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRules:MwsNetworkConnectivityConfigEgressConfigDefaultRules": {
            "properties": {
                "awsStableIpRule": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule"
                },
                "azureServiceEndpointRule": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule",
                    "description": "This provides a list of subnets. These subnets need to be allowed in your Azure resources in order for Databricks to access. See `default_rules.azure_service_endpoint_rule.target_services` for the supported Azure services.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule": {
            "properties": {
                "cidrBlocks": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule:MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule": {
            "properties": {
                "subnets": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "targetRegion": {
                    "type": "string"
                },
                "targetServices": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRules:MwsNetworkConnectivityConfigEgressConfigTargetRules": {
            "properties": {
                "azurePrivateEndpointRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule:MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule": {
            "properties": {
                "connectionState": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "deactivated": {
                    "type": "boolean"
                },
                "deactivatedAt": {
                    "type": "integer"
                },
                "endpointName": {
                    "type": "string"
                },
                "groupId": {
                    "type": "string"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "resourceId": {
                    "type": "string"
                },
                "ruleId": {
                    "type": "string"
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage": {
            "properties": {
                "errorMessage": {
                    "type": "string"
                },
                "errorType": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo": {
            "properties": {
                "networkProjectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID of the VPC network.\n",
                    "willReplaceOnChanges": true
                },
                "podIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.\n",
                    "willReplaceOnChanges": true
                },
                "serviceIpRangeName": {
                    "type": "string",
                    "description": "The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.\n",
                    "willReplaceOnChanges": true
                },
                "subnetId": {
                    "type": "string",
                    "description": "The ID of the subnet associated with this network.\n",
                    "willReplaceOnChanges": true
                },
                "subnetRegion": {
                    "type": "string",
                    "description": "The Google Cloud region of the workspace data plane. For example, `us-east4`.\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "networkProjectId",
                "podIpRangeName",
                "serviceIpRangeName",
                "subnetId",
                "subnetRegion",
                "vpcId"
            ]
        },
        "databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints": {
            "properties": {
                "dataplaneRelays": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "restApis": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "dataplaneRelays",
                "restApis"
            ]
        },
        "databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo": {
            "properties": {
                "endpointRegion": {
                    "type": "string",
                    "description": "Region of the PSC endpoint.\n",
                    "willReplaceOnChanges": true
                },
                "projectId": {
                    "type": "string",
                    "description": "The Google Cloud project ID of the VPC network where the PSC connection resides.\n",
                    "willReplaceOnChanges": true
                },
                "pscConnectionId": {
                    "type": "string",
                    "description": "The unique ID of this PSC connection.\n"
                },
                "pscEndpointName": {
                    "type": "string",
                    "description": "The name of the PSC endpoint in the Google Cloud project.\n",
                    "willReplaceOnChanges": true
                },
                "serviceAttachmentId": {
                    "type": "string",
                    "description": "The service attachment this PSC connection connects to.\n"
                }
            },
            "type": "object",
            "required": [
                "endpointRegion",
                "projectId",
                "pscEndpointName"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "endpointRegion",
                        "projectId",
                        "pscConnectionId",
                        "pscEndpointName",
                        "serviceAttachmentId"
                    ]
                }
            }
        },
        "databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer": {
            "properties": {
                "gcp": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp",
                    "description": "A block that consists of the following field:\n"
                }
            },
            "type": "object",
            "required": [
                "gcp"
            ]
        },
        "databricks:index/MwsWorkspacesCloudResourceContainerGcp:MwsWorkspacesCloudResourceContainerGcp": {
            "properties": {
                "projectId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "projectId"
            ]
        },
        "databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo": {
            "properties": {
                "authoritativeUserEmail": {
                    "type": "string"
                },
                "authoritativeUserFullName": {
                    "type": "string"
                },
                "customerName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "authoritativeUserEmail",
                "authoritativeUserFullName",
                "customerName"
            ]
        },
        "databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig": {
            "properties": {
                "gkeClusterPodIpRange": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "gkeClusterServiceIpRange": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "subnetCidr": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "gkeClusterPodIpRange",
                "gkeClusterServiceIpRange",
                "subnetCidr"
            ]
        },
        "databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig": {
            "properties": {
                "connectivityType": {
                    "type": "string",
                    "description": "Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: `PRIVATE_NODE_PUBLIC_MASTER`, `PUBLIC_NODE_PUBLIC_MASTER`.\n",
                    "willReplaceOnChanges": true
                },
                "masterIpRange": {
                    "type": "string",
                    "description": "The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as `/28`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "connectivityType",
                "masterIpRange"
            ]
        },
        "databricks:index/MwsWorkspacesToken:MwsWorkspacesToken": {
            "properties": {
                "comment": {
                    "type": "string"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "Token expiry lifetime. By default its 2592000 (30 days).\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "secret": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "tokenId",
                        "tokenValue"
                    ]
                }
            }
        },
        "databricks:index/OnlineTableSpec:OnlineTableSpec": {
            "properties": {
                "performFullCopy": {
                    "type": "boolean",
                    "description": "Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports \"triggered\" scheduling policy.\n",
                    "willReplaceOnChanges": true
                },
                "pipelineId": {
                    "type": "string",
                    "description": "ID of the associated Delta Live Table pipeline.\n"
                },
                "primaryKeyColumns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "list of the columns comprising the primary key.\n",
                    "willReplaceOnChanges": true
                },
                "runContinuously": {
                    "$ref": "#/types/databricks:index/OnlineTableSpecRunContinuously:OnlineTableSpecRunContinuously",
                    "description": "empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with `run_triggered`.\n",
                    "willReplaceOnChanges": true
                },
                "runTriggered": {
                    "$ref": "#/types/databricks:index/OnlineTableSpecRunTriggered:OnlineTableSpecRunTriggered",
                    "description": "empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers).\n",
                    "willReplaceOnChanges": true
                },
                "sourceTableFullName": {
                    "type": "string",
                    "description": "full name of the source table.\n",
                    "willReplaceOnChanges": true
                },
                "timeseriesKey": {
                    "type": "string",
                    "description": "Time series key to deduplicate (tie-break) rows with the same primary key.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pipelineId"
                    ]
                }
            }
        },
        "databricks:index/OnlineTableSpecRunContinuously:OnlineTableSpecRunContinuously": {
            "type": "object"
        },
        "databricks:index/OnlineTableSpecRunTriggered:OnlineTableSpecRunTriggered": {
            "type": "object"
        },
        "databricks:index/OnlineTableStatus:OnlineTableStatus": {
            "properties": {
                "continuousUpdateStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusContinuousUpdateStatus:OnlineTableStatusContinuousUpdateStatus"
                },
                "detailedState": {
                    "type": "string",
                    "description": "The state of the online table.\n"
                },
                "failedStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusFailedStatus:OnlineTableStatusFailedStatus"
                },
                "message": {
                    "type": "string",
                    "description": "A text description of the current state of the online table.\n"
                },
                "provisioningStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusProvisioningStatus:OnlineTableStatusProvisioningStatus"
                },
                "triggeredUpdateStatus": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusTriggeredUpdateStatus:OnlineTableStatusTriggeredUpdateStatus"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusContinuousUpdateStatus:OnlineTableStatusContinuousUpdateStatus": {
            "properties": {
                "initialPipelineSyncProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress:OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress"
                },
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress:OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusFailedStatus:OnlineTableStatusFailedStatus": {
            "properties": {
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusProvisioningStatus:OnlineTableStatusProvisioningStatus": {
            "properties": {
                "initialPipelineSyncProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress:OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress:OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusTriggeredUpdateStatus:OnlineTableStatusTriggeredUpdateStatus": {
            "properties": {
                "lastProcessedCommitVersion": {
                    "type": "integer"
                },
                "timestamp": {
                    "type": "string"
                },
                "triggeredUpdateProgress": {
                    "$ref": "#/types/databricks:index/OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress:OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress"
                }
            },
            "type": "object"
        },
        "databricks:index/OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress:OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress": {
            "properties": {
                "estimatedCompletionTimeSeconds": {
                    "type": "number"
                },
                "latestVersionCurrentlyProcessing": {
                    "type": "integer"
                },
                "syncProgressCompletion": {
                    "type": "number"
                },
                "syncedRowCount": {
                    "type": "integer"
                },
                "totalRowCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/PermissionsAccessControl:PermissionsAccessControl": {
            "properties": {
                "groupName": {
                    "type": "string"
                },
                "permissionLevel": {
                    "type": "string"
                },
                "servicePrincipalName": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "permissionLevel"
            ]
        },
        "databricks:index/PipelineCluster:PipelineCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineClusterInitScript:PipelineClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "label": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "driverNodeTypeId",
                        "enableLocalDiskEncryption",
                        "nodeTypeId"
                    ]
                }
            }
        },
        "databricks:index/PipelineClusterAutoscale:PipelineClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                },
                "mode": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAwsAttributes:PipelineClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterAzureAttributes:PipelineClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConf:PipelineClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterClusterLogConfDbfs:PipelineClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterClusterLogConfS3:PipelineClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterGcpAttributes:PipelineClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScript:PipelineClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs",
                    "deprecationMessage": "For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'."
                },
                "file": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptVolumes:PipelineClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/PipelineClusterInitScriptWorkspace:PipelineClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineClusterInitScriptAbfss:PipelineClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptDbfs:PipelineClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptFile:PipelineClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptGcs:PipelineClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptS3:PipelineClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptVolumes:PipelineClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineClusterInitScriptWorkspace:PipelineClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/PipelineDeployment:PipelineDeployment": {
            "properties": {
                "kind": {
                    "type": "string"
                },
                "metadataFilePath": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineFilters:PipelineFilters": {
            "properties": {
                "excludes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "includes": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibrary:PipelineLibrary": {
            "properties": {
                "file": {
                    "$ref": "#/types/databricks:index/PipelineLibraryFile:PipelineLibraryFile"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/PipelineLibraryMaven:PipelineLibraryMaven"
                },
                "notebook": {
                    "$ref": "#/types/databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/PipelineLibraryFile:PipelineLibraryFile": {
            "properties": {
                "path": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/PipelineLibraryMaven:PipelineLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/PipelineLibraryNotebook:PipelineLibraryNotebook": {
            "properties": {
                "path": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/PipelineNotification:PipelineNotification": {
            "properties": {
                "alerts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list\n* `on-update-success` - a pipeline update completes successfully.\n* `on-update-failure` - a pipeline update fails with a retryable error.\n* `on-update-fatal-failure` - a pipeline update fails with a non-retryable (fatal) error.\n* `on-flow-failure` - a single data flow fails.\n"
                },
                "emailRecipients": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "non-empty list of emails to notify.\n"
                }
            },
            "type": "object",
            "required": [
                "alerts",
                "emailRecipients"
            ]
        },
        "databricks:index/RecipientIpAccessList:RecipientIpAccessList": {
            "properties": {
                "allowedIpAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs": {
            "properties": {
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "a map of string key-value pairs with recipient's properties.  Properties with name starting with `databricks.` are reserved.\n"
                }
            },
            "type": "object",
            "required": [
                "properties"
            ]
        },
        "databricks:index/RecipientToken:RecipientToken": {
            "properties": {
                "activationUrl": {
                    "type": "string",
                    "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of recipient creator.\n"
                },
                "expirationTime": {
                    "type": "integer",
                    "description": "Expiration timestamp of the token in epoch milliseconds.\n"
                },
                "id": {
                    "type": "string",
                    "description": "Unique ID of the recipient token.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of recipient Token updater.\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "activationUrl",
                        "createdAt",
                        "createdBy",
                        "expirationTime",
                        "id",
                        "updatedAt",
                        "updatedBy"
                    ]
                }
            }
        },
        "databricks:index/RepoSparseCheckout:RepoSparseCheckout": {
            "properties": {
                "patterns": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.\n\nAddition or removal of the `sparse_checkout` configuration block will lead to recreation of the repo.\n"
                }
            },
            "type": "object",
            "required": [
                "patterns"
            ]
        },
        "databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins": {
            "properties": {
                "status": {
                    "type": "string",
                    "description": "The restrict workspace admins status for the workspace.\n"
                }
            },
            "type": "object",
            "required": [
                "status"
            ]
        },
        "databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata": {
            "properties": {
                "dnsName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "required": [
                "dnsName",
                "resourceId"
            ]
        },
        "databricks:index/ShareObject:ShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer"
                },
                "addedBy": {
                    "type": "string"
                },
                "cdfEnabled": {
                    "type": "boolean",
                    "description": "Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field `history_data_sharing_status` can not be set.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n"
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the data object, currently `TABLE`, `SCHEMA`, `VOLUME`, and `MODEL` are supported.\n"
                },
                "historyDataSharingStatus": {
                    "type": "string",
                    "description": "Whether to enable history sharing, one of: `ENABLED`, `DISABLED`. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. *NOTE*: The start_version should be less than or equal the current version of the object. When this field is set, field `cdf_enabled` can not be set.\n\nTo share only part of a table when you add the table to a share, you can provide partition specifications. This is specified by a number of `partition` blocks. Each entry in `partition` block takes a list of `value` blocks. The field is documented below.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Full name of the object, e.g. `catalog.schema.name` for a tables, volumes and models, or `catalog.schema` for schemas.\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObjectPartition:ShareObjectPartition"
                    }
                },
                "sharedAs": {
                    "type": "string",
                    "description": "A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the `shared_as` name. The `shared_as` name must be unique within a Share. Change forces creation of a new resource.\n"
                },
                "startVersion": {
                    "type": "integer",
                    "description": "The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of the object, one of: `ACTIVE`, `PERMISSION_DENIED`.\n"
                }
            },
            "type": "object",
            "required": [
                "dataObjectType",
                "name"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "addedAt",
                        "addedBy",
                        "dataObjectType",
                        "name",
                        "status"
                    ]
                }
            }
        },
        "databricks:index/ShareObjectPartition:ShareObjectPartition": {
            "properties": {
                "values": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObjectPartitionValue:ShareObjectPartitionValue"
                    }
                }
            },
            "type": "object",
            "required": [
                "values"
            ]
        },
        "databricks:index/ShareObjectPartitionValue:ShareObjectPartitionValue": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the partition column.\n"
                },
                "op": {
                    "type": "string",
                    "description": "The operator to apply for the value, one of: `EQUAL`, `LIKE`\n"
                },
                "recipientPropertyKey": {
                    "type": "string",
                    "description": "The key of a Delta Sharing recipient's property. For example `databricks-account-id`. When this field is set, field `value` can not be set.\n"
                },
                "value": {
                    "type": "string",
                    "description": "The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipient_property_key` can not be set.\n"
                }
            },
            "type": "object",
            "required": [
                "name",
                "op"
            ]
        },
        "databricks:index/SqlAlertOptions:SqlAlertOptions": {
            "properties": {
                "column": {
                    "type": "string",
                    "description": "Name of column in the query result to compare in alert evaluation.\n"
                },
                "customBody": {
                    "type": "string",
                    "description": "Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "customSubject": {
                    "type": "string",
                    "description": "Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.\n"
                },
                "emptyResultState": {
                    "type": "string",
                    "description": "State that alert evaluates to when query result is empty.  Currently supported values are `unknown`, `triggered`, `ok` - check [API documentation](https://docs.databricks.com/api/workspace/alerts/create) for full list of supported values.\n"
                },
                "muted": {
                    "type": "boolean",
                    "description": "Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.\n"
                },
                "op": {
                    "type": "string",
                    "description": "Operator used to compare in alert evaluation. (Enum: `\u003e`, `\u003e=`, `\u003c`, `\u003c=`, `==`, `!=`)\n"
                },
                "value": {
                    "type": "string",
                    "description": "Value used to compare in alert evaluation.\n"
                }
            },
            "type": "object",
            "required": [
                "column",
                "op",
                "value"
            ]
        },
        "databricks:index/SqlEndpointChannel:SqlEndpointChannel": {
            "properties": {
                "dbsqlVersion": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointHealth:SqlEndpointHealth": {
            "properties": {
                "details": {
                    "type": "string"
                },
                "failureReason": {
                    "$ref": "#/types/databricks:index/SqlEndpointHealthFailureReason:SqlEndpointHealthFailureReason"
                },
                "message": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "summary": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointHealthFailureReason:SqlEndpointHealthFailureReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams": {
            "properties": {
                "hostname": {
                    "type": "string"
                },
                "path": {
                    "type": "string"
                },
                "port": {
                    "type": "integer"
                },
                "protocol": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointTags:SqlEndpointTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/SqlEndpointTagsCustomTag:SqlEndpointTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "key",
                "value"
            ]
        },
        "databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment": {
            "properties": {
                "principal": {
                    "type": "string",
                    "description": "`display_name` for a databricks.Group or databricks_user, `application_id` for a databricks_service_principal.\n"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "set of available privilege names in upper case.\n\n\n[Available](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html) privilege names are:\n[Available](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html) privilege names are:\n"
                }
            },
            "type": "object",
            "required": [
                "principal",
                "privileges"
            ]
        },
        "databricks:index/SqlQueryParameter:SqlQueryParameter": {
            "properties": {
                "date": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDate:SqlQueryParameterDate"
                },
                "dateRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange"
                },
                "datetime": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime"
                },
                "datetimeRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange"
                },
                "datetimesec": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec"
                },
                "datetimesecRange": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange"
                },
                "enum": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum"
                },
                "name": {
                    "type": "string",
                    "description": "The literal parameter marker that appears between double curly braces in the query text.\nParameters can have several different types. Type is specified using one of the following configuration blocks: `text`, `number`, `enum`, `query`, `date`, `datetime`, `datetimesec`, `date_range`, `datetime_range`, `datetimesec_range`.\n\nFor `text`, `number`, `date`, `datetime`, `datetimesec` block\n"
                },
                "number": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber"
                },
                "query": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery",
                    "description": "The text of the query to be run.\n"
                },
                "text": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterText:SqlQueryParameterText"
                },
                "title": {
                    "type": "string",
                    "description": "The text displayed in a parameter picking widget.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/SqlQueryParameterDate:SqlQueryParameterDate": {
            "properties": {
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDateRange:SqlQueryParameterDateRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDateRangeRange:SqlQueryParameterDateRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetime:SqlQueryParameterDatetime": {
            "properties": {
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimeRange:SqlQueryParameterDatetimeRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimeRangeRange:SqlQueryParameterDatetimeRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesec:SqlQueryParameterDatetimesec": {
            "properties": {
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterDatetimesecRange:SqlQueryParameterDatetimesecRange": {
            "properties": {
                "range": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryParameterDatetimesecRangeRange:SqlQueryParameterDatetimesecRangeRange": {
            "properties": {
                "end": {
                    "type": "string"
                },
                "start": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "end",
                "start"
            ]
        },
        "databricks:index/SqlQueryParameterEnum:SqlQueryParameterEnum": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple"
                },
                "options": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "value": {
                    "type": "string"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "options"
            ]
        },
        "databricks:index/SqlQueryParameterEnumMultiple:SqlQueryParameterEnumMultiple": {
            "properties": {
                "prefix": {
                    "type": "string"
                },
                "separator": {
                    "type": "string"
                },
                "suffix": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "separator"
            ]
        },
        "databricks:index/SqlQueryParameterNumber:SqlQueryParameterNumber": {
            "properties": {
                "value": {
                    "type": "number"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQueryParameterQuery:SqlQueryParameterQuery": {
            "properties": {
                "multiple": {
                    "$ref": "#/types/databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple"
                },
                "queryId": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/SqlQueryParameterQueryMultiple:SqlQueryParameterQueryMultiple": {
            "properties": {
                "prefix": {
                    "type": "string"
                },
                "separator": {
                    "type": "string"
                },
                "suffix": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "separator"
            ]
        },
        "databricks:index/SqlQueryParameterText:SqlQueryParameterText": {
            "properties": {
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "value"
            ]
        },
        "databricks:index/SqlQuerySchedule:SqlQuerySchedule": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous"
                },
                "daily": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily"
                },
                "weekly": {
                    "$ref": "#/types/databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly"
                }
            },
            "type": "object"
        },
        "databricks:index/SqlQueryScheduleContinuous:SqlQueryScheduleContinuous": {
            "properties": {
                "intervalSeconds": {
                    "type": "integer"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "intervalSeconds"
            ]
        },
        "databricks:index/SqlQueryScheduleDaily:SqlQueryScheduleDaily": {
            "properties": {
                "intervalDays": {
                    "type": "integer"
                },
                "timeOfDay": {
                    "type": "string"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "intervalDays",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlQueryScheduleWeekly:SqlQueryScheduleWeekly": {
            "properties": {
                "dayOfWeek": {
                    "type": "string"
                },
                "intervalWeeks": {
                    "type": "integer"
                },
                "timeOfDay": {
                    "type": "string"
                },
                "untilDate": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "dayOfWeek",
                "intervalWeeks",
                "timeOfDay"
            ]
        },
        "databricks:index/SqlTableColumn:SqlTableColumn": {
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "name": {
                    "type": "string",
                    "description": "User-visible name of column\n"
                },
                "nullable": {
                    "type": "boolean",
                    "description": "Whether field is nullable (Default: `true`)\n"
                },
                "type": {
                    "type": "string",
                    "description": "Column type spec (with metadata) as SQL text. Not supported for `VIEW` table_type.\n"
                }
            },
            "type": "object",
            "required": [
                "name"
            ]
        },
        "databricks:index/SqlWidgetParameter:SqlWidgetParameter": {
            "properties": {
                "mapTo": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                },
                "values": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "name",
                "type"
            ]
        },
        "databricks:index/SqlWidgetPosition:SqlWidgetPosition": {
            "properties": {
                "autoHeight": {
                    "type": "boolean"
                },
                "posX": {
                    "type": "integer"
                },
                "posY": {
                    "type": "integer"
                },
                "sizeX": {
                    "type": "integer"
                },
                "sizeY": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "sizeX",
                "sizeY"
            ]
        },
        "databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n\n`azure_managed_identity` optional configuration block for using managed identity as credential details for Azure (recommended over service principal):\n"
                },
                "unityCatalogIamArn": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "externalId",
                        "roleArn",
                        "unityCatalogIamArn"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.\n"
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.\n\n`databricks_gcp_service_account` optional configuration block for creating a Databricks-managed GCP Service Account:\n"
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ],
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "accessConnectorId",
                        "credentialId"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n"
                },
                "clientSecret": {
                    "type": "string",
                    "description": "The client secret generated for the above app ID in AAD. **This field is redacted on output**\n"
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n"
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n\n`azure_service_principal` optional configuration block to use service principal as credential details for Azure (Legacy):\n"
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "credentialId",
                        "email"
                    ]
                }
            }
        },
        "databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey": {
            "properties": {
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n\n`azure_service_principal` optional configuration block to use service principal as credential details for Azure (Legacy):\n"
                },
                "privateKey": {
                    "type": "string",
                    "secret": true
                },
                "privateKeyId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "email",
                "privateKey",
                "privateKeyId"
            ]
        },
        "databricks:index/TableColumn:TableColumn": {
            "properties": {
                "comment": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "nullable": {
                    "type": "boolean"
                },
                "partitionIndex": {
                    "type": "integer"
                },
                "position": {
                    "type": "integer"
                },
                "typeIntervalType": {
                    "type": "string"
                },
                "typeJson": {
                    "type": "string"
                },
                "typeName": {
                    "type": "string"
                },
                "typePrecision": {
                    "type": "integer"
                },
                "typeScale": {
                    "type": "integer"
                },
                "typeText": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name",
                "position",
                "typeName",
                "typeText"
            ]
        },
        "databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus": {
            "properties": {
                "message": {
                    "type": "string",
                    "description": "Additional status message.\n"
                },
                "state": {
                    "type": "string",
                    "description": "Current state of the endpoint. Currently following values are supported: `PROVISIONING`, `ONLINE`, and `OFFLINE`.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec": {
            "properties": {
                "embeddingSourceColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding source.  Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "embeddingVectorColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn"
                    },
                    "willReplaceOnChanges": true
                },
                "pipelineId": {
                    "type": "string",
                    "description": "ID of the associated Delta Live Table pipeline.\n"
                },
                "pipelineType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sourceTable": {
                    "type": "string",
                    "description": "The name of the source table.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object",
            "language": {
                "nodejs": {
                    "requiredOutputs": [
                        "pipelineId"
                    ]
                }
            }
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn": {
            "properties": {
                "embeddingModelEndpointName": {
                    "type": "string",
                    "description": "The name of the embedding model endpoint\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn:VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn": {
            "properties": {
                "embeddingDimension": {
                    "type": "integer",
                    "description": "Dimension of the embedding vector.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec": {
            "properties": {
                "embeddingSourceColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn"
                    },
                    "description": "array of objects representing columns that contain the embedding source.  Each entry consists of:\n",
                    "willReplaceOnChanges": true
                },
                "embeddingVectorColumns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn"
                    },
                    "willReplaceOnChanges": true
                },
                "schemaJson": {
                    "type": "string",
                    "description": "The schema of the index in JSON format.  Check the [API documentation](https://docs.databricks.com/api/workspace/vectorsearchindexes/createindex#direct_access_index_spec-schema_json) for a list of supported data types.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn": {
            "properties": {
                "embeddingModelEndpointName": {
                    "type": "string",
                    "description": "The name of the embedding model endpoint\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn:VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn": {
            "properties": {
                "embeddingDimension": {
                    "type": "integer",
                    "description": "Dimension of the embedding vector.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                }
            },
            "type": "object"
        },
        "databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus": {
            "properties": {
                "indexUrl": {
                    "type": "string",
                    "description": "Index API Url to be used to perform operations on the index\n"
                },
                "indexedRowCount": {
                    "type": "integer",
                    "description": "Number of rows indexed\n"
                },
                "message": {
                    "type": "string",
                    "description": "Message associated with the index status\n"
                },
                "ready": {
                    "type": "boolean",
                    "description": "Whether the index is ready for search\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfo:getClusterClusterInfo": {
            "properties": {
                "autoscale": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes"
                },
                "clusterCores": {
                    "type": "number"
                },
                "clusterId": {
                    "type": "string",
                    "description": "The id of the cluster\n"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf"
                },
                "clusterLogStatus": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus"
                },
                "clusterMemoryMb": {
                    "type": "integer"
                },
                "clusterName": {
                    "type": "string",
                    "description": "The exact name of the cluster to search\n"
                },
                "clusterSource": {
                    "type": "string"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Additional tags for cluster resources.\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage"
                },
                "driver": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "Use autoscaling local storage.\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Enable local disk encryption.\n"
                },
                "executors": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor"
                    }
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "The pool of idle instances the cluster is attached to.\n"
                },
                "jdbcPort": {
                    "type": "integer"
                },
                "lastActivityTime": {
                    "type": "integer"
                },
                "lastStateLossTime": {
                    "type": "integer"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id.\n"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string",
                    "description": "Identifier of Cluster Policy to validate cluster and preset certain defaults.\n"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime of the cluster\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with key-value pairs to fine-tune Spark clusters.\n"
                },
                "sparkContextId": {
                    "type": "integer"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster.\n"
                },
                "startTime": {
                    "type": "integer"
                },
                "state": {
                    "type": "string"
                },
                "stateMessage": {
                    "type": "string"
                },
                "terminateTime": {
                    "type": "integer"
                },
                "terminationReason": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason"
                }
            },
            "type": "object",
            "required": [
                "clusterSource",
                "defaultTags",
                "driverInstancePoolId",
                "sparkVersion",
                "state"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "defaultTags",
                        "sparkVersion",
                        "state"
                    ]
                }
            }
        },
        "databricks:index/getClusterClusterInfoAutoscale:getClusterClusterInfoAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAwsAttributes:getClusterClusterInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoAzureAttributes:getClusterClusterInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConf:getClusterClusterInfoClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoClusterLogConfDbfs:getClusterClusterInfoClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogConfS3:getClusterClusterInfoClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoClusterLogStatus:getClusterClusterInfoClusterLogStatus": {
            "properties": {
                "lastAttempted": {
                    "type": "integer"
                },
                "lastException": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDockerImage:getClusterClusterInfoDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getClusterClusterInfoDockerImageBasicAuth:getClusterClusterInfoDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getClusterClusterInfoDriver:getClusterClusterInfoDriver": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string"
                },
                "instanceId": {
                    "type": "string"
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes"
                },
                "nodeId": {
                    "type": "string"
                },
                "privateIp": {
                    "type": "string"
                },
                "publicDns": {
                    "type": "string"
                },
                "startTimestamp": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoDriverNodeAwsAttributes:getClusterClusterInfoDriverNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutor:getClusterClusterInfoExecutor": {
            "properties": {
                "hostPrivateIp": {
                    "type": "string"
                },
                "instanceId": {
                    "type": "string"
                },
                "nodeAwsAttributes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes"
                },
                "nodeId": {
                    "type": "string"
                },
                "privateIp": {
                    "type": "string"
                },
                "publicDns": {
                    "type": "string"
                },
                "startTimestamp": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoExecutorNodeAwsAttributes:getClusterClusterInfoExecutorNodeAwsAttributes": {
            "properties": {
                "isSpot": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoGcpAttributes:getClusterClusterInfoGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScript:getClusterClusterInfoInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptAbfss:getClusterClusterInfoInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptFile:getClusterClusterInfoInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptGcs:getClusterClusterInfoInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptVolumes:getClusterClusterInfoInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getClusterClusterInfoInitScriptWorkspace:getClusterClusterInfoInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getClusterClusterInfoInitScriptAbfss:getClusterClusterInfoInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptDbfs:getClusterClusterInfoInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptFile:getClusterClusterInfoInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptGcs:getClusterClusterInfoInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptS3:getClusterClusterInfoInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptVolumes:getClusterClusterInfoInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoInitScriptWorkspace:getClusterClusterInfoInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getClusterClusterInfoTerminationReason:getClusterClusterInfoTerminationReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo": {
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Timestamp (in milliseconds) when the current metastore was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "the ID of the identity that created the current metastore.\n"
                },
                "defaultDataAccessConfigId": {
                    "type": "string",
                    "description": "the ID of the default data access configuration.\n"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "the expiration duration in seconds on recipient data access tokens.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "globalMetastoreId": {
                    "type": "string",
                    "description": "Identifier in form of `\u003ccloud\u003e:\u003cregion\u003e:\u003cmetastore_id\u003e` for use in Databricks to Databricks Delta Sharing.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Metastore ID.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/group name/sp application_id of the metastore owner.\n"
                },
                "privilegeModelVersion": {
                    "type": "string",
                    "description": "the version of the privilege model used by the metastore.\n"
                },
                "region": {
                    "type": "string",
                    "description": "(Mandatory for account-level) The region of the metastore.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored.\n"
                },
                "storageRootCredentialId": {
                    "type": "string",
                    "description": "ID of a storage credential used for the `storage_root`.\n"
                },
                "storageRootCredentialName": {
                    "type": "string",
                    "description": "Name of a storage credential used for the `storage_root`.\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Timestamp (in milliseconds) when the current metastore was updated.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "the ID of the identity that updated the current metastore.\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList": {
            "properties": {
                "fileSize": {
                    "type": "integer"
                },
                "path": {
                    "type": "string",
                    "description": "Path on DBFS for the file to perform listing\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo": {
            "properties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "browseOnly": {
                    "type": "boolean"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied comment.\n"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "credentialId": {
                    "type": "string"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfoEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the storage credential\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfoEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetails": {
            "properties": {
                "sseEncryptionDetails": {
                    "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails"
                }
            },
            "type": "object"
        },
        "databricks:index/getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails:getExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails": {
            "properties": {
                "algorithm": {
                    "type": "string"
                },
                "awsKmsKeyArn": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo": {
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer"
                },
                "instancePoolFleetAttributes": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string"
                },
                "maxCapacity": {
                    "type": "integer"
                },
                "minIdleInstances": {
                    "type": "integer"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "state": {
                    "type": "string"
                },
                "stats": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats"
                }
            },
            "type": "object",
            "required": [
                "defaultTags",
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "idleInstanceAutoterminationMinutes",
                        "instancePoolName"
                    ]
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAwsAttributes:getInstancePoolPoolInfoAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "zoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoAzureAttributes:getInstancePoolPoolInfoAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpec:getInstancePoolPoolInfoDiskSpec": {
            "properties": {
                "diskCount": {
                    "type": "integer"
                },
                "diskSize": {
                    "type": "integer"
                },
                "diskType": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoDiskSpecDiskType:getInstancePoolPoolInfoDiskSpecDiskType": {
            "properties": {
                "azureDiskVolumeType": {
                    "type": "string"
                },
                "ebsVolumeType": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstancePoolPoolInfoGcpAttributes:getInstancePoolPoolInfoGcpAttributes": {
            "properties": {
                "gcpAvailability": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localSsdCount",
                "zoneId"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttribute:getInstancePoolPoolInfoInstancePoolFleetAttribute": {
            "properties": {
                "fleetOnDemandOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption"
                },
                "fleetSpotOption": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption"
                },
                "launchTemplateOverrides": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride"
                    }
                }
            },
            "type": "object",
            "required": [
                "launchTemplateOverrides"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string"
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption:getInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption": {
            "properties": {
                "allocationStrategy": {
                    "type": "string"
                },
                "instancePoolsToUseCount": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "allocationStrategy"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride:getInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride": {
            "properties": {
                "availabilityZone": {
                    "type": "string"
                },
                "instanceType": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "availabilityZone",
                "instanceType"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImage:getInstancePoolPoolInfoPreloadedDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoPreloadedDockerImageBasicAuth:getInstancePoolPoolInfoPreloadedDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getInstancePoolPoolInfoStats:getInstancePoolPoolInfoStats": {
            "properties": {
                "idleCount": {
                    "type": "integer"
                },
                "pendingIdleCount": {
                    "type": "integer"
                },
                "pendingUsedCount": {
                    "type": "integer"
                },
                "usedCount": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile": {
            "properties": {
                "arn": {
                    "type": "string",
                    "description": "ARN of the instance profile.\n"
                },
                "isMeta": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile or not.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the instance profile.\n"
                },
                "roleArn": {
                    "type": "string",
                    "description": "ARN of the role attached to the instance profile.\n"
                }
            },
            "type": "object",
            "required": [
                "arn",
                "isMeta",
                "name",
                "roleArn"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettings:getJobJobSettings": {
            "properties": {
                "createdTime": {
                    "type": "integer"
                },
                "creatorUserName": {
                    "type": "string"
                },
                "jobId": {
                    "type": "integer"
                },
                "runAsUserName": {
                    "type": "string"
                },
                "settings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings"
                }
            },
            "type": "object",
            "required": [
                "runAsUserName"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettings:getJobJobSettingsSettings": {
            "properties": {
                "continuous": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsContinuous:getJobJobSettingsSettingsContinuous"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsDeployment:getJobJobSettingsSettingsDeployment"
                },
                "description": {
                    "type": "string"
                },
                "editMode": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEnvironment:getJobJobSettingsSettingsEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsHealth:getJobJobSettingsSettingsHealth"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster"
                    }
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary"
                    }
                },
                "maxConcurrentRuns": {
                    "type": "integer"
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "the job name of databricks.Job if the resource was matched by id.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNotificationSettings:getJobJobSettingsSettingsNotificationSettings"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsParameter:getJobJobSettingsSettingsParameter"
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsQueue:getJobJobSettingsSettingsQueue"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsRunAs:getJobJobSettingsSettingsRunAs"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsRunJobTask:getJobJobSettingsSettingsRunJobTask"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTrigger:getJobJobSettingsSettingsTrigger"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "format",
                "runAs"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsContinuous:getJobJobSettingsSettingsContinuous": {
            "properties": {
                "pauseStatus": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsDbtTask:getJobJobSettingsSettingsDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsDeployment:getJobJobSettingsSettingsDeployment": {
            "properties": {
                "kind": {
                    "type": "string"
                },
                "metadataFilePath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "kind"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsEmailNotifications:getJobJobSettingsSettingsEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsEnvironment:getJobJobSettingsSettingsEnvironment": {
            "properties": {
                "environmentKey": {
                    "type": "string"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsEnvironmentSpec:getJobJobSettingsSettingsEnvironmentSpec"
                }
            },
            "type": "object",
            "required": [
                "environmentKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsEnvironmentSpec:getJobJobSettingsSettingsEnvironmentSpec": {
            "properties": {
                "client": {
                    "type": "string"
                },
                "dependencies": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object",
            "required": [
                "client"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsGitSource:getJobJobSettingsSettingsGitSource": {
            "properties": {
                "branch": {
                    "type": "string"
                },
                "commit": {
                    "type": "string"
                },
                "jobSource": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsGitSourceJobSource:getJobJobSettingsSettingsGitSourceJobSource"
                },
                "provider": {
                    "type": "string"
                },
                "tag": {
                    "type": "string"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsGitSourceJobSource:getJobJobSettingsSettingsGitSourceJobSource": {
            "properties": {
                "dirtyState": {
                    "type": "string"
                },
                "importFromGitBranch": {
                    "type": "string"
                },
                "jobConfigPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "importFromGitBranch",
                "jobConfigPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsHealth:getJobJobSettingsSettingsHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsHealthRule:getJobJobSettingsSettingsHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsHealthRule:getJobJobSettingsSettingsHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobCluster:getJobJobSettingsSettingsJobCluster": {
            "properties": {
                "jobClusterKey": {
                    "type": "string"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewCluster:getJobJobSettingsSettingsJobClusterNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAutoscale:getJobJobSettingsSettingsJobClusterNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes:getJobJobSettingsSettingsJobClusterNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes:getJobJobSettingsSettingsJobClusterNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3:getJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImage:getJobJobSettingsSettingsJobClusterNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes:getJobJobSettingsSettingsJobClusterNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScript:getJobJobSettingsSettingsJobClusterNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes:getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace:getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss:getJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile:getJobJobSettingsSettingsJobClusterNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs:getJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3:getJobJobSettingsSettingsJobClusterNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes:getJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace:getJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadType:getJobJobSettingsSettingsJobClusterNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients:getJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibrary:getJobJobSettingsSettingsLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsLibraryCran:getJobJobSettingsSettingsLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryMaven:getJobJobSettingsSettingsLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsLibraryPypi:getJobJobSettingsSettingsLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewCluster:getJobJobSettingsSettingsNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAutoscale:getJobJobSettingsSettingsNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAwsAttributes:getJobJobSettingsSettingsNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterAzureAttributes:getJobJobSettingsSettingsNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConf:getJobJobSettingsSettingsNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterLogConfS3:getJobJobSettingsSettingsNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfo:getJobJobSettingsSettingsNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImage:getJobJobSettingsSettingsNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterGcpAttributes:getJobJobSettingsSettingsNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScript:getJobJobSettingsSettingsNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptVolumes:getJobJobSettingsSettingsNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterInitScriptWorkspace:getJobJobSettingsSettingsNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptAbfss:getJobJobSettingsSettingsNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptDbfs:getJobJobSettingsSettingsNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptFile:getJobJobSettingsSettingsNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptGcs:getJobJobSettingsSettingsNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptS3:getJobJobSettingsSettingsNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptVolumes:getJobJobSettingsSettingsNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterInitScriptWorkspace:getJobJobSettingsSettingsNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadType:getJobJobSettingsSettingsNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNewClusterWorkloadTypeClients:getJobJobSettingsSettingsNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsNotebookTask:getJobJobSettingsSettingsNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsNotificationSettings:getJobJobSettingsSettingsNotificationSettings": {
            "properties": {
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsParameter:getJobJobSettingsSettingsParameter": {
            "properties": {
                "default": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the job name of databricks.Job if the resource was matched by id.\n"
                }
            },
            "type": "object",
            "required": [
                "default",
                "name"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPipelineTask:getJobJobSettingsSettingsPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsPythonWheelTask:getJobJobSettingsSettingsPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsQueue:getJobJobSettingsSettingsQueue": {
            "properties": {
                "enabled": {
                    "type": "boolean"
                }
            },
            "type": "object",
            "required": [
                "enabled"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsRunAs:getJobJobSettingsSettingsRunAs": {
            "properties": {
                "servicePrincipalName": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsRunJobTask:getJobJobSettingsSettingsRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSchedule:getJobJobSettingsSettingsSchedule": {
            "properties": {
                "pauseStatus": {
                    "type": "string"
                },
                "quartzCronExpression": {
                    "type": "string"
                },
                "timezoneId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "quartzCronExpression",
                "timezoneId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSparkJarTask:getJobJobSettingsSettingsSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsSparkPythonTask:getJobJobSettingsSettingsSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsSparkSubmitTask:getJobJobSettingsSettingsSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTask:getJobJobSettingsSettingsTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskConditionTask:getJobJobSettingsSettingsTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn"
                    }
                },
                "description": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "forEachTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTask:getJobJobSettingsSettingsTaskForEachTask"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskHealth:getJobJobSettingsSettingsTaskHealth"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary"
                    }
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNotificationSettings:getJobJobSettingsSettingsTaskNotificationSettings"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runIf": {
                    "type": "string"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskRunJobTask:getJobJobSettingsSettingsTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask"
                },
                "taskKey": {
                    "type": "string"
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotifications:getJobJobSettingsSettingsTaskWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "retryOnTimeout"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskConditionTask:getJobJobSettingsSettingsTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "right": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskDbtTask:getJobJobSettingsSettingsTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskDependsOn:getJobJobSettingsSettingsTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string"
                },
                "taskKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskEmailNotifications:getJobJobSettingsSettingsTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTask:getJobJobSettingsSettingsTaskForEachTask": {
            "properties": {
                "concurrency": {
                    "type": "integer"
                },
                "inputs": {
                    "type": "string"
                },
                "task": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTask:getJobJobSettingsSettingsTaskForEachTaskTask"
                }
            },
            "type": "object",
            "required": [
                "inputs",
                "task"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTask:getJobJobSettingsSettingsTaskForEachTaskTask": {
            "properties": {
                "conditionTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask:getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask:getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask"
                },
                "dependsOns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn:getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn"
                    }
                },
                "description": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications"
                },
                "environmentKey": {
                    "type": "string"
                },
                "existingClusterId": {
                    "type": "string"
                },
                "health": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealth:getJobJobSettingsSettingsTaskForEachTaskTaskHealth"
                },
                "jobClusterKey": {
                    "type": "string"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibrary:getJobJobSettingsSettingsTaskForEachTaskTaskLibrary"
                    }
                },
                "maxRetries": {
                    "type": "integer"
                },
                "minRetryIntervalMillis": {
                    "type": "integer"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster:getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask:getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings:getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings"
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask:getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask:getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask"
                },
                "retryOnTimeout": {
                    "type": "boolean"
                },
                "runIf": {
                    "type": "string"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask:getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask"
                },
                "sqlTask": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask"
                },
                "taskKey": {
                    "type": "string"
                },
                "timeoutSeconds": {
                    "type": "integer"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications"
                }
            },
            "type": "object",
            "required": [
                "retryOnTimeout"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": []
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask:getJobJobSettingsSettingsTaskForEachTaskTaskConditionTask": {
            "properties": {
                "left": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "right": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "left",
                "op",
                "right"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask:getJobJobSettingsSettingsTaskForEachTaskTaskDbtTask": {
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "commands": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "profilesDirectory": {
                    "type": "string"
                },
                "projectDirectory": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "commands"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn:getJobJobSettingsSettingsTaskForEachTaskTaskDependsOn": {
            "properties": {
                "outcome": {
                    "type": "string"
                },
                "taskKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "taskKey"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications": {
            "properties": {
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                },
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealth:getJobJobSettingsSettingsTaskForEachTaskTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule:getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule:getJobJobSettingsSettingsTaskForEachTaskTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibrary:getJobJobSettingsSettingsTaskForEachTaskTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi:getJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster:getJobJobSettingsSettingsTaskForEachTaskTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask:getJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings:getJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask:getJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask:getJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask:getJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask:getJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId",
                "subscriptions"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string"
                },
                "dashboardId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery:getJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskHealth:getJobJobSettingsSettingsTaskHealth": {
            "properties": {
                "rules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskHealthRule:getJobJobSettingsSettingsTaskHealthRule"
                    }
                }
            },
            "type": "object",
            "required": [
                "rules"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskHealthRule:getJobJobSettingsSettingsTaskHealthRule": {
            "properties": {
                "metric": {
                    "type": "string"
                },
                "op": {
                    "type": "string"
                },
                "value": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibrary:getJobJobSettingsSettingsTaskLibrary": {
            "properties": {
                "cran": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryCran:getJobJobSettingsSettingsTaskLibraryCran": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryMaven:getJobJobSettingsSettingsTaskLibraryMaven": {
            "properties": {
                "coordinates": {
                    "type": "string"
                },
                "exclusions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "coordinates"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskLibraryPypi:getJobJobSettingsSettingsTaskLibraryPypi": {
            "properties": {
                "package": {
                    "type": "string"
                },
                "repo": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "package"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewCluster:getJobJobSettingsSettingsTaskNewCluster": {
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "dataSecurityMode": {
                    "type": "string"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string"
                },
                "driverNodeTypeId": {
                    "type": "string"
                },
                "enableElasticDisk": {
                    "type": "boolean"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string"
                },
                "nodeTypeId": {
                    "type": "string"
                },
                "numWorkers": {
                    "type": "integer"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string"
                },
                "singleUserName": {
                    "type": "string"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "sparkVersion": {
                    "type": "string"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType"
                }
            },
            "type": "object",
            "required": [
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "numWorkers",
                "sparkVersion"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "numWorkers",
                        "sparkVersion"
                    ]
                }
            }
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAutoscale:getJobJobSettingsSettingsTaskNewClusterAutoscale": {
            "properties": {
                "maxWorkers": {
                    "type": "integer"
                },
                "minWorkers": {
                    "type": "integer"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAwsAttributes:getJobJobSettingsSettingsTaskNewClusterAwsAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "ebsVolumeCount": {
                    "type": "integer"
                },
                "ebsVolumeSize": {
                    "type": "integer"
                },
                "ebsVolumeType": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "spotBidPricePercent": {
                    "type": "integer"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterAzureAttributes:getJobJobSettingsSettingsTaskNewClusterAzureAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "firstOnDemand": {
                    "type": "integer"
                },
                "spotBidMaxPrice": {
                    "type": "number"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConf:getJobJobSettingsSettingsTaskNewClusterClusterLogConf": {
            "properties": {
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs:getJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3:getJobJobSettingsSettingsTaskNewClusterClusterLogConfS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfo": {
            "properties": {
                "localMountDirPath": {
                    "type": "string"
                },
                "networkFilesystemInfo": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo"
                },
                "remoteMountDirPath": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "localMountDirPath",
                "networkFilesystemInfo"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo:getJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo": {
            "properties": {
                "mountOptions": {
                    "type": "string"
                },
                "serverAddress": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "serverAddress"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImage:getJobJobSettingsSettingsTaskNewClusterDockerImage": {
            "properties": {
                "basicAuth": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth"
                },
                "url": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth:getJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth": {
            "properties": {
                "password": {
                    "type": "string",
                    "secret": true
                },
                "username": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "password",
                "username"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterGcpAttributes:getJobJobSettingsSettingsTaskNewClusterGcpAttributes": {
            "properties": {
                "availability": {
                    "type": "string"
                },
                "bootDiskSize": {
                    "type": "integer"
                },
                "googleServiceAccount": {
                    "type": "string"
                },
                "localSsdCount": {
                    "type": "integer"
                },
                "usePreemptibleExecutors": {
                    "type": "boolean"
                },
                "zoneId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScript:getJobJobSettingsSettingsTaskNewClusterInitScript": {
            "properties": {
                "abfss": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss"
                },
                "dbfs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile"
                },
                "gcs": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3"
                },
                "volumes": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes"
                },
                "workspace": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss:getJobJobSettingsSettingsTaskNewClusterInitScriptAbfss": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs:getJobJobSettingsSettingsTaskNewClusterInitScriptDbfs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptFile:getJobJobSettingsSettingsTaskNewClusterInitScriptFile": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptGcs:getJobJobSettingsSettingsTaskNewClusterInitScriptGcs": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptS3:getJobJobSettingsSettingsTaskNewClusterInitScriptS3": {
            "properties": {
                "cannedAcl": {
                    "type": "string"
                },
                "destination": {
                    "type": "string"
                },
                "enableEncryption": {
                    "type": "boolean"
                },
                "encryptionType": {
                    "type": "string"
                },
                "endpoint": {
                    "type": "string"
                },
                "kmsKey": {
                    "type": "string"
                },
                "region": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes:getJobJobSettingsSettingsTaskNewClusterInitScriptVolumes": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace:getJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace": {
            "properties": {
                "destination": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "destination"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadType:getJobJobSettingsSettingsTaskNewClusterWorkloadType": {
            "properties": {
                "clients": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients"
                }
            },
            "type": "object",
            "required": [
                "clients"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients:getJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients": {
            "properties": {
                "jobs": {
                    "type": "boolean"
                },
                "notebooks": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskNotebookTask:getJobJobSettingsSettingsTaskNotebookTask": {
            "properties": {
                "baseParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "notebookPath": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "notebookPath"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskNotificationSettings:getJobJobSettingsSettingsTaskNotificationSettings": {
            "properties": {
                "alertOnLastAttempt": {
                    "type": "boolean"
                },
                "noAlertForCanceledRuns": {
                    "type": "boolean"
                },
                "noAlertForSkippedRuns": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskPipelineTask:getJobJobSettingsSettingsTaskPipelineTask": {
            "properties": {
                "fullRefresh": {
                    "type": "boolean"
                },
                "pipelineId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pipelineId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskPythonWheelTask:getJobJobSettingsSettingsTaskPythonWheelTask": {
            "properties": {
                "entryPoint": {
                    "type": "string"
                },
                "namedParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "packageName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskRunJobTask:getJobJobSettingsSettingsTaskRunJobTask": {
            "properties": {
                "jobId": {
                    "type": "integer"
                },
                "jobParameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                }
            },
            "type": "object",
            "required": [
                "jobId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkJarTask:getJobJobSettingsSettingsTaskSparkJarTask": {
            "properties": {
                "jarUri": {
                    "type": "string"
                },
                "mainClassName": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkPythonTask:getJobJobSettingsSettingsTaskSparkPythonTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "pythonFile": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "pythonFile"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSparkSubmitTask:getJobJobSettingsSettingsTaskSparkSubmitTask": {
            "properties": {
                "parameters": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTask:getJobJobSettingsSettingsTaskSqlTask": {
            "properties": {
                "alert": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert"
                },
                "dashboard": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard"
                },
                "file": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskFile:getJobJobSettingsSettingsTaskSqlTaskFile"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "query": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery"
                },
                "warehouseId": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlert:getJobJobSettingsSettingsTaskSqlTaskAlert": {
            "properties": {
                "alertId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskSqlTaskAlertSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "alertId",
                "subscriptions"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskAlertSubscription:getJobJobSettingsSettingsTaskSqlTaskAlertSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboard:getJobJobSettingsSettingsTaskSqlTaskDashboard": {
            "properties": {
                "customSubject": {
                    "type": "string"
                },
                "dashboardId": {
                    "type": "string"
                },
                "pauseSubscriptions": {
                    "type": "boolean"
                },
                "subscriptions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription"
                    }
                }
            },
            "type": "object",
            "required": [
                "dashboardId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription:getJobJobSettingsSettingsTaskSqlTaskDashboardSubscription": {
            "properties": {
                "destinationId": {
                    "type": "string"
                },
                "userName": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskFile:getJobJobSettingsSettingsTaskSqlTaskFile": {
            "properties": {
                "path": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "path"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskSqlTaskQuery:getJobJobSettingsSettingsTaskSqlTaskQuery": {
            "properties": {
                "queryId": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "queryId"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotifications:getJobJobSettingsSettingsTaskWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskWebhookNotificationsOnStart"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure:getJobJobSettingsSettingsTaskWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnStart:getJobJobSettingsSettingsTaskWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess:getJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTrigger:getJobJobSettingsSettingsTrigger": {
            "properties": {
                "fileArrival": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTriggerFileArrival:getJobJobSettingsSettingsTriggerFileArrival"
                },
                "pauseStatus": {
                    "type": "string"
                },
                "tableUpdate": {
                    "$ref": "#/types/databricks:index/getJobJobSettingsSettingsTriggerTableUpdate:getJobJobSettingsSettingsTriggerTableUpdate"
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsTriggerFileArrival:getJobJobSettingsSettingsTriggerFileArrival": {
            "properties": {
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer"
                },
                "url": {
                    "type": "string"
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "url"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsTriggerTableUpdate:getJobJobSettingsSettingsTriggerTableUpdate": {
            "properties": {
                "condition": {
                    "type": "string"
                },
                "minTimeBetweenTriggersSeconds": {
                    "type": "integer"
                },
                "tableNames": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "waitAfterLastChangeSeconds": {
                    "type": "integer"
                }
            },
            "type": "object",
            "required": [
                "tableNames"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotifications:getJobJobSettingsSettingsWebhookNotifications": {
            "properties": {
                "onDurationWarningThresholdExceededs": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded"
                    }
                },
                "onFailures": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure"
                    }
                },
                "onStarts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart"
                    }
                },
                "onSuccesses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded:getJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnFailure:getJobJobSettingsSettingsWebhookNotificationsOnFailure": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnStart:getJobJobSettingsSettingsWebhookNotificationsOnStart": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getJobJobSettingsSettingsWebhookNotificationsOnSuccess:getJobJobSettingsSettingsWebhookNotificationsOnSuccess": {
            "properties": {
                "id": {
                    "type": "string",
                    "description": "the id of databricks.Job if the resource was matched by name.\n"
                }
            },
            "type": "object",
            "required": [
                "id"
            ]
        },
        "databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo": {
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Used to set expiration duration in seconds on recipient data access tokens.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Id of the metastore\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the metastore\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "privilegeModelVersion": {
                    "type": "string"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored.\n"
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "storageRootCredentialName": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion": {
            "properties": {
                "creationTimestamp": {
                    "type": "integer"
                },
                "currentStage": {
                    "type": "string"
                },
                "description": {
                    "type": "string",
                    "description": "User-specified description for the object.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the registered model.\n"
                },
                "runId": {
                    "type": "string"
                },
                "runLink": {
                    "type": "string"
                },
                "source": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "statusMessage": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getMlflowModelLatestVersionTag:getMlflowModelLatestVersionTag"
                    },
                    "description": "Array of tags associated with the model.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "The username of the user that created the object.\n"
                },
                "version": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelLatestVersionTag:getMlflowModelLatestVersionTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getMlflowModelTag:getMlflowModelTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList": {
            "properties": {
                "language": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "Path to workspace directory\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getShareObject:getShareObject": {
            "properties": {
                "addedAt": {
                    "type": "integer"
                },
                "addedBy": {
                    "type": "string"
                },
                "cdfEnabled": {
                    "type": "boolean"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the object.\n"
                },
                "dataObjectType": {
                    "type": "string",
                    "description": "Type of the object.\n"
                },
                "historyDataSharingStatus": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the share\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getShareObjectPartition:getShareObjectPartition"
                    }
                },
                "sharedAs": {
                    "type": "string"
                },
                "startVersion": {
                    "type": "integer"
                },
                "status": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "addedAt",
                "addedBy",
                "dataObjectType",
                "name",
                "status"
            ],
            "language": {
                "nodejs": {
                    "requiredInputs": [
                        "dataObjectType",
                        "name"
                    ]
                }
            }
        },
        "databricks:index/getShareObjectPartition:getShareObjectPartition": {
            "properties": {
                "values": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getShareObjectPartitionValue:getShareObjectPartitionValue"
                    }
                }
            },
            "type": "object",
            "required": [
                "values"
            ]
        },
        "databricks:index/getShareObjectPartitionValue:getShareObjectPartitionValue": {
            "properties": {
                "name": {
                    "type": "string",
                    "description": "The name of the share\n"
                },
                "op": {
                    "type": "string"
                },
                "recipientPropertyKey": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "name",
                "op"
            ]
        },
        "databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel": {
            "properties": {
                "dbsqlVersion": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse to search (case-sensitive).\n"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth": {
            "properties": {
                "details": {
                    "type": "string"
                },
                "failureReason": {
                    "$ref": "#/types/databricks:index/getSqlWarehouseHealthFailureReason:getSqlWarehouseHealthFailureReason"
                },
                "message": {
                    "type": "string"
                },
                "status": {
                    "type": "string"
                },
                "summary": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseHealthFailureReason:getSqlWarehouseHealthFailureReason": {
            "properties": {
                "code": {
                    "type": "string"
                },
                "parameters": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "type": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams": {
            "properties": {
                "hostname": {
                    "type": "string"
                },
                "path": {
                    "type": "string"
                },
                "port": {
                    "type": "integer"
                },
                "protocol": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseTags:getSqlWarehouseTags": {
            "properties": {
                "customTags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag"
                    }
                }
            },
            "type": "object"
        },
        "databricks:index/getSqlWarehouseTagsCustomTag:getSqlWarehouseTagsCustomTag": {
            "properties": {
                "key": {
                    "type": "string"
                },
                "value": {
                    "type": "string"
                }
            },
            "type": "object"
        },
        "databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo": {
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAwsIamRole:getStorageCredentialStorageCredentialInfoAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAzureManagedIdentity:getStorageCredentialStorageCredentialInfoAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoAzureServicePrincipal:getStorageCredentialStorageCredentialInfoAzureServicePrincipal"
                },
                "cloudflareApiToken": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoCloudflareApiToken:getStorageCredentialStorageCredentialInfoCloudflareApiToken"
                },
                "comment": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount:getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount"
                },
                "id": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the storage credential\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                },
                "usedForManagedStorage": {
                    "type": "boolean"
                }
            },
            "type": "object"
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAwsIamRole:getStorageCredentialStorageCredentialInfoAwsIamRole": {
            "properties": {
                "externalId": {
                    "type": "string",
                    "description": "(output only) - The external ID used in role assumption to prevent confused deputy problem.\n"
                },
                "roleArn": {
                    "type": "string",
                    "description": "The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`\n"
                },
                "unityCatalogIamArn": {
                    "type": "string",
                    "description": "(output only) - The Amazon Resource Name (ARN) of the AWS IAM user managed by Databricks. This is the identity that is going to assume the AWS IAM role.\n"
                }
            },
            "type": "object",
            "required": [
                "roleArn"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAzureManagedIdentity:getStorageCredentialStorageCredentialInfoAzureManagedIdentity": {
            "properties": {
                "accessConnectorId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.\n"
                },
                "credentialId": {
                    "type": "string"
                },
                "managedIdentityId": {
                    "type": "string",
                    "description": "The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.\n"
                }
            },
            "type": "object",
            "required": [
                "accessConnectorId"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoAzureServicePrincipal:getStorageCredentialStorageCredentialInfoAzureServicePrincipal": {
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "The application ID of the application registration within the referenced AAD tenant\n"
                },
                "clientSecret": {
                    "type": "string"
                },
                "directoryId": {
                    "type": "string",
                    "description": "The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application\n"
                }
            },
            "type": "object",
            "required": [
                "applicationId",
                "clientSecret",
                "directoryId"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoCloudflareApiToken:getStorageCredentialStorageCredentialInfoCloudflareApiToken": {
            "properties": {
                "accessKeyId": {
                    "type": "string"
                },
                "accountId": {
                    "type": "string"
                },
                "secretAccessKey": {
                    "type": "string"
                }
            },
            "type": "object",
            "required": [
                "accessKeyId",
                "accountId",
                "secretAccessKey"
            ]
        },
        "databricks:index/getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount:getStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount": {
            "properties": {
                "credentialId": {
                    "type": "string"
                },
                "email": {
                    "type": "string",
                    "description": "The email of the GCP service account created, to be granted access to relevant buckets.\n"
                }
            },
            "type": "object"
        }
    },
    "provider": {
        "description": "The provider type for the databricks package. By default, resources use package-wide configuration\nsettings, however an explicit `Provider` instance may be created and passed during resource\nconstruction to achieve fine-grained programmatic control over provider settings. See the\n[documentation](https://www.pulumi.com/docs/reference/programming-model/#providers) for more information.\n",
        "properties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        },
        "inputProperties": {
            "accountId": {
                "type": "string"
            },
            "authType": {
                "type": "string"
            },
            "azureClientId": {
                "type": "string"
            },
            "azureClientSecret": {
                "type": "string",
                "secret": true
            },
            "azureEnvironment": {
                "type": "string"
            },
            "azureLoginAppId": {
                "type": "string"
            },
            "azureTenantId": {
                "type": "string"
            },
            "azureUseMsi": {
                "type": "boolean"
            },
            "azureWorkspaceResourceId": {
                "type": "string"
            },
            "clientId": {
                "type": "string"
            },
            "clientSecret": {
                "type": "string",
                "secret": true
            },
            "clusterId": {
                "type": "string"
            },
            "configFile": {
                "type": "string"
            },
            "databricksCliPath": {
                "type": "string"
            },
            "debugHeaders": {
                "type": "boolean"
            },
            "debugTruncateBytes": {
                "type": "integer"
            },
            "googleCredentials": {
                "type": "string",
                "secret": true
            },
            "googleServiceAccount": {
                "type": "string"
            },
            "host": {
                "type": "string"
            },
            "httpTimeoutSeconds": {
                "type": "integer"
            },
            "metadataServiceUrl": {
                "type": "string",
                "secret": true
            },
            "password": {
                "type": "string",
                "secret": true
            },
            "profile": {
                "type": "string"
            },
            "rateLimit": {
                "type": "integer"
            },
            "retryTimeoutSeconds": {
                "type": "integer"
            },
            "skipVerify": {
                "type": "boolean"
            },
            "token": {
                "type": "string",
                "secret": true
            },
            "username": {
                "type": "string"
            },
            "warehouseId": {
                "type": "string"
            }
        }
    },
    "resources": {
        "databricks:index/accessControlRuleSet:AccessControlRuleSet": {
            "description": "\u003e **Note** This resource could be used with account or workspace-level provider.\n\nThis resource allows you to manage access rules on Databricks account level resources. For convenience we allow accessing this resource through the Databricks account and workspace.\n\n\u003e **Note** Currently, we only support managing access rules on service principal, group and account resources through `databricks.AccessControlRuleSet`.\n\n\u003e **Warning** `databricks.AccessControlRuleSet` cannot be used to manage access rules for resources supported by databricks_permissions. Refer to its documentation for more information.\n\n## Service principal rule set usage\n\nThrough a Databricks workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.then(ds =\u003e ds.aclPrincipalId)],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[databricks.AccessControlRuleSetGrantRuleArgs(\n        principals=[ds.acl_principal_id],\n        role=\"roles/servicePrincipal.user\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\tds, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(ds.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()        \n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(automationSp.applicationId().applyValue(applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.applyValue(getGroupResult -\u003e getGroupResult.aclPrincipalId()))\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: Data Science\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[databricks.AccessControlRuleSetGrantRuleArgs(\n        principals=[ds.acl_principal_id],\n        role=\"roles/servicePrincipal.user\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()        \n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()        \n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(automationSp.applicationId().applyValue(applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {\n    applicationId: \"00000000-0000-0000-0000-000000000000\",\n    displayName: \"SP_FOR_AUTOMATION\",\n});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\",\n    application_id=\"00000000-0000-0000-0000-000000000000\",\n    display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[databricks.AccessControlRuleSetGrantRuleArgs(\n        principals=[ds.acl_principal_id],\n        role=\"roles/servicePrincipal.user\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tDisplayName:   pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()        \n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(automationSp.applicationId().applyValue(applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThrough GCP Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group creation\nconst ds = new databricks.Group(\"ds\", {displayName: \"Data Science\"});\nconst automationSp = new databricks.ServicePrincipal(\"automation_sp\", {displayName: \"SP_FOR_AUTOMATION\"});\nconst automationSpRuleSet = new databricks.AccessControlRuleSet(\"automation_sp_rule_set\", {\n    name: pulumi.interpolate`accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default`,\n    grantRules: [{\n        principals: [ds.aclPrincipalId],\n        role: \"roles/servicePrincipal.user\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group creation\nds = databricks.Group(\"ds\", display_name=\"Data Science\")\nautomation_sp = databricks.ServicePrincipal(\"automation_sp\", display_name=\"SP_FOR_AUTOMATION\")\nautomation_sp_rule_set = databricks.AccessControlRuleSet(\"automation_sp_rule_set\",\n    name=automation_sp.application_id.apply(lambda application_id: f\"accounts/{account_id}/servicePrincipals/{application_id}/ruleSets/default\"),\n    grant_rules=[databricks.AccessControlRuleSetGrantRuleArgs(\n        principals=[ds.acl_principal_id],\n        role=\"roles/servicePrincipal.user\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group creation\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var automationSp = new Databricks.ServicePrincipal(\"automation_sp\", new()\n    {\n        DisplayName = \"SP_FOR_AUTOMATION\",\n    });\n\n    var automationSpRuleSet = new Databricks.AccessControlRuleSet(\"automation_sp_rule_set\", new()\n    {\n        Name = automationSp.ApplicationId.Apply(applicationId =\u003e $\"accounts/{accountId}/servicePrincipals/{applicationId}/ruleSets/default\"),\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.AclPrincipalId,\n                },\n                Role = \"roles/servicePrincipal.user\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group creation\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Science\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tautomationSp, err := databricks.NewServicePrincipal(ctx, \"automation_sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"SP_FOR_AUTOMATION\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"automation_sp_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: automationSp.ApplicationId.ApplyT(func(applicationId string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"accounts/%v/servicePrincipals/%v/ruleSets/default\", accountId, applicationId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tds.AclPrincipalId,\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.user\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group creation\n        var ds = new Group(\"ds\", GroupArgs.builder()        \n            .displayName(\"Data Science\")\n            .build());\n\n        var automationSp = new ServicePrincipal(\"automationSp\", ServicePrincipalArgs.builder()        \n            .displayName(\"SP_FOR_AUTOMATION\")\n            .build());\n\n        var automationSpRuleSet = new AccessControlRuleSet(\"automationSpRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(automationSp.applicationId().applyValue(applicationId -\u003e String.format(\"accounts/%s/servicePrincipals/%s/ruleSets/default\", accountId,applicationId)))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(ds.aclPrincipalId())\n                .role(\"roles/servicePrincipal.user\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # account level group creation\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: Data Science\n  automationSp:\n    type: databricks:ServicePrincipal\n    name: automation_sp\n    properties:\n      displayName: SP_FOR_AUTOMATION\n  automationSpRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: automation_sp_rule_set\n    properties:\n      name: accounts/${accountId}/servicePrincipals/${automationSp.applicationId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.user\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Group rule set usage\n\nRefer to the appropriate provider configuration as shown in the examples for service principal rule set.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\nconst john = databricks.getUser({\n    userName: \"john.doe@example.com\",\n});\nconst dsGroupRuleSet = new databricks.AccessControlRuleSet(\"ds_group_rule_set\", {\n    name: `accounts/${accountId}/groups/${dsDatabricksGroup.id}/ruleSets/default`,\n    grantRules: [{\n        principals: [john.then(john =\u003e john.aclPrincipalId)],\n        role: \"roles/group.manager\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\njohn = databricks.get_user(user_name=\"john.doe@example.com\")\nds_group_rule_set = databricks.AccessControlRuleSet(\"ds_group_rule_set\",\n    name=f\"accounts/{account_id}/groups/{ds_databricks_group['id']}/ruleSets/default\",\n    grant_rules=[databricks.AccessControlRuleSetGrantRuleArgs(\n        principals=[john.acl_principal_id],\n        role=\"roles/group.manager\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    var john = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"john.doe@example.com\",\n    });\n\n    var dsGroupRuleSet = new Databricks.AccessControlRuleSet(\"ds_group_rule_set\", new()\n    {\n        Name = $\"accounts/{accountId}/groups/{dsDatabricksGroup.Id}/ruleSets/default\",\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    john.Apply(getUserResult =\u003e getUserResult.AclPrincipalId),\n                },\n                Role = \"roles/group.manager\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\t_, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjohn, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"john.doe@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"ds_group_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: pulumi.String(fmt.Sprintf(\"accounts/%v/groups/%v/ruleSets/default\", accountId, dsDatabricksGroup.Id)),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(john.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/group.manager\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        final var john = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"john.doe@example.com\")\n            .build());\n\n        var dsGroupRuleSet = new AccessControlRuleSet(\"dsGroupRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(String.format(\"accounts/%s/groups/%s/ruleSets/default\", accountId,dsDatabricksGroup.id()))\n            .grantRules(AccessControlRuleSetGrantRuleArgs.builder()\n                .principals(john.applyValue(getUserResult -\u003e getUserResult.aclPrincipalId()))\n                .role(\"roles/group.manager\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dsGroupRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: ds_group_rule_set\n    properties:\n      name: accounts/${accountId}/groups/${dsDatabricksGroup.id}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${john.aclPrincipalId}\n          role: roles/group.manager\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: Data Science\n  john:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: john.doe@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Account rule set usage\n\nRefer to the appropriate provider configuration as shown in the examples for service principal rule set.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountId = \"00000000-0000-0000-0000-000000000000\";\n// account level group\nconst ds = databricks.getGroup({\n    displayName: \"Data Science\",\n});\n// account level group\nconst marketplaceAdmins = databricks.getGroup({\n    displayName: \"Marketplace Admins\",\n});\nconst john = databricks.getUser({\n    userName: \"john.doe@example.com\",\n});\nconst accountRuleSet = new databricks.AccessControlRuleSet(\"account_rule_set\", {\n    name: `accounts/${accountId}/ruleSets/default`,\n    grantRules: [\n        {\n            principals: [john.then(john =\u003e john.aclPrincipalId)],\n            role: \"roles/group.manager\",\n        },\n        {\n            principals: [ds.then(ds =\u003e ds.aclPrincipalId)],\n            role: \"roles/servicePrincipal.manager\",\n        },\n        {\n            principals: [marketplaceAdmins.then(marketplaceAdmins =\u003e marketplaceAdmins.aclPrincipalId)],\n            role: \"roles/marketplace.admin\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_id = \"00000000-0000-0000-0000-000000000000\"\n# account level group\nds = databricks.get_group(display_name=\"Data Science\")\n# account level group\nmarketplace_admins = databricks.get_group(display_name=\"Marketplace Admins\")\njohn = databricks.get_user(user_name=\"john.doe@example.com\")\naccount_rule_set = databricks.AccessControlRuleSet(\"account_rule_set\",\n    name=f\"accounts/{account_id}/ruleSets/default\",\n    grant_rules=[\n        databricks.AccessControlRuleSetGrantRuleArgs(\n            principals=[john.acl_principal_id],\n            role=\"roles/group.manager\",\n        ),\n        databricks.AccessControlRuleSetGrantRuleArgs(\n            principals=[ds.acl_principal_id],\n            role=\"roles/servicePrincipal.manager\",\n        ),\n        databricks.AccessControlRuleSetGrantRuleArgs(\n            principals=[marketplace_admins.acl_principal_id],\n            role=\"roles/marketplace.admin\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n    // account level group\n    var ds = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Data Science\",\n    });\n\n    // account level group\n    var marketplaceAdmins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"Marketplace Admins\",\n    });\n\n    var john = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"john.doe@example.com\",\n    });\n\n    var accountRuleSet = new Databricks.AccessControlRuleSet(\"account_rule_set\", new()\n    {\n        Name = $\"accounts/{accountId}/ruleSets/default\",\n        GrantRules = new[]\n        {\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    john.Apply(getUserResult =\u003e getUserResult.AclPrincipalId),\n                },\n                Role = \"roles/group.manager\",\n            },\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    ds.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/servicePrincipal.manager\",\n            },\n            new Databricks.Inputs.AccessControlRuleSetGrantRuleArgs\n            {\n                Principals = new[]\n                {\n                    marketplaceAdmins.Apply(getGroupResult =\u003e getGroupResult.AclPrincipalId),\n                },\n                Role = \"roles/marketplace.admin\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\taccountId := \"00000000-0000-0000-0000-000000000000\"\n\t\t// account level group\n\t\tds, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Data Science\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// account level group\n\t\tmarketplaceAdmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"Marketplace Admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjohn, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"john.doe@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewAccessControlRuleSet(ctx, \"account_rule_set\", \u0026databricks.AccessControlRuleSetArgs{\n\t\t\tName: pulumi.String(fmt.Sprintf(\"accounts/%v/ruleSets/default\", accountId)),\n\t\t\tGrantRules: databricks.AccessControlRuleSetGrantRuleArray{\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(john.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/group.manager\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(ds.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/servicePrincipal.manager\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.AccessControlRuleSetGrantRuleArgs{\n\t\t\t\t\tPrincipals: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(marketplaceAdmins.AclPrincipalId),\n\t\t\t\t\t},\n\t\t\t\t\tRole: pulumi.String(\"roles/marketplace.admin\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.AccessControlRuleSet;\nimport com.pulumi.databricks.AccessControlRuleSetArgs;\nimport com.pulumi.databricks.inputs.AccessControlRuleSetGrantRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var accountId = \"00000000-0000-0000-0000-000000000000\";\n\n        // account level group\n        final var ds = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Data Science\")\n            .build());\n\n        // account level group\n        final var marketplaceAdmins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"Marketplace Admins\")\n            .build());\n\n        final var john = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"john.doe@example.com\")\n            .build());\n\n        var accountRuleSet = new AccessControlRuleSet(\"accountRuleSet\", AccessControlRuleSetArgs.builder()        \n            .name(String.format(\"accounts/%s/ruleSets/default\", accountId))\n            .grantRules(            \n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(john.applyValue(getUserResult -\u003e getUserResult.aclPrincipalId()))\n                    .role(\"roles/group.manager\")\n                    .build(),\n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(ds.applyValue(getGroupResult -\u003e getGroupResult.aclPrincipalId()))\n                    .role(\"roles/servicePrincipal.manager\")\n                    .build(),\n                AccessControlRuleSetGrantRuleArgs.builder()\n                    .principals(marketplaceAdmins.applyValue(getGroupResult -\u003e getGroupResult.aclPrincipalId()))\n                    .role(\"roles/marketplace.admin\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountRuleSet:\n    type: databricks:AccessControlRuleSet\n    name: account_rule_set\n    properties:\n      name: accounts/${accountId}/ruleSets/default\n      grantRules:\n        - principals:\n            - ${john.aclPrincipalId}\n          role: roles/group.manager\n        - principals:\n            - ${ds.aclPrincipalId}\n          role: roles/servicePrincipal.manager\n        - principals:\n            - ${marketplaceAdmins.aclPrincipalId}\n          role: roles/marketplace.admin\nvariables:\n  accountId: 00000000-0000-0000-0000-000000000000\n  # account level group\n  ds:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: Data Science\n  # account level group\n  marketplaceAdmins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: Marketplace Admins\n  john:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: john.doe@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Group\n* databricks.User\n* databricks.ServicePrincipal\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "grantRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                    },
                    "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e **Warning** Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default`\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default`\n* `accounts/{account_id}/ruleSets/default`\n"
                }
            },
            "required": [
                "etag",
                "name"
            ],
            "inputProperties": {
                "grantRules": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                    },
                    "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e **Warning** Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default`\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default`\n* `accounts/{account_id}/ruleSets/default`\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering AccessControlRuleSet resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "grantRules": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/AccessControlRuleSetGrantRule:AccessControlRuleSetGrantRule"
                        },
                        "description": "The access control rules to be granted by this rule set, consisting of a set of principals and roles to be granted to them.\n\n!\u003e **Warning** Name uniquely identifies a rule set resource. Ensure all the grant_rules blocks for a rule set name are present in one `databricks.AccessControlRuleSet` resource block. Otherwise, after applying changes, users might lose their role assignment even if that was not intended.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Unique identifier of a rule set. The name determines the resource to which the rule set applies. Currently, only default rule sets are supported. The following rule set formats are supported:\n* `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default`\n* `accounts/{account_id}/groups/{group_id}/ruleSets/default`\n* `accounts/{account_id}/ruleSets/default`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/artifactAllowlist:ArtifactAllowlist": {
            "description": "## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst initScripts = new databricks.ArtifactAllowlist(\"init_scripts\", {\n    artifactType: \"INIT_SCRIPT\",\n    artifactMatchers: [{\n        artifact: \"/Volumes/inits\",\n        matchType: \"PREFIX_MATCH\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninit_scripts = databricks.ArtifactAllowlist(\"init_scripts\",\n    artifact_type=\"INIT_SCRIPT\",\n    artifact_matchers=[databricks.ArtifactAllowlistArtifactMatcherArgs(\n        artifact=\"/Volumes/inits\",\n        match_type=\"PREFIX_MATCH\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var initScripts = new Databricks.ArtifactAllowlist(\"init_scripts\", new()\n    {\n        ArtifactType = \"INIT_SCRIPT\",\n        ArtifactMatchers = new[]\n        {\n            new Databricks.Inputs.ArtifactAllowlistArtifactMatcherArgs\n            {\n                Artifact = \"/Volumes/inits\",\n                MatchType = \"PREFIX_MATCH\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewArtifactAllowlist(ctx, \"init_scripts\", \u0026databricks.ArtifactAllowlistArgs{\n\t\t\tArtifactType: pulumi.String(\"INIT_SCRIPT\"),\n\t\t\tArtifactMatchers: databricks.ArtifactAllowlistArtifactMatcherArray{\n\t\t\t\t\u0026databricks.ArtifactAllowlistArtifactMatcherArgs{\n\t\t\t\t\tArtifact:  pulumi.String(\"/Volumes/inits\"),\n\t\t\t\t\tMatchType: pulumi.String(\"PREFIX_MATCH\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ArtifactAllowlist;\nimport com.pulumi.databricks.ArtifactAllowlistArgs;\nimport com.pulumi.databricks.inputs.ArtifactAllowlistArtifactMatcherArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var initScripts = new ArtifactAllowlist(\"initScripts\", ArtifactAllowlistArgs.builder()        \n            .artifactType(\"INIT_SCRIPT\")\n            .artifactMatchers(ArtifactAllowlistArtifactMatcherArgs.builder()\n                .artifact(\"/Volumes/inits\")\n                .matchType(\"PREFIX_MATCH\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  initScripts:\n    type: databricks:ArtifactAllowlist\n    name: init_scripts\n    properties:\n      artifactType: INIT_SCRIPT\n      artifactMatchers:\n        - artifact: /Volumes/inits\n          matchType: PREFIX_MATCH\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/artifactAllowlist:ArtifactAllowlist this '\u003cmetastore_id\u003e|\u003cartifact_type\u003e'\n```\n\n",
            "properties": {
                "artifactMatchers": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                    }
                },
                "artifactType": {
                    "type": "string",
                    "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this artifact allowlist was set.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Identity that set the artifact allowlist.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                }
            },
            "required": [
                "artifactMatchers",
                "artifactType",
                "createdAt",
                "createdBy",
                "metastoreId"
            ],
            "inputProperties": {
                "artifactMatchers": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                    }
                },
                "artifactType": {
                    "type": "string",
                    "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this artifact allowlist was set.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Identity that set the artifact allowlist.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                }
            },
            "requiredInputs": [
                "artifactMatchers",
                "artifactType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ArtifactAllowlist resources.\n",
                "properties": {
                    "artifactMatchers": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ArtifactAllowlistArtifactMatcher:ArtifactAllowlistArtifactMatcher"
                        }
                    },
                    "artifactType": {
                        "type": "string",
                        "description": "The artifact type of the allowlist. Can be `INIT_SCRIPT`, `LIBRARY_JAR` or `LIBRARY_MAVEN`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this artifact allowlist was set.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Identity that set the artifact allowlist.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "ID of the parent metastore.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/catalog:Catalog": {
            "description": "## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/catalog:Catalog this \u003cname\u003e\n```\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "connectionName": {
                    "type": "string",
                    "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n"
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "enablePredictiveOptimization",
                "isolationMode",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "connectionName": {
                    "type": "string",
                    "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete catalog regardless of its contents.\n"
                },
                "isolationMode": {
                    "type": "string",
                    "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "ID of the parent metastore.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Catalog relative to parent metastore.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the catalog owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Catalog properties.\n"
                },
                "providerName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "shareName": {
                    "type": "string",
                    "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Catalog resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "connectionName": {
                        "type": "string",
                        "description": "For Foreign Catalogs: the name of the connection to an external data source. Changes forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "enablePredictiveOptimization": {
                        "type": "string",
                        "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete catalog regardless of its contents.\n"
                    },
                    "isolationMode": {
                        "type": "string",
                        "description": "Whether the catalog is accessible from all workspaces or a specific set of workspaces. Can be `ISOLATED` or `OPEN`. Setting the catalog to `ISOLATED` will automatically allow access from the current workspace.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "ID of the parent metastore.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Catalog relative to parent metastore.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "For Foreign Catalogs: the name of the entity from an external data source that maps to a catalog. For example, the database name in a PostgreSQL server.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the catalog owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Catalog properties.\n"
                    },
                    "providerName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the delta sharing provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "shareName": {
                        "type": "string",
                        "description": "For Delta Sharing Catalogs: the name of the share under the share provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the catalog. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the metastore root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/catalogWorkspaceBinding:CatalogWorkspaceBinding": {
            "description": "## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    isolationMode: \"ISOLATED\",\n});\nconst sandboxCatalogWorkspaceBinding = new databricks.CatalogWorkspaceBinding(\"sandbox\", {\n    securableName: sandbox.name,\n    workspaceId: other.workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    isolation_mode=\"ISOLATED\")\nsandbox_catalog_workspace_binding = databricks.CatalogWorkspaceBinding(\"sandbox\",\n    securable_name=sandbox.name,\n    workspace_id=other[\"workspaceId\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        IsolationMode = \"ISOLATED\",\n    });\n\n    var sandboxCatalogWorkspaceBinding = new Databricks.CatalogWorkspaceBinding(\"sandbox\", new()\n    {\n        SecurableName = sandbox.Name,\n        WorkspaceId = other.WorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:          pulumi.String(\"sandbox\"),\n\t\t\tIsolationMode: pulumi.String(\"ISOLATED\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCatalogWorkspaceBinding(ctx, \"sandbox\", \u0026databricks.CatalogWorkspaceBindingArgs{\n\t\t\tSecurableName: sandbox.Name,\n\t\t\tWorkspaceId:   pulumi.Any(other.WorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.CatalogWorkspaceBinding;\nimport com.pulumi.databricks.CatalogWorkspaceBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .name(\"sandbox\")\n            .isolationMode(\"ISOLATED\")\n            .build());\n\n        var sandboxCatalogWorkspaceBinding = new CatalogWorkspaceBinding(\"sandboxCatalogWorkspaceBinding\", CatalogWorkspaceBindingArgs.builder()        \n            .securableName(sandbox.name())\n            .workspaceId(other.workspaceId())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      isolationMode: ISOLATED\n  sandboxCatalogWorkspaceBinding:\n    type: databricks:CatalogWorkspaceBinding\n    name: sandbox\n    properties:\n      securableName: ${sandbox.name}\n      workspaceId: ${other.workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by using combination of workspace ID, securable type and name:\n\n```sh\n$ pulumi import databricks:index/catalogWorkspaceBinding:CatalogWorkspaceBinding this \"\u003cworkspace_id\u003e|\u003csecurable_type\u003e|\u003csecurable_name\u003e\"\n```\n\n",
            "properties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n"
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead."
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n"
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n"
                }
            },
            "inputProperties": {
                "bindingType": {
                    "type": "string",
                    "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n",
                    "willReplaceOnChanges": true
                },
                "catalogName": {
                    "type": "string",
                    "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                    "willReplaceOnChanges": true
                },
                "securableName": {
                    "type": "string",
                    "description": "Name of securable. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "securableType": {
                    "type": "string",
                    "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "ID of the workspace. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering CatalogWorkspaceBinding resources.\n",
                "properties": {
                    "bindingType": {
                        "type": "string",
                        "description": "Binding mode. Default to `BINDING_TYPE_READ_WRITE`. Possible values are `BINDING_TYPE_READ_ONLY`, `BINDING_TYPE_READ_WRITE`\n",
                        "willReplaceOnChanges": true
                    },
                    "catalogName": {
                        "type": "string",
                        "deprecationMessage": "Please use 'securable_name' and 'securable_type instead.",
                        "willReplaceOnChanges": true
                    },
                    "securableName": {
                        "type": "string",
                        "description": "Name of securable. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "securableType": {
                        "type": "string",
                        "description": "Type of securable. Default to `catalog`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "ID of the workspace. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/cluster:Cluster": {
            "description": "This resource allows you to manage [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n\n\u003e **Note** In case of `Cannot access cluster ####-######-####### that was terminated or unpinned more than 30 days ago` command.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Group and databricks.User can control which groups or individual users can create clusters.\n* databricks.ClusterPolicy can control which kinds of clusters users can create.\n* Users, who have access to Cluster Policy, but do not have an `allow_cluster_create` argument set would still be able to create clusters, but within the boundary of the policy.\n* databricks.Permissions can control which groups or individual users can *Manage*, *Restart* or *Attach to* individual clusters.\n* `instance_profile_arn` *(AWS only)* can control which data a given cluster can access through cloud-native controls.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* Dynamic Passthrough Clusters for a Group guide.\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n* databricks.getNodeType data to get the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n* databricks.getZones data to fetch all available AWS availability zones on your workspace on AWS.\n\n## Import\n\nThe resource cluster can be imported using cluster id.\n\nbash\n\n```sh\n$ pulumi import databricks:index/cluster:Cluster this \u003ccluster-id\u003e\n```\n\n",
            "properties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "cloneFrom": {
                    "$ref": "#/types/databricks:index/ClusterCloneFrom:ClusterCloneFrom"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "clusterSource": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.Any(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.Any(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"ResourceClass\": pulumi.Any(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()        \n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                },
                "defaultTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n"
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": true,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": True,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", true },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.Any(true),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.Any(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.Any(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", true),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "state": {
                    "type": "string",
                    "description": "(string) State of the cluster.\n"
                },
                "url": {
                    "type": "string"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "required": [
                "clusterId",
                "clusterSource",
                "defaultTags",
                "driverInstancePoolId",
                "driverNodeTypeId",
                "enableElasticDisk",
                "enableLocalDiskEncryption",
                "nodeTypeId",
                "sparkVersion",
                "state",
                "url"
            ],
            "inputProperties": {
                "applyPolicyDefaultValues": {
                    "type": "boolean",
                    "description": "Whether to use policy default values for missing cluster attributes.\n"
                },
                "autoscale": {
                    "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                },
                "autoterminationMinutes": {
                    "type": "integer",
                    "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                },
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                },
                "cloneFrom": {
                    "$ref": "#/types/databricks:index/ClusterCloneFrom:ClusterCloneFrom"
                },
                "clusterLogConf": {
                    "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                },
                "clusterMountInfos": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                    }
                },
                "clusterName": {
                    "type": "string",
                    "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.Any(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.Any(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"ResourceClass\": pulumi.Any(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()        \n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "dataSecurityMode": {
                    "type": "string",
                    "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                },
                "dockerImage": {
                    "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                },
                "driverInstancePoolId": {
                    "type": "string",
                    "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                },
                "driverNodeTypeId": {
                    "type": "string",
                    "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                },
                "enableLocalDiskEncryption": {
                    "type": "boolean",
                    "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                },
                "idempotencyToken": {
                    "type": "string",
                    "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                    "willReplaceOnChanges": true
                },
                "initScripts": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                    }
                },
                "instancePoolId": {
                    "type": "string",
                    "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                },
                "isPinned": {
                    "type": "boolean",
                    "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": true,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": True,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", true },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.Any(true),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.Any(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.Any(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", true),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                    }
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                },
                "numWorkers": {
                    "type": "integer",
                    "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                },
                "policyId": {
                    "type": "string"
                },
                "runtimeEngine": {
                    "type": "string",
                    "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                },
                "singleUserName": {
                    "type": "string",
                    "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                },
                "sparkConf": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                },
                "sparkEnvVars": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                },
                "sparkVersion": {
                    "type": "string",
                    "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                },
                "sshPublicKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                },
                "workloadType": {
                    "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                }
            },
            "requiredInputs": [
                "sparkVersion"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Cluster resources.\n",
                "properties": {
                    "applyPolicyDefaultValues": {
                        "type": "boolean",
                        "description": "Whether to use policy default values for missing cluster attributes.\n"
                    },
                    "autoscale": {
                        "$ref": "#/types/databricks:index/ClusterAutoscale:ClusterAutoscale"
                    },
                    "autoterminationMinutes": {
                        "type": "integer",
                        "description": "Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*\n"
                    },
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAwsAttributes:ClusterAwsAttributes"
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/ClusterAzureAttributes:ClusterAzureAttributes"
                    },
                    "cloneFrom": {
                        "$ref": "#/types/databricks:index/ClusterCloneFrom:ClusterCloneFrom"
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterLogConf": {
                        "$ref": "#/types/databricks:index/ClusterClusterLogConf:ClusterClusterLogConf"
                    },
                    "clusterMountInfos": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterClusterMountInfo:ClusterClusterMountInfo"
                        }
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "Cluster name, which doesn’t have to be unique. If not specified at creation, the cluster name will be an empty string.\n"
                    },
                    "clusterSource": {
                        "type": "string"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "should have tag `ResourceClass` set to value `Serverless`\n\nFor example:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst clusterWithTableAccessControl = new databricks.Cluster(\"cluster_with_table_access_control\", {\n    clusterName: \"Shared High-Concurrency\",\n    sparkVersion: latestLts.id,\n    nodeTypeId: smallest.id,\n    autoterminationMinutes: 20,\n    sparkConf: {\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ncluster_with_table_access_control = databricks.Cluster(\"cluster_with_table_access_control\",\n    cluster_name=\"Shared High-Concurrency\",\n    spark_version=latest_lts[\"id\"],\n    node_type_id=smallest[\"id\"],\n    autotermination_minutes=20,\n    spark_conf={\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.cluster.profile\": \"serverless\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var clusterWithTableAccessControl = new Databricks.Cluster(\"cluster_with_table_access_control\", new()\n    {\n        ClusterName = \"Shared High-Concurrency\",\n        SparkVersion = latestLts.Id,\n        NodeTypeId = smallest.Id,\n        AutoterminationMinutes = 20,\n        SparkConf = \n        {\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewCluster(ctx, \"cluster_with_table_access_control\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared High-Concurrency\"),\n\t\t\tSparkVersion:           pulumi.Any(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.Any(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\": pulumi.Any(\"python,sql\"),\n\t\t\t\t\"spark.databricks.cluster.profile\":       pulumi.Any(\"serverless\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"ResourceClass\": pulumi.Any(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var clusterWithTableAccessControl = new Cluster(\"clusterWithTableAccessControl\", ClusterArgs.builder()        \n            .clusterName(\"Shared High-Concurrency\")\n            .sparkVersion(latestLts.id())\n            .nodeTypeId(smallest.id())\n            .autoterminationMinutes(20)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  clusterWithTableAccessControl:\n    type: databricks:Cluster\n    name: cluster_with_table_access_control\n    properties:\n      clusterName: Shared High-Concurrency\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      sparkConf:\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.cluster.profile: serverless\n      customTags:\n        ResourceClass: Serverless\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                    },
                    "dataSecurityMode": {
                        "type": "string",
                        "description": "Select the security features of the cluster. [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, no security features are enabled. In the Databricks UI, this has been recently been renamed *Access Mode* and `USER_ISOLATION` has been renamed *Shared*, but use these terms here.\n"
                    },
                    "defaultTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: \u003cusername_of_creator\u003e, ClusterName: \u003cname_of_cluster\u003e, ClusterId: \u003cid_of_cluster\u003e, Name: \u003cDatabricks internal use\u003e, and any workspace and pool tags.\n"
                    },
                    "dockerImage": {
                        "$ref": "#/types/databricks:index/ClusterDockerImage:ClusterDockerImage"
                    },
                    "driverInstancePoolId": {
                        "type": "string",
                        "description": "similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.\n"
                    },
                    "driverNodeTypeId": {
                        "type": "string",
                        "description": "The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.\n"
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "If you don’t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster’s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance’s local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).\n"
                    },
                    "enableLocalDiskEncryption": {
                        "type": "boolean",
                        "description": "Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster’s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*\n"
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/ClusterGcpAttributes:ClusterGcpAttributes"
                    },
                    "idempotencyToken": {
                        "type": "string",
                        "description": "An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "initScripts": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterInitScript:ClusterInitScript"
                        }
                    },
                    "instancePoolId": {
                        "type": "string",
                        "description": "To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster’s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.\n"
                    },
                    "isPinned": {
                        "type": "boolean",
                        "description": "boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).\n\nThe following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latestLts = databricks.getSparkVersion({\n    longTermSupport: true,\n});\nconst sharedAutoscaling = new databricks.Cluster(\"shared_autoscaling\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latestLts.then(latestLts =\u003e latestLts.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    sparkConf: {\n        \"spark.databricks.io.cache.enabled\": true,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type(local_disk=True)\nlatest_lts = databricks.get_spark_version(long_term_support=True)\nshared_autoscaling = databricks.Cluster(\"shared_autoscaling\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest_lts.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    spark_conf={\n        \"spark.databricks.io.cache.enabled\": True,\n        \"spark.databricks.io.cache.maxDiskUsage\": \"50g\",\n        \"spark.databricks.io.cache.maxMetaDataCache\": \"1g\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latestLts = Databricks.GetSparkVersion.Invoke(new()\n    {\n        LongTermSupport = true,\n    });\n\n    var sharedAutoscaling = new Databricks.Cluster(\"shared_autoscaling\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latestLts.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        SparkConf = \n        {\n            { \"spark.databricks.io.cache.enabled\", true },\n            { \"spark.databricks.io.cache.maxDiskUsage\", \"50g\" },\n            { \"spark.databricks.io.cache.maxMetaDataCache\", \"1g\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatestLts, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tLongTermSupport: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"shared_autoscaling\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latestLts.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.io.cache.enabled\":          pulumi.Any(true),\n\t\t\t\t\"spark.databricks.io.cache.maxDiskUsage\":     pulumi.Any(\"50g\"),\n\t\t\t\t\"spark.databricks.io.cache.maxMetaDataCache\": pulumi.Any(\"1g\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .longTermSupport(true)\n            .build());\n\n        var sharedAutoscaling = new Cluster(\"sharedAutoscaling\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latestLts.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.io.cache.enabled\", true),\n                Map.entry(\"spark.databricks.io.cache.maxDiskUsage\", \"50g\"),\n                Map.entry(\"spark.databricks.io.cache.maxMetaDataCache\", \"1g\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedAutoscaling:\n    type: databricks:Cluster\n    name: shared_autoscaling\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latestLts.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      sparkConf:\n        spark.databricks.io.cache.enabled: true\n        spark.databricks.io.cache.maxDiskUsage: 50g\n        spark.databricks.io.cache.maxMetaDataCache: 1g\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latestLts:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        longTermSupport: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterLibrary:ClusterLibrary"
                        }
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "Any supported databricks.getNodeType id. If `instance_pool_id` is specified, this field is not needed.\n"
                    },
                    "numWorkers": {
                        "type": "integer",
                        "description": "Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.\n"
                    },
                    "policyId": {
                        "type": "string"
                    },
                    "runtimeEngine": {
                        "type": "string",
                        "description": "The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.\n"
                    },
                    "singleUserName": {
                        "type": "string",
                        "description": "The optional user name of the user to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).\n"
                    },
                    "sparkConf": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "should have following items:\n* `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!\n* `spark.databricks.cluster.profile` set to `serverless`\n"
                    },
                    "sparkEnvVars": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.\n"
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "[Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.\n"
                    },
                    "sshPublicKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "(string) State of the cluster.\n"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workloadType": {
                        "$ref": "#/types/databricks:index/ClusterWorkloadType:ClusterWorkloadType"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/clusterPolicy:ClusterPolicy": {
            "description": "This resource creates a cluster policy, which limits the ability to create clusters based on a set of rules. The policy rules limit the attributes or attribute values available for cluster creation. cluster policies have ACLs that limit their use to specific users and groups. Only admin users can create, edit, and delete policies. Admin users also have access to all policies.\n\nCluster policies let you:\n\n* Limit users to create clusters with prescribed settings.\n* Simplify the user interface and enable more users to create their own clusters (by fixing and hiding some values).\n* Control cost by limiting per cluster maximum cost (by setting limits on attributes whose values contribute to hourly price).\n\nCluster policy permissions limit which policies a user can select in the Policy drop-down when the user creates a cluster:\n\n* If no policies have been created in the workspace, the Policy drop-down does not display.\n* A user who has cluster create permission can select the `Free form` policy and create fully-configurable clusters.\n* A user who has both cluster create permission and access to cluster policies can select the Free form policy and policies they have access to.\n* A user that has access to only cluster policies, can select the policies they have access to.\n\n### Overriding the built-in cluster policies\n\nYou can override built-in cluster policies by creating a `databricks.ClusterPolicy` resource with following attributes:\n\n* `name` - the name of the built-in cluster policy.\n* `policy_family_id` - the ID of the cluster policy family used for built-in cluster policy.\n* `policy_family_definition_overrides` - settings to override in the built-in cluster policy.\n\nYou can obtain the list of defined cluster policies families using the `databricks policy-families list` command of the new [Databricks CLI](https://docs.databricks.com/en/dev-tools/cli/index.html), or via [list policy families](https://docs.databricks.com/api/workspace/policyfamilies/list) REST API.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst personalVmOverride = {\n    autotermination_minutes: {\n        type: \"fixed\",\n        value: 220,\n        hidden: true,\n    },\n    \"custom_tags.Team\": {\n        type: \"fixed\",\n        value: team,\n    },\n};\nconst personalVm = new databricks.ClusterPolicy(\"personal_vm\", {\n    policyFamilyId: \"personal-vm\",\n    policyFamilyDefinitionOverrides: JSON.stringify(personalVmOverride),\n    name: \"Personal Compute\",\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\npersonal_vm_override = {\n    \"autotermination_minutes\": {\n        \"type\": \"fixed\",\n        \"value\": 220,\n        \"hidden\": True,\n    },\n    \"custom_tags.Team\": {\n        \"type\": \"fixed\",\n        \"value\": team,\n    },\n}\npersonal_vm = databricks.ClusterPolicy(\"personal_vm\",\n    policy_family_id=\"personal-vm\",\n    policy_family_definition_overrides=json.dumps(personal_vm_override),\n    name=\"Personal Compute\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var personalVmOverride = \n    {\n        { \"autotermination_minutes\", \n        {\n            { \"type\", \"fixed\" },\n            { \"value\", 220 },\n            { \"hidden\", true },\n        } },\n        { \"custom_tags.Team\", \n        {\n            { \"type\", \"fixed\" },\n            { \"value\", team },\n        } },\n    };\n\n    var personalVm = new Databricks.ClusterPolicy(\"personal_vm\", new()\n    {\n        PolicyFamilyId = \"personal-vm\",\n        PolicyFamilyDefinitionOverrides = JsonSerializer.Serialize(personalVmOverride),\n        Name = \"Personal Compute\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpersonalVmOverride := map[string]interface{}{\n\t\t\t\"autotermination_minutes\": map[string]interface{}{\n\t\t\t\t\"type\":   \"fixed\",\n\t\t\t\t\"value\":  220,\n\t\t\t\t\"hidden\": true,\n\t\t\t},\n\t\t\t\"custom_tags.Team\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": team,\n\t\t\t},\n\t\t}\n\t\ttmpJSON0, err := json.Marshal(personalVmOverride)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"personal_vm\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tPolicyFamilyId:                  pulumi.String(\"personal-vm\"),\n\t\t\tPolicyFamilyDefinitionOverrides: pulumi.String(json0),\n\t\t\tName:                            pulumi.String(\"Personal Compute\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var personalVmOverride = %!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);\n\n        var personalVm = new ClusterPolicy(\"personalVm\", ClusterPolicyArgs.builder()        \n            .policyFamilyId(\"personal-vm\")\n            .policyFamilyDefinitionOverrides(serializeJson(\n                personalVmOverride))\n            .name(\"Personal Compute\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  personalVm:\n    type: databricks:ClusterPolicy\n    name: personal_vm\n    properties:\n      policyFamilyId: personal-vm\n      policyFamilyDefinitionOverrides:\n        fn::toJSON: ${personalVmOverride}\n      name: Personal Compute\nvariables:\n  personalVmOverride:\n    autotermination_minutes:\n      type: fixed\n      value: 220\n      hidden: true\n    custom_tags.Team:\n      type: fixed\n      value: ${team}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* Dynamic Passthrough Clusters for a Group guide.\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.getNodeType data to get the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.getSparkVersion data to get [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.WorkspaceConf to manage workspace configuration for expert usage.\n\n## Import\n\nThe resource cluster policy can be imported using the policy id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/clusterPolicy:ClusterPolicy this \u003ccluster-policy-id\u003e\n```\n\n",
            "properties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                },
                "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                    },
                    "description": "blocks defining individual libraries that will be installed on the cluster that uses a given cluster policy. See databricks.Cluster for more details about supported library types.\n"
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyFamilyDefinitionOverrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                },
                "policyFamilyId": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                },
                "policyId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the cluster policy.\n"
                }
            },
            "required": [
                "definition",
                "name",
                "policyId"
            ],
            "inputProperties": {
                "definition": {
                    "type": "string",
                    "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                },
                "description": {
                    "type": "string",
                    "description": "Additional human-readable description of the cluster policy.\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                    },
                    "description": "blocks defining individual libraries that will be installed on the cluster that uses a given cluster policy. See databricks.Cluster for more details about supported library types.\n"
                },
                "maxClustersPerUser": {
                    "type": "integer",
                    "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                },
                "policyFamilyDefinitionOverrides": {
                    "type": "string",
                    "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                },
                "policyFamilyId": {
                    "type": "string",
                    "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ClusterPolicy resources.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition). Cannot be used with `policy_family_id`\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "Additional human-readable description of the cluster policy.\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ClusterPolicyLibrary:ClusterPolicyLibrary"
                        },
                        "description": "blocks defining individual libraries that will be installed on the cluster that uses a given cluster policy. See databricks.Cluster for more details about supported library types.\n"
                    },
                    "maxClustersPerUser": {
                        "type": "integer",
                        "description": "Maximum number of clusters allowed per user. When omitted, there is no limit. If specified, value must be greater than zero.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Cluster policy name. This must be unique. Length must be between 1 and 100 characters.\n"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "type": "string",
                        "description": "Policy definition JSON document expressed in Databricks Policy Definition Language. The JSON document must be passed as a string and cannot be embedded in the requests. You can use this to customize the policy definition inherited from the policy family. Policy rules specified here are merged into the inherited policy definition.\n"
                    },
                    "policyFamilyId": {
                        "type": "string",
                        "description": "ID of the policy family. The cluster policy's policy definition inherits the policy family's policy definition. Cannot be used with `definition`. Use `policy_family_definition_overrides` instead to customize the policy definition.\n"
                    },
                    "policyId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the cluster policy.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/connection:Connection": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nLakehouse Federation is the query federation platform for Databricks. Databricks uses Unity Catalog to manage query federation. To make a dataset available for read-only querying using Lakehouse Federation, you create the following:\n\n- A connection, a securable object in Unity Catalog that specifies a path and credentials for accessing an external database system.\n- A foreign catalog\n\nThis resource manages connections in Unity Catalog\n\n## Example Usage\n\nCreate a connection to a MySQL database\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst mysql = new databricks.Connection(\"mysql\", {\n    name: \"mysql_connection\",\n    connectionType: \"MYSQL\",\n    comment: \"this is a connection to mysql db\",\n    options: {\n        host: \"test.mysql.database.azure.com\",\n        port: \"3306\",\n        user: \"user\",\n        password: \"password\",\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmysql = databricks.Connection(\"mysql\",\n    name=\"mysql_connection\",\n    connection_type=\"MYSQL\",\n    comment=\"this is a connection to mysql db\",\n    options={\n        \"host\": \"test.mysql.database.azure.com\",\n        \"port\": \"3306\",\n        \"user\": \"user\",\n        \"password\": \"password\",\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var mysql = new Databricks.Connection(\"mysql\", new()\n    {\n        Name = \"mysql_connection\",\n        ConnectionType = \"MYSQL\",\n        Comment = \"this is a connection to mysql db\",\n        Options = \n        {\n            { \"host\", \"test.mysql.database.azure.com\" },\n            { \"port\", \"3306\" },\n            { \"user\", \"user\" },\n            { \"password\", \"password\" },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewConnection(ctx, \"mysql\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"mysql_connection\"),\n\t\t\tConnectionType: pulumi.String(\"MYSQL\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to mysql db\"),\n\t\t\tOptions: pulumi.Map{\n\t\t\t\t\"host\":     pulumi.Any(\"test.mysql.database.azure.com\"),\n\t\t\t\t\"port\":     pulumi.Any(\"3306\"),\n\t\t\t\t\"user\":     pulumi.Any(\"user\"),\n\t\t\t\t\"password\": pulumi.Any(\"password\"),\n\t\t\t},\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mysql = new Connection(\"mysql\", ConnectionArgs.builder()        \n            .name(\"mysql_connection\")\n            .connectionType(\"MYSQL\")\n            .comment(\"this is a connection to mysql db\")\n            .options(Map.ofEntries(\n                Map.entry(\"host\", \"test.mysql.database.azure.com\"),\n                Map.entry(\"port\", \"3306\"),\n                Map.entry(\"user\", \"user\"),\n                Map.entry(\"password\", \"password\")\n            ))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  mysql:\n    type: databricks:Connection\n    properties:\n      name: mysql_connection\n      connectionType: MYSQL\n      comment: this is a connection to mysql db\n      options:\n        host: test.mysql.database.azure.com\n        port: '3306'\n        user: user\n        password: password\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreate a connection to a BigQuery database\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst bigquery = new databricks.Connection(\"bigquery\", {\n    name: \"bq_connection\",\n    connectionType: \"BIGQUERY\",\n    comment: \"this is a connection to BQ\",\n    options: {\n        GoogleServiceAccountKeyJson: JSON.stringify({\n            type: \"service_account\",\n            project_id: \"PROJECT_ID\",\n            private_key_id: \"KEY_ID\",\n            private_key: `-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n`,\n            client_email: \"SERVICE_ACCOUNT_EMAIL\",\n            client_id: \"CLIENT_ID\",\n            auth_uri: \"https://accounts.google.com/o/oauth2/auth\",\n            token_uri: \"https://accounts.google.com/o/oauth2/token\",\n            auth_provider_x509_cert_url: \"https://www.googleapis.com/oauth2/v1/certs\",\n            client_x509_cert_url: \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n            universe_domain: \"googleapis.com\",\n        }),\n    },\n    properties: {\n        purpose: \"testing\",\n    },\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nbigquery = databricks.Connection(\"bigquery\",\n    name=\"bq_connection\",\n    connection_type=\"BIGQUERY\",\n    comment=\"this is a connection to BQ\",\n    options={\n        \"GoogleServiceAccountKeyJson\": json.dumps({\n            \"type\": \"service_account\",\n            \"project_id\": \"PROJECT_ID\",\n            \"private_key_id\": \"KEY_ID\",\n            \"private_key\": \"\"\"-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n\"\"\",\n            \"client_email\": \"SERVICE_ACCOUNT_EMAIL\",\n            \"client_id\": \"CLIENT_ID\",\n            \"auth_uri\": \"https://accounts.google.com/o/oauth2/auth\",\n            \"token_uri\": \"https://accounts.google.com/o/oauth2/token\",\n            \"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n            \"client_x509_cert_url\": \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n            \"universe_domain\": \"googleapis.com\",\n        }),\n    },\n    properties={\n        \"purpose\": \"testing\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var bigquery = new Databricks.Connection(\"bigquery\", new()\n    {\n        Name = \"bq_connection\",\n        ConnectionType = \"BIGQUERY\",\n        Comment = \"this is a connection to BQ\",\n        Options = \n        {\n            { \"GoogleServiceAccountKeyJson\", JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"service_account\",\n                [\"project_id\"] = \"PROJECT_ID\",\n                [\"private_key_id\"] = \"KEY_ID\",\n                [\"private_key\"] = @\"-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n\",\n                [\"client_email\"] = \"SERVICE_ACCOUNT_EMAIL\",\n                [\"client_id\"] = \"CLIENT_ID\",\n                [\"auth_uri\"] = \"https://accounts.google.com/o/oauth2/auth\",\n                [\"token_uri\"] = \"https://accounts.google.com/o/oauth2/token\",\n                [\"auth_provider_x509_cert_url\"] = \"https://www.googleapis.com/oauth2/v1/certs\",\n                [\"client_x509_cert_url\"] = \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n                [\"universe_domain\"] = \"googleapis.com\",\n            }) },\n        },\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"type\":                        \"service_account\",\n\t\t\t\"project_id\":                  \"PROJECT_ID\",\n\t\t\t\"private_key_id\":              \"KEY_ID\",\n\t\t\t\"private_key\":                 \"-----BEGIN PRIVATE KEY-----\\nPRIVATE_KEY\\n-----END PRIVATE KEY-----\\n\",\n\t\t\t\"client_email\":                \"SERVICE_ACCOUNT_EMAIL\",\n\t\t\t\"client_id\":                   \"CLIENT_ID\",\n\t\t\t\"auth_uri\":                    \"https://accounts.google.com/o/oauth2/auth\",\n\t\t\t\"token_uri\":                   \"https://accounts.google.com/o/oauth2/token\",\n\t\t\t\"auth_provider_x509_cert_url\": \"https://www.googleapis.com/oauth2/v1/certs\",\n\t\t\t\"client_x509_cert_url\":        \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\",\n\t\t\t\"universe_domain\":             \"googleapis.com\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewConnection(ctx, \"bigquery\", \u0026databricks.ConnectionArgs{\n\t\t\tName:           pulumi.String(\"bq_connection\"),\n\t\t\tConnectionType: pulumi.String(\"BIGQUERY\"),\n\t\t\tComment:        pulumi.String(\"this is a connection to BQ\"),\n\t\t\tOptions: pulumi.Map{\n\t\t\t\t\"GoogleServiceAccountKeyJson\": pulumi.String(json0),\n\t\t\t},\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Connection;\nimport com.pulumi.databricks.ConnectionArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var bigquery = new Connection(\"bigquery\", ConnectionArgs.builder()        \n            .name(\"bq_connection\")\n            .connectionType(\"BIGQUERY\")\n            .comment(\"this is a connection to BQ\")\n            .options(Map.of(\"GoogleServiceAccountKeyJson\", serializeJson(\n                jsonObject(\n                    jsonProperty(\"type\", \"service_account\"),\n                    jsonProperty(\"project_id\", \"PROJECT_ID\"),\n                    jsonProperty(\"private_key_id\", \"KEY_ID\"),\n                    jsonProperty(\"private_key\", \"\"\"\n-----BEGIN PRIVATE KEY-----\nPRIVATE_KEY\n-----END PRIVATE KEY-----\n                    \"\"\"),\n                    jsonProperty(\"client_email\", \"SERVICE_ACCOUNT_EMAIL\"),\n                    jsonProperty(\"client_id\", \"CLIENT_ID\"),\n                    jsonProperty(\"auth_uri\", \"https://accounts.google.com/o/oauth2/auth\"),\n                    jsonProperty(\"token_uri\", \"https://accounts.google.com/o/oauth2/token\"),\n                    jsonProperty(\"auth_provider_x509_cert_url\", \"https://www.googleapis.com/oauth2/v1/certs\"),\n                    jsonProperty(\"client_x509_cert_url\", \"https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\"),\n                    jsonProperty(\"universe_domain\", \"googleapis.com\")\n                ))))\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  bigquery:\n    type: databricks:Connection\n    properties:\n      name: bq_connection\n      connectionType: BIGQUERY\n      comment: this is a connection to BQ\n      options:\n        GoogleServiceAccountKeyJson:\n          fn::toJSON:\n            type: service_account\n            project_id: PROJECT_ID\n            private_key_id: KEY_ID\n            private_key: |\n              -----BEGIN PRIVATE KEY-----\n              PRIVATE_KEY\n              -----END PRIVATE KEY-----\n            client_email: SERVICE_ACCOUNT_EMAIL\n            client_id: CLIENT_ID\n            auth_uri: https://accounts.google.com/o/oauth2/auth\n            token_uri: https://accounts.google.com/o/oauth2/token\n            auth_provider_x509_cert_url: https://www.googleapis.com/oauth2/v1/certs\n            client_x509_cert_url: https://www.googleapis.com/robot/v1/metadata/x509/SERVICE_ACCOUNT_EMAIL\n            universe_domain: googleapis.com\n      properties:\n        purpose: testing\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by `id`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/connection:Connection this '\u003cmetastore_id\u003e|\u003cname\u003e'\n```\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "connectionType": {
                    "type": "string",
                    "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER` or `DATABRICKS` are supported. [Up-to-date list of connection type supported](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Connection.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                    "secret": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the connection owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Free-form connection properties.\n"
                },
                "readOnly": {
                    "type": "boolean"
                }
            },
            "required": [
                "connectionType",
                "metastoreId",
                "name",
                "options",
                "owner",
                "readOnly"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n",
                    "willReplaceOnChanges": true
                },
                "connectionType": {
                    "type": "string",
                    "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER` or `DATABRICKS` are supported. [Up-to-date list of connection type supported](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n",
                    "willReplaceOnChanges": true
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Connection.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                    "secret": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the connection owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Free-form connection properties.\n",
                    "willReplaceOnChanges": true
                },
                "readOnly": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "connectionType",
                "options"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Connection resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "Free-form text.\n",
                        "willReplaceOnChanges": true
                    },
                    "connectionType": {
                        "type": "string",
                        "description": "Connection type. `BIGQUERY` `MYSQL` `POSTGRESQL` `SNOWFLAKE` `REDSHIFT` `SQLDW` `SQLSERVER` or `DATABRICKS` are supported. [Up-to-date list of connection type supported](https://docs.databricks.com/query-federation/index.html#supported-data-sources)\n",
                        "willReplaceOnChanges": true
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Connection.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "The key value of options required by the connection, e.g. `host`, `port`, `user`, `password` or `GoogleServiceAccountKeyJson`. Please consult the [documentation](https://docs.databricks.com/query-federation/index.html#supported-data-sources) for the required option.\n",
                        "secret": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the connection owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Free-form connection properties.\n",
                        "willReplaceOnChanges": true
                    },
                    "readOnly": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/dbfsFile:DbfsFile": {
            "description": "\n\n## Import\n\nThe resource dbfs file can be imported using the path of the file:\n\nbash\n\n```sh\n$ pulumi import databricks:index/dbfsFile:DbfsFile this \u003cpath\u003e\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "dbfsPath": {
                    "type": "string",
                    "description": "Path, but with `dbfs:` prefix.\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "dbfsPath",
                "fileSize",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "md5": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save.\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DbfsFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "dbfsPath": {
                        "type": "string",
                        "description": "Path, but with `dbfs:` prefix.\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save.\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/defaultNamespaceSetting:DefaultNamespaceSetting": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nThe `databricks.DefaultNamespaceSetting` resource allows you to operate the setting configuration for the default namespace in the Databricks workspace.\nSetting the default catalog for the workspace determines the catalog that is used when queries do not reference\na fully qualified 3 level name. For example, if the default catalog is set to 'retail_prod' then a query\n'SELECT * FROM myTable' would reference the object 'retail_prod.default.myTable'\n(the schema 'default' is always assumed).\nThis setting requires a restart of clusters and SQL warehouses to take effect. Additionally, the default namespace only applies when using Unity Catalog-enabled compute.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.DefaultNamespaceSetting(\"this\", {namespace: {\n    value: \"namespace_value\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.DefaultNamespaceSetting(\"this\", namespace=databricks.DefaultNamespaceSettingNamespaceArgs(\n    value=\"namespace_value\",\n))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.DefaultNamespaceSetting(\"this\", new()\n    {\n        Namespace = new Databricks.Inputs.DefaultNamespaceSettingNamespaceArgs\n        {\n            Value = \"namespace_value\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewDefaultNamespaceSetting(ctx, \"this\", \u0026databricks.DefaultNamespaceSettingArgs{\n\t\t\tNamespace: \u0026databricks.DefaultNamespaceSettingNamespaceArgs{\n\t\t\t\tValue: pulumi.String(\"namespace_value\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DefaultNamespaceSetting;\nimport com.pulumi.databricks.DefaultNamespaceSettingArgs;\nimport com.pulumi.databricks.inputs.DefaultNamespaceSettingNamespaceArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new DefaultNamespaceSetting(\"this\", DefaultNamespaceSettingArgs.builder()        \n            .namespace(DefaultNamespaceSettingNamespaceArgs.builder()\n                .value(\"namespace_value\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:DefaultNamespaceSetting\n    properties:\n      namespace:\n        value: namespace_value\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/defaultNamespaceSetting:DefaultNamespaceSetting this global\n```\n\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "namespace": {
                    "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "etag",
                "namespace",
                "settingName"
            ],
            "inputProperties": {
                "etag": {
                    "type": "string"
                },
                "namespace": {
                    "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "namespace"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering DefaultNamespaceSetting resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "namespace": {
                        "$ref": "#/types/databricks:index/DefaultNamespaceSettingNamespace:DefaultNamespaceSettingNamespace",
                        "description": "The configuration details.\n"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/directory:Directory": {
            "description": "\n\n## Import\n\nThe resource directory can be imported using directory path:\n\nbash\n\n```sh\n$ pulumi import databricks:index/directory:Directory this /path/to/directory\n```\n\n",
            "properties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "objectId",
                "path",
                "workspacePath"
            ],
            "inputProperties": {
                "deleteRecursive": {
                    "type": "boolean"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a DIRECTORY\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Directory resources.\n",
                "properties": {
                    "deleteRecursive": {
                        "type": "boolean"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a DIRECTORY\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/entitlements:Entitlements": {
            "description": "This resource allows you to set entitlements to existing databricks_users, databricks.Group or databricks_service_principal.\n\n\u003e **Note** You must define entitlements of a principal using either `databricks.Entitlements` or directly within one of databricks_users, databricks.Group or databricks_service_principal. Having entitlements defined in both resources will result in non-deterministic behaviour.\n\n## Example Usage\n\nSetting entitlements for a regular user:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst meEntitlements = new databricks.Entitlements(\"me\", {\n    userId: me.then(me =\u003e me.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_user(user_name=\"me@example.com\")\nme_entitlements = databricks.Entitlements(\"me\",\n    user_id=me.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var meEntitlements = new Databricks.Entitlements(\"me\", new()\n    {\n        UserId = me.Apply(getUserResult =\u003e getUserResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"me\", \u0026databricks.EntitlementsArgs{\n\t\t\tUserId:                  pulumi.String(me.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var meEntitlements = new Entitlements(\"meEntitlements\", EntitlementsArgs.builder()        \n            .userId(me.applyValue(getUserResult -\u003e getUserResult.id()))\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  meEntitlements:\n    type: databricks:Entitlements\n    name: me\n    properties:\n      userId: ${me.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nSetting entitlements for a service principal:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst thisEntitlements = new databricks.Entitlements(\"this\", {\n    servicePrincipalId: _this.then(_this =\u003e _this.spId),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nthis_entitlements = databricks.Entitlements(\"this\",\n    service_principal_id=this.sp_id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var thisEntitlements = new Databricks.Entitlements(\"this\", new()\n    {\n        ServicePrincipalId = @this.Apply(@this =\u003e @this.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.SpId)),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"this\", \u0026databricks.EntitlementsArgs{\n\t\t\tServicePrincipalId:      pulumi.String(this.SpId),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var thisEntitlements = new Entitlements(\"thisEntitlements\", EntitlementsArgs.builder()        \n            .servicePrincipalId(this_.spId())\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisEntitlements:\n    type: databricks:Entitlements\n    name: this\n    properties:\n      servicePrincipalId: ${this.spId}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getServicePrincipal\n      Arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nSetting entitlements to all users in a workspace - referencing special `users` databricks.Group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst workspace_users = new databricks.Entitlements(\"workspace-users\", {\n    groupId: users.then(users =\u003e users.id),\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusers = databricks.get_group(display_name=\"users\")\nworkspace_users = databricks.Entitlements(\"workspace-users\",\n    group_id=users.id,\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var workspace_users = new Databricks.Entitlements(\"workspace-users\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewEntitlements(ctx, \"workspace-users\", \u0026databricks.EntitlementsArgs{\n\t\t\tGroupId:                 pulumi.String(users.Id),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.Entitlements;\nimport com.pulumi.databricks.EntitlementsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var workspace_users = new Entitlements(\"workspace-users\", EntitlementsArgs.builder()        \n            .groupId(users.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  workspace-users:\n    type: databricks:Entitlements\n    properties:\n      groupId: ${users.id}\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\nvariables:\n  users:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: users\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are:\n\n* `user/user_id` - user `user_id`.\n\n* `group/group_id` - group `group_id`.\n\n* `spn/spn_id` - service principal `spn_id`.\n\nbash\n\n```sh\n$ pulumi import databricks:index/entitlements:Entitlements me user/\u003cuser-id\u003e\n```\n\n",
            "properties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "inputProperties": {
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the group.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "Canonical unique identifier for the user.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Entitlements resources.\n",
                "properties": {
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "groupId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the group.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the service principal.\n\nThe following entitlements are available.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "Canonical unique identifier for the user.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the principal to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/externalLocation:ExternalLocation": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- databricks.StorageCredential represent authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- `databricks.ExternalLocation` are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.id,\n    comment: \"Managed by TF\",\n});\nconst someGrants = new databricks.Grants(\"some\", {\n    externalLocation: some.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\n            \"CREATE_EXTERNAL_TABLE\",\n            \"READ_FILES\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role=databricks.StorageCredentialAwsIamRoleArgs(\n        role_arn=external_data_access[\"arn\"],\n    ),\n    comment=\"Managed by TF\")\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external.id,\n    comment=\"Managed by TF\")\nsome_grants = databricks.Grants(\"some\",\n    external_location=some.id,\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\n            \"CREATE_EXTERNAL_TABLE\",\n            \"READ_FILES\",\n        ],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Id,\n        Comment = \"Managed by TF\",\n    });\n\n    var someGrants = new Databricks.Grants(\"some\", new()\n    {\n        ExternalLocation = some.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external\"),\n\t\t\tUrl:            pulumi.String(fmt.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id)),\n\t\t\tCredentialName: external.ID(),\n\t\t\tComment:        pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"some\", \u0026databricks.GrantsArgs{\n\t\t\tExternalLocation: some.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t\tpulumi.String(\"READ_FILES\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()        \n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()        \n            .name(\"external\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.id())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var someGrants = new Grants(\"someGrants\", GrantsArgs.builder()        \n            .externalLocation(some.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(                \n                    \"CREATE_EXTERNAL_TABLE\",\n                    \"READ_FILES\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.id}\n      comment: Managed by TF\n  someGrants:\n    type: databricks:Grants\n    name: some\n    properties:\n      externalLocation: ${some.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n            - READ_FILES\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n## Import\n\nThis resource can be imported by `name`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/externalLocation:ExternalLocation this \u003cname\u003e\n```\n\n",
            "properties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy external location regardless of its dependents.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update external location regardless of its dependents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "required": [
                "credentialName",
                "metastoreId",
                "name",
                "owner",
                "url"
            ],
            "inputProperties": {
                "accessPoint": {
                    "type": "string",
                    "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "credentialName": {
                    "type": "string",
                    "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                },
                "encryptionDetails": {
                    "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                    "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy external location regardless of its dependents.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update external location regardless of its dependents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the external location owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the external location is read-only.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the external location\n"
                },
                "url": {
                    "type": "string",
                    "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                }
            },
            "requiredInputs": [
                "credentialName",
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ExternalLocation resources.\n",
                "properties": {
                    "accessPoint": {
                        "type": "string",
                        "description": "The ARN of the s3 access point to use with the external location (AWS).\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "credentialName": {
                        "type": "string",
                        "description": "Name of the databricks.StorageCredential to use with this external location.\n"
                    },
                    "encryptionDetails": {
                        "$ref": "#/types/databricks:index/ExternalLocationEncryptionDetails:ExternalLocationEncryptionDetails",
                        "description": "The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy external location regardless of its dependents.\n"
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "description": "Update external location regardless of its dependents.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of External Location, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the external location owner.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "description": "Indicates whether the external location is read-only.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the external location\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/file:File": {
            "description": "\n\n## Import\n\nThe resource `databricks_file` can be imported using the path of the file:\n\nbash\n\n```sh\n$ pulumi import databricks:index/file:File this \u003cpath\u003e\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Contents in base 64 format. Conflicts with `source`.\n"
                },
                "fileSize": {
                    "type": "integer",
                    "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                },
                "md5": {
                    "type": "string"
                },
                "modificationTime": {
                    "type": "string",
                    "description": "The last time stamp when the file was modified\n"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n"
                },
                "remoteFileModified": {
                    "type": "boolean"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "required": [
                "fileSize",
                "modificationTime",
                "path"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string",
                    "description": "Contents in base 64 format. Conflicts with `source`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "path": {
                    "type": "string",
                    "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n",
                    "willReplaceOnChanges": true
                },
                "remoteFileModified": {
                    "type": "boolean"
                },
                "source": {
                    "type": "string",
                    "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering File resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string",
                        "description": "Contents in base 64 format. Conflicts with `source`.\n"
                    },
                    "fileSize": {
                        "type": "integer",
                        "description": "The file size of the file that is being tracked by this resource in bytes.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "modificationTime": {
                        "type": "string",
                        "description": "The last time stamp when the file was modified\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The path of the file in which you wish to save. For example, `/Volumes/main/default/volume1/file.txt`.\n",
                        "willReplaceOnChanges": true
                    },
                    "remoteFileModified": {
                        "type": "boolean"
                    },
                    "source": {
                        "type": "string",
                        "description": "The full absolute path to the file. Conflicts with `content_base64`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/gitCredential:GitCredential": {
            "description": "\n\n## Import\n\nThe resource cluster can be imported using ID of Git credential that could be obtained via REST API:\n\nbash\n\n```sh\n$ pulumi import databricks:index/gitCredential:GitCredential this \u003cgit-credential-id\u003e\n```\n\n",
            "properties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, that has a non-empty value.\n"
                }
            },
            "required": [
                "gitProvider"
            ],
            "inputProperties": {
                "force": {
                    "type": "boolean",
                    "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "gitUsername": {
                    "type": "string",
                    "description": "user name at Git provider.\n"
                },
                "personalAccessToken": {
                    "type": "string",
                    "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, that has a non-empty value.\n"
                }
            },
            "requiredInputs": [
                "gitProvider"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GitCredential resources.\n",
                "properties": {
                    "force": {
                        "type": "boolean",
                        "description": "specify if settings need to be enforced - right now, Databricks allows only single Git credential, so if it's already configured, the apply operation will fail.\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Git Credentials API documentation](https://docs.databricks.com/dev-tools/api/latest/gitcredentials.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                    },
                    "gitUsername": {
                        "type": "string",
                        "description": "user name at Git provider.\n"
                    },
                    "personalAccessToken": {
                        "type": "string",
                        "description": "The personal access token used to authenticate to the corresponding Git provider. If value is not provided, it's sourced from the first environment variable of `GITHUB_TOKEN`, that has a non-empty value.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/globalInitScript:GlobalInitScript": {
            "description": "\n\n## Import\n\nThe resource global init script can be imported using script ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/globalInitScript:GlobalInitScript this script_id\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "required": [
                "name",
                "position"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "enabled": {
                    "type": "boolean",
                    "description": "specifies if the script is enabled for execution, or not\n"
                },
                "md5": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "the name of the script.  It should be unique\n"
                },
                "position": {
                    "type": "integer",
                    "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GlobalInitScript resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "enabled": {
                        "type": "boolean",
                        "description": "specifies if the script is enabled for execution, or not\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "the name of the script.  It should be unique\n"
                    },
                    "position": {
                        "type": "integer",
                        "description": "the position of a global init script, where `0` represents the first global init script to run, `1` is the second global init script to run, and so on. When omitted, the script gets the last position.\n"
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to script's source code on local filesystem. Conflicts with `content_base64`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grant:Grant": {
            "description": "\n\n## Import\n\nThe resource can be imported using combination of securable type (`table`, `catalog`, `foreign_connection`, ...), it's name and `principal`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/grant:Grant this catalog/abc/user_name\n```\n\n",
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "foreignConnection": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "metastore": {
                    "type": "string"
                },
                "model": {
                    "type": "string"
                },
                "pipeline": {
                    "type": "string"
                },
                "principal": {
                    "type": "string"
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "recipient": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "share": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "volume": {
                    "type": "string"
                }
            },
            "required": [
                "principal",
                "privileges"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "foreignConnection": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "model": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "pipeline": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "privileges": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "recipient": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "share": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "volume": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "principal",
                "privileges"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grant resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "foreignConnection": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "model": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "pipeline": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "privileges": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "recipient": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "share": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "volume": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/grants:Grants": {
            "description": "\n\n## Import\n\nThe resource can be imported using combination of securable type (`table`, `catalog`, `foreign_connection`, ...) and it's name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/grants:Grants this catalog/abc\n```\n\n",
            "properties": {
                "catalog": {
                    "type": "string"
                },
                "externalLocation": {
                    "type": "string"
                },
                "foreignConnection": {
                    "type": "string"
                },
                "function": {
                    "type": "string"
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string"
                },
                "model": {
                    "type": "string"
                },
                "pipeline": {
                    "type": "string"
                },
                "recipient": {
                    "type": "string"
                },
                "schema": {
                    "type": "string"
                },
                "share": {
                    "type": "string"
                },
                "storageCredential": {
                    "type": "string"
                },
                "table": {
                    "type": "string"
                },
                "volume": {
                    "type": "string"
                }
            },
            "required": [
                "grants"
            ],
            "inputProperties": {
                "catalog": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalLocation": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "foreignConnection": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "function": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "grants": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                    },
                    "language": {
                        "csharp": {
                            "name": "GrantDetails"
                        }
                    }
                },
                "metastore": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "model": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "pipeline": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "recipient": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "schema": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "share": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredential": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "table": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "volume": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "grants"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Grants resources.\n",
                "properties": {
                    "catalog": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalLocation": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "foreignConnection": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "function": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "grants": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/GrantsGrant:GrantsGrant"
                        },
                        "language": {
                            "csharp": {
                                "name": "GrantDetails"
                            }
                        }
                    },
                    "metastore": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "model": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "pipeline": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "recipient": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "schema": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "share": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredential": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "table": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "volume": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/group:Group": {
            "description": "This resource allows you to manage both [account groups and workspace-local groups](https://docs.databricks.com/administration-guide/users-groups/groups.html). You can use the databricks.GroupMember resource to assign Databricks users, service principals as well as other groups as members of the group. This is useful if you are using an application to sync users \u0026 groups with SCIM API.\n\n\u003e **Note** To assign an account level group to a workspace use databricks_mws_permission_assignment.\n\n\u003e **Note** Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level groups.  Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level groups.\n\nTo create account groups in the Databricks account, the provider must be configured accordingly. On AWS deployment with `host = \"https://accounts.cloud.databricks.com\"` and `account_id = \"00000000-0000-0000-0000-000000000000\"`. On Azure deployments `host = \"https://accounts.azuredatabricks.net\"`, `account_id = \"00000000-0000-0000-0000-000000000000\"` and using AAD tokens as authentication.\n\nRecommended to use along with Identity Provider SCIM provisioning to populate users into those groups:\n\n* [Azure Active Directory](https://docs.microsoft.com/en-us/azure/databricks/administration-guide/users-groups/scim/aad)\n* [Okta](https://docs.databricks.com/administration-guide/users-groups/scim/okta.html)\n* [OneLogin](https://docs.databricks.com/administration-guide/users-groups/scim/onelogin.html)\n\n## Example Usage\n\nCreating some group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {\n    displayName: \"Some Group\",\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\",\n    display_name=\"Some Group\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName:             pulumi.String(\"Some Group\"),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()        \n            .displayName(\"Some Group\")\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAdding databricks.User as databricks.GroupMember of some group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {\n    displayName: \"Some Group\",\n    allowClusterCreate: true,\n    allowInstancePoolCreate: true,\n});\nconst thisUser = new databricks.User(\"this\", {userName: \"someone@example.com\"});\nconst vipMember = new databricks.GroupMember(\"vip_member\", {\n    groupId: _this.id,\n    memberId: thisUser.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\",\n    display_name=\"Some Group\",\n    allow_cluster_create=True,\n    allow_instance_pool_create=True)\nthis_user = databricks.User(\"this\", user_name=\"someone@example.com\")\nvip_member = databricks.GroupMember(\"vip_member\",\n    group_id=this.id,\n    member_id=this_user.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n        AllowClusterCreate = true,\n        AllowInstancePoolCreate = true,\n    });\n\n    var thisUser = new Databricks.User(\"this\", new()\n    {\n        UserName = \"someone@example.com\",\n    });\n\n    var vipMember = new Databricks.GroupMember(\"vip_member\", new()\n    {\n        GroupId = @this.Id,\n        MemberId = thisUser.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName:             pulumi.String(\"Some Group\"),\n\t\t\tAllowClusterCreate:      pulumi.Bool(true),\n\t\t\tAllowInstancePoolCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisUser, err := databricks.NewUser(ctx, \"this\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"someone@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"vip_member\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  this.ID(),\n\t\t\tMemberId: thisUser.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()        \n            .displayName(\"Some Group\")\n            .allowClusterCreate(true)\n            .allowInstancePoolCreate(true)\n            .build());\n\n        var thisUser = new User(\"thisUser\", UserArgs.builder()        \n            .userName(\"someone@example.com\")\n            .build());\n\n        var vipMember = new GroupMember(\"vipMember\", GroupMemberArgs.builder()        \n            .groupId(this_.id())\n            .memberId(thisUser.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n      allowClusterCreate: true\n      allowInstancePoolCreate: true\n  thisUser:\n    type: databricks:User\n    name: this\n    properties:\n      userName: someone@example.com\n  vipMember:\n    type: databricks:GroupMember\n    name: vip_member\n    properties:\n      groupId: ${this.id}\n      memberId: ${thisUser.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating group in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {displayName: \"Some Group\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\", display_name=\"Some Group\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Some Group\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()        \n            .displayName(\"Some Group\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating group in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Group(\"this\", {displayName: \"Some Group\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Group(\"this\", display_name=\"Some Group\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Group(\"this\", new()\n    {\n        DisplayName = \"Some Group\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewGroup(ctx, \"this\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Some Group\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Group(\"this\", GroupArgs.builder()        \n            .displayName(\"Some Group\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Group\n    properties:\n      displayName: Some Group\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nYou can import a `databricks_group` resource with the name `my_group` like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/group:Group my_group \u003cgroup_id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "aclPrincipalId",
                "displayName",
                "url"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is the display name for the given group.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the group in an external identity provider.\n",
                    "willReplaceOnChanges": true
                },
                "force": {
                    "type": "boolean"
                },
                "url": {
                    "type": "string"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Group resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have cluster create privileges. More fine grained permissions could be assigned with databricks.Permissions and cluster_id argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have instance pool create privileges. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is the display name for the given group.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n",
                        "willReplaceOnChanges": true
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "url": {
                        "type": "string"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupInstanceProfile:GroupInstanceProfile": {
            "description": "\u003e **Deprecated** Please migrate to databricks_group_role.\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_group.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupInstanceProfile = new databricks.GroupInstanceProfile(\"my_group_instance_profile\", {\n    groupId: myGroup.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_instance_profile = databricks.GroupInstanceProfile(\"my_group_instance_profile\",\n    group_id=my_group.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupInstanceProfile = new Databricks.GroupInstanceProfile(\"my_group_instance_profile\", new()\n    {\n        GroupId = myGroup.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"my_group_instance_profile\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           myGroup.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()        \n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupInstanceProfile = new GroupInstanceProfile(\"myGroupInstanceProfile\", GroupInstanceProfileArgs.builder()        \n            .groupId(myGroup.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupInstanceProfile:\n    type: databricks:GroupInstanceProfile\n    name: my_group_instance_profile\n    properties:\n      groupId: ${myGroup.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "instanceProfileId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "instanceProfileId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupInstanceProfile resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupMember:GroupMember": {
            "description": "This resource allows you to attach users, service_principal, and groups as group members.\n\nTo attach members to groups in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments\n\n## Example Usage\n\nAfter the following example, Bradley would have direct membership in group B and transitive membership in group A.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst a = new databricks.Group(\"a\", {displayName: \"A\"});\nconst b = new databricks.Group(\"b\", {displayName: \"B\"});\nconst ab = new databricks.GroupMember(\"ab\", {\n    groupId: a.id,\n    memberId: b.id,\n});\nconst bradley = new databricks.User(\"bradley\", {userName: \"bradley@example.com\"});\nconst bb = new databricks.GroupMember(\"bb\", {\n    groupId: b.id,\n    memberId: bradley.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\na = databricks.Group(\"a\", display_name=\"A\")\nb = databricks.Group(\"b\", display_name=\"B\")\nab = databricks.GroupMember(\"ab\",\n    group_id=a.id,\n    member_id=b.id)\nbradley = databricks.User(\"bradley\", user_name=\"bradley@example.com\")\nbb = databricks.GroupMember(\"bb\",\n    group_id=b.id,\n    member_id=bradley.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var a = new Databricks.Group(\"a\", new()\n    {\n        DisplayName = \"A\",\n    });\n\n    var b = new Databricks.Group(\"b\", new()\n    {\n        DisplayName = \"B\",\n    });\n\n    var ab = new Databricks.GroupMember(\"ab\", new()\n    {\n        GroupId = a.Id,\n        MemberId = b.Id,\n    });\n\n    var bradley = new Databricks.User(\"bradley\", new()\n    {\n        UserName = \"bradley@example.com\",\n    });\n\n    var bb = new Databricks.GroupMember(\"bb\", new()\n    {\n        GroupId = b.Id,\n        MemberId = bradley.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ta, err := databricks.NewGroup(ctx, \"a\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"A\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tb, err := databricks.NewGroup(ctx, \"b\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"B\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"ab\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  a.ID(),\n\t\t\tMemberId: b.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tbradley, err := databricks.NewUser(ctx, \"bradley\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"bradley@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"bb\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  b.ID(),\n\t\t\tMemberId: bradley.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var a = new Group(\"a\", GroupArgs.builder()        \n            .displayName(\"A\")\n            .build());\n\n        var b = new Group(\"b\", GroupArgs.builder()        \n            .displayName(\"B\")\n            .build());\n\n        var ab = new GroupMember(\"ab\", GroupMemberArgs.builder()        \n            .groupId(a.id())\n            .memberId(b.id())\n            .build());\n\n        var bradley = new User(\"bradley\", UserArgs.builder()        \n            .userName(\"bradley@example.com\")\n            .build());\n\n        var bb = new GroupMember(\"bb\", GroupMemberArgs.builder()        \n            .groupId(b.id())\n            .memberId(bradley.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  a:\n    type: databricks:Group\n    properties:\n      displayName: A\n  b:\n    type: databricks:Group\n    properties:\n      displayName: B\n  ab:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${a.id}\n      memberId: ${b.id}\n  bradley:\n    type: databricks:User\n    properties:\n      userName: bradley@example.com\n  bb:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${b.id}\n      memberId: ${bradley.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.IpAccessList to allow access from [predefined IP ranges](https://docs.databricks.com/security/network/ip-access-list.html).\n* databricks.ServicePrincipal to grant access to a workspace to an automation tool or application.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\nYou can import a `databricks_group_member` resource with name `my_group_member` like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/groupMember:GroupMember my_group_member \"\u003cgroup_id\u003e|\u003cmember_id\u003e\"\n```\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the group, service principal, or user.\n"
                }
            },
            "required": [
                "groupId",
                "memberId"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "memberId": {
                    "type": "string",
                    "description": "This is the id of the group, service principal, or user.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "memberId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupMember resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "memberId": {
                        "type": "string",
                        "description": "This is the id of the group, service principal, or user.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/groupRole:GroupRole": {
            "description": "This resource allows you to attach a role to databricks_group. This role could be a pre-defined role such as account admin, or an instance profile ARN.\n\n## Example Usage\n\nAttach an instance profile to a group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupInstanceProfile = new databricks.GroupRole(\"my_group_instance_profile\", {\n    groupId: myGroup.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_instance_profile = databricks.GroupRole(\"my_group_instance_profile\",\n    group_id=my_group.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupInstanceProfile = new Databricks.GroupRole(\"my_group_instance_profile\", new()\n    {\n        GroupId = myGroup.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupRole(ctx, \"my_group_instance_profile\", \u0026databricks.GroupRoleArgs{\n\t\t\tGroupId: myGroup.ID(),\n\t\t\tRole:    instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupRole;\nimport com.pulumi.databricks.GroupRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()        \n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupInstanceProfile = new GroupRole(\"myGroupInstanceProfile\", GroupRoleArgs.builder()        \n            .groupId(myGroup.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupInstanceProfile:\n    type: databricks:GroupRole\n    name: my_group_instance_profile\n    properties:\n      groupId: ${myGroup.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAttach account admin role to an account-level group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myGroup = new databricks.Group(\"my_group\", {displayName: \"my_group_name\"});\nconst myGroupAccountAdmin = new databricks.GroupRole(\"my_group_account_admin\", {\n    groupId: myGroup.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_group = databricks.Group(\"my_group\", display_name=\"my_group_name\")\nmy_group_account_admin = databricks.GroupRole(\"my_group_account_admin\",\n    group_id=my_group.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myGroup = new Databricks.Group(\"my_group\", new()\n    {\n        DisplayName = \"my_group_name\",\n    });\n\n    var myGroupAccountAdmin = new Databricks.GroupRole(\"my_group_account_admin\", new()\n    {\n        GroupId = myGroup.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyGroup, err := databricks.NewGroup(ctx, \"my_group\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"my_group_name\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupRole(ctx, \"my_group_account_admin\", \u0026databricks.GroupRoleArgs{\n\t\t\tGroupId: myGroup.ID(),\n\t\t\tRole:    pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.GroupRole;\nimport com.pulumi.databricks.GroupRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myGroup = new Group(\"myGroup\", GroupArgs.builder()        \n            .displayName(\"my_group_name\")\n            .build());\n\n        var myGroupAccountAdmin = new GroupRole(\"myGroupAccountAdmin\", GroupRoleArgs.builder()        \n            .groupId(myGroup.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myGroup:\n    type: databricks:Group\n    name: my_group\n    properties:\n      displayName: my_group_name\n  myGroupAccountAdmin:\n    type: databricks:GroupRole\n    name: my_group_account_admin\n    properties:\n      groupId: ${myGroup.id}\n      role: account_admin\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n"
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                }
            },
            "required": [
                "groupId",
                "role"
            ],
            "inputProperties": {
                "groupId": {
                    "type": "string",
                    "description": "This is the id of the group resource.\n",
                    "willReplaceOnChanges": true
                },
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "groupId",
                "role"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering GroupRole resources.\n",
                "properties": {
                    "groupId": {
                        "type": "string",
                        "description": "This is the id of the group resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instancePool:InstancePool": {
            "description": "This resource allows you to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances. An instance pool reduces cluster start and auto-scaling times by maintaining a set of idle, ready-to-use cloud instances. When a cluster attached to a pool needs an instance, it first attempts to allocate one of the pool’s idle instances. If the pool has no idle instances, it expands by allocating a new instance from the instance provider in order to accommodate the cluster’s request. When a cluster releases an instance, it returns to the pool and is free for another cluster to use. Only clusters attached to a pool can use that pool’s idle instances.\n\n\u003e **Note** It is important to know that different cloud service providers have different `node_type_id`, `disk_specs` and potentially other configurations.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst smallest = databricks.getNodeType({});\nconst smallestNodes = new databricks.InstancePool(\"smallest_nodes\", {\n    instancePoolName: \"Smallest Nodes\",\n    minIdleInstances: 0,\n    maxCapacity: 300,\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    awsAttributes: {\n        availability: \"ON_DEMAND\",\n        zoneId: \"us-east-1a\",\n        spotBidPricePercent: 100,\n    },\n    idleInstanceAutoterminationMinutes: 10,\n    diskSpec: {\n        diskType: {\n            ebsVolumeType: \"GENERAL_PURPOSE_SSD\",\n        },\n        diskSize: 80,\n        diskCount: 1,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsmallest = databricks.get_node_type()\nsmallest_nodes = databricks.InstancePool(\"smallest_nodes\",\n    instance_pool_name=\"Smallest Nodes\",\n    min_idle_instances=0,\n    max_capacity=300,\n    node_type_id=smallest.id,\n    aws_attributes=databricks.InstancePoolAwsAttributesArgs(\n        availability=\"ON_DEMAND\",\n        zone_id=\"us-east-1a\",\n        spot_bid_price_percent=100,\n    ),\n    idle_instance_autotermination_minutes=10,\n    disk_spec=databricks.InstancePoolDiskSpecArgs(\n        disk_type=databricks.InstancePoolDiskSpecDiskTypeArgs(\n            ebs_volume_type=\"GENERAL_PURPOSE_SSD\",\n        ),\n        disk_size=80,\n        disk_count=1,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var smallest = Databricks.GetNodeType.Invoke();\n\n    var smallestNodes = new Databricks.InstancePool(\"smallest_nodes\", new()\n    {\n        InstancePoolName = \"Smallest Nodes\",\n        MinIdleInstances = 0,\n        MaxCapacity = 300,\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AwsAttributes = new Databricks.Inputs.InstancePoolAwsAttributesArgs\n        {\n            Availability = \"ON_DEMAND\",\n            ZoneId = \"us-east-1a\",\n            SpotBidPricePercent = 100,\n        },\n        IdleInstanceAutoterminationMinutes = 10,\n        DiskSpec = new Databricks.Inputs.InstancePoolDiskSpecArgs\n        {\n            DiskType = new Databricks.Inputs.InstancePoolDiskSpecDiskTypeArgs\n            {\n                EbsVolumeType = \"GENERAL_PURPOSE_SSD\",\n            },\n            DiskSize = 80,\n            DiskCount = 1,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsmallest, err := databricks.GetNodeType(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstancePool(ctx, \"smallest_nodes\", \u0026databricks.InstancePoolArgs{\n\t\t\tInstancePoolName: pulumi.String(\"Smallest Nodes\"),\n\t\t\tMinIdleInstances: pulumi.Int(0),\n\t\t\tMaxCapacity:      pulumi.Int(300),\n\t\t\tNodeTypeId:       pulumi.String(smallest.Id),\n\t\t\tAwsAttributes: \u0026databricks.InstancePoolAwsAttributesArgs{\n\t\t\t\tAvailability:        pulumi.String(\"ON_DEMAND\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1a\"),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t\tIdleInstanceAutoterminationMinutes: pulumi.Int(10),\n\t\t\tDiskSpec: \u0026databricks.InstancePoolDiskSpecArgs{\n\t\t\t\tDiskType: \u0026databricks.InstancePoolDiskSpecDiskTypeArgs{\n\t\t\t\t\tEbsVolumeType: pulumi.String(\"GENERAL_PURPOSE_SSD\"),\n\t\t\t\t},\n\t\t\t\tDiskSize:  pulumi.Int(80),\n\t\t\t\tDiskCount: pulumi.Int(1),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.InstancePool;\nimport com.pulumi.databricks.InstancePoolArgs;\nimport com.pulumi.databricks.inputs.InstancePoolAwsAttributesArgs;\nimport com.pulumi.databricks.inputs.InstancePoolDiskSpecArgs;\nimport com.pulumi.databricks.inputs.InstancePoolDiskSpecDiskTypeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var smallest = DatabricksFunctions.getNodeType();\n\n        var smallestNodes = new InstancePool(\"smallestNodes\", InstancePoolArgs.builder()        \n            .instancePoolName(\"Smallest Nodes\")\n            .minIdleInstances(0)\n            .maxCapacity(300)\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .awsAttributes(InstancePoolAwsAttributesArgs.builder()\n                .availability(\"ON_DEMAND\")\n                .zoneId(\"us-east-1a\")\n                .spotBidPricePercent(\"100\")\n                .build())\n            .idleInstanceAutoterminationMinutes(10)\n            .diskSpec(InstancePoolDiskSpecArgs.builder()\n                .diskType(InstancePoolDiskSpecDiskTypeArgs.builder()\n                    .ebsVolumeType(\"GENERAL_PURPOSE_SSD\")\n                    .build())\n                .diskSize(80)\n                .diskCount(1)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  smallestNodes:\n    type: databricks:InstancePool\n    name: smallest_nodes\n    properties:\n      instancePoolName: Smallest Nodes\n      minIdleInstances: 0\n      maxCapacity: 300\n      nodeTypeId: ${smallest.id}\n      awsAttributes:\n        availability: ON_DEMAND\n        zoneId: us-east-1a\n        spotBidPricePercent: '100'\n      idleInstanceAutoterminationMinutes: 10\n      diskSpec:\n        diskType:\n          ebsVolumeType: GENERAL_PURPOSE_SSD\n        diskSize: 80\n        diskCount: 1\nvariables:\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Group and databricks.User can control which groups or individual users can create instance pools.\n* databricks.Permissions can control which groups or individual users can *Manage* or *Attach to* individual instance pools.\n\n## Import\n\nThe resource instance pool can be imported using it's id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/instancePool:InstancePool this \u003cinstance-pool-id\u003e\n```\n\n",
            "properties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes"
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec"
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n"
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes"
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n"
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    }
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n"
                }
            },
            "required": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolId",
                "instancePoolName"
            ],
            "inputProperties": {
                "awsAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                    "willReplaceOnChanges": true
                },
                "azureAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                    "willReplaceOnChanges": true
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                },
                "diskSpec": {
                    "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                    "willReplaceOnChanges": true
                },
                "enableElasticDisk": {
                    "type": "boolean",
                    "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                    "willReplaceOnChanges": true
                },
                "gcpAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                    "willReplaceOnChanges": true
                },
                "idleInstanceAutoterminationMinutes": {
                    "type": "integer",
                    "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                },
                "instancePoolFleetAttributes": {
                    "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string"
                },
                "instancePoolName": {
                    "type": "string",
                    "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                },
                "maxCapacity": {
                    "type": "integer",
                    "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                },
                "minIdleInstances": {
                    "type": "integer",
                    "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                },
                "nodeTypeId": {
                    "type": "string",
                    "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                    "willReplaceOnChanges": true
                },
                "preloadedDockerImages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                    },
                    "willReplaceOnChanges": true
                },
                "preloadedSparkVersions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "idleInstanceAutoterminationMinutes",
                "instancePoolName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstancePool resources.\n",
                "properties": {
                    "awsAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAwsAttributes:InstancePoolAwsAttributes",
                        "willReplaceOnChanges": true
                    },
                    "azureAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolAzureAttributes:InstancePoolAzureAttributes",
                        "willReplaceOnChanges": true
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "(Map) Additional tags for instance pool resources. Databricks tags all pool resources (e.g. AWS \u0026 Azure instances and Disk volumes). The tags of the instance pool will propagate to the clusters using the pool (see the [official documentation](https://docs.databricks.com/administration-guide/account-settings/usage-detail-tags-aws.html#tag-propagation)). Attempting to set the same tags in both cluster and instance pool will raise an error. *Databricks allows at most 43 custom tags.*\n"
                    },
                    "diskSpec": {
                        "$ref": "#/types/databricks:index/InstancePoolDiskSpec:InstancePoolDiskSpec",
                        "willReplaceOnChanges": true
                    },
                    "enableElasticDisk": {
                        "type": "boolean",
                        "description": "(Bool) Autoscaling Local Storage: when enabled, the instances in the pool dynamically acquire additional disk space when they are running low on disk space.\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolGcpAttributes:InstancePoolGcpAttributes",
                        "willReplaceOnChanges": true
                    },
                    "idleInstanceAutoterminationMinutes": {
                        "type": "integer",
                        "description": "(Integer) The number of minutes that idle instances in excess of the min_idle_instances are maintained by the pool before being terminated. If not specified, excess idle instances are terminated automatically after a default timeout period. If specified, the time must be between 0 and 10000 minutes. If you specify 0, excess idle instances are removed as soon as possible.\n"
                    },
                    "instancePoolFleetAttributes": {
                        "$ref": "#/types/databricks:index/InstancePoolInstancePoolFleetAttributes:InstancePoolInstancePoolFleetAttributes",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string"
                    },
                    "instancePoolName": {
                        "type": "string",
                        "description": "(String) The name of the instance pool. This is required for create and edit operations. It must be unique, non-empty, and less than 100 characters.\n"
                    },
                    "maxCapacity": {
                        "type": "integer",
                        "description": "(Integer) The maximum number of instances the pool can contain, including both idle instances and ones in use by clusters. Once the maximum capacity is reached, you cannot create new clusters from the pool and existing clusters cannot autoscale up until some instances are made idle in the pool via cluster termination or down-scaling. There is no default limit, but as a [best practice](https://docs.databricks.com/clusters/instance-pools/pool-best-practices.html#configure-pools-to-control-cost), this should be set based on anticipated usage.\n"
                    },
                    "minIdleInstances": {
                        "type": "integer",
                        "description": "(Integer) The minimum number of idle instances maintained by the pool. This is in addition to any instances in use by active clusters.\n"
                    },
                    "nodeTypeId": {
                        "type": "string",
                        "description": "(String) The node type for the instances in the pool. All clusters attached to the pool inherit this node type and the pool’s idle instances are allocated based on this type. You can retrieve a list of available node types by using the [List Node Types API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistnodetypes) call.\n",
                        "willReplaceOnChanges": true
                    },
                    "preloadedDockerImages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/InstancePoolPreloadedDockerImage:InstancePoolPreloadedDockerImage"
                        },
                        "willReplaceOnChanges": true
                    },
                    "preloadedSparkVersions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "(List) A list with at most one runtime version the pool installs on each instance. Pool clusters that use a preloaded runtime version start faster as they do not have to wait for the image to download. You can retrieve them via databricks.getSparkVersion data source or via  [Runtime Versions API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistsparkversions) call.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/instanceProfile:InstanceProfile": {
            "description": "This resource allows you to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount. The following example demonstrates how to create an instance profile and create a cluster with it. When creating a new `databricks.InstanceProfile`, Databricks validates that it has sufficient permissions to launch instances with the instance profile. This validation uses AWS dry-run mode for the [AWS EC2 RunInstances API](https://docs.aws.amazon.com/AWSEC2/latest/APIReference/API_RunInstances.html).\n\n\u003e **Note** Please switch to databricks.StorageCredential with Unity Catalog to manage storage credentials, which provides a better and faster way for managing credential security.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Role that you've specified on https://accounts.cloud.databricks.com/#aws\nconst crossaccountRoleName = config.require(\"crossaccountRoleName\");\nconst assumeRoleForEc2 = aws.iam.getPolicyDocument({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            identifiers: [\"ec2.amazonaws.com\"],\n            type: \"Service\",\n        }],\n    }],\n});\nconst roleForS3Access = new aws.iam.Role(\"role_for_s3_access\", {\n    name: \"shared-ec2-role-for-s3\",\n    description: \"Role for shared access\",\n    assumeRolePolicy: assumeRoleForEc2.then(assumeRoleForEc2 =\u003e assumeRoleForEc2.json),\n});\nconst passRoleForS3Access = aws.iam.getPolicyDocumentOutput({\n    statements: [{\n        effect: \"Allow\",\n        actions: [\"iam:PassRole\"],\n        resources: [roleForS3Access.arn],\n    }],\n});\nconst passRoleForS3AccessPolicy = new aws.iam.Policy(\"pass_role_for_s3_access\", {\n    name: \"shared-pass-role-for-s3-access\",\n    path: \"/\",\n    policy: passRoleForS3Access.apply(passRoleForS3Access =\u003e passRoleForS3Access.json),\n});\nconst crossAccount = new aws.iam.RolePolicyAttachment(\"cross_account\", {\n    policyArn: passRoleForS3AccessPolicy.arn,\n    role: crossaccountRoleName,\n});\nconst shared = new aws.iam.InstanceProfile(\"shared\", {\n    name: \"shared-instance-profile\",\n    role: roleForS3Access.name,\n});\nconst sharedInstanceProfile = new databricks.InstanceProfile(\"shared\", {instanceProfileArn: shared.arn});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Cluster(\"this\", {\n    clusterName: \"Shared Autoscaling\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n    awsAttributes: {\n        instanceProfileArn: sharedInstanceProfile.id,\n        availability: \"SPOT\",\n        zoneId: \"us-east-1\",\n        firstOnDemand: 1,\n        spotBidPricePercent: 100,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Role that you've specified on https://accounts.cloud.databricks.com/#aws\ncrossaccount_role_name = config.require(\"crossaccountRoleName\")\nassume_role_for_ec2 = aws.iam.get_policy_document(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"sts:AssumeRole\"],\n    principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n        identifiers=[\"ec2.amazonaws.com\"],\n        type=\"Service\",\n    )],\n)])\nrole_for_s3_access = aws.iam.Role(\"role_for_s3_access\",\n    name=\"shared-ec2-role-for-s3\",\n    description=\"Role for shared access\",\n    assume_role_policy=assume_role_for_ec2.json)\npass_role_for_s3_access = aws.iam.get_policy_document_output(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    effect=\"Allow\",\n    actions=[\"iam:PassRole\"],\n    resources=[role_for_s3_access.arn],\n)])\npass_role_for_s3_access_policy = aws.iam.Policy(\"pass_role_for_s3_access\",\n    name=\"shared-pass-role-for-s3-access\",\n    path=\"/\",\n    policy=pass_role_for_s3_access.json)\ncross_account = aws.iam.RolePolicyAttachment(\"cross_account\",\n    policy_arn=pass_role_for_s3_access_policy.arn,\n    role=crossaccount_role_name)\nshared = aws.iam.InstanceProfile(\"shared\",\n    name=\"shared-instance-profile\",\n    role=role_for_s3_access.name)\nshared_instance_profile = databricks.InstanceProfile(\"shared\", instance_profile_arn=shared.arn)\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Cluster(\"this\",\n    cluster_name=\"Shared Autoscaling\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ),\n    aws_attributes=databricks.ClusterAwsAttributesArgs(\n        instance_profile_arn=shared_instance_profile.id,\n        availability=\"SPOT\",\n        zone_id=\"us-east-1\",\n        first_on_demand=1,\n        spot_bid_price_percent=100,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Role that you've specified on https://accounts.cloud.databricks.com/#aws\n    var crossaccountRoleName = config.Require(\"crossaccountRoleName\");\n    var assumeRoleForEc2 = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Identifiers = new[]\n                        {\n                            \"ec2.amazonaws.com\",\n                        },\n                        Type = \"Service\",\n                    },\n                },\n            },\n        },\n    });\n\n    var roleForS3Access = new Aws.Iam.Role(\"role_for_s3_access\", new()\n    {\n        Name = \"shared-ec2-role-for-s3\",\n        Description = \"Role for shared access\",\n        AssumeRolePolicy = assumeRoleForEc2.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var passRoleForS3Access = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"iam:PassRole\",\n                },\n                Resources = new[]\n                {\n                    roleForS3Access.Arn,\n                },\n            },\n        },\n    });\n\n    var passRoleForS3AccessPolicy = new Aws.Iam.Policy(\"pass_role_for_s3_access\", new()\n    {\n        Name = \"shared-pass-role-for-s3-access\",\n        Path = \"/\",\n        PolicyDocument = passRoleForS3Access.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var crossAccount = new Aws.Iam.RolePolicyAttachment(\"cross_account\", new()\n    {\n        PolicyArn = passRoleForS3AccessPolicy.Arn,\n        Role = crossaccountRoleName,\n    });\n\n    var shared = new Aws.Iam.InstanceProfile(\"shared\", new()\n    {\n        Name = \"shared-instance-profile\",\n        Role = roleForS3Access.Name,\n    });\n\n    var sharedInstanceProfile = new Databricks.InstanceProfile(\"shared\", new()\n    {\n        InstanceProfileArn = shared.Arn,\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        ClusterName = \"Shared Autoscaling\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n        AwsAttributes = new Databricks.Inputs.ClusterAwsAttributesArgs\n        {\n            InstanceProfileArn = sharedInstanceProfile.Id,\n            Availability = \"SPOT\",\n            ZoneId = \"us-east-1\",\n            FirstOnDemand = 1,\n            SpotBidPricePercent = 100,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Role that you've specified on https://accounts.cloud.databricks.com/#aws\n\t\tcrossaccountRoleName := cfg.Require(\"crossaccountRoleName\")\n\t\tassumeRoleForEc2, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tEffect: pulumi.StringRef(\"Allow\"),\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"ec2.amazonaws.com\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t\tType: \"Service\",\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\troleForS3Access, err := iam.NewRole(ctx, \"role_for_s3_access\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(\"shared-ec2-role-for-s3\"),\n\t\t\tDescription:      pulumi.String(\"Role for shared access\"),\n\t\t\tAssumeRolePolicy: pulumi.String(assumeRoleForEc2.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpassRoleForS3Access := iam.GetPolicyDocumentOutput(ctx, iam.GetPolicyDocumentOutputArgs{\n\t\t\tStatements: iam.GetPolicyDocumentStatementArray{\n\t\t\t\t\u0026iam.GetPolicyDocumentStatementArgs{\n\t\t\t\t\tEffect: pulumi.String(\"Allow\"),\n\t\t\t\t\tActions: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"iam:PassRole\"),\n\t\t\t\t\t},\n\t\t\t\t\tResources: pulumi.StringArray{\n\t\t\t\t\t\troleForS3Access.Arn,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tpassRoleForS3AccessPolicy, err := iam.NewPolicy(ctx, \"pass_role_for_s3_access\", \u0026iam.PolicyArgs{\n\t\t\tName: pulumi.String(\"shared-pass-role-for-s3-access\"),\n\t\t\tPath: pulumi.String(\"/\"),\n\t\t\tPolicy: passRoleForS3Access.ApplyT(func(passRoleForS3Access iam.GetPolicyDocumentResult) (*string, error) {\n\t\t\t\treturn \u0026passRoleForS3Access.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"cross_account\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: passRoleForS3AccessPolicy.Arn,\n\t\t\tRole:      pulumi.String(crossaccountRoleName),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tshared, err := iam.NewInstanceProfile(ctx, \"shared\", \u0026iam.InstanceProfileArgs{\n\t\t\tName: pulumi.String(\"shared-instance-profile\"),\n\t\t\tRole: roleForS3Access.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedInstanceProfile, err := databricks.NewInstanceProfile(ctx, \"shared\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: shared.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Autoscaling\"),\n\t\t\tSparkVersion:           pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t\tAwsAttributes: \u0026databricks.ClusterAwsAttributesArgs{\n\t\t\t\tInstanceProfileArn:  sharedInstanceProfile.ID(),\n\t\t\t\tAvailability:        pulumi.String(\"SPOT\"),\n\t\t\t\tZoneId:              pulumi.String(\"us-east-1\"),\n\t\t\t\tFirstOnDemand:       pulumi.Int(1),\n\t\t\t\tSpotBidPricePercent: pulumi.Int(100),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.aws.iam.InstanceProfile;\nimport com.pulumi.aws.iam.InstanceProfileArgs;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport com.pulumi.databricks.inputs.ClusterAwsAttributesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var crossaccountRoleName = config.get(\"crossaccountRoleName\");\n        final var assumeRoleForEc2 = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .identifiers(\"ec2.amazonaws.com\")\n                    .type(\"Service\")\n                    .build())\n                .build())\n            .build());\n\n        var roleForS3Access = new Role(\"roleForS3Access\", RoleArgs.builder()        \n            .name(\"shared-ec2-role-for-s3\")\n            .description(\"Role for shared access\")\n            .assumeRolePolicy(assumeRoleForEc2.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        final var passRoleForS3Access = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .effect(\"Allow\")\n                .actions(\"iam:PassRole\")\n                .resources(roleForS3Access.arn())\n                .build())\n            .build());\n\n        var passRoleForS3AccessPolicy = new Policy(\"passRoleForS3AccessPolicy\", PolicyArgs.builder()        \n            .name(\"shared-pass-role-for-s3-access\")\n            .path(\"/\")\n            .policy(passRoleForS3Access.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult).applyValue(passRoleForS3Access -\u003e passRoleForS3Access.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json())))\n            .build());\n\n        var crossAccount = new RolePolicyAttachment(\"crossAccount\", RolePolicyAttachmentArgs.builder()        \n            .policyArn(passRoleForS3AccessPolicy.arn())\n            .role(crossaccountRoleName)\n            .build());\n\n        var shared = new InstanceProfile(\"shared\", InstanceProfileArgs.builder()        \n            .name(\"shared-instance-profile\")\n            .role(roleForS3Access.name())\n            .build());\n\n        var sharedInstanceProfile = new InstanceProfile(\"sharedInstanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(shared.arn())\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion();\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()        \n            .clusterName(\"Shared Autoscaling\")\n            .sparkVersion(latest.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .awsAttributes(ClusterAwsAttributesArgs.builder()\n                .instanceProfileArn(sharedInstanceProfile.id())\n                .availability(\"SPOT\")\n                .zoneId(\"us-east-1\")\n                .firstOnDemand(1)\n                .spotBidPricePercent(100)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  crossaccountRoleName:\n    type: string\nresources:\n  roleForS3Access:\n    type: aws:iam:Role\n    name: role_for_s3_access\n    properties:\n      name: shared-ec2-role-for-s3\n      description: Role for shared access\n      assumeRolePolicy: ${assumeRoleForEc2.json}\n  passRoleForS3AccessPolicy:\n    type: aws:iam:Policy\n    name: pass_role_for_s3_access\n    properties:\n      name: shared-pass-role-for-s3-access\n      path: /\n      policy: ${passRoleForS3Access.json}\n  crossAccount:\n    type: aws:iam:RolePolicyAttachment\n    name: cross_account\n    properties:\n      policyArn: ${passRoleForS3AccessPolicy.arn}\n      role: ${crossaccountRoleName}\n  shared:\n    type: aws:iam:InstanceProfile\n    properties:\n      name: shared-instance-profile\n      role: ${roleForS3Access.name}\n  sharedInstanceProfile:\n    type: databricks:InstanceProfile\n    name: shared\n    properties:\n      instanceProfileArn: ${shared.arn}\n  this:\n    type: databricks:Cluster\n    properties:\n      clusterName: Shared Autoscaling\n      sparkVersion: ${latest.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\n      awsAttributes:\n        instanceProfileArn: ${sharedInstanceProfile.id}\n        availability: SPOT\n        zoneId: us-east-1\n        firstOnDemand: 1\n        spotBidPricePercent: 100\nvariables:\n  assumeRoleForEc2:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        statements:\n          - effect: Allow\n            actions:\n              - sts:AssumeRole\n            principals:\n              - identifiers:\n                  - ec2.amazonaws.com\n                type: Service\n  passRoleForS3Access:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        statements:\n          - effect: Allow\n            actions:\n              - iam:PassRole\n            resources:\n              - ${roleForS3Access.arn}\n  latest:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments: {}\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Usage with Cluster Policies\n\nIt is advised to keep all common configurations in Cluster Policies to maintain control of the environments launched, so `databricks.Cluster` above could be replaced with `databricks.ClusterPolicy`:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ClusterPolicy(\"this\", {\n    name: \"Policy with predefined instance profile\",\n    definition: JSON.stringify({\n        \"aws_attributes.instance_profile_arn\": {\n            type: \"fixed\",\n            value: shared.arn,\n        },\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\nthis = databricks.ClusterPolicy(\"this\",\n    name=\"Policy with predefined instance profile\",\n    definition=json.dumps({\n        \"aws_attributes.instance_profile_arn\": {\n            \"type\": \"fixed\",\n            \"value\": shared[\"arn\"],\n        },\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ClusterPolicy(\"this\", new()\n    {\n        Name = \"Policy with predefined instance profile\",\n        Definition = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"aws_attributes.instance_profile_arn\"] = new Dictionary\u003cstring, object?\u003e\n            {\n                [\"type\"] = \"fixed\",\n                [\"value\"] = shared.Arn,\n            },\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"aws_attributes.instance_profile_arn\": map[string]interface{}{\n\t\t\t\t\"type\":  \"fixed\",\n\t\t\t\t\"value\": shared.Arn,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewClusterPolicy(ctx, \"this\", \u0026databricks.ClusterPolicyArgs{\n\t\t\tName:       pulumi.String(\"Policy with predefined instance profile\"),\n\t\t\tDefinition: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ClusterPolicy;\nimport com.pulumi.databricks.ClusterPolicyArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ClusterPolicy(\"this\", ClusterPolicyArgs.builder()        \n            .name(\"Policy with predefined instance profile\")\n            .definition(serializeJson(\n                jsonObject(\n                    jsonProperty(\"aws_attributes.instance_profile_arn\", jsonObject(\n                        jsonProperty(\"type\", \"fixed\"),\n                        jsonProperty(\"value\", shared.arn())\n                    ))\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ClusterPolicy\n    properties:\n      name: Policy with predefined instance profile\n      definition:\n        fn::toJSON:\n          aws_attributes.instance_profile_arn:\n            type: fixed\n            value: ${shared.arn}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Granting access to all users\n\nYou can make instance profile available to all users by associating it with the special group called `users` through databricks.Group data source.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.InstanceProfile(\"this\", {instanceProfileArn: shared.arn});\nconst users = databricks.getGroup({\n    displayName: \"users\",\n});\nconst all = new databricks.GroupInstanceProfile(\"all\", {\n    groupId: users.then(users =\u003e users.id),\n    instanceProfileId: _this.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.InstanceProfile(\"this\", instance_profile_arn=shared[\"arn\"])\nusers = databricks.get_group(display_name=\"users\")\nall = databricks.GroupInstanceProfile(\"all\",\n    group_id=users.id,\n    instance_profile_id=this.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = shared.Arn,\n    });\n\n    var users = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"users\",\n    });\n\n    var all = new Databricks.GroupInstanceProfile(\"all\", new()\n    {\n        GroupId = users.Apply(getGroupResult =\u003e getGroupResult.Id),\n        InstanceProfileId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.Any(shared.Arn),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tusers, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"users\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupInstanceProfile(ctx, \"all\", \u0026databricks.GroupInstanceProfileArgs{\n\t\t\tGroupId:           pulumi.String(users.Id),\n\t\t\tInstanceProfileId: this.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.GroupInstanceProfile;\nimport com.pulumi.databricks.GroupInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new InstanceProfile(\"this\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(shared.arn())\n            .build());\n\n        final var users = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"users\")\n            .build());\n\n        var all = new GroupInstanceProfile(\"all\", GroupInstanceProfileArgs.builder()        \n            .groupId(users.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .instanceProfileId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:InstanceProfile\n    properties:\n      instanceProfileArn: ${shared.arn}\n  all:\n    type: databricks:GroupInstanceProfile\n    properties:\n      groupId: ${users.id}\n      instanceProfileId: ${this.id}\nvariables:\n  users:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: users\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Usage with Databricks SQL serverless\n\nWhen the instance profile ARN and its associated IAM role ARN don't match and the instance profile is intended for use with Databricks SQL serverless, the `iam_role_arn` parameter can be specified.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sqlServerlessAssumeRole = aws.iam.getPolicyDocument({\n    statements: [{\n        actions: [\"sts:AssumeRole\"],\n        principals: [{\n            type: \"AWS\",\n            identifiers: [\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n        }],\n        conditions: [{\n            test: \"StringEquals\",\n            variable: \"sts:ExternalID\",\n            values: [\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n            ],\n        }],\n    }],\n});\nconst _this = new aws.iam.Role(\"this\", {\n    name: \"my-databricks-sql-serverless-role\",\n    assumeRolePolicy: sqlServerlessAssumeRole.then(sqlServerlessAssumeRole =\u003e sqlServerlessAssumeRole.json),\n});\nconst thisInstanceProfile = new aws.iam.InstanceProfile(\"this\", {\n    name: \"my-databricks-sql-serverless-instance-profile\",\n    role: _this.name,\n});\nconst thisInstanceProfile2 = new databricks.InstanceProfile(\"this\", {\n    instanceProfileArn: thisInstanceProfile.arn,\n    iamRoleArn: _this.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nsql_serverless_assume_role = aws.iam.get_policy_document(statements=[aws.iam.GetPolicyDocumentStatementArgs(\n    actions=[\"sts:AssumeRole\"],\n    principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n        type=\"AWS\",\n        identifiers=[\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\"],\n    )],\n    conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n        test=\"StringEquals\",\n        variable=\"sts:ExternalID\",\n        values=[\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n        ],\n    )],\n)])\nthis = aws.iam.Role(\"this\",\n    name=\"my-databricks-sql-serverless-role\",\n    assume_role_policy=sql_serverless_assume_role.json)\nthis_instance_profile = aws.iam.InstanceProfile(\"this\",\n    name=\"my-databricks-sql-serverless-instance-profile\",\n    role=this.name)\nthis_instance_profile2 = databricks.InstanceProfile(\"this\",\n    instance_profile_arn=this_instance_profile.arn,\n    iam_role_arn=this.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sqlServerlessAssumeRole = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n                        },\n                    },\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"StringEquals\",\n                        Variable = \"sts:ExternalID\",\n                        Values = new[]\n                        {\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                            \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var @this = new Aws.Iam.Role(\"this\", new()\n    {\n        Name = \"my-databricks-sql-serverless-role\",\n        AssumeRolePolicy = sqlServerlessAssumeRole.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var thisInstanceProfile = new Aws.Iam.InstanceProfile(\"this\", new()\n    {\n        Name = \"my-databricks-sql-serverless-instance-profile\",\n        Role = @this.Name,\n    });\n\n    var thisInstanceProfile2 = new Databricks.InstanceProfile(\"this\", new()\n    {\n        InstanceProfileArn = thisInstanceProfile.Arn,\n        IamRoleArn = @this.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsqlServerlessAssumeRole, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\n\t\t\tStatements: []iam.GetPolicyDocumentStatement{\n\t\t\t\t{\n\t\t\t\t\tActions: []string{\n\t\t\t\t\t\t\"sts:AssumeRole\",\n\t\t\t\t\t},\n\t\t\t\t\tPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tType: \"AWS\",\n\t\t\t\t\t\t\tIdentifiers: []string{\n\t\t\t\t\t\t\t\t\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t\tConditions: []iam.GetPolicyDocumentStatementCondition{\n\t\t\t\t\t\t{\n\t\t\t\t\t\t\tTest:     \"StringEquals\",\n\t\t\t\t\t\t\tVariable: \"sts:ExternalID\",\n\t\t\t\t\t\t\tValues: []string{\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n\t\t\t\t\t\t\t\t\"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\",\n\t\t\t\t\t\t\t},\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := iam.NewRole(ctx, \"this\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(\"my-databricks-sql-serverless-role\"),\n\t\t\tAssumeRolePolicy: pulumi.String(sqlServerlessAssumeRole.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisInstanceProfile, err := iam.NewInstanceProfile(ctx, \"this\", \u0026iam.InstanceProfileArgs{\n\t\t\tName: pulumi.String(\"my-databricks-sql-serverless-instance-profile\"),\n\t\t\tRole: this.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewInstanceProfile(ctx, \"this\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: thisInstanceProfile.Arn,\n\t\t\tIamRoleArn:         this.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.InstanceProfile;\nimport com.pulumi.aws.iam.InstanceProfileArgs;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sqlServerlessAssumeRole = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(GetPolicyDocumentStatementArgs.builder()\n                .actions(\"sts:AssumeRole\")\n                .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                    .type(\"AWS\")\n                    .identifiers(\"arn:aws:iam::790110701330:role/serverless-customer-resource-role\")\n                    .build())\n                .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                    .test(\"StringEquals\")\n                    .variable(\"sts:ExternalID\")\n                    .values(                    \n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\",\n                        \"databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\")\n                    .build())\n                .build())\n            .build());\n\n        var this_ = new Role(\"this\", RoleArgs.builder()        \n            .name(\"my-databricks-sql-serverless-role\")\n            .assumeRolePolicy(sqlServerlessAssumeRole.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var thisInstanceProfile = new InstanceProfile(\"thisInstanceProfile\", InstanceProfileArgs.builder()        \n            .name(\"my-databricks-sql-serverless-instance-profile\")\n            .role(this_.name())\n            .build());\n\n        var thisInstanceProfile2 = new InstanceProfile(\"thisInstanceProfile2\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(thisInstanceProfile.arn())\n            .iamRoleArn(this_.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: aws:iam:Role\n    properties:\n      name: my-databricks-sql-serverless-role\n      assumeRolePolicy: ${sqlServerlessAssumeRole.json}\n  thisInstanceProfile:\n    type: aws:iam:InstanceProfile\n    name: this\n    properties:\n      name: my-databricks-sql-serverless-instance-profile\n      role: ${this.name}\n  thisInstanceProfile2:\n    type: databricks:InstanceProfile\n    name: this\n    properties:\n      instanceProfileArn: ${thisInstanceProfile.arn}\n      iamRoleArn: ${this.arn}\nvariables:\n  sqlServerlessAssumeRole:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        statements:\n          - actions:\n              - sts:AssumeRole\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::790110701330:role/serverless-customer-resource-role\n            conditions:\n              - test: StringEquals\n                variable: sts:ExternalID\n                values:\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID1\u003e\n                  - databricks-serverless-\u003cYOUR_WORKSPACE_ID2\u003e\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource instance profile can be imported using the ARN of it\n\nbash\n\n```sh\n$ pulumi import databricks:index/instanceProfile:InstanceProfile this \u003cinstance-profile-arn\u003e\n```\n\n",
            "properties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "required": [
                "instanceProfileArn",
                "skipValidation"
            ],
            "inputProperties": {
                "iamRoleArn": {
                    "type": "string",
                    "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                },
                "isMetaInstanceProfile": {
                    "type": "boolean",
                    "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                }
            },
            "requiredInputs": [
                "instanceProfileArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering InstanceProfile resources.\n",
                "properties": {
                    "iamRoleArn": {
                        "type": "string",
                        "description": "The AWS IAM role ARN of the role associated with the instance profile. It must have the form `arn:aws:iam::\u003caccount-id\u003e:role/\u003cname\u003e`. This field is required if your role name and instance profile name do not match and you want to use the instance profile with Databricks SQL Serverless.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "`ARN` attribute of `aws_iam_instance_profile` output, the EC2 instance profile association to AWS IAM role. This ARN would be validated upon resource creation.\n"
                    },
                    "isMetaInstanceProfile": {
                        "type": "boolean",
                        "description": "Whether the instance profile is a meta instance profile. Used only in [IAM credential passthrough](https://docs.databricks.com/security/credential-passthrough/iam-passthrough.html).\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "**For advanced usage only.** If validation fails with an error message that does not indicate an IAM related permission issue, (e.g. \"Your requested instance type is not supported in your requested availability zone\"), you can pass this flag to skip the validation and forcibly add the instance profile.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/ipAccessList:IpAccessList": {
            "description": "Security-conscious enterprises that use cloud SaaS applications need to restrict access to their own employees. Authentication helps to prove user identity, but that does not enforce network location of the users. Accessing a cloud service from an unsecured network can pose security risks to an enterprise, especially when the user may have authorized access to sensitive or personal data. Enterprise network perimeters apply security policies and limit access to external services (for example, firewalls, proxies, DLP, and logging), so access beyond these controls are assumed to be untrusted. Please see [IP Access List](https://docs.databricks.com/security/network/ip-access-list.html) for full feature documentation.\n\n\u003e **Note** The total number of IP addresses and CIDR scopes provided across all ACL Lists in a workspace can not exceed 1000.  Refer to the docs above for specifics.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\nconst allowed_list = new databricks.IpAccessList(\"allowed-list\", {\n    label: \"allow_in\",\n    listType: \"ALLOW\",\n    ipAddresses: [\n        \"1.1.1.1\",\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n}, {\n    dependsOn: [_this],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\nallowed_list = databricks.IpAccessList(\"allowed-list\",\n    label=\"allow_in\",\n    list_type=\"ALLOW\",\n    ip_addresses=[\n        \"1.1.1.1\",\n        \"1.2.3.0/24\",\n        \"1.2.5.0/24\",\n    ],\n    opts=pulumi.ResourceOptions(depends_on=[this]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n    var allowed_list = new Databricks.IpAccessList(\"allowed-list\", new()\n    {\n        Label = \"allow_in\",\n        ListType = \"ALLOW\",\n        IpAddresses = new[]\n        {\n            \"1.1.1.1\",\n            \"1.2.3.0/24\",\n            \"1.2.5.0/24\",\n        },\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            @this,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.Map{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewIpAccessList(ctx, \"allowed-list\", \u0026databricks.IpAccessListArgs{\n\t\t\tLabel:    pulumi.String(\"allow_in\"),\n\t\t\tListType: pulumi.String(\"ALLOW\"),\n\t\t\tIpAddresses: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"1.1.1.1\"),\n\t\t\t\tpulumi.String(\"1.2.3.0/24\"),\n\t\t\t\tpulumi.String(\"1.2.5.0/24\"),\n\t\t\t},\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthis,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport com.pulumi.databricks.IpAccessList;\nimport com.pulumi.databricks.IpAccessListArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n        var allowed_list = new IpAccessList(\"allowed-list\", IpAccessListArgs.builder()        \n            .label(\"allow_in\")\n            .listType(\"ALLOW\")\n            .ipAddresses(            \n                \"1.1.1.1\",\n                \"1.2.3.0/24\",\n                \"1.2.5.0/24\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(this_)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n  allowed-list:\n    type: databricks:IpAccessList\n    properties:\n      label: allow_in\n      listType: ALLOW\n      ipAddresses:\n        - 1.1.1.1\n        - 1.2.3.0/24\n        - 1.2.5.0/24\n    options:\n      dependson:\n        - ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsPrivateAccessSettings to create a [Private Access Setting](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html#step-5-create-a-private-access-settings-configuration-using-the-databricks-account-api) that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nThe databricks_ip_access_list can be imported using id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/ipAccessList:IpAccessList this \u003clist-id\u003e\n```\n\n",
            "properties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A string list of IP addresses and CIDR ranges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                }
            },
            "required": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "inputProperties": {
                "enabled": {
                    "type": "boolean",
                    "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                },
                "ipAddresses": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "A string list of IP addresses and CIDR ranges.\n"
                },
                "label": {
                    "type": "string",
                    "description": "This is the display name for the given IP ACL List.\n"
                },
                "listType": {
                    "type": "string",
                    "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                }
            },
            "requiredInputs": [
                "ipAddresses",
                "label",
                "listType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering IpAccessList resources.\n",
                "properties": {
                    "enabled": {
                        "type": "boolean",
                        "description": "Boolean `true` or `false` indicating whether this list should be active.  Defaults to `true`\n"
                    },
                    "ipAddresses": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "A string list of IP addresses and CIDR ranges.\n"
                    },
                    "label": {
                        "type": "string",
                        "description": "This is the display name for the given IP ACL List.\n"
                    },
                    "listType": {
                        "type": "string",
                        "description": "Can only be \"ALLOW\" or \"BLOCK\".\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/job:Job": {
            "description": "\n\n## Import\n\nThe resource job can be imported using the id of the job\n\nbash\n\n```sh\n$ pulumi import databricks:index/job:Job this \u003cjob-id\u003e\n```\n\n",
            "properties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                    "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                },
                "continuous": {
                    "$ref": "#/types/databricks:index/JobContinuous:JobContinuous"
                },
                "controlRunState": {
                    "type": "boolean",
                    "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "editMode": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                    "description": "An optional block that specifies the health conditions for the job (described below).\n"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    },
                    "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section of the databricks.Cluster resource for more information.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level (described below).\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/JobQueue:JobQueue"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/JobRunAs:JobRunAs"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/JobTrigger:JobTrigger"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the job on the given workspace\n"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "required": [
                "format",
                "name",
                "runAs",
                "url"
            ],
            "inputProperties": {
                "alwaysRunning": {
                    "type": "boolean",
                    "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                    "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                },
                "continuous": {
                    "$ref": "#/types/databricks:index/JobContinuous:JobContinuous"
                },
                "controlRunState": {
                    "type": "boolean",
                    "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                },
                "dbtTask": {
                    "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                },
                "description": {
                    "type": "string",
                    "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                },
                "editMode": {
                    "type": "string"
                },
                "emailNotifications": {
                    "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                    "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                },
                "environments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                    }
                },
                "existingClusterId": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "gitSource": {
                    "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                },
                "health": {
                    "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                    "description": "An optional block that specifies the health conditions for the job (described below).\n"
                },
                "jobClusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                    },
                    "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                    },
                    "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section of the databricks.Cluster resource for more information.\n"
                },
                "maxConcurrentRuns": {
                    "type": "integer",
                    "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                },
                "maxRetries": {
                    "type": "integer",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "minRetryIntervalMillis": {
                    "type": "integer",
                    "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "name": {
                    "type": "string",
                    "description": "An optional name for the job. The default value is Untitled.\n"
                },
                "newCluster": {
                    "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                },
                "notebookTask": {
                    "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "notificationSettings": {
                    "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                    "description": "An optional block controlling the notification settings on the job level (described below).\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                    }
                },
                "pipelineTask": {
                    "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "pythonWheelTask": {
                    "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "queue": {
                    "$ref": "#/types/databricks:index/JobQueue:JobQueue"
                },
                "retryOnTimeout": {
                    "type": "boolean",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "runAs": {
                    "$ref": "#/types/databricks:index/JobRunAs:JobRunAs"
                },
                "runJobTask": {
                    "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                    "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                },
                "sparkJarTask": {
                    "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkPythonTask": {
                    "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "sparkSubmitTask": {
                    "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                    "deprecationMessage": "should be used inside a task block and not inside a job block"
                },
                "tags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "tasks": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/JobTask:JobTask"
                    }
                },
                "timeoutSeconds": {
                    "type": "integer",
                    "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                },
                "trigger": {
                    "$ref": "#/types/databricks:index/JobTrigger:JobTrigger"
                },
                "webhookNotifications": {
                    "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                    "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Job resources.\n",
                "properties": {
                    "alwaysRunning": {
                        "type": "boolean",
                        "description": "(Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.\n",
                        "deprecationMessage": "always_running will be replaced by control_run_state in the next major release."
                    },
                    "continuous": {
                        "$ref": "#/types/databricks:index/JobContinuous:JobContinuous"
                    },
                    "controlRunState": {
                        "type": "boolean",
                        "description": "(Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.\n\nWhen migrating from `always_running` to `control_run_state`, set `continuous` as follows:\n\n"
                    },
                    "dbtTask": {
                        "$ref": "#/types/databricks:index/JobDbtTask:JobDbtTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "deployment": {
                        "$ref": "#/types/databricks:index/JobDeployment:JobDeployment"
                    },
                    "description": {
                        "type": "string",
                        "description": "An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.\n"
                    },
                    "editMode": {
                        "type": "string"
                    },
                    "emailNotifications": {
                        "$ref": "#/types/databricks:index/JobEmailNotifications:JobEmailNotifications",
                        "description": "(List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.\n"
                    },
                    "environments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobEnvironment:JobEnvironment"
                        }
                    },
                    "existingClusterId": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "gitSource": {
                        "$ref": "#/types/databricks:index/JobGitSource:JobGitSource"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/JobHealth:JobHealth",
                        "description": "An optional block that specifies the health conditions for the job (described below).\n"
                    },
                    "jobClusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobJobCluster:JobJobCluster"
                        },
                        "description": "A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*\n"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobLibrary:JobLibrary"
                        },
                        "description": "(List) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section of the databricks.Cluster resource for more information.\n"
                    },
                    "maxConcurrentRuns": {
                        "type": "integer",
                        "description": "(Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.\n"
                    },
                    "maxRetries": {
                        "type": "integer",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "minRetryIntervalMillis": {
                        "type": "integer",
                        "description": "(Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.\n",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "name": {
                        "type": "string",
                        "description": "An optional name for the job. The default value is Untitled.\n"
                    },
                    "newCluster": {
                        "$ref": "#/types/databricks:index/JobNewCluster:JobNewCluster"
                    },
                    "notebookTask": {
                        "$ref": "#/types/databricks:index/JobNotebookTask:JobNotebookTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "notificationSettings": {
                        "$ref": "#/types/databricks:index/JobNotificationSettings:JobNotificationSettings",
                        "description": "An optional block controlling the notification settings on the job level (described below).\n"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobParameter:JobParameter"
                        }
                    },
                    "pipelineTask": {
                        "$ref": "#/types/databricks:index/JobPipelineTask:JobPipelineTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "pythonWheelTask": {
                        "$ref": "#/types/databricks:index/JobPythonWheelTask:JobPythonWheelTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "queue": {
                        "$ref": "#/types/databricks:index/JobQueue:JobQueue"
                    },
                    "retryOnTimeout": {
                        "type": "boolean",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "runAs": {
                        "$ref": "#/types/databricks:index/JobRunAs:JobRunAs"
                    },
                    "runJobTask": {
                        "$ref": "#/types/databricks:index/JobRunJobTask:JobRunJobTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/JobSchedule:JobSchedule",
                        "description": "(List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.\n"
                    },
                    "sparkJarTask": {
                        "$ref": "#/types/databricks:index/JobSparkJarTask:JobSparkJarTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "sparkPythonTask": {
                        "$ref": "#/types/databricks:index/JobSparkPythonTask:JobSparkPythonTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "sparkSubmitTask": {
                        "$ref": "#/types/databricks:index/JobSparkSubmitTask:JobSparkSubmitTask",
                        "deprecationMessage": "should be used inside a task block and not inside a job block"
                    },
                    "tags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        }
                    },
                    "tasks": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/JobTask:JobTask"
                        }
                    },
                    "timeoutSeconds": {
                        "type": "integer",
                        "description": "(Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.\n"
                    },
                    "trigger": {
                        "$ref": "#/types/databricks:index/JobTrigger:JobTrigger"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the job on the given workspace\n"
                    },
                    "webhookNotifications": {
                        "$ref": "#/types/databricks:index/JobWebhookNotifications:JobWebhookNotifications",
                        "description": "(List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/lakehouseMonitor:LakehouseMonitor": {
            "description": "This resource allows you to manage [Lakehouse Monitors](https://docs.databricks.com/en/lakehouse-monitoring/index.html) in Databricks. \n\nA `databricks.LakehouseMonitor` is attached to a databricks.SqlTable and can be of type timeseries, snapshot or inference. \n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.SqlTable;\nimport com.pulumi.databricks.SqlTableArgs;\nimport com.pulumi.databricks.inputs.SqlTableColumnArgs;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorTimeSeriesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var myTestTable = new SqlTable(\"myTestTable\", SqlTableArgs.builder()        \n            .catalogName(\"main\")\n            .schemaName(things.name())\n            .name(\"bar\")\n            .tableType(\"MANAGED\")\n            .dataSourceFormat(\"DELTA\")\n            .columns(SqlTableColumnArgs.builder()\n                .name(\"timestamp\")\n                .position(1)\n                .type(\"int\")\n                .build())\n            .build());\n\n        var testTimeseriesMonitor = new LakehouseMonitor(\"testTimeseriesMonitor\", LakehouseMonitorArgs.builder()        \n            .tableName(Output.tuple(sandbox.name(), things.name(), myTestTable.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                var myTestTableName = values.t3;\n                return String.format(\"%s.%s.%s\", sandboxName,thingsName,myTestTableName);\n            }))\n            .assetsDir(myTestTable.name().applyValue(name -\u003e String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", name)))\n            .outputSchemaName(Output.tuple(sandbox.name(), things.name()).applyValue(values -\u003e {\n                var sandboxName = values.t1;\n                var thingsName = values.t2;\n                return String.format(\"%s.%s\", sandboxName,thingsName);\n            }))\n            .timeSeries(LakehouseMonitorTimeSeriesArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n  myTestTable:\n    type: databricks:SqlTable\n    properties:\n      catalogName: main\n      schemaName: ${things.name}\n      name: bar\n      tableType: MANAGED\n      dataSourceFormat: DELTA\n      columns:\n        - name: timestamp\n          position: 1\n          type: int\n  testTimeseriesMonitor:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      timeSeries:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Inference Monitor\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.LakehouseMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    inferenceLog: {\n        granularities: [\"1 hour\"],\n        timestampCol: \"timestamp\",\n        predictionCol: \"prediction\",\n        modelIdCol: \"model_id\",\n        problemType: \"PROBLEM_TYPE_REGRESSION\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.LakehouseMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_lakehouse_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    inference_log=databricks.LakehouseMonitorInferenceLogArgs(\n        granularities=[\"1 hour\"],\n        timestamp_col=\"timestamp\",\n        prediction_col=\"prediction\",\n        model_id_col=\"model_id\",\n        problem_type=\"PROBLEM_TYPE_REGRESSION\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.LakehouseMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_lakehouse_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        InferenceLog = new Databricks.Inputs.LakehouseMonitorInferenceLogArgs\n        {\n            Granularities = new[]\n            {\n                \"1 hour\",\n            },\n            TimestampCol = \"timestamp\",\n            PredictionCol = \"prediction\",\n            ModelIdCol = \"model_id\",\n            ProblemType = \"PROBLEM_TYPE_REGRESSION\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLakehouseMonitor(ctx, \"testMonitorInference\", \u0026databricks.LakehouseMonitorArgs{\n\t\t\tTableName:        pulumi.String(fmt.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name)),\n\t\t\tAssetsDir:        pulumi.String(fmt.Sprintf(\"/Shared/provider-test/databricks_lakehouse_monitoring/%v\", myTestTable.Name)),\n\t\t\tOutputSchemaName: pulumi.String(fmt.Sprintf(\"%v.%v\", sandbox.Name, things.Name)),\n\t\t\tInferenceLog: \u0026databricks.LakehouseMonitorInferenceLogArgs{\n\t\t\t\tGranularities: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"1 hour\"),\n\t\t\t\t},\n\t\t\t\tTimestampCol:  pulumi.String(\"timestamp\"),\n\t\t\t\tPredictionCol: pulumi.String(\"prediction\"),\n\t\t\t\tModelIdCol:    pulumi.String(\"model_id\"),\n\t\t\t\tProblemType:   pulumi.String(\"PROBLEM_TYPE_REGRESSION\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorInferenceLogArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new LakehouseMonitor(\"testMonitorInference\", LakehouseMonitorArgs.builder()        \n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .inferenceLog(LakehouseMonitorInferenceLogArgs.builder()\n                .granularities(\"1 hour\")\n                .timestampCol(\"timestamp\")\n                .predictionCol(\"prediction\")\n                .modelIdCol(\"model_id\")\n                .problemType(\"PROBLEM_TYPE_REGRESSION\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      inferenceLog:\n        granularities:\n          - 1 hour\n        timestampCol: timestamp\n        predictionCol: prediction\n        modelIdCol: model_id\n        problemType: PROBLEM_TYPE_REGRESSION\n```\n\u003c!--End PulumiCodeChooser --\u003e\n### Snapshot Monitor\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst testMonitorInference = new databricks.LakehouseMonitor(\"testMonitorInference\", {\n    tableName: `${sandbox.name}.${things.name}.${myTestTable.name}`,\n    assetsDir: `/Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}`,\n    outputSchemaName: `${sandbox.name}.${things.name}`,\n    snapshot: {},\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest_monitor_inference = databricks.LakehouseMonitor(\"testMonitorInference\",\n    table_name=f\"{sandbox['name']}.{things['name']}.{my_test_table['name']}\",\n    assets_dir=f\"/Shared/provider-test/databricks_lakehouse_monitoring/{my_test_table['name']}\",\n    output_schema_name=f\"{sandbox['name']}.{things['name']}\",\n    snapshot=databricks.LakehouseMonitorSnapshotArgs())\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var testMonitorInference = new Databricks.LakehouseMonitor(\"testMonitorInference\", new()\n    {\n        TableName = $\"{sandbox.Name}.{things.Name}.{myTestTable.Name}\",\n        AssetsDir = $\"/Shared/provider-test/databricks_lakehouse_monitoring/{myTestTable.Name}\",\n        OutputSchemaName = $\"{sandbox.Name}.{things.Name}\",\n        Snapshot = null,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLakehouseMonitor(ctx, \"testMonitorInference\", \u0026databricks.LakehouseMonitorArgs{\n\t\t\tTableName:        pulumi.String(fmt.Sprintf(\"%v.%v.%v\", sandbox.Name, things.Name, myTestTable.Name)),\n\t\t\tAssetsDir:        pulumi.String(fmt.Sprintf(\"/Shared/provider-test/databricks_lakehouse_monitoring/%v\", myTestTable.Name)),\n\t\t\tOutputSchemaName: pulumi.String(fmt.Sprintf(\"%v.%v\", sandbox.Name, things.Name)),\n\t\t\tSnapshot:         nil,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.LakehouseMonitor;\nimport com.pulumi.databricks.LakehouseMonitorArgs;\nimport com.pulumi.databricks.inputs.LakehouseMonitorSnapshotArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var testMonitorInference = new LakehouseMonitor(\"testMonitorInference\", LakehouseMonitorArgs.builder()        \n            .tableName(String.format(\"%s.%s.%s\", sandbox.name(),things.name(),myTestTable.name()))\n            .assetsDir(String.format(\"/Shared/provider-test/databricks_lakehouse_monitoring/%s\", myTestTable.name()))\n            .outputSchemaName(String.format(\"%s.%s\", sandbox.name(),things.name()))\n            .snapshot()\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  testMonitorInference:\n    type: databricks:LakehouseMonitor\n    properties:\n      tableName: ${sandbox.name}.${things.name}.${myTestTable.name}\n      assetsDir: /Shared/provider-test/databricks_lakehouse_monitoring/${myTestTable.name}\n      outputSchemaName: ${sandbox.name}.${things.name}\n      snapshot: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.Catalog\n* databricks.Schema\n* databricks.SqlTable\n",
            "properties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dashboardId": {
                    "type": "string",
                    "description": "The ID of the generated dashboard.\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "driftMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "monitorVersion": {
                    "type": "string",
                    "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "profileMetricsTableName": {
                    "type": "string",
                    "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of the Monitor\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                }
            },
            "required": [
                "assetsDir",
                "dashboardId",
                "driftMetricsTableName",
                "monitorVersion",
                "outputSchemaName",
                "profileMetricsTableName",
                "status",
                "tableName"
            ],
            "inputProperties": {
                "assetsDir": {
                    "type": "string",
                    "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                },
                "baselineTableName": {
                    "type": "string",
                    "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                },
                "customMetrics": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                    },
                    "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                },
                "dataClassificationConfig": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                    "description": "The data classification config for the monitor\n"
                },
                "inferenceLog": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                    "description": "Configuration for the inference log monitor\n"
                },
                "latestMonitorFailureMsg": {
                    "type": "string"
                },
                "notifications": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                    "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                },
                "outputSchemaName": {
                    "type": "string",
                    "description": "Schema where output metric tables are created\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                    "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                },
                "skipBuiltinDashboard": {
                    "type": "boolean",
                    "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                },
                "slicingExprs": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                },
                "snapshot": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                    "description": "Configuration for monitoring snapshot tables.\n"
                },
                "tableName": {
                    "type": "string",
                    "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                },
                "timeSeries": {
                    "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                    "description": "Configuration for monitoring timeseries tables.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                }
            },
            "requiredInputs": [
                "assetsDir",
                "outputSchemaName",
                "tableName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering LakehouseMonitor resources.\n",
                "properties": {
                    "assetsDir": {
                        "type": "string",
                        "description": "The directory to store the monitoring assets (Eg. Dashboard and Metric Tables)\n"
                    },
                    "baselineTableName": {
                        "type": "string",
                        "description": "Name of the baseline table from which drift metrics are computed from.Columns in the monitored table should also be present in the baseline\ntable.\n"
                    },
                    "customMetrics": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/LakehouseMonitorCustomMetric:LakehouseMonitorCustomMetric"
                        },
                        "description": "Custom metrics to compute on the monitored table. These can be aggregate metrics, derived metrics (from already computed aggregate metrics), or drift metrics (comparing metrics across time windows).\n"
                    },
                    "dashboardId": {
                        "type": "string",
                        "description": "The ID of the generated dashboard.\n"
                    },
                    "dataClassificationConfig": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorDataClassificationConfig:LakehouseMonitorDataClassificationConfig",
                        "description": "The data classification config for the monitor\n"
                    },
                    "driftMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the drift metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "inferenceLog": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorInferenceLog:LakehouseMonitorInferenceLog",
                        "description": "Configuration for the inference log monitor\n"
                    },
                    "latestMonitorFailureMsg": {
                        "type": "string"
                    },
                    "monitorVersion": {
                        "type": "string",
                        "description": "The version of the monitor config (e.g. 1,2,3). If negative, the monitor may be corrupted\n"
                    },
                    "notifications": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorNotifications:LakehouseMonitorNotifications",
                        "description": "The notification settings for the monitor.  The following optional blocks are supported, each consisting of the single string array field with name `email_addresses` containing a list of emails to notify:\n"
                    },
                    "outputSchemaName": {
                        "type": "string",
                        "description": "Schema where output metric tables are created\n"
                    },
                    "profileMetricsTableName": {
                        "type": "string",
                        "description": "The full name of the profile metrics table. Format: __catalog_name__.__schema_name__.__table_name__.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorSchedule:LakehouseMonitorSchedule",
                        "description": "The schedule for automatically updating and refreshing metric tables.  This block consists of following fields:\n"
                    },
                    "skipBuiltinDashboard": {
                        "type": "boolean",
                        "description": "Whether to skip creating a default dashboard summarizing data quality metrics.\n"
                    },
                    "slicingExprs": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of column expressions to slice data with for targeted analysis. The data is grouped by each expression independently, resulting in a separate slice for each predicate and its complements. For high-cardinality columns, only the top 100 unique values by frequency will generate slices.\n"
                    },
                    "snapshot": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorSnapshot:LakehouseMonitorSnapshot",
                        "description": "Configuration for monitoring snapshot tables.\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of the Monitor\n"
                    },
                    "tableName": {
                        "type": "string",
                        "description": "The full name of the table to attach the monitor too. Its of the format {catalog}.{schema}.{tableName}\n"
                    },
                    "timeSeries": {
                        "$ref": "#/types/databricks:index/LakehouseMonitorTimeSeries:LakehouseMonitorTimeSeries",
                        "description": "Configuration for monitoring timeseries tables.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "Optional argument to specify the warehouse for dashboard creation. If not specified, the first running warehouse will be used.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/library:Library": {
            "description": "Installs a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster. Each different type of library has a slightly different syntax. It's possible to set only one type of library within one resource. Otherwise, the plan will fail with an error.\n\n\u003e **Note** `databricks.Library` resource would always start the associated cluster if it's not running, so make sure to have auto-termination configured. It's not possible to atomically change the version of the same library without cluster restart. Libraries are fully removed from the cluster only after restart.\n\n## Installing library on all clusters\n\nYou can install libraries on all clusters with the help of databricks.getClusters data resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const all = await databricks.getClusters({});\n    const cli: databricks.Library[] = [];\n    for (const range of all.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        cli.push(new databricks.Library(`cli-${range.key}`, {\n            clusterId: range.key,\n            pypi: {\n                \"package\": \"databricks-cli\",\n            },\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\ncli = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(all.ids)]:\n    cli.append(databricks.Library(f\"cli-{range['key']}\",\n        cluster_id=range[\"key\"],\n        pypi=databricks.LibraryPypiArgs(\n            package=\"databricks-cli\",\n        )))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var all = await Databricks.GetClusters.InvokeAsync();\n\n    var cli = new List\u003cDatabricks.Library\u003e();\n    foreach (var range in )\n    {\n        cli.Add(new Databricks.Library($\"cli-{range.Key}\", new()\n        {\n            ClusterId = range.Key,\n            Pypi = new Databricks.Inputs.LibraryPypiArgs\n            {\n                Package = \"databricks-cli\",\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetClusters(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar cli []*databricks.Library\n\t\tfor key0, _ := range all.Ids {\n\t\t\t__res, err := databricks.NewLibrary(ctx, fmt.Sprintf(\"cli-%v\", key0), \u0026databricks.LibraryArgs{\n\t\t\t\tClusterId: pulumi.Float64(key0),\n\t\t\t\tPypi: \u0026databricks.LibraryPypiArgs{\n\t\t\t\t\tPackage: pulumi.String(\"databricks-cli\"),\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tcli = append(cli, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters();\n\n        final var cli = all.applyValue(getClustersResult -\u003e {\n            final var resources = new ArrayList\u003cLibrary\u003e();\n            for (var range : KeyedValue.of(getClustersResult.ids()) {\n                var resource = new Library(\"cli-\" + range.key(), LibraryArgs.builder()                \n                    .clusterId(range.key())\n                    .pypi(LibraryPypiArgs.builder()\n                        .package_(\"databricks-cli\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  cli:\n    type: databricks:Library\n    properties:\n      clusterId: ${range.key}\n      pypi:\n        package: databricks-cli\n    options: {}\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getClusters\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Java/Scala Maven\n\nInstalling artifacts from Maven repository. You can also optionally specify a `repo` parameter for a custom Maven-style repository, that should be accessible without any authentication. Maven libraries are resolved in Databricks Control Plane, so repo should be accessible from it. It can even be properly configured [maven s3 wagon](https://github.com/seahen/maven-s3-wagon), [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) or [Azure Artifacts](https://azure.microsoft.com/en-us/services/devops/artifacts/).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst deequ = new databricks.Library(\"deequ\", {\n    clusterId: _this.id,\n    maven: {\n        coordinates: \"com.amazon.deequ:deequ:1.0.4\",\n        exclusions: [\"org.apache.avro:avro\"],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndeequ = databricks.Library(\"deequ\",\n    cluster_id=this[\"id\"],\n    maven=databricks.LibraryMavenArgs(\n        coordinates=\"com.amazon.deequ:deequ:1.0.4\",\n        exclusions=[\"org.apache.avro:avro\"],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var deequ = new Databricks.Library(\"deequ\", new()\n    {\n        ClusterId = @this.Id,\n        Maven = new Databricks.Inputs.LibraryMavenArgs\n        {\n            Coordinates = \"com.amazon.deequ:deequ:1.0.4\",\n            Exclusions = new[]\n            {\n                \"org.apache.avro:avro\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"deequ\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tMaven: \u0026databricks.LibraryMavenArgs{\n\t\t\t\tCoordinates: pulumi.String(\"com.amazon.deequ:deequ:1.0.4\"),\n\t\t\t\tExclusions: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"org.apache.avro:avro\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryMavenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var deequ = new Library(\"deequ\", LibraryArgs.builder()        \n            .clusterId(this_.id())\n            .maven(LibraryMavenArgs.builder()\n                .coordinates(\"com.amazon.deequ:deequ:1.0.4\")\n                .exclusions(\"org.apache.avro:avro\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  deequ:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      maven:\n        coordinates: com.amazon.deequ:deequ:1.0.4\n        exclusions:\n          - org.apache.avro:avro\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Python PyPI\n\nInstalling Python PyPI artifacts. You can optionally also specify the `repo` parameter for a custom PyPI mirror, which should be accessible without any authentication for the network that cluster runs in.\n\n\u003e **Note** `repo` host should be accessible from the Internet by Databricks control plane. If connectivity to custom PyPI repositories is required, please modify cluster-node `/etc/pip.conf` through databricks_global_init_script.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fbprophet = new databricks.Library(\"fbprophet\", {\n    clusterId: _this.id,\n    pypi: {\n        \"package\": \"fbprophet==0.6\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfbprophet = databricks.Library(\"fbprophet\",\n    cluster_id=this[\"id\"],\n    pypi=databricks.LibraryPypiArgs(\n        package=\"fbprophet==0.6\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fbprophet = new Databricks.Library(\"fbprophet\", new()\n    {\n        ClusterId = @this.Id,\n        Pypi = new Databricks.Inputs.LibraryPypiArgs\n        {\n            Package = \"fbprophet==0.6\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"fbprophet\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tPypi: \u0026databricks.LibraryPypiArgs{\n\t\t\t\tPackage: pulumi.String(\"fbprophet==0.6\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryPypiArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fbprophet = new Library(\"fbprophet\", LibraryArgs.builder()        \n            .clusterId(this_.id())\n            .pypi(LibraryPypiArgs.builder()\n                .package_(\"fbprophet==0.6\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fbprophet:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      pypi:\n        package: fbprophet==0.6\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## R CRan\n\nInstalling artifacts from CRan. You can also optionally specify a `repo` parameter for a custom cran mirror.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst rkeops = new databricks.Library(\"rkeops\", {\n    clusterId: _this.id,\n    cran: {\n        \"package\": \"rkeops\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nrkeops = databricks.Library(\"rkeops\",\n    cluster_id=this[\"id\"],\n    cran=databricks.LibraryCranArgs(\n        package=\"rkeops\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var rkeops = new Databricks.Library(\"rkeops\", new()\n    {\n        ClusterId = @this.Id,\n        Cran = new Databricks.Inputs.LibraryCranArgs\n        {\n            Package = \"rkeops\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewLibrary(ctx, \"rkeops\", \u0026databricks.LibraryArgs{\n\t\t\tClusterId: pulumi.Any(this.Id),\n\t\t\tCran: \u0026databricks.LibraryCranArgs{\n\t\t\t\tPackage: pulumi.String(\"rkeops\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Library;\nimport com.pulumi.databricks.LibraryArgs;\nimport com.pulumi.databricks.inputs.LibraryCranArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var rkeops = new Library(\"rkeops\", LibraryArgs.builder()        \n            .clusterId(this_.id())\n            .cran(LibraryCranArgs.builder()\n                .package_(\"rkeops\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  rkeops:\n    type: databricks:Library\n    properties:\n      clusterId: ${this.id}\n      cran:\n        package: rkeops\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getClusters data to retrieve a list of databricks.Cluster ids.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "clusterId": {
                    "type": "string"
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran"
                },
                "egg": {
                    "type": "string"
                },
                "jar": {
                    "type": "string"
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven"
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi"
                },
                "whl": {
                    "type": "string"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "cran": {
                    "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                    "willReplaceOnChanges": true
                },
                "egg": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "jar": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "maven": {
                    "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                    "willReplaceOnChanges": true
                },
                "pypi": {
                    "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                    "willReplaceOnChanges": true
                },
                "whl": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "clusterId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Library resources.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "cran": {
                        "$ref": "#/types/databricks:index/LibraryCran:LibraryCran",
                        "willReplaceOnChanges": true
                    },
                    "egg": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "jar": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "maven": {
                        "$ref": "#/types/databricks:index/LibraryMaven:LibraryMaven",
                        "willReplaceOnChanges": true
                    },
                    "pypi": {
                        "$ref": "#/types/databricks:index/LibraryPypi:LibraryPypi",
                        "willReplaceOnChanges": true
                    },
                    "whl": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastore:Metastore": {
            "description": "\u003e **Note** This resource could be used with account or workspace-level provider.\n\nA metastore is the top-level container of objects in Unity Catalog. It stores data assets (tables and views) and the permissions that govern access to them. Databricks account admins can create metastores and assign them to Databricks workspaces in order to control which workloads use each metastore.\n\nUnity Catalog offers a new metastore with built in security and auditing. This is distinct to the metastore used in previous versions of Databricks (based on the Hive Metastore).\n\nA Unity Catalog metastore can be created without a root location \u0026 credential to maintain strict separation of storage across catalogs or environments.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"this\", {\n    metastoreId: _this.id,\n    workspaceId: workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"this\",\n    metastore_id=this.id,\n    workspace_id=workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        WorkspaceId = workspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", metastore.Id)),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"this\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tWorkspaceId: pulumi.Any(workspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()        \n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()        \n            .metastoreId(this_.id())\n            .workspaceId(workspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      workspaceId: ${workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n## Import\n\nThis resource can be imported by ID:\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastore:Metastore this \u003cid\u003e\n```\n\n",
            "properties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string",
                    "description": "The region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n"
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "required": [
                "cloud",
                "createdAt",
                "createdBy",
                "globalMetastoreId",
                "metastoreId",
                "name",
                "owner",
                "region",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "cloud": {
                    "type": "string"
                },
                "createdAt": {
                    "type": "integer"
                },
                "createdBy": {
                    "type": "string"
                },
                "defaultDataAccessConfigId": {
                    "type": "string"
                },
                "deltaSharingOrganizationName": {
                    "type": "string",
                    "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                },
                "deltaSharingRecipientTokenLifetimeInSeconds": {
                    "type": "integer",
                    "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                },
                "deltaSharingScope": {
                    "type": "string",
                    "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Destroy metastore regardless of its contents.\n"
                },
                "globalMetastoreId": {
                    "type": "string"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of metastore.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the metastore owner.\n"
                },
                "region": {
                    "type": "string",
                    "description": "The region of the metastore\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n",
                    "willReplaceOnChanges": true
                },
                "storageRootCredentialId": {
                    "type": "string"
                },
                "updatedAt": {
                    "type": "integer"
                },
                "updatedBy": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Metastore resources.\n",
                "properties": {
                    "cloud": {
                        "type": "string"
                    },
                    "createdAt": {
                        "type": "integer"
                    },
                    "createdBy": {
                        "type": "string"
                    },
                    "defaultDataAccessConfigId": {
                        "type": "string"
                    },
                    "deltaSharingOrganizationName": {
                        "type": "string",
                        "description": "The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing. Once this is set it cannot be removed and can only be modified to another valid value. To delete this value please taint and recreate the resource.\n"
                    },
                    "deltaSharingRecipientTokenLifetimeInSeconds": {
                        "type": "integer",
                        "description": "Required along with `delta_sharing_scope`. Used to set expiration duration in seconds on recipient data access tokens. Set to 0 for unlimited duration.\n"
                    },
                    "deltaSharingScope": {
                        "type": "string",
                        "description": "Required along with `delta_sharing_recipient_token_lifetime_in_seconds`. Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.  INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Destroy metastore regardless of its contents.\n"
                    },
                    "globalMetastoreId": {
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of metastore.\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the metastore owner.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "The region of the metastore\n"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource. If no `storage_root` is defined for the metastore, each catalog must have a `storage_root` defined.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageRootCredentialId": {
                        "type": "string"
                    },
                    "updatedAt": {
                        "type": "integer"
                    },
                    "updatedBy": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreAssignment:MetastoreAssignment": {
            "description": "\u003e **Note** This resource could be only used with account-level provider!\n\nA single databricks.Metastore can be shared across Databricks workspaces, and each linked workspace has a consistent view of the data and a single set of access policies. You can only create a single metastore for each region in which your organization operates.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreAssignment = new databricks.MetastoreAssignment(\"this\", {\n    metastoreId: _this.id,\n    workspaceId: workspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_assignment = databricks.MetastoreAssignment(\"this\",\n    metastore_id=this.id,\n    workspace_id=workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreAssignment = new Databricks.MetastoreAssignment(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        WorkspaceId = workspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", metastore.Id)),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreAssignment(ctx, \"this\", \u0026databricks.MetastoreAssignmentArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tWorkspaceId: pulumi.Any(workspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreAssignment;\nimport com.pulumi.databricks.MetastoreAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()        \n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreAssignment = new MetastoreAssignment(\"thisMetastoreAssignment\", MetastoreAssignmentArgs.builder()        \n            .metastoreId(this_.id())\n            .workspaceId(workspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreAssignment:\n    type: databricks:MetastoreAssignment\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      workspaceId: ${workspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by combination of workspace id and metastore id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastoreAssignment:MetastoreAssignment this '\u003cworkspace_id\u003e|\u003cmetastore_id\u003e'\n```\n\n",
            "properties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "id of the workspace for the assignment\n"
                }
            },
            "required": [
                "metastoreId",
                "workspaceId"
            ],
            "inputProperties": {
                "defaultCatalogName": {
                    "type": "string",
                    "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "id of the workspace for the assignment\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "metastoreId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreAssignment resources.\n",
                "properties": {
                    "defaultCatalogName": {
                        "type": "string",
                        "description": "Default catalog used for this assignment, default to `hive_metastore`\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "id of the workspace for the assignment\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreDataAccess:MetastoreDataAccess": {
            "description": "\u003e **Note** This resource could be used with account or workspace-level provider.\n\nOptionally, each databricks.Metastore can have a default databricks.StorageCredential defined as `databricks.MetastoreDataAccess`. This will be used by Unity Catalog to access data in the root storage location if defined.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: \"uc admins\",\n    region: \"us-east-1\",\n    forceDestroy: true,\n});\nconst thisMetastoreDataAccess = new databricks.MetastoreDataAccess(\"this\", {\n    metastoreId: _this.id,\n    name: metastoreDataAccess.name,\n    awsIamRole: {\n        roleArn: metastoreDataAccess.arn,\n    },\n    isDefault: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=\"uc admins\",\n    region=\"us-east-1\",\n    force_destroy=True)\nthis_metastore_data_access = databricks.MetastoreDataAccess(\"this\",\n    metastore_id=this.id,\n    name=metastore_data_access[\"name\"],\n    aws_iam_role=databricks.MetastoreDataAccessAwsIamRoleArgs(\n        role_arn=metastore_data_access[\"arn\"],\n    ),\n    is_default=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = \"uc admins\",\n        Region = \"us-east-1\",\n        ForceDestroy = true,\n    });\n\n    var thisMetastoreDataAccess = new Databricks.MetastoreDataAccess(\"this\", new()\n    {\n        MetastoreId = @this.Id,\n        Name = metastoreDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.MetastoreDataAccessAwsIamRoleArgs\n        {\n            RoleArn = metastoreDataAccess.Arn,\n        },\n        IsDefault = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", metastore.Id)),\n\t\t\tOwner:        pulumi.String(\"uc admins\"),\n\t\t\tRegion:       pulumi.String(\"us-east-1\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMetastoreDataAccess(ctx, \"this\", \u0026databricks.MetastoreDataAccessArgs{\n\t\t\tMetastoreId: this.ID(),\n\t\t\tName:        pulumi.Any(metastoreDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.MetastoreDataAccessAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(metastoreDataAccess.Arn),\n\t\t\t},\n\t\t\tIsDefault: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.MetastoreDataAccess;\nimport com.pulumi.databricks.MetastoreDataAccessArgs;\nimport com.pulumi.databricks.inputs.MetastoreDataAccessAwsIamRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Metastore(\"this\", MetastoreArgs.builder()        \n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(\"uc admins\")\n            .region(\"us-east-1\")\n            .forceDestroy(true)\n            .build());\n\n        var thisMetastoreDataAccess = new MetastoreDataAccess(\"thisMetastoreDataAccess\", MetastoreDataAccessArgs.builder()        \n            .metastoreId(this_.id())\n            .name(metastoreDataAccess.name())\n            .awsIamRole(MetastoreDataAccessAwsIamRoleArgs.builder()\n                .roleArn(metastoreDataAccess.arn())\n                .build())\n            .isDefault(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Metastore\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: uc admins\n      region: us-east-1\n      forceDestroy: true\n  thisMetastoreDataAccess:\n    type: databricks:MetastoreDataAccess\n    name: this\n    properties:\n      metastoreId: ${this.id}\n      name: ${metastoreDataAccess.name}\n      awsIamRole:\n        roleArn: ${metastoreDataAccess.arn}\n      isDefault: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure using managed identity as credential (recommended)\n\n## Import\n\nThis resource can be imported by combination of metastore id and the data access name.\n\nbash\n\n```sh\n$ pulumi import databricks:index/metastoreDataAccess:MetastoreDataAccess this '\u003cmetastore_id\u003e|\u003cname\u003e'\n```\n\n",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean"
                },
                "forceUpdate": {
                    "type": "boolean"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey"
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "readOnly": {
                    "type": "boolean"
                },
                "skipValidation": {
                    "type": "boolean"
                }
            },
            "required": [
                "databricksGcpServiceAccount",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                    "willReplaceOnChanges": true
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                    "willReplaceOnChanges": true
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "forceUpdate": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                    "willReplaceOnChanges": true
                },
                "isDefault": {
                    "type": "boolean",
                    "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n",
                    "willReplaceOnChanges": true
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string"
                },
                "readOnly": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "skipValidation": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreDataAccess resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAwsIamRole:MetastoreDataAccessAwsIamRole",
                        "willReplaceOnChanges": true
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureManagedIdentity:MetastoreDataAccessAzureManagedIdentity",
                        "willReplaceOnChanges": true
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessAzureServicePrincipal:MetastoreDataAccessAzureServicePrincipal",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "databricksGcpServiceAccount": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessDatabricksGcpServiceAccount:MetastoreDataAccessDatabricksGcpServiceAccount"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/MetastoreDataAccessGcpServiceAccountKey:MetastoreDataAccessGcpServiceAccountKey",
                        "willReplaceOnChanges": true
                    },
                    "isDefault": {
                        "type": "boolean",
                        "description": "whether to set this credential as the default for the metastore. In practice, this should always be true.\n",
                        "willReplaceOnChanges": true
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/metastoreProvider:MetastoreProvider": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nIn Delta Sharing, a provider is an entity that shares data with a recipient. Within a metastore, Unity Catalog provides the ability to create a provider which contains a list of shares that have been shared with you.\n\nA `databricks.MetastoreProvider` is contained within databricks.Metastore and can contain a list of shares that have been shared with you.\n\nNote that Databricks to Databricks sharing automatically creates the provider.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dbprovider = new databricks.MetastoreProvider(\"dbprovider\", {\n    name: \"terraform-test-provider\",\n    comment: \"made by terraform 2\",\n    authenticationType: \"TOKEN\",\n    recipientProfileStr: JSON.stringify({\n        shareCredentialsVersion: 1,\n        bearerToken: \"token\",\n        endpoint: \"endpoint\",\n        expirationTime: \"expiration-time\",\n    }),\n});\n```\n```python\nimport pulumi\nimport json\nimport pulumi_databricks as databricks\n\ndbprovider = databricks.MetastoreProvider(\"dbprovider\",\n    name=\"terraform-test-provider\",\n    comment=\"made by terraform 2\",\n    authentication_type=\"TOKEN\",\n    recipient_profile_str=json.dumps({\n        \"shareCredentialsVersion\": 1,\n        \"bearerToken\": \"token\",\n        \"endpoint\": \"endpoint\",\n        \"expirationTime\": \"expiration-time\",\n    }))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Text.Json;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dbprovider = new Databricks.MetastoreProvider(\"dbprovider\", new()\n    {\n        Name = \"terraform-test-provider\",\n        Comment = \"made by terraform 2\",\n        AuthenticationType = \"TOKEN\",\n        RecipientProfileStr = JsonSerializer.Serialize(new Dictionary\u003cstring, object?\u003e\n        {\n            [\"shareCredentialsVersion\"] = 1,\n            [\"bearerToken\"] = \"token\",\n            [\"endpoint\"] = \"endpoint\",\n            [\"expirationTime\"] = \"expiration-time\",\n        }),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"encoding/json\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttmpJSON0, err := json.Marshal(map[string]interface{}{\n\t\t\t\"shareCredentialsVersion\": 1,\n\t\t\t\"bearerToken\":             \"token\",\n\t\t\t\"endpoint\":                \"endpoint\",\n\t\t\t\"expirationTime\":          \"expiration-time\",\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tjson0 := string(tmpJSON0)\n\t\t_, err = databricks.NewMetastoreProvider(ctx, \"dbprovider\", \u0026databricks.MetastoreProviderArgs{\n\t\t\tName:                pulumi.String(\"terraform-test-provider\"),\n\t\t\tComment:             pulumi.String(\"made by terraform 2\"),\n\t\t\tAuthenticationType:  pulumi.String(\"TOKEN\"),\n\t\t\tRecipientProfileStr: pulumi.String(json0),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MetastoreProvider;\nimport com.pulumi.databricks.MetastoreProviderArgs;\nimport static com.pulumi.codegen.internal.Serialization.*;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dbprovider = new MetastoreProvider(\"dbprovider\", MetastoreProviderArgs.builder()        \n            .name(\"terraform-test-provider\")\n            .comment(\"made by terraform 2\")\n            .authenticationType(\"TOKEN\")\n            .recipientProfileStr(serializeJson(\n                jsonObject(\n                    jsonProperty(\"shareCredentialsVersion\", 1),\n                    jsonProperty(\"bearerToken\", \"token\"),\n                    jsonProperty(\"endpoint\", \"endpoint\"),\n                    jsonProperty(\"expirationTime\", \"expiration-time\")\n                )))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dbprovider:\n    type: databricks:MetastoreProvider\n    properties:\n      name: terraform-test-provider\n      comment: made by terraform 2\n      authenticationType: TOKEN\n      recipientProfileStr:\n        fn::toJSON:\n          shareCredentialsVersion: 1\n          bearerToken: token\n          endpoint: endpoint\n          expirationTime: expiration-time\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n",
            "properties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n"
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "required": [
                "authenticationType",
                "name",
                "recipientProfileStr"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the provider.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of provider. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "recipientProfileStr": {
                    "type": "string",
                    "description": "This is the json file that is created from a recipient url.\n",
                    "secret": true
                }
            },
            "requiredInputs": [
                "authenticationType",
                "recipientProfileStr"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MetastoreProvider resources.\n",
                "properties": {
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN`.\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the provider.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of provider. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "recipientProfileStr": {
                        "type": "string",
                        "description": "This is the json file that is created from a recipient url.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowExperiment:MlflowExperiment": {
            "description": "This resource allows you to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.MlflowExperiment(\"this\", {\n    name: me.then(me =\u003e `${me.home}/Sample`),\n    artifactLocation: \"dbfs:/tmp/my-experiment\",\n    description: \"My MLflow experiment description\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.MlflowExperiment(\"this\",\n    name=f\"{me.home}/Sample\",\n    artifact_location=\"dbfs:/tmp/my-experiment\",\n    description=\"My MLflow experiment description\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.MlflowExperiment(\"this\", new()\n    {\n        Name = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/Sample\",\n        ArtifactLocation = \"dbfs:/tmp/my-experiment\",\n        Description = \"My MLflow experiment description\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowExperiment(ctx, \"this\", \u0026databricks.MlflowExperimentArgs{\n\t\t\tName:             pulumi.String(fmt.Sprintf(\"%v/Sample\", me.Home)),\n\t\t\tArtifactLocation: pulumi.String(\"dbfs:/tmp/my-experiment\"),\n\t\t\tDescription:      pulumi.String(\"My MLflow experiment description\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.MlflowExperiment;\nimport com.pulumi.databricks.MlflowExperimentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new MlflowExperiment(\"this\", MlflowExperimentArgs.builder()        \n            .name(String.format(\"%s/Sample\", me.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.home())))\n            .artifactLocation(\"dbfs:/tmp/my-experiment\")\n            .description(\"My MLflow experiment description\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MlflowExperiment\n    properties:\n      name: ${me.home}/Sample\n      artifactLocation: dbfs:/tmp/my-experiment\n      description: My MLflow experiment description\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, or *Manage* individual experiments.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create models in the [workspace model registry](https://docs.databricks.com/en/mlflow/model-registry.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe experiment resource can be imported using the id of the experiment\n\nbash\n\n```sh\n$ pulumi import databricks:index/mlflowExperiment:MlflowExperiment this \u003cexperiment-id\u003e\n```\n\n",
            "properties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "required": [
                "creationTime",
                "experimentId",
                "lastUpdateTime",
                "lifecycleStage",
                "name"
            ],
            "inputProperties": {
                "artifactLocation": {
                    "type": "string",
                    "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow experiment.\n"
                },
                "experimentId": {
                    "type": "string"
                },
                "lastUpdateTime": {
                    "type": "integer"
                },
                "lifecycleStage": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowExperiment resources.\n",
                "properties": {
                    "artifactLocation": {
                        "type": "string",
                        "description": "Path to dbfs:/ or s3:// artifact location of the MLflow experiment.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow experiment.\n"
                    },
                    "experimentId": {
                        "type": "string"
                    },
                    "lastUpdateTime": {
                        "type": "integer"
                    },
                    "lifecycleStage": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow experiment. It must be an absolute path within the Databricks workspace, e.g. `/Users/\u003csome-username\u003e/my-experiment`. For more information about changes to experiment naming conventions, see [mlflow docs](https://docs.databricks.com/applications/mlflow/experiments.html#experiment-migration).\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowModel:MlflowModel": {
            "description": "This resource allows you to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n\n**Note** This documentation covers the Workspace Model Registry. Databricks recommends using Models in Unity Catalog. Models in Unity Catalog provides centralized model governance, cross-workspace access, lineage, and deployment.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst test = new databricks.MlflowModel(\"test\", {\n    name: \"My MLflow Model\",\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntest = databricks.MlflowModel(\"test\",\n    name=\"My MLflow Model\",\n    description=\"My MLflow model description\",\n    tags=[\n        databricks.MlflowModelTagArgs(\n            key=\"key1\",\n            value=\"value1\",\n        ),\n        databricks.MlflowModelTagArgs(\n            key=\"key2\",\n            value=\"value2\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var test = new Databricks.MlflowModel(\"test\", new()\n    {\n        Name = \"My MLflow Model\",\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"test\", \u0026databricks.MlflowModelArgs{\n\t\t\tName:        pulumi.String(\"My MLflow Model\"),\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: databricks.MlflowModelTagArray{\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var test = new MlflowModel(\"test\", MlflowModelArgs.builder()        \n            .name(\"My MLflow Model\")\n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  test:\n    type: databricks:MlflowModel\n    properties:\n      name: My MLflow Model\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Read*, *Edit*, *Manage Staging Versions*, *Manage Production Versions*, and *Manage* individual models.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.Directory to manage directories in [Databricks Workspace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe model resource can be imported using the name\n\nbash\n\n```sh\n$ pulumi import databricks:index/mlflowModel:MlflowModel this \u003cname\u003e\n```\n\n",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                }
            },
            "required": [
                "name",
                "registeredModelId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "The description of the MLflow model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of MLflow model. Change of name triggers new resource.\n",
                    "willReplaceOnChanges": true
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                    },
                    "description": "Tags for the MLflow model.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowModel resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "The description of the MLflow model.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of MLflow model. Change of name triggers new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MlflowModelTag:MlflowModelTag"
                        },
                        "description": "Tags for the MLflow model.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mlflowWebhook:MlflowWebhook": {
            "description": "This resource allows you to create [MLflow Model Registry Webhooks](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) in Databricks.  Webhooks enable you to listen for Model Registry events so your integrations can automatically trigger actions. You can use webhooks to automate and integrate your machine learning pipeline with existing CI/CD tools and workflows. Webhooks allow trigger execution of a Databricks job or call a web service on specific event(s) that is generated in the MLflow Registry - stage transitioning, creation of registered model, creation of transition request, etc.\n\n## Example Usage\n\n### Triggering Databricks job\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as std from \"@pulumi/std\";\n\nconst me = databricks.getCurrentUser({});\nconst latest = databricks.getSparkVersion({});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst _this = new databricks.Notebook(\"this\", {\n    path: me.then(me =\u003e `${me.home}/MLFlowWebhook`),\n    language: \"PYTHON\",\n    contentBase64: std.base64encode({\n        input: `import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n`,\n    }).then(invoke =\u003e invoke.result),\n});\nconst thisJob = new databricks.Job(\"this\", {\n    name: me.then(me =\u003e `Terraform MLflowWebhook Demo (${me.alphanumeric})`),\n    tasks: [{\n        taskKey: \"task1\",\n        newCluster: {\n            numWorkers: 1,\n            sparkVersion: latest.then(latest =\u003e latest.id),\n            nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n        },\n        notebookTask: {\n            notebookPath: _this.path,\n        },\n    }],\n});\nconst patForWebhook = new databricks.Token(\"pat_for_webhook\", {\n    comment: \"MLflow Webhook\",\n    lifetimeSeconds: 86400000,\n});\nconst job = new databricks.MlflowWebhook(\"job\", {\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    description: \"Databricks Job webhook trigger\",\n    status: \"ACTIVE\",\n    jobSpec: {\n        jobId: thisJob.id,\n        workspaceUrl: me.then(me =\u003e me.workspaceUrl),\n        accessToken: patForWebhook.tokenValue,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_std as std\n\nme = databricks.get_current_user()\nlatest = databricks.get_spark_version()\nsmallest = databricks.get_node_type(local_disk=True)\nthis = databricks.Notebook(\"this\",\n    path=f\"{me.home}/MLFlowWebhook\",\n    language=\"PYTHON\",\n    content_base64=std.base64encode(input=\"\"\"import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n\"\"\").result)\nthis_job = databricks.Job(\"this\",\n    name=f\"Terraform MLflowWebhook Demo ({me.alphanumeric})\",\n    tasks=[databricks.JobTaskArgs(\n        task_key=\"task1\",\n        new_cluster=databricks.JobTaskNewClusterArgs(\n            num_workers=1,\n            spark_version=latest.id,\n            node_type_id=smallest.id,\n        ),\n        notebook_task=databricks.JobTaskNotebookTaskArgs(\n            notebook_path=this.path,\n        ),\n    )])\npat_for_webhook = databricks.Token(\"pat_for_webhook\",\n    comment=\"MLflow Webhook\",\n    lifetime_seconds=86400000)\njob = databricks.MlflowWebhook(\"job\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    description=\"Databricks Job webhook trigger\",\n    status=\"ACTIVE\",\n    job_spec=databricks.MlflowWebhookJobSpecArgs(\n        job_id=this_job.id,\n        workspace_url=me.workspace_url,\n        access_token=pat_for_webhook.token_value,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Std = Pulumi.Std;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var @this = new Databricks.Notebook(\"this\", new()\n    {\n        Path = $\"{me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Home)}/MLFlowWebhook\",\n        Language = \"PYTHON\",\n        ContentBase64 = Std.Base64encode.Invoke(new()\n        {\n            Input = @\"import json\n \nevent_message = dbutils.widgets.get(\"\"event_message\"\")\nevent_message_dict = json.loads(event_message)\nprint(f\"\"event data={event_message_dict}\"\")\n\",\n        }).Apply(invoke =\u003e invoke.Result),\n    });\n\n    var thisJob = new Databricks.Job(\"this\", new()\n    {\n        Name = $\"Terraform MLflowWebhook Demo ({me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)})\",\n        Tasks = new[]\n        {\n            new Databricks.Inputs.JobTaskArgs\n            {\n                TaskKey = \"task1\",\n                NewCluster = new Databricks.Inputs.JobTaskNewClusterArgs\n                {\n                    NumWorkers = 1,\n                    SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n                    NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n                },\n                NotebookTask = new Databricks.Inputs.JobTaskNotebookTaskArgs\n                {\n                    NotebookPath = @this.Path,\n                },\n            },\n        },\n    });\n\n    var patForWebhook = new Databricks.Token(\"pat_for_webhook\", new()\n    {\n        Comment = \"MLflow Webhook\",\n        LifetimeSeconds = 86400000,\n    });\n\n    var job = new Databricks.MlflowWebhook(\"job\", new()\n    {\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        Description = \"Databricks Job webhook trigger\",\n        Status = \"ACTIVE\",\n        JobSpec = new Databricks.Inputs.MlflowWebhookJobSpecArgs\n        {\n            JobId = thisJob.Id,\n            WorkspaceUrl = me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.WorkspaceUrl),\n            AccessToken = patForWebhook.TokenValue,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-std/sdk/go/std\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tinvokeBase64encode, err := std.Base64encode(ctx, \u0026std.Base64encodeArgs{\n\t\t\tInput: `import json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n`,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewNotebook(ctx, \"this\", \u0026databricks.NotebookArgs{\n\t\t\tPath:          pulumi.String(fmt.Sprintf(\"%v/MLFlowWebhook\", me.Home)),\n\t\t\tLanguage:      pulumi.String(\"PYTHON\"),\n\t\t\tContentBase64: invokeBase64encode.Result,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisJob, err := databricks.NewJob(ctx, \"this\", \u0026databricks.JobArgs{\n\t\t\tName: pulumi.String(fmt.Sprintf(\"Terraform MLflowWebhook Demo (%v)\", me.Alphanumeric)),\n\t\t\tTasks: databricks.JobTaskArray{\n\t\t\t\t\u0026databricks.JobTaskArgs{\n\t\t\t\t\tTaskKey: pulumi.String(\"task1\"),\n\t\t\t\t\tNewCluster: \u0026databricks.JobTaskNewClusterArgs{\n\t\t\t\t\t\tNumWorkers:   pulumi.Int(1),\n\t\t\t\t\t\tSparkVersion: pulumi.String(latest.Id),\n\t\t\t\t\t\tNodeTypeId:   pulumi.String(smallest.Id),\n\t\t\t\t\t},\n\t\t\t\t\tNotebookTask: \u0026databricks.JobTaskNotebookTaskArgs{\n\t\t\t\t\t\tNotebookPath: this.Path,\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpatForWebhook, err := databricks.NewToken(ctx, \"pat_for_webhook\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"MLflow Webhook\"),\n\t\t\tLifetimeSeconds: pulumi.Int(86400000),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMlflowWebhook(ctx, \"job\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tDescription: pulumi.String(\"Databricks Job webhook trigger\"),\n\t\t\tStatus:      pulumi.String(\"ACTIVE\"),\n\t\t\tJobSpec: \u0026databricks.MlflowWebhookJobSpecArgs{\n\t\t\t\tJobId:        thisJob.ID(),\n\t\t\t\tWorkspaceUrl: pulumi.String(me.WorkspaceUrl),\n\t\t\t\tAccessToken:  patForWebhook.TokenValue,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.NotebookArgs;\nimport com.pulumi.databricks.Job;\nimport com.pulumi.databricks.JobArgs;\nimport com.pulumi.databricks.inputs.JobTaskArgs;\nimport com.pulumi.databricks.inputs.JobTaskNewClusterArgs;\nimport com.pulumi.databricks.inputs.JobTaskNotebookTaskArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookJobSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        final var latest = DatabricksFunctions.getSparkVersion();\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        var this_ = new Notebook(\"this\", NotebookArgs.builder()        \n            .path(String.format(\"%s/MLFlowWebhook\", me.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.home())))\n            .language(\"PYTHON\")\n            .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()\n                .input(\"\"\"\nimport json\n \nevent_message = dbutils.widgets.get(\"event_message\")\nevent_message_dict = json.loads(event_message)\nprint(f\"event data={event_message_dict}\")\n                \"\"\")\n                .build()).result())\n            .build());\n\n        var thisJob = new Job(\"thisJob\", JobArgs.builder()        \n            .name(String.format(\"Terraform MLflowWebhook Demo (%s)\", me.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.alphanumeric())))\n            .tasks(JobTaskArgs.builder()\n                .taskKey(\"task1\")\n                .newCluster(JobTaskNewClusterArgs.builder()\n                    .numWorkers(1)\n                    .sparkVersion(latest.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n                    .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n                    .build())\n                .notebookTask(JobTaskNotebookTaskArgs.builder()\n                    .notebookPath(this_.path())\n                    .build())\n                .build())\n            .build());\n\n        var patForWebhook = new Token(\"patForWebhook\", TokenArgs.builder()        \n            .comment(\"MLflow Webhook\")\n            .lifetimeSeconds(86400000)\n            .build());\n\n        var job = new MlflowWebhook(\"job\", MlflowWebhookArgs.builder()        \n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .description(\"Databricks Job webhook trigger\")\n            .status(\"ACTIVE\")\n            .jobSpec(MlflowWebhookJobSpecArgs.builder()\n                .jobId(thisJob.id())\n                .workspaceUrl(me.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.workspaceUrl()))\n                .accessToken(patForWebhook.tokenValue())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Notebook\n    properties:\n      path: ${me.home}/MLFlowWebhook\n      language: PYTHON\n      contentBase64:\n        fn::invoke:\n          Function: std:base64encode\n          Arguments:\n            input: \"import json\\n \\nevent_message = dbutils.widgets.get(\\\"event_message\\\")\\nevent_message_dict = json.loads(event_message)\\nprint(f\\\"event data={event_message_dict}\\\")\\n\"\n          Return: result\n  thisJob:\n    type: databricks:Job\n    name: this\n    properties:\n      name: Terraform MLflowWebhook Demo (${me.alphanumeric})\n      tasks:\n        - taskKey: task1\n          newCluster:\n            numWorkers: 1\n            sparkVersion: ${latest.id}\n            nodeTypeId: ${smallest.id}\n          notebookTask:\n            notebookPath: ${this.path}\n  patForWebhook:\n    type: databricks:Token\n    name: pat_for_webhook\n    properties:\n      comment: MLflow Webhook\n      lifetimeSeconds: 8.64e+07\n  job:\n    type: databricks:MlflowWebhook\n    properties:\n      events:\n        - TRANSITION_REQUEST_CREATED\n      description: Databricks Job webhook trigger\n      status: ACTIVE\n      jobSpec:\n        jobId: ${thisJob.id}\n        workspaceUrl: ${me.workspaceUrl}\n        accessToken: ${patForWebhook.tokenValue}\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n  latest:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments: {}\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### POSTing to URL\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst url = new databricks.MlflowWebhook(\"url\", {\n    events: [\"TRANSITION_REQUEST_CREATED\"],\n    description: \"URL webhook trigger\",\n    httpUrlSpec: {\n        url: \"https://my_cool_host/webhook\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nurl = databricks.MlflowWebhook(\"url\",\n    events=[\"TRANSITION_REQUEST_CREATED\"],\n    description=\"URL webhook trigger\",\n    http_url_spec=databricks.MlflowWebhookHttpUrlSpecArgs(\n        url=\"https://my_cool_host/webhook\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var url = new Databricks.MlflowWebhook(\"url\", new()\n    {\n        Events = new[]\n        {\n            \"TRANSITION_REQUEST_CREATED\",\n        },\n        Description = \"URL webhook trigger\",\n        HttpUrlSpec = new Databricks.Inputs.MlflowWebhookHttpUrlSpecArgs\n        {\n            Url = \"https://my_cool_host/webhook\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowWebhook(ctx, \"url\", \u0026databricks.MlflowWebhookArgs{\n\t\t\tEvents: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"TRANSITION_REQUEST_CREATED\"),\n\t\t\t},\n\t\t\tDescription: pulumi.String(\"URL webhook trigger\"),\n\t\t\tHttpUrlSpec: \u0026databricks.MlflowWebhookHttpUrlSpecArgs{\n\t\t\t\tUrl: pulumi.String(\"https://my_cool_host/webhook\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowWebhook;\nimport com.pulumi.databricks.MlflowWebhookArgs;\nimport com.pulumi.databricks.inputs.MlflowWebhookHttpUrlSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var url = new MlflowWebhook(\"url\", MlflowWebhookArgs.builder()        \n            .events(\"TRANSITION_REQUEST_CREATED\")\n            .description(\"URL webhook trigger\")\n            .httpUrlSpec(MlflowWebhookHttpUrlSpecArgs.builder()\n                .url(\"https://my_cool_host/webhook\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  url:\n    type: databricks:MlflowWebhook\n    properties:\n      events:\n        - TRANSITION_REQUEST_CREATED\n      description: URL webhook trigger\n      httpUrlSpec:\n        url: https://my_cool_host/webhook\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* MLflow webhooks could be configured only by workspace admins.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.MlflowModel to create [MLflow models](https://docs.databricks.com/applications/mlflow/models.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "required": [
                "events"
            ],
            "inputProperties": {
                "description": {
                    "type": "string",
                    "description": "Optional description of the MLflow webhook.\n"
                },
                "events": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                },
                "httpUrlSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                },
                "jobSpec": {
                    "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                },
                "modelName": {
                    "type": "string",
                    "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                }
            },
            "requiredInputs": [
                "events"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MlflowWebhook resources.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "Optional description of the MLflow webhook.\n"
                    },
                    "events": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of events that will trigger execution of Databricks job or POSTing to an URL, for example, `MODEL_VERSION_CREATED`, `MODEL_VERSION_TRANSITIONED_STAGE`, `TRANSITION_REQUEST_CREATED`, etc.  Refer to the [Webhooks API documentation](https://docs.databricks.com/dev-tools/api/latest/mlflow.html#operation/create-registry-webhook) for a full list of supported events.\n\nConfiguration must include one of `http_url_spec` or `job_spec` blocks, but not both.\n"
                    },
                    "httpUrlSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookHttpUrlSpec:MlflowWebhookHttpUrlSpec"
                    },
                    "jobSpec": {
                        "$ref": "#/types/databricks:index/MlflowWebhookJobSpec:MlflowWebhookJobSpec"
                    },
                    "modelName": {
                        "type": "string",
                        "description": "Name of MLflow model for which webhook will be created. If the model name is not specified, a registry-wide webhook is created that listens for the specified events across all versions of all registered models.\n"
                    },
                    "status": {
                        "type": "string",
                        "description": "Optional status of webhook. Possible values are `ACTIVE`, `TEST_MODE`, `DISABLED`. Default is `ACTIVE`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/modelServing:ModelServing": {
            "description": "This resource allows you to manage [Model Serving](https://docs.databricks.com/machine-learning/model-serving/index.html) endpoints in Databricks.\n\n**Note** If you replace `served_models` with `served_entities` in an existing serving endpoint, the serving endpoint will briefly go into an update state (~30 seconds) and increment the config version.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.ModelServing(\"this\", {\n    name: \"ads-serving-endpoint\",\n    config: {\n        servedEntities: [\n            {\n                name: \"prod_model\",\n                entityName: \"ads-model\",\n                entityVersion: \"2\",\n                workloadSize: \"Small\",\n                scaleToZeroEnabled: true,\n            },\n            {\n                name: \"candidate_model\",\n                entityName: \"ads-model\",\n                entityVersion: \"4\",\n                workloadSize: \"Small\",\n                scaleToZeroEnabled: false,\n            },\n        ],\n        trafficConfig: {\n            routes: [\n                {\n                    servedModelName: \"prod_model\",\n                    trafficPercentage: 90,\n                },\n                {\n                    servedModelName: \"candidate_model\",\n                    trafficPercentage: 10,\n                },\n            ],\n        },\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.ModelServing(\"this\",\n    name=\"ads-serving-endpoint\",\n    config=databricks.ModelServingConfigArgs(\n        served_entities=[\n            databricks.ModelServingConfigServedEntityArgs(\n                name=\"prod_model\",\n                entity_name=\"ads-model\",\n                entity_version=\"2\",\n                workload_size=\"Small\",\n                scale_to_zero_enabled=True,\n            ),\n            databricks.ModelServingConfigServedEntityArgs(\n                name=\"candidate_model\",\n                entity_name=\"ads-model\",\n                entity_version=\"4\",\n                workload_size=\"Small\",\n                scale_to_zero_enabled=False,\n            ),\n        ],\n        traffic_config=databricks.ModelServingConfigTrafficConfigArgs(\n            routes=[\n                databricks.ModelServingConfigTrafficConfigRouteArgs(\n                    served_model_name=\"prod_model\",\n                    traffic_percentage=90,\n                ),\n                databricks.ModelServingConfigTrafficConfigRouteArgs(\n                    served_model_name=\"candidate_model\",\n                    traffic_percentage=10,\n                ),\n            ],\n        ),\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.ModelServing(\"this\", new()\n    {\n        Name = \"ads-serving-endpoint\",\n        Config = new Databricks.Inputs.ModelServingConfigArgs\n        {\n            ServedEntities = new[]\n            {\n                new Databricks.Inputs.ModelServingConfigServedEntityArgs\n                {\n                    Name = \"prod_model\",\n                    EntityName = \"ads-model\",\n                    EntityVersion = \"2\",\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = true,\n                },\n                new Databricks.Inputs.ModelServingConfigServedEntityArgs\n                {\n                    Name = \"candidate_model\",\n                    EntityName = \"ads-model\",\n                    EntityVersion = \"4\",\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = false,\n                },\n            },\n            TrafficConfig = new Databricks.Inputs.ModelServingConfigTrafficConfigArgs\n            {\n                Routes = new[]\n                {\n                    new Databricks.Inputs.ModelServingConfigTrafficConfigRouteArgs\n                    {\n                        ServedModelName = \"prod_model\",\n                        TrafficPercentage = 90,\n                    },\n                    new Databricks.Inputs.ModelServingConfigTrafficConfigRouteArgs\n                    {\n                        ServedModelName = \"candidate_model\",\n                        TrafficPercentage = 10,\n                    },\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewModelServing(ctx, \"this\", \u0026databricks.ModelServingArgs{\n\t\t\tName: pulumi.String(\"ads-serving-endpoint\"),\n\t\t\tConfig: \u0026databricks.ModelServingConfigArgs{\n\t\t\t\tServedEntities: databricks.ModelServingConfigServedEntityArray{\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedEntityArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"prod_model\"),\n\t\t\t\t\t\tEntityName:         pulumi.String(\"ads-model\"),\n\t\t\t\t\t\tEntityVersion:      pulumi.String(\"2\"),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(true),\n\t\t\t\t\t},\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedEntityArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"candidate_model\"),\n\t\t\t\t\t\tEntityName:         pulumi.String(\"ads-model\"),\n\t\t\t\t\t\tEntityVersion:      pulumi.String(\"4\"),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(false),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\tTrafficConfig: \u0026databricks.ModelServingConfigTrafficConfigArgs{\n\t\t\t\t\tRoutes: databricks.ModelServingConfigTrafficConfigRouteArray{\n\t\t\t\t\t\t\u0026databricks.ModelServingConfigTrafficConfigRouteArgs{\n\t\t\t\t\t\t\tServedModelName:   pulumi.String(\"prod_model\"),\n\t\t\t\t\t\t\tTrafficPercentage: pulumi.Int(90),\n\t\t\t\t\t\t},\n\t\t\t\t\t\t\u0026databricks.ModelServingConfigTrafficConfigRouteArgs{\n\t\t\t\t\t\t\tServedModelName:   pulumi.String(\"candidate_model\"),\n\t\t\t\t\t\t\tTrafficPercentage: pulumi.Int(10),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ModelServing;\nimport com.pulumi.databricks.ModelServingArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigTrafficConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new ModelServing(\"this\", ModelServingArgs.builder()        \n            .name(\"ads-serving-endpoint\")\n            .config(ModelServingConfigArgs.builder()\n                .servedEntities(                \n                    ModelServingConfigServedEntityArgs.builder()\n                        .name(\"prod_model\")\n                        .entityName(\"ads-model\")\n                        .entityVersion(\"2\")\n                        .workloadSize(\"Small\")\n                        .scaleToZeroEnabled(true)\n                        .build(),\n                    ModelServingConfigServedEntityArgs.builder()\n                        .name(\"candidate_model\")\n                        .entityName(\"ads-model\")\n                        .entityVersion(\"4\")\n                        .workloadSize(\"Small\")\n                        .scaleToZeroEnabled(false)\n                        .build())\n                .trafficConfig(ModelServingConfigTrafficConfigArgs.builder()\n                    .routes(                    \n                        ModelServingConfigTrafficConfigRouteArgs.builder()\n                            .servedModelName(\"prod_model\")\n                            .trafficPercentage(90)\n                            .build(),\n                        ModelServingConfigTrafficConfigRouteArgs.builder()\n                            .servedModelName(\"candidate_model\")\n                            .trafficPercentage(10)\n                            .build())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:ModelServing\n    properties:\n      name: ads-serving-endpoint\n      config:\n        servedEntities:\n          - name: prod_model\n            entityName: ads-model\n            entityVersion: '2'\n            workloadSize: Small\n            scaleToZeroEnabled: true\n          - name: candidate_model\n            entityName: ads-model\n            entityVersion: '4'\n            workloadSize: Small\n            scaleToZeroEnabled: false\n        trafficConfig:\n          routes:\n            - servedModelName: prod_model\n              trafficPercentage: 90\n            - servedModelName: candidate_model\n              trafficPercentage: 10\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Permissions can control which groups or individual users can *Manage*, *Query* or *View* individual serving endpoints.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.RegisteredModel to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n* End to end workspace management guide.\n* databricks.Directory to manage directories in [Databricks Workspace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.MlflowModel to create models in the [workspace model registry](https://docs.databricks.com/en/mlflow/model-registry.html) in Databricks.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Notebook data to export a notebook from Databricks Workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n\n## Import\n\nThe model serving resource can be imported using the name of the endpoint.\n\nbash\n\n```sh\n$ pulumi import databricks:index/modelServing:ModelServing this \u003cmodel-serving-endpoint-name\u003e\n```\n\n",
            "properties": {
                "config": {
                    "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                    "description": "The model serving endpoint configuration.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the update name.\n"
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                    },
                    "description": "A list of rate limits to be applied to the serving endpoint. NOTE: only external and foundation model endpoints are supported as of now.\n"
                },
                "servingEndpointId": {
                    "type": "string",
                    "description": "Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                    },
                    "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                }
            },
            "required": [
                "config",
                "name",
                "servingEndpointId"
            ],
            "inputProperties": {
                "config": {
                    "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                    "description": "The model serving endpoint configuration.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the update name.\n",
                    "willReplaceOnChanges": true
                },
                "rateLimits": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                    },
                    "description": "A list of rate limits to be applied to the serving endpoint. NOTE: only external and foundation model endpoints are supported as of now.\n"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                    },
                    "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                }
            },
            "requiredInputs": [
                "config"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ModelServing resources.\n",
                "properties": {
                    "config": {
                        "$ref": "#/types/databricks:index/ModelServingConfig:ModelServingConfig",
                        "description": "The model serving endpoint configuration.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the model serving endpoint. This field is required and must be unique across a workspace. An endpoint name can consist of alphanumeric characters, dashes, and underscores. NOTE: Changing this name will delete the existing endpoint and create a new endpoint with the update name.\n",
                        "willReplaceOnChanges": true
                    },
                    "rateLimits": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ModelServingRateLimit:ModelServingRateLimit"
                        },
                        "description": "A list of rate limits to be applied to the serving endpoint. NOTE: only external and foundation model endpoints are supported as of now.\n"
                    },
                    "servingEndpointId": {
                        "type": "string",
                        "description": "Unique identifier of the serving endpoint primarily used to set permissions and refer to this instance for other operations.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ModelServingTag:ModelServingTag"
                        },
                        "description": "Tags to be attached to the serving endpoint and automatically propagated to billing logs.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mount:Mount": {
            "description": "This resource will mount your cloud storage\n* `gs` - to [mount Google Cloud Storage](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html)\n* `abfs` - to [mount ADLS Gen2](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/) using Azure Blob Filesystem (ABFS) driver\n* `adl` - to [mount ADLS Gen1](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake) using Azure Data Lake (ADL) driver\n* `wasb`  - to [mount Azure Blob Storage](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage) using Windows Azure Storage Blob (WASB) driver\n\n1. Use generic arguments - you have a responsibility for providing all necessary parameters that are required to mount specific storage. This is most flexible option\n\n## Common arguments\n\n* `cluster_id` - (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it's going to be started, so be aware to set auto-termination rules on it.\n* `name` - (Optional, String) Name, under which mount will be accessible in `dbfs:/mnt/\u003cMOUNT_NAME\u003e`. If not specified, provider will try to infer it from depending on the resource type:\n  * `bucket_name` for AWS S3 and Google Cloud Storage\n  * `container_name` for ADLS Gen2 and Azure Blob Storage\n  * `storage_resource_name` for ADLS Gen1\n* `uri` - (Optional, String) the URI for accessing specific storage (`s3a://....`, `abfss://....`, `gs://....`, etc.)\n* `extra_configs` - (Optional, String map) configuration parameters that are necessary for mounting of specific storage\n* `resource_id` - (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account \u0026 container names on Azure.\n* `encryption_type` - (Optional, String) encryption type. Currently used only for [AWS S3 mounts](https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#encrypt-data-in-s3-buckets)\n\n### Example mounting ADLS Gen2 using uri and extra_configs\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst tenantId = \"00000000-1111-2222-3333-444444444444\";\nconst clientId = \"55555555-6666-7777-8888-999999999999\";\nconst secretScope = \"some-kv\";\nconst secretKey = \"some-sp-secret\";\nconst container = \"test\";\nconst storageAcc = \"lrs\";\nconst _this = new databricks.Mount(\"this\", {\n    name: \"tf-abfss\",\n    uri: `abfss://${container}@${storageAcc}.dfs.core.windows.net`,\n    extraConfigs: {\n        \"fs.azure.account.auth.type\": \"OAuth\",\n        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"fs.azure.account.oauth2.client.id\": clientId,\n        \"fs.azure.account.oauth2.client.secret\": `{{secrets/${secretScope}/${secretKey}}}`,\n        \"fs.azure.account.oauth2.client.endpoint\": `https://login.microsoftonline.com/${tenantId}/oauth2/token`,\n        \"fs.azure.createRemoteFileSystemDuringInitialization\": \"false\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ntenant_id = \"00000000-1111-2222-3333-444444444444\"\nclient_id = \"55555555-6666-7777-8888-999999999999\"\nsecret_scope = \"some-kv\"\nsecret_key = \"some-sp-secret\"\ncontainer = \"test\"\nstorage_acc = \"lrs\"\nthis = databricks.Mount(\"this\",\n    name=\"tf-abfss\",\n    uri=f\"abfss://{container}@{storage_acc}.dfs.core.windows.net\",\n    extra_configs={\n        \"fs.azure.account.auth.type\": \"OAuth\",\n        \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"fs.azure.account.oauth2.client.id\": client_id,\n        \"fs.azure.account.oauth2.client.secret\": f\"{{{{secrets/{secret_scope}/{secret_key}}}}}\",\n        \"fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\",\n        \"fs.azure.createRemoteFileSystemDuringInitialization\": \"false\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var tenantId = \"00000000-1111-2222-3333-444444444444\";\n\n    var clientId = \"55555555-6666-7777-8888-999999999999\";\n\n    var secretScope = \"some-kv\";\n\n    var secretKey = \"some-sp-secret\";\n\n    var container = \"test\";\n\n    var storageAcc = \"lrs\";\n\n    var @this = new Databricks.Mount(\"this\", new()\n    {\n        Name = \"tf-abfss\",\n        Uri = $\"abfss://{container}@{storageAcc}.dfs.core.windows.net\",\n        ExtraConfigs = \n        {\n            { \"fs.azure.account.auth.type\", \"OAuth\" },\n            { \"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" },\n            { \"fs.azure.account.oauth2.client.id\", clientId },\n            { \"fs.azure.account.oauth2.client.secret\", $\"{{{{secrets/{secretScope}/{secretKey}}}}}\" },\n            { \"fs.azure.account.oauth2.client.endpoint\", $\"https://login.microsoftonline.com/{tenantId}/oauth2/token\" },\n            { \"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\ttenantId := \"00000000-1111-2222-3333-444444444444\"\n\t\tclientId := \"55555555-6666-7777-8888-999999999999\"\n\t\tsecretScope := \"some-kv\"\n\t\tsecretKey := \"some-sp-secret\"\n\t\tcontainer := \"test\"\n\t\tstorageAcc := \"lrs\"\n\t\t_, err := databricks.NewMount(ctx, \"this\", \u0026databricks.MountArgs{\n\t\t\tName: pulumi.String(\"tf-abfss\"),\n\t\t\tUri:  pulumi.String(fmt.Sprintf(\"abfss://%v@%v.dfs.core.windows.net\", container, storageAcc)),\n\t\t\tExtraConfigs: pulumi.Map{\n\t\t\t\t\"fs.azure.account.auth.type\":                          pulumi.Any(\"OAuth\"),\n\t\t\t\t\"fs.azure.account.oauth.provider.type\":                pulumi.Any(\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n\t\t\t\t\"fs.azure.account.oauth2.client.id\":                   pulumi.String(clientId),\n\t\t\t\t\"fs.azure.account.oauth2.client.secret\":               pulumi.Any(fmt.Sprintf(\"{{secrets/%v/%v}}\", secretScope, secretKey)),\n\t\t\t\t\"fs.azure.account.oauth2.client.endpoint\":             pulumi.Any(fmt.Sprintf(\"https://login.microsoftonline.com/%v/oauth2/token\", tenantId)),\n\t\t\t\t\"fs.azure.createRemoteFileSystemDuringInitialization\": pulumi.Any(\"false\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var tenantId = \"00000000-1111-2222-3333-444444444444\";\n\n        final var clientId = \"55555555-6666-7777-8888-999999999999\";\n\n        final var secretScope = \"some-kv\";\n\n        final var secretKey = \"some-sp-secret\";\n\n        final var container = \"test\";\n\n        final var storageAcc = \"lrs\";\n\n        var this_ = new Mount(\"this\", MountArgs.builder()        \n            .name(\"tf-abfss\")\n            .uri(String.format(\"abfss://%s@%s.dfs.core.windows.net\", container,storageAcc))\n            .extraConfigs(Map.ofEntries(\n                Map.entry(\"fs.azure.account.auth.type\", \"OAuth\"),\n                Map.entry(\"fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n                Map.entry(\"fs.azure.account.oauth2.client.id\", clientId),\n                Map.entry(\"fs.azure.account.oauth2.client.secret\", String.format(\"{{{{secrets/%s/%s}}}}\", secretScope,secretKey)),\n                Map.entry(\"fs.azure.account.oauth2.client.endpoint\", String.format(\"https://login.microsoftonline.com/%s/oauth2/token\", tenantId)),\n                Map.entry(\"fs.azure.createRemoteFileSystemDuringInitialization\", \"false\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:Mount\n    properties:\n      name: tf-abfss\n      uri: abfss://${container}@${storageAcc}.dfs.core.windows.net\n      extraConfigs:\n        fs.azure.account.auth.type: OAuth\n        fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n        fs.azure.account.oauth2.client.id: ${clientId}\n        fs.azure.account.oauth2.client.secret: '{{secrets/${secretScope}/${secretKey}}}'\n        fs.azure.account.oauth2.client.endpoint: https://login.microsoftonline.com/${tenantId}/oauth2/token\n        fs.azure.createRemoteFileSystemDuringInitialization: 'false'\nvariables:\n  tenantId: 00000000-1111-2222-3333-444444444444\n  clientId: 55555555-6666-7777-8888-999999999999\n  secretScope: some-kv\n  secretKey: some-sp-secret\n  container: test\n  storageAcc: lrs\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Example mounting ADLS Gen2 with AAD passthrough\n\n\u003e **Note** AAD passthrough is considered a legacy data access pattern. Use Unity Catalog for fine-grained data access control.\n\n\u003e **Note** Mounts using AAD passthrough cannot be created using a service principal.\n\nTo mount ALDS Gen2 with Azure Active Directory Credentials passthrough we need to execute the mount commands using the cluster configured with AAD Credentials passthrough \u0026 provide necessary configuration parameters (see [documentation](https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough#--mount-azure-data-lake-storage-to-dbfs-using-credential-passthrough) for more details).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as azure from \"@pulumi/azure\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Resource group for Databricks Workspace\nconst resourceGroup = config.require(\"resourceGroup\");\n// Name of the Databricks Workspace\nconst workspaceName = config.require(\"workspaceName\");\nconst this = azure.databricks.getWorkspace({\n    name: workspaceName,\n    resourceGroupName: resourceGroup,\n});\nconst smallest = databricks.getNodeType({\n    localDisk: true,\n});\nconst latest = databricks.getSparkVersion({});\nconst sharedPassthrough = new databricks.Cluster(\"shared_passthrough\", {\n    clusterName: \"Shared Passthrough for mount\",\n    sparkVersion: latest.then(latest =\u003e latest.id),\n    nodeTypeId: smallest.then(smallest =\u003e smallest.id),\n    autoterminationMinutes: 10,\n    numWorkers: 1,\n    sparkConf: {\n        \"spark.databricks.cluster.profile\": \"serverless\",\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.passthrough.enabled\": \"true\",\n        \"spark.databricks.pyspark.enableProcessIsolation\": \"true\",\n    },\n    customTags: {\n        ResourceClass: \"Serverless\",\n    },\n});\n// Name of the ADLS Gen2 storage container\nconst storageAcc = config.require(\"storageAcc\");\n// Name of container inside storage account\nconst container = config.require(\"container\");\nconst passthrough = new databricks.Mount(\"passthrough\", {\n    name: \"passthrough-test\",\n    clusterId: sharedPassthrough.id,\n    uri: `abfss://${container}@${storageAcc}.dfs.core.windows.net`,\n    extraConfigs: {\n        \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n        \"fs.azure.account.custom.token.provider.class\": \"{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_azure as azure\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Resource group for Databricks Workspace\nresource_group = config.require(\"resourceGroup\")\n# Name of the Databricks Workspace\nworkspace_name = config.require(\"workspaceName\")\nthis = azure.databricks.get_workspace(name=workspace_name,\n    resource_group_name=resource_group)\nsmallest = databricks.get_node_type(local_disk=True)\nlatest = databricks.get_spark_version()\nshared_passthrough = databricks.Cluster(\"shared_passthrough\",\n    cluster_name=\"Shared Passthrough for mount\",\n    spark_version=latest.id,\n    node_type_id=smallest.id,\n    autotermination_minutes=10,\n    num_workers=1,\n    spark_conf={\n        \"spark.databricks.cluster.profile\": \"serverless\",\n        \"spark.databricks.repl.allowedLanguages\": \"python,sql\",\n        \"spark.databricks.passthrough.enabled\": \"true\",\n        \"spark.databricks.pyspark.enableProcessIsolation\": \"true\",\n    },\n    custom_tags={\n        \"ResourceClass\": \"Serverless\",\n    })\n# Name of the ADLS Gen2 storage container\nstorage_acc = config.require(\"storageAcc\")\n# Name of container inside storage account\ncontainer = config.require(\"container\")\npassthrough = databricks.Mount(\"passthrough\",\n    name=\"passthrough-test\",\n    cluster_id=shared_passthrough.id,\n    uri=f\"abfss://{container}@{storage_acc}.dfs.core.windows.net\",\n    extra_configs={\n        \"fs.azure.account.auth.type\": \"CustomAccessToken\",\n        \"fs.azure.account.custom.token.provider.class\": \"{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Azure = Pulumi.Azure;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Resource group for Databricks Workspace\n    var resourceGroup = config.Require(\"resourceGroup\");\n    // Name of the Databricks Workspace\n    var workspaceName = config.Require(\"workspaceName\");\n    var @this = Azure.DataBricks.GetWorkspace.Invoke(new()\n    {\n        Name = workspaceName,\n        ResourceGroupName = resourceGroup,\n    });\n\n    var smallest = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n    });\n\n    var latest = Databricks.GetSparkVersion.Invoke();\n\n    var sharedPassthrough = new Databricks.Cluster(\"shared_passthrough\", new()\n    {\n        ClusterName = \"Shared Passthrough for mount\",\n        SparkVersion = latest.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = smallest.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 10,\n        NumWorkers = 1,\n        SparkConf = \n        {\n            { \"spark.databricks.cluster.profile\", \"serverless\" },\n            { \"spark.databricks.repl.allowedLanguages\", \"python,sql\" },\n            { \"spark.databricks.passthrough.enabled\", \"true\" },\n            { \"spark.databricks.pyspark.enableProcessIsolation\", \"true\" },\n        },\n        CustomTags = \n        {\n            { \"ResourceClass\", \"Serverless\" },\n        },\n    });\n\n    // Name of the ADLS Gen2 storage container\n    var storageAcc = config.Require(\"storageAcc\");\n    // Name of container inside storage account\n    var container = config.Require(\"container\");\n    var passthrough = new Databricks.Mount(\"passthrough\", new()\n    {\n        Name = \"passthrough-test\",\n        ClusterId = sharedPassthrough.Id,\n        Uri = $\"abfss://{container}@{storageAcc}.dfs.core.windows.net\",\n        ExtraConfigs = \n        {\n            { \"fs.azure.account.auth.type\", \"CustomAccessToken\" },\n            { \"fs.azure.account.custom.token.provider.class\", \"{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\tazuredatabricks \"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/databricks\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Resource group for Databricks Workspace\n\t\tresourceGroup := cfg.Require(\"resourceGroup\")\n\t\t// Name of the Databricks Workspace\n\t\tworkspaceName := cfg.Require(\"workspaceName\")\n\t\t_, err := azuredatabricks.LookupWorkspace(ctx, \u0026databricks.LookupWorkspaceArgs{\n\t\t\tName:              workspaceName,\n\t\t\tResourceGroupName: resourceGroup,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsmallest, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tlatest, err := databricks.GetSparkVersion(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsharedPassthrough, err := databricks.NewCluster(ctx, \"shared_passthrough\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Shared Passthrough for mount\"),\n\t\t\tSparkVersion:           pulumi.String(latest.Id),\n\t\t\tNodeTypeId:             pulumi.String(smallest.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(10),\n\t\t\tNumWorkers:             pulumi.Int(1),\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"spark.databricks.cluster.profile\":                pulumi.Any(\"serverless\"),\n\t\t\t\t\"spark.databricks.repl.allowedLanguages\":          pulumi.Any(\"python,sql\"),\n\t\t\t\t\"spark.databricks.passthrough.enabled\":            pulumi.Any(\"true\"),\n\t\t\t\t\"spark.databricks.pyspark.enableProcessIsolation\": pulumi.Any(\"true\"),\n\t\t\t},\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"ResourceClass\": pulumi.Any(\"Serverless\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Name of the ADLS Gen2 storage container\n\t\tstorageAcc := cfg.Require(\"storageAcc\")\n\t\t// Name of container inside storage account\n\t\tcontainer := cfg.Require(\"container\")\n\t\t_, err = databricks.NewMount(ctx, \"passthrough\", \u0026databricks.MountArgs{\n\t\t\tName:      pulumi.String(\"passthrough-test\"),\n\t\t\tClusterId: sharedPassthrough.ID(),\n\t\t\tUri:       pulumi.String(fmt.Sprintf(\"abfss://%v@%v.dfs.core.windows.net\", container, storageAcc)),\n\t\t\tExtraConfigs: pulumi.Map{\n\t\t\t\t\"fs.azure.account.auth.type\":                   pulumi.Any(\"CustomAccessToken\"),\n\t\t\t\t\"fs.azure.account.custom.token.provider.class\": pulumi.Any(\"{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.azure.databricks.DatabricksFunctions;\nimport com.pulumi.azure.databricks.inputs.GetWorkspaceArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var resourceGroup = config.get(\"resourceGroup\");\n        final var workspaceName = config.get(\"workspaceName\");\n        final var this = DatabricksFunctions.getWorkspace(GetWorkspaceArgs.builder()\n            .name(workspaceName)\n            .resourceGroupName(resourceGroup)\n            .build());\n\n        final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .build());\n\n        final var latest = DatabricksFunctions.getSparkVersion();\n\n        var sharedPassthrough = new Cluster(\"sharedPassthrough\", ClusterArgs.builder()        \n            .clusterName(\"Shared Passthrough for mount\")\n            .sparkVersion(latest.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(smallest.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(10)\n            .numWorkers(1)\n            .sparkConf(Map.ofEntries(\n                Map.entry(\"spark.databricks.cluster.profile\", \"serverless\"),\n                Map.entry(\"spark.databricks.repl.allowedLanguages\", \"python,sql\"),\n                Map.entry(\"spark.databricks.passthrough.enabled\", \"true\"),\n                Map.entry(\"spark.databricks.pyspark.enableProcessIsolation\", \"true\")\n            ))\n            .customTags(Map.of(\"ResourceClass\", \"Serverless\"))\n            .build());\n\n        final var storageAcc = config.get(\"storageAcc\");\n        final var container = config.get(\"container\");\n        var passthrough = new Mount(\"passthrough\", MountArgs.builder()        \n            .name(\"passthrough-test\")\n            .clusterId(sharedPassthrough.id())\n            .uri(String.format(\"abfss://%s@%s.dfs.core.windows.net\", container,storageAcc))\n            .extraConfigs(Map.ofEntries(\n                Map.entry(\"fs.azure.account.auth.type\", \"CustomAccessToken\"),\n                Map.entry(\"fs.azure.account.custom.token.provider.class\", \"{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}\")\n            ))\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  resourceGroup:\n    type: string\n  workspaceName:\n    type: string\n  storageAcc:\n    type: string\n  container:\n    type: string\nresources:\n  sharedPassthrough:\n    type: databricks:Cluster\n    name: shared_passthrough\n    properties:\n      clusterName: Shared Passthrough for mount\n      sparkVersion: ${latest.id}\n      nodeTypeId: ${smallest.id}\n      autoterminationMinutes: 10\n      numWorkers: 1\n      sparkConf:\n        spark.databricks.cluster.profile: serverless\n        spark.databricks.repl.allowedLanguages: python,sql\n        spark.databricks.passthrough.enabled: 'true'\n        spark.databricks.pyspark.enableProcessIsolation: 'true'\n      customTags:\n        ResourceClass: Serverless\n  passthrough:\n    type: databricks:Mount\n    properties:\n      name: passthrough-test\n      clusterId: ${sharedPassthrough.id}\n      uri: abfss://${container}@${storageAcc}.dfs.core.windows.net\n      extraConfigs:\n        fs.azure.account.auth.type: CustomAccessToken\n        fs.azure.account.custom.token.provider.class: '{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}'\nvariables:\n  this:\n    fn::invoke:\n      Function: azure:databricks:getWorkspace\n      Arguments:\n        name: ${workspaceName}\n        resourceGroupName: ${resourceGroup}\n  smallest:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n  latest:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## s3 block\n\nThis block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `s3` block:\n\n* `instance_profile` - (Optional) (String) ARN of registered instance profile for data access.  If it's not specified, then the `cluster_id` should be provided, and the cluster should have an instance profile attached to it. If both `cluster_id` \u0026 `instance_profile` are specified, then `cluster_id` takes precedence.\n* `bucket_name` - (Required) (String) S3 bucket name to be mounted.\n\n### Example of mounting S3\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// now you can do `%fs ls /mnt/experiments` in notebooks\nconst _this = new databricks.Mount(\"this\", {\n    name: \"experiments\",\n    s3: {\n        instanceProfile: ds.id,\n        bucketName: thisAwsS3Bucket.bucket,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# now you can do `%fs ls /mnt/experiments` in notebooks\nthis = databricks.Mount(\"this\",\n    name=\"experiments\",\n    s3=databricks.MountS3Args(\n        instance_profile=ds[\"id\"],\n        bucket_name=this_aws_s3_bucket[\"bucket\"],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // now you can do `%fs ls /mnt/experiments` in notebooks\n    var @this = new Databricks.Mount(\"this\", new()\n    {\n        Name = \"experiments\",\n        S3 = new Databricks.Inputs.MountS3Args\n        {\n            InstanceProfile = ds.Id,\n            BucketName = thisAwsS3Bucket.Bucket,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// now you can do `%fs ls /mnt/experiments` in notebooks\n\t\t_, err := databricks.NewMount(ctx, \"this\", \u0026databricks.MountArgs{\n\t\t\tName: pulumi.String(\"experiments\"),\n\t\t\tS3: \u0026databricks.MountS3Args{\n\t\t\t\tInstanceProfile: pulumi.Any(ds.Id),\n\t\t\t\tBucketName:      pulumi.Any(thisAwsS3Bucket.Bucket),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport com.pulumi.databricks.inputs.MountS3Args;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // now you can do `%fs ls /mnt/experiments` in notebooks\n        var this_ = new Mount(\"this\", MountArgs.builder()        \n            .name(\"experiments\")\n            .s3(MountS3Args.builder()\n                .instanceProfile(ds.id())\n                .bucketName(thisAwsS3Bucket.bucket())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  # now you can do `%fs ls /mnt/experiments` in notebooks\n  this:\n    type: databricks:Mount\n    properties:\n      name: experiments\n      s3:\n        instanceProfile: ${ds.id}\n        bucketName: ${thisAwsS3Bucket.bucket}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## abfs block\n\nThis block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `abfs` block:\n\n* `client_id` - (Required) (String) This is the client_id (Application Object ID) for the enterprise application for the service principal.\n* `tenant_id` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenant_id` from it).\n* `client_secret_key` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.\n* `client_secret_scope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.\n* `container_name` - (Required) (String) ADLS gen2 container name. (Could be omitted if `resource_id` is provided)\n* `storage_account_name` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resource_id` is provided)\n* `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a \"/\".\n* `initialize_file_system` - (Required) (Bool) either or not initialize FS for the first use\n\n### Creating mount for ADLS Gen2 using abfs block\n\nIn this example, we're using Azure authentication, so we can omit some parameters (`tenant_id`, `storage_account_name`, and `container_name`) that will be detected automatically.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as azure from \"@pulumi/azure\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst terraform = new databricks.SecretScope(\"terraform\", {\n    name: \"application\",\n    initialManagePrincipal: \"users\",\n});\nconst servicePrincipalKey = new databricks.Secret(\"service_principal_key\", {\n    key: \"service_principal_key\",\n    stringValue: ARM_CLIENT_SECRET,\n    scope: terraform.name,\n});\nconst _this = new azure.storage.Account(\"this\", {\n    name: `${prefix}datalake`,\n    resourceGroupName: resourceGroupName,\n    location: resourceGroupLocation,\n    accountTier: \"Standard\",\n    accountReplicationType: \"GRS\",\n    accountKind: \"StorageV2\",\n    isHnsEnabled: true,\n});\nconst thisAssignment = new azure.authorization.Assignment(\"this\", {\n    scope: _this.id,\n    roleDefinitionName: \"Storage Blob Data Contributor\",\n    principalId: current.objectId,\n});\nconst thisContainer = new azure.storage.Container(\"this\", {\n    name: \"marketing\",\n    storageAccountName: _this.name,\n    containerAccessType: \"private\",\n});\nconst marketing = new databricks.Mount(\"marketing\", {\n    name: \"marketing\",\n    resourceId: thisContainer.resourceManagerId,\n    abfs: {\n        clientId: current.clientId,\n        clientSecretScope: terraform.name,\n        clientSecretKey: servicePrincipalKey.key,\n        initializeFileSystem: true,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_azure as azure\nimport pulumi_databricks as databricks\n\nterraform = databricks.SecretScope(\"terraform\",\n    name=\"application\",\n    initial_manage_principal=\"users\")\nservice_principal_key = databricks.Secret(\"service_principal_key\",\n    key=\"service_principal_key\",\n    string_value=ar_m__clien_t__secret,\n    scope=terraform.name)\nthis = azure.storage.Account(\"this\",\n    name=f\"{prefix}datalake\",\n    resource_group_name=resource_group_name,\n    location=resource_group_location,\n    account_tier=\"Standard\",\n    account_replication_type=\"GRS\",\n    account_kind=\"StorageV2\",\n    is_hns_enabled=True)\nthis_assignment = azure.authorization.Assignment(\"this\",\n    scope=this.id,\n    role_definition_name=\"Storage Blob Data Contributor\",\n    principal_id=current[\"objectId\"])\nthis_container = azure.storage.Container(\"this\",\n    name=\"marketing\",\n    storage_account_name=this.name,\n    container_access_type=\"private\")\nmarketing = databricks.Mount(\"marketing\",\n    name=\"marketing\",\n    resource_id=this_container.resource_manager_id,\n    abfs=databricks.MountAbfsArgs(\n        client_id=current[\"clientId\"],\n        client_secret_scope=terraform.name,\n        client_secret_key=service_principal_key.key,\n        initialize_file_system=True,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Azure = Pulumi.Azure;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var terraform = new Databricks.SecretScope(\"terraform\", new()\n    {\n        Name = \"application\",\n        InitialManagePrincipal = \"users\",\n    });\n\n    var servicePrincipalKey = new Databricks.Secret(\"service_principal_key\", new()\n    {\n        Key = \"service_principal_key\",\n        StringValue = ARM_CLIENT_SECRET,\n        Scope = terraform.Name,\n    });\n\n    var @this = new Azure.Storage.Account(\"this\", new()\n    {\n        Name = $\"{prefix}datalake\",\n        ResourceGroupName = resourceGroupName,\n        Location = resourceGroupLocation,\n        AccountTier = \"Standard\",\n        AccountReplicationType = \"GRS\",\n        AccountKind = \"StorageV2\",\n        IsHnsEnabled = true,\n    });\n\n    var thisAssignment = new Azure.Authorization.Assignment(\"this\", new()\n    {\n        Scope = @this.Id,\n        RoleDefinitionName = \"Storage Blob Data Contributor\",\n        PrincipalId = current.ObjectId,\n    });\n\n    var thisContainer = new Azure.Storage.Container(\"this\", new()\n    {\n        Name = \"marketing\",\n        StorageAccountName = @this.Name,\n        ContainerAccessType = \"private\",\n    });\n\n    var marketing = new Databricks.Mount(\"marketing\", new()\n    {\n        Name = \"marketing\",\n        ResourceId = thisContainer.ResourceManagerId,\n        Abfs = new Databricks.Inputs.MountAbfsArgs\n        {\n            ClientId = current.ClientId,\n            ClientSecretScope = terraform.Name,\n            ClientSecretKey = servicePrincipalKey.Key,\n            InitializeFileSystem = true,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/authorization\"\n\t\"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/storage\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tterraform, err := databricks.NewSecretScope(ctx, \"terraform\", \u0026databricks.SecretScopeArgs{\n\t\t\tName:                   pulumi.String(\"application\"),\n\t\t\tInitialManagePrincipal: pulumi.String(\"users\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tservicePrincipalKey, err := databricks.NewSecret(ctx, \"service_principal_key\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"service_principal_key\"),\n\t\t\tStringValue: pulumi.Any(ARM_CLIENT_SECRET),\n\t\t\tScope:       terraform.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := storage.NewAccount(ctx, \"this\", \u0026storage.AccountArgs{\n\t\t\tName:                   pulumi.String(fmt.Sprintf(\"%vdatalake\", prefix)),\n\t\t\tResourceGroupName:      pulumi.Any(resourceGroupName),\n\t\t\tLocation:               pulumi.Any(resourceGroupLocation),\n\t\t\tAccountTier:            pulumi.String(\"Standard\"),\n\t\t\tAccountReplicationType: pulumi.String(\"GRS\"),\n\t\t\tAccountKind:            pulumi.String(\"StorageV2\"),\n\t\t\tIsHnsEnabled:           pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = authorization.NewAssignment(ctx, \"this\", \u0026authorization.AssignmentArgs{\n\t\t\tScope:              this.ID(),\n\t\t\tRoleDefinitionName: pulumi.String(\"Storage Blob Data Contributor\"),\n\t\t\tPrincipalId:        pulumi.Any(current.ObjectId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisContainer, err := storage.NewContainer(ctx, \"this\", \u0026storage.ContainerArgs{\n\t\t\tName:                pulumi.String(\"marketing\"),\n\t\t\tStorageAccountName:  this.Name,\n\t\t\tContainerAccessType: pulumi.String(\"private\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMount(ctx, \"marketing\", \u0026databricks.MountArgs{\n\t\t\tName:       pulumi.String(\"marketing\"),\n\t\t\tResourceId: thisContainer.ResourceManagerId,\n\t\t\tAbfs: \u0026databricks.MountAbfsArgs{\n\t\t\t\tClientId:             pulumi.Any(current.ClientId),\n\t\t\t\tClientSecretScope:    terraform.Name,\n\t\t\t\tClientSecretKey:      servicePrincipalKey.Key,\n\t\t\t\tInitializeFileSystem: pulumi.Bool(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport com.pulumi.azure.storage.Account;\nimport com.pulumi.azure.storage.AccountArgs;\nimport com.pulumi.azure.authorization.Assignment;\nimport com.pulumi.azure.authorization.AssignmentArgs;\nimport com.pulumi.azure.storage.Container;\nimport com.pulumi.azure.storage.ContainerArgs;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport com.pulumi.databricks.inputs.MountAbfsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var terraform = new SecretScope(\"terraform\", SecretScopeArgs.builder()        \n            .name(\"application\")\n            .initialManagePrincipal(\"users\")\n            .build());\n\n        var servicePrincipalKey = new Secret(\"servicePrincipalKey\", SecretArgs.builder()        \n            .key(\"service_principal_key\")\n            .stringValue(ARM_CLIENT_SECRET)\n            .scope(terraform.name())\n            .build());\n\n        var this_ = new Account(\"this\", AccountArgs.builder()        \n            .name(String.format(\"%sdatalake\", prefix))\n            .resourceGroupName(resourceGroupName)\n            .location(resourceGroupLocation)\n            .accountTier(\"Standard\")\n            .accountReplicationType(\"GRS\")\n            .accountKind(\"StorageV2\")\n            .isHnsEnabled(true)\n            .build());\n\n        var thisAssignment = new Assignment(\"thisAssignment\", AssignmentArgs.builder()        \n            .scope(this_.id())\n            .roleDefinitionName(\"Storage Blob Data Contributor\")\n            .principalId(current.objectId())\n            .build());\n\n        var thisContainer = new Container(\"thisContainer\", ContainerArgs.builder()        \n            .name(\"marketing\")\n            .storageAccountName(this_.name())\n            .containerAccessType(\"private\")\n            .build());\n\n        var marketing = new Mount(\"marketing\", MountArgs.builder()        \n            .name(\"marketing\")\n            .resourceId(thisContainer.resourceManagerId())\n            .abfs(MountAbfsArgs.builder()\n                .clientId(current.clientId())\n                .clientSecretScope(terraform.name())\n                .clientSecretKey(servicePrincipalKey.key())\n                .initializeFileSystem(true)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  terraform:\n    type: databricks:SecretScope\n    properties:\n      name: application\n      initialManagePrincipal: users\n  servicePrincipalKey:\n    type: databricks:Secret\n    name: service_principal_key\n    properties:\n      key: service_principal_key\n      stringValue: ${ARM_CLIENT_SECRET}\n      scope: ${terraform.name}\n  this:\n    type: azure:storage:Account\n    properties:\n      name: ${prefix}datalake\n      resourceGroupName: ${resourceGroupName}\n      location: ${resourceGroupLocation}\n      accountTier: Standard\n      accountReplicationType: GRS\n      accountKind: StorageV2\n      isHnsEnabled: true\n  thisAssignment:\n    type: azure:authorization:Assignment\n    name: this\n    properties:\n      scope: ${this.id}\n      roleDefinitionName: Storage Blob Data Contributor\n      principalId: ${current.objectId}\n  thisContainer:\n    type: azure:storage:Container\n    name: this\n    properties:\n      name: marketing\n      storageAccountName: ${this.name}\n      containerAccessType: private\n  marketing:\n    type: databricks:Mount\n    properties:\n      name: marketing\n      resourceId: ${thisContainer.resourceManagerId}\n      abfs:\n        clientId: ${current.clientId}\n        clientSecretScope: ${terraform.name}\n        clientSecretKey: ${servicePrincipalKey.key}\n        initializeFileSystem: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## gs block\n\nThis block allows specifying parameters for mounting of the Google Cloud Storage. The following arguments are required inside the `gs` block:\n\n* `service_account` - (Optional) (String) email of registered [Google Service Account](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html#step-1-set-up-google-cloud-service-account-using-google-cloud-console) for data access.  If it's not specified, then the `cluster_id` should be provided, and the cluster should have a Google service account attached to it.\n* `bucket_name` - (Required) (String) GCS bucket name to be mounted.\n\n### Example mounting Google Cloud Storage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisGs = new databricks.Mount(\"this_gs\", {\n    name: \"gs-mount\",\n    gs: {\n        serviceAccount: \"acc@company.iam.gserviceaccount.com\",\n        bucketName: \"mybucket\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_gs = databricks.Mount(\"this_gs\",\n    name=\"gs-mount\",\n    gs=databricks.MountGsArgs(\n        service_account=\"acc@company.iam.gserviceaccount.com\",\n        bucket_name=\"mybucket\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisGs = new Databricks.Mount(\"this_gs\", new()\n    {\n        Name = \"gs-mount\",\n        Gs = new Databricks.Inputs.MountGsArgs\n        {\n            ServiceAccount = \"acc@company.iam.gserviceaccount.com\",\n            BucketName = \"mybucket\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMount(ctx, \"this_gs\", \u0026databricks.MountArgs{\n\t\t\tName: pulumi.String(\"gs-mount\"),\n\t\t\tGs: \u0026databricks.MountGsArgs{\n\t\t\t\tServiceAccount: pulumi.String(\"acc@company.iam.gserviceaccount.com\"),\n\t\t\t\tBucketName:     pulumi.String(\"mybucket\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport com.pulumi.databricks.inputs.MountGsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisGs = new Mount(\"thisGs\", MountArgs.builder()        \n            .name(\"gs-mount\")\n            .gs(MountGsArgs.builder()\n                .serviceAccount(\"acc@company.iam.gserviceaccount.com\")\n                .bucketName(\"mybucket\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisGs:\n    type: databricks:Mount\n    name: this_gs\n    properties:\n      name: gs-mount\n      gs:\n        serviceAccount: acc@company.iam.gserviceaccount.com\n        bucketName: mybucket\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## adl block\n\nThis block allows specifying parameters for mounting of the ADLS Gen1. The following arguments are required inside the `adl` block:\n\n* `client_id` - (Required) (String) This is the client_id for the enterprise application for the service principal.\n* `tenant_id` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenant_id` from it)\n* `client_secret_key` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.\n* `client_secret_scope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.\n\n* `storage_resource_name` - (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if `resource_id` is provided)\n* `spark_conf_prefix` - (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are `fs.adl`, `dfs.adls`. Use `fs.adl` for runtime 6.0 and above for the clusters. Otherwise use `dfs.adls`. The default value is: `fs.adl`.\n* `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a \"/\".\n\n### Example mounting ADLS Gen1\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst mount = new databricks.Mount(\"mount\", {\n    name: \"{var.RANDOM}\",\n    adl: {\n        storageResourceName: \"{env.TEST_STORAGE_ACCOUNT_NAME}\",\n        tenantId: current.tenantId,\n        clientId: current.clientId,\n        clientSecretScope: terraform.name,\n        clientSecretKey: servicePrincipalKey.key,\n        sparkConfPrefix: \"fs.adl\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmount = databricks.Mount(\"mount\",\n    name=\"{var.RANDOM}\",\n    adl=databricks.MountAdlArgs(\n        storage_resource_name=\"{env.TEST_STORAGE_ACCOUNT_NAME}\",\n        tenant_id=current[\"tenantId\"],\n        client_id=current[\"clientId\"],\n        client_secret_scope=terraform[\"name\"],\n        client_secret_key=service_principal_key[\"key\"],\n        spark_conf_prefix=\"fs.adl\",\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var mount = new Databricks.Mount(\"mount\", new()\n    {\n        Name = \"{var.RANDOM}\",\n        Adl = new Databricks.Inputs.MountAdlArgs\n        {\n            StorageResourceName = \"{env.TEST_STORAGE_ACCOUNT_NAME}\",\n            TenantId = current.TenantId,\n            ClientId = current.ClientId,\n            ClientSecretScope = terraform.Name,\n            ClientSecretKey = servicePrincipalKey.Key,\n            SparkConfPrefix = \"fs.adl\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMount(ctx, \"mount\", \u0026databricks.MountArgs{\n\t\t\tName: pulumi.String(\"{var.RANDOM}\"),\n\t\t\tAdl: \u0026databricks.MountAdlArgs{\n\t\t\t\tStorageResourceName: pulumi.String(\"{env.TEST_STORAGE_ACCOUNT_NAME}\"),\n\t\t\t\tTenantId:            pulumi.Any(current.TenantId),\n\t\t\t\tClientId:            pulumi.Any(current.ClientId),\n\t\t\t\tClientSecretScope:   pulumi.Any(terraform.Name),\n\t\t\t\tClientSecretKey:     pulumi.Any(servicePrincipalKey.Key),\n\t\t\t\tSparkConfPrefix:     pulumi.String(\"fs.adl\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport com.pulumi.databricks.inputs.MountAdlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var mount = new Mount(\"mount\", MountArgs.builder()        \n            .name(\"{var.RANDOM}\")\n            .adl(MountAdlArgs.builder()\n                .storageResourceName(\"{env.TEST_STORAGE_ACCOUNT_NAME}\")\n                .tenantId(current.tenantId())\n                .clientId(current.clientId())\n                .clientSecretScope(terraform.name())\n                .clientSecretKey(servicePrincipalKey.key())\n                .sparkConfPrefix(\"fs.adl\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  mount:\n    type: databricks:Mount\n    properties:\n      name: '{var.RANDOM}'\n      adl:\n        storageResourceName: '{env.TEST_STORAGE_ACCOUNT_NAME}'\n        tenantId: ${current.tenantId}\n        clientId: ${current.clientId}\n        clientSecretScope: ${terraform.name}\n        clientSecretKey: ${servicePrincipalKey.key}\n        sparkConfPrefix: fs.adl\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## wasb block\n\nThis block allows specifying parameters for mounting of the Azure Blob Storage. The following arguments are required inside the `wasb` block:\n\n* `auth_type` - (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (`SAS`) or account access keys (`ACCESS_KEY`).\n* `token_secret_scope` - (Required) (String) This is the secret scope in which your auth type token is stored.\n* `token_secret_key` - (Required) (String) This is the secret key in which your auth type token is stored.\n* `container_name` - (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if `resource_id` is provided)\n* `storage_account_name` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resource_id` is provided)\n* `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a \"/\".\n\n### Example mounting Azure Blob Storage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as azure from \"@pulumi/azure\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst blobaccount = new azure.storage.Account(\"blobaccount\", {\n    name: `${prefix}blob`,\n    resourceGroupName: resourceGroupName,\n    location: resourceGroupLocation,\n    accountTier: \"Standard\",\n    accountReplicationType: \"LRS\",\n    accountKind: \"StorageV2\",\n});\nconst marketing = new azure.storage.Container(\"marketing\", {\n    name: \"marketing\",\n    storageAccountName: blobaccount.name,\n    containerAccessType: \"private\",\n});\nconst terraform = new databricks.SecretScope(\"terraform\", {\n    name: \"application\",\n    initialManagePrincipal: \"users\",\n});\nconst storageKey = new databricks.Secret(\"storage_key\", {\n    key: \"blob_storage_key\",\n    stringValue: blobaccount.primaryAccessKey,\n    scope: terraform.name,\n});\nconst marketingMount = new databricks.Mount(\"marketing\", {\n    name: \"marketing\",\n    wasb: {\n        containerName: marketing.name,\n        storageAccountName: blobaccount.name,\n        authType: \"ACCESS_KEY\",\n        tokenSecretScope: terraform.name,\n        tokenSecretKey: storageKey.key,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_azure as azure\nimport pulumi_databricks as databricks\n\nblobaccount = azure.storage.Account(\"blobaccount\",\n    name=f\"{prefix}blob\",\n    resource_group_name=resource_group_name,\n    location=resource_group_location,\n    account_tier=\"Standard\",\n    account_replication_type=\"LRS\",\n    account_kind=\"StorageV2\")\nmarketing = azure.storage.Container(\"marketing\",\n    name=\"marketing\",\n    storage_account_name=blobaccount.name,\n    container_access_type=\"private\")\nterraform = databricks.SecretScope(\"terraform\",\n    name=\"application\",\n    initial_manage_principal=\"users\")\nstorage_key = databricks.Secret(\"storage_key\",\n    key=\"blob_storage_key\",\n    string_value=blobaccount.primary_access_key,\n    scope=terraform.name)\nmarketing_mount = databricks.Mount(\"marketing\",\n    name=\"marketing\",\n    wasb=databricks.MountWasbArgs(\n        container_name=marketing.name,\n        storage_account_name=blobaccount.name,\n        auth_type=\"ACCESS_KEY\",\n        token_secret_scope=terraform.name,\n        token_secret_key=storage_key.key,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Azure = Pulumi.Azure;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var blobaccount = new Azure.Storage.Account(\"blobaccount\", new()\n    {\n        Name = $\"{prefix}blob\",\n        ResourceGroupName = resourceGroupName,\n        Location = resourceGroupLocation,\n        AccountTier = \"Standard\",\n        AccountReplicationType = \"LRS\",\n        AccountKind = \"StorageV2\",\n    });\n\n    var marketing = new Azure.Storage.Container(\"marketing\", new()\n    {\n        Name = \"marketing\",\n        StorageAccountName = blobaccount.Name,\n        ContainerAccessType = \"private\",\n    });\n\n    var terraform = new Databricks.SecretScope(\"terraform\", new()\n    {\n        Name = \"application\",\n        InitialManagePrincipal = \"users\",\n    });\n\n    var storageKey = new Databricks.Secret(\"storage_key\", new()\n    {\n        Key = \"blob_storage_key\",\n        StringValue = blobaccount.PrimaryAccessKey,\n        Scope = terraform.Name,\n    });\n\n    var marketingMount = new Databricks.Mount(\"marketing\", new()\n    {\n        Name = \"marketing\",\n        Wasb = new Databricks.Inputs.MountWasbArgs\n        {\n            ContainerName = marketing.Name,\n            StorageAccountName = blobaccount.Name,\n            AuthType = \"ACCESS_KEY\",\n            TokenSecretScope = terraform.Name,\n            TokenSecretKey = storageKey.Key,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-azure/sdk/v5/go/azure/storage\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tblobaccount, err := storage.NewAccount(ctx, \"blobaccount\", \u0026storage.AccountArgs{\n\t\t\tName:                   pulumi.String(fmt.Sprintf(\"%vblob\", prefix)),\n\t\t\tResourceGroupName:      pulumi.Any(resourceGroupName),\n\t\t\tLocation:               pulumi.Any(resourceGroupLocation),\n\t\t\tAccountTier:            pulumi.String(\"Standard\"),\n\t\t\tAccountReplicationType: pulumi.String(\"LRS\"),\n\t\t\tAccountKind:            pulumi.String(\"StorageV2\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmarketing, err := storage.NewContainer(ctx, \"marketing\", \u0026storage.ContainerArgs{\n\t\t\tName:                pulumi.String(\"marketing\"),\n\t\t\tStorageAccountName:  blobaccount.Name,\n\t\t\tContainerAccessType: pulumi.String(\"private\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tterraform, err := databricks.NewSecretScope(ctx, \"terraform\", \u0026databricks.SecretScopeArgs{\n\t\t\tName:                   pulumi.String(\"application\"),\n\t\t\tInitialManagePrincipal: pulumi.String(\"users\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tstorageKey, err := databricks.NewSecret(ctx, \"storage_key\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"blob_storage_key\"),\n\t\t\tStringValue: blobaccount.PrimaryAccessKey,\n\t\t\tScope:       terraform.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMount(ctx, \"marketing\", \u0026databricks.MountArgs{\n\t\t\tName: pulumi.String(\"marketing\"),\n\t\t\tWasb: \u0026databricks.MountWasbArgs{\n\t\t\t\tContainerName:      marketing.Name,\n\t\t\t\tStorageAccountName: blobaccount.Name,\n\t\t\t\tAuthType:           pulumi.String(\"ACCESS_KEY\"),\n\t\t\t\tTokenSecretScope:   terraform.Name,\n\t\t\t\tTokenSecretKey:     storageKey.Key,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.azure.storage.Account;\nimport com.pulumi.azure.storage.AccountArgs;\nimport com.pulumi.azure.storage.Container;\nimport com.pulumi.azure.storage.ContainerArgs;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport com.pulumi.databricks.Mount;\nimport com.pulumi.databricks.MountArgs;\nimport com.pulumi.databricks.inputs.MountWasbArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var blobaccount = new Account(\"blobaccount\", AccountArgs.builder()        \n            .name(String.format(\"%sblob\", prefix))\n            .resourceGroupName(resourceGroupName)\n            .location(resourceGroupLocation)\n            .accountTier(\"Standard\")\n            .accountReplicationType(\"LRS\")\n            .accountKind(\"StorageV2\")\n            .build());\n\n        var marketing = new Container(\"marketing\", ContainerArgs.builder()        \n            .name(\"marketing\")\n            .storageAccountName(blobaccount.name())\n            .containerAccessType(\"private\")\n            .build());\n\n        var terraform = new SecretScope(\"terraform\", SecretScopeArgs.builder()        \n            .name(\"application\")\n            .initialManagePrincipal(\"users\")\n            .build());\n\n        var storageKey = new Secret(\"storageKey\", SecretArgs.builder()        \n            .key(\"blob_storage_key\")\n            .stringValue(blobaccount.primaryAccessKey())\n            .scope(terraform.name())\n            .build());\n\n        var marketingMount = new Mount(\"marketingMount\", MountArgs.builder()        \n            .name(\"marketing\")\n            .wasb(MountWasbArgs.builder()\n                .containerName(marketing.name())\n                .storageAccountName(blobaccount.name())\n                .authType(\"ACCESS_KEY\")\n                .tokenSecretScope(terraform.name())\n                .tokenSecretKey(storageKey.key())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  blobaccount:\n    type: azure:storage:Account\n    properties:\n      name: ${prefix}blob\n      resourceGroupName: ${resourceGroupName}\n      location: ${resourceGroupLocation}\n      accountTier: Standard\n      accountReplicationType: LRS\n      accountKind: StorageV2\n  marketing:\n    type: azure:storage:Container\n    properties:\n      name: marketing\n      storageAccountName: ${blobaccount.name}\n      containerAccessType: private\n  terraform:\n    type: databricks:SecretScope\n    properties:\n      name: application\n      initialManagePrincipal: users\n  storageKey:\n    type: databricks:Secret\n    name: storage_key\n    properties:\n      key: blob_storage_key\n      stringValue: ${blobaccount.primaryAccessKey}\n      scope: ${terraform.name}\n  marketingMount:\n    type: databricks:Mount\n    name: marketing\n    properties:\n      name: marketing\n      wasb:\n        containerName: ${marketing.name}\n        storageAccountName: ${blobaccount.name}\n        authType: ACCESS_KEY\n        tokenSecretScope: ${terraform.name}\n        tokenSecretKey: ${storageKey.key}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Migration from other mount resources\n\nMigration from the specific mount resource is straightforward:\n\n* rename `mount_name` to `name`\n* wrap storage-specific settings (`container_name`, ...) into corresponding block (`adl`, `abfs`, `s3`, `wasbs`)\n* for S3 mounts, rename `s3_bucket_name` to `bucket_name`\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs"
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl"
                },
                "clusterId": {
                    "type": "string"
                },
                "encryptionType": {
                    "type": "string"
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs"
                },
                "name": {
                    "type": "string"
                },
                "resourceId": {
                    "type": "string"
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3"
                },
                "source": {
                    "type": "string",
                    "description": "(String) HDFS-compatible url\n"
                },
                "uri": {
                    "type": "string"
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb"
                }
            },
            "required": [
                "clusterId",
                "name",
                "source"
            ],
            "inputProperties": {
                "abfs": {
                    "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                    "willReplaceOnChanges": true
                },
                "adl": {
                    "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "encryptionType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "extraConfigs": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "willReplaceOnChanges": true
                },
                "gs": {
                    "$ref": "#/types/databricks:index/MountGs:MountGs",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "s3": {
                    "$ref": "#/types/databricks:index/MountS3:MountS3",
                    "willReplaceOnChanges": true
                },
                "uri": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "wasb": {
                    "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Mount resources.\n",
                "properties": {
                    "abfs": {
                        "$ref": "#/types/databricks:index/MountAbfs:MountAbfs",
                        "willReplaceOnChanges": true
                    },
                    "adl": {
                        "$ref": "#/types/databricks:index/MountAdl:MountAdl",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "encryptionType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "extraConfigs": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "willReplaceOnChanges": true
                    },
                    "gs": {
                        "$ref": "#/types/databricks:index/MountGs:MountGs",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "s3": {
                        "$ref": "#/types/databricks:index/MountS3:MountS3",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "(String) HDFS-compatible url\n"
                    },
                    "uri": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "wasb": {
                        "$ref": "#/types/databricks:index/MountWasb:MountWasb",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCredentials:MwsCredentials": {
            "description": "\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nThis resource to configure the cross-account role for creation of new workspaces within AWS.\n\nPlease follow this complete runnable example Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n* `credentials_name` - (Required) name of credentials to register\n* `role_arn` - (Required) ARN of cross-account role\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\nThis resource can be imported by the combination of its identifier and the account id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/mwsCredentials:MwsCredentials this \u003caccount_id\u003e/\u003ccredentials_id\u003e\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource."
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string"
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string"
                }
            },
            "required": [
                "creationTime",
                "credentialsId",
                "credentialsName",
                "externalId",
                "roleArn"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource.",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time of credentials registration\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "(String) identifier of credentials\n"
                },
                "credentialsName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string"
                },
                "roleArn": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "credentialsName",
                "roleArn"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCredentials resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "deprecationMessage": "`account_id` should be set as part of the Databricks Config, not in the resource.",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time of credentials registration\n"
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "(String) identifier of credentials\n"
                    },
                    "credentialsName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "roleArn": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsCustomerManagedKeys:MwsCustomerManagedKeys": {
            "description": "## Example Usage\n\n\u003e **Note** If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.\n\n### Customer-managed key for managed services\n\nYou must configure this during workspace creation\n\n### For AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst current = aws.getCallerIdentity({});\nconst databricksManagedServicesCmk = current.then(current =\u003e aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for control plane managed services\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources: [\"*\"],\n        },\n    ],\n}));\nconst managedServicesCustomerManagedKey = new aws.kms.Key(\"managed_services_customer_managed_key\", {policy: databricksManagedServicesCmk.then(databricksManagedServicesCmk =\u003e databricksManagedServicesCmk.json)});\nconst managedServicesCustomerManagedKeyAlias = new aws.kms.Alias(\"managed_services_customer_managed_key_alias\", {\n    name: \"alias/managed-services-customer-managed-key-alias\",\n    targetKeyId: managedServicesCustomerManagedKey.keyId,\n});\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managed_services\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: managedServicesCustomerManagedKey.arn,\n        keyAlias: managedServicesCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ncurrent = aws.get_caller_identity()\ndatabricks_managed_services_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[current.account_id],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for control plane managed services\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n            ],\n            resources=[\"*\"],\n        ),\n    ])\nmanaged_services_customer_managed_key = aws.kms.Key(\"managed_services_customer_managed_key\", policy=databricks_managed_services_cmk.json)\nmanaged_services_customer_managed_key_alias = aws.kms.Alias(\"managed_services_customer_managed_key_alias\",\n    name=\"alias/managed-services-customer-managed-key-alias\",\n    target_key_id=managed_services_customer_managed_key.key_id)\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managed_services\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=managed_services_customer_managed_key.arn,\n        key_alias=managed_services_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var current = Aws.GetCallerIdentity.Invoke();\n\n    var databricksManagedServicesCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.Apply(getCallerIdentityResult =\u003e getCallerIdentityResult.AccountId),\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for control plane managed services\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n        },\n    });\n\n    var managedServicesCustomerManagedKey = new Aws.Kms.Key(\"managed_services_customer_managed_key\", new()\n    {\n        Policy = databricksManagedServicesCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var managedServicesCustomerManagedKeyAlias = new Aws.Kms.Alias(\"managed_services_customer_managed_key_alias\", new()\n    {\n        Name = \"alias/managed-services-customer-managed-key-alias\",\n        TargetKeyId = managedServicesCustomerManagedKey.KeyId,\n    });\n\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managed_services\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = managedServicesCustomerManagedKey.Arn,\n            KeyAlias = managedServicesCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/kms\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\nfunc main() {\npulumi.Run(func(ctx *pulumi.Context) error {\ncfg := config.New(ctx, \"\")\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\ncurrent, err := aws.GetCallerIdentity(ctx, nil, nil);\nif err != nil {\nreturn err\n}\ndatabricksManagedServicesCmk, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\nVersion: pulumi.StringRef(\"2012-10-17\"),\nStatements: []iam.GetPolicyDocumentStatement{\n{\nSid: pulumi.StringRef(\"Enable IAM User Permissions\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ncurrent.AccountId,\n},\n},\n},\nActions: []string{\n\"kms:*\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for control plane managed services\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n},\nResources: []string{\n\"*\",\n},\n},\n},\n}, nil);\nif err != nil {\nreturn err\n}\nmanagedServicesCustomerManagedKey, err := kms.NewKey(ctx, \"managed_services_customer_managed_key\", \u0026kms.KeyArgs{\nPolicy: pulumi.String(databricksManagedServicesCmk.Json),\n})\nif err != nil {\nreturn err\n}\nmanagedServicesCustomerManagedKeyAlias, err := kms.NewAlias(ctx, \"managed_services_customer_managed_key_alias\", \u0026kms.AliasArgs{\nName: pulumi.String(\"alias/managed-services-customer-managed-key-alias\"),\nTargetKeyId: managedServicesCustomerManagedKey.KeyId,\n})\nif err != nil {\nreturn err\n}\n_, err = databricks.NewMwsCustomerManagedKeys(ctx, \"managed_services\", \u0026databricks.MwsCustomerManagedKeysArgs{\nAccountId: pulumi.Any(databricksAccountId),\nAwsKeyInfo: \u0026databricks.MwsCustomerManagedKeysAwsKeyInfoArgs{\nKeyArn: managedServicesCustomerManagedKey.Arn,\nKeyAlias: managedServicesCustomerManagedKeyAlias.Name,\n},\nUseCases: pulumi.StringArray{\npulumi.String(\"MANAGED_SERVICES\"),\n},\n})\nif err != nil {\nreturn err\n}\nreturn nil\n})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.AwsFunctions;\nimport com.pulumi.aws.inputs.GetCallerIdentityArgs;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var current = AwsFunctions.getCallerIdentity();\n\n        final var databricksManagedServicesCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.applyValue(getCallerIdentityResult -\u003e getCallerIdentityResult.accountId()))\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for control plane managed services\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\")\n                    .resources(\"*\")\n                    .build())\n            .build());\n\n        var managedServicesCustomerManagedKey = new Key(\"managedServicesCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksManagedServicesCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var managedServicesCustomerManagedKeyAlias = new Alias(\"managedServicesCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .name(\"alias/managed-services-customer-managed-key-alias\")\n            .targetKeyId(managedServicesCustomerManagedKey.keyId())\n            .build());\n\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(managedServicesCustomerManagedKey.arn())\n                .keyAlias(managedServicesCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  managedServicesCustomerManagedKey:\n    type: aws:kms:Key\n    name: managed_services_customer_managed_key\n    properties:\n      policy: ${databricksManagedServicesCmk.json}\n  managedServicesCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    name: managed_services_customer_managed_key_alias\n    properties:\n      name: alias/managed-services-customer-managed-key-alias\n      targetKeyId: ${managedServicesCustomerManagedKey.keyId}\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    name: managed_services\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${managedServicesCustomerManagedKey.arn}\n        keyAlias: ${managedServicesCustomerManagedKeyAlias.name}\n      useCases:\n        - MANAGED_SERVICES\nvariables:\n  current:\n    fn::invoke:\n      Function: aws:getCallerIdentity\n      Arguments: {}\n  databricksManagedServicesCmk:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for control plane managed services\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n            resources:\n              - '*'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### For GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\n// Id of a google_kms_crypto_key\nconst cmekResourceId = config.requireObject(\"cmekResourceId\");\nconst managedServices = new databricks.MwsCustomerManagedKeys(\"managed_services\", {\n    accountId: databricksAccountId,\n    gcpKeyInfo: {\n        kmsKeyId: cmekResourceId,\n    },\n    useCases: [\"MANAGED_SERVICES\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# Id of a google_kms_crypto_key\ncmek_resource_id = config.require_object(\"cmekResourceId\")\nmanaged_services = databricks.MwsCustomerManagedKeys(\"managed_services\",\n    account_id=databricks_account_id,\n    gcp_key_info=databricks.MwsCustomerManagedKeysGcpKeyInfoArgs(\n        kms_key_id=cmek_resource_id,\n    ),\n    use_cases=[\"MANAGED_SERVICES\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // Id of a google_kms_crypto_key\n    var cmekResourceId = config.RequireObject\u003cdynamic\u003e(\"cmekResourceId\");\n    var managedServices = new Databricks.MwsCustomerManagedKeys(\"managed_services\", new()\n    {\n        AccountId = databricksAccountId,\n        GcpKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysGcpKeyInfoArgs\n        {\n            KmsKeyId = cmekResourceId,\n        },\n        UseCases = new[]\n        {\n            \"MANAGED_SERVICES\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// Id of a google_kms_crypto_key\n\t\tcmekResourceId := cfg.RequireObject(\"cmekResourceId\")\n\t\t_, err := databricks.NewMwsCustomerManagedKeys(ctx, \"managed_services\", \u0026databricks.MwsCustomerManagedKeysArgs{\n\t\t\tAccountId: pulumi.Any(databricksAccountId),\n\t\t\tGcpKeyInfo: \u0026databricks.MwsCustomerManagedKeysGcpKeyInfoArgs{\n\t\t\t\tKmsKeyId: pulumi.Any(cmekResourceId),\n\t\t\t},\n\t\t\tUseCases: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"MANAGED_SERVICES\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysGcpKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var cmekResourceId = config.get(\"cmekResourceId\");\n        var managedServices = new MwsCustomerManagedKeys(\"managedServices\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .gcpKeyInfo(MwsCustomerManagedKeysGcpKeyInfoArgs.builder()\n                .kmsKeyId(cmekResourceId)\n                .build())\n            .useCases(\"MANAGED_SERVICES\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  cmekResourceId:\n    type: dynamic\nresources:\n  managedServices:\n    type: databricks:MwsCustomerManagedKeys\n    name: managed_services\n    properties:\n      accountId: ${databricksAccountId}\n      gcpKeyInfo:\n        kmsKeyId: ${cmekResourceId}\n      useCases:\n        - MANAGED_SERVICES\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Customer-managed key for workspace storage\n\n### For AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\n// AWS ARN for the Databricks cross account role\nconst databricksCrossAccountRole = config.requireObject(\"databricksCrossAccountRole\");\nconst databricksStorageCmk = aws.iam.getPolicyDocument({\n    version: \"2012-10-17\",\n    statements: [\n        {\n            sid: \"Enable IAM User Permissions\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [current.accountId],\n            }],\n            actions: [\"kms:*\"],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [\"arn:aws:iam::414351767826:root\"],\n            }],\n            actions: [\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"Bool\",\n                variable: \"kms:GrantIsForAWSResource\",\n                values: [\"true\"],\n            }],\n        },\n        {\n            sid: \"Allow Databricks to use KMS key for EBS\",\n            effect: \"Allow\",\n            principals: [{\n                type: \"AWS\",\n                identifiers: [databricksCrossAccountRole],\n            }],\n            actions: [\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources: [\"*\"],\n            conditions: [{\n                test: \"ForAnyValue:StringLike\",\n                variable: \"kms:ViaService\",\n                values: [\"ec2.*.amazonaws.com\"],\n            }],\n        },\n    ],\n});\nconst storageCustomerManagedKey = new aws.kms.Key(\"storage_customer_managed_key\", {policy: databricksStorageCmk.then(databricksStorageCmk =\u003e databricksStorageCmk.json)});\nconst storageCustomerManagedKeyAlias = new aws.kms.Alias(\"storage_customer_managed_key_alias\", {\n    name: \"alias/storage-customer-managed-key-alias\",\n    targetKeyId: storageCustomerManagedKey.keyId,\n});\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    awsKeyInfo: {\n        keyArn: storageCustomerManagedKey.arn,\n        keyAlias: storageCustomerManagedKeyAlias.name,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# AWS ARN for the Databricks cross account role\ndatabricks_cross_account_role = config.require_object(\"databricksCrossAccountRole\")\ndatabricks_storage_cmk = aws.iam.get_policy_document(version=\"2012-10-17\",\n    statements=[\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Enable IAM User Permissions\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[current[\"accountId\"]],\n            )],\n            actions=[\"kms:*\"],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:Encrypt\",\n                \"kms:Decrypt\",\n                \"kms:ReEncrypt*\",\n                \"kms:GenerateDataKey*\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for DBFS (Grants)\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[\"arn:aws:iam::414351767826:root\"],\n            )],\n            actions=[\n                \"kms:CreateGrant\",\n                \"kms:ListGrants\",\n                \"kms:RevokeGrant\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"Bool\",\n                variable=\"kms:GrantIsForAWSResource\",\n                values=[\"true\"],\n            )],\n        ),\n        aws.iam.GetPolicyDocumentStatementArgs(\n            sid=\"Allow Databricks to use KMS key for EBS\",\n            effect=\"Allow\",\n            principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n                type=\"AWS\",\n                identifiers=[databricks_cross_account_role],\n            )],\n            actions=[\n                \"kms:Decrypt\",\n                \"kms:GenerateDataKey*\",\n                \"kms:CreateGrant\",\n                \"kms:DescribeKey\",\n            ],\n            resources=[\"*\"],\n            conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n                test=\"ForAnyValue:StringLike\",\n                variable=\"kms:ViaService\",\n                values=[\"ec2.*.amazonaws.com\"],\n            )],\n        ),\n    ])\nstorage_customer_managed_key = aws.kms.Key(\"storage_customer_managed_key\", policy=databricks_storage_cmk.json)\nstorage_customer_managed_key_alias = aws.kms.Alias(\"storage_customer_managed_key_alias\",\n    name=\"alias/storage-customer-managed-key-alias\",\n    target_key_id=storage_customer_managed_key.key_id)\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    aws_key_info=databricks.MwsCustomerManagedKeysAwsKeyInfoArgs(\n        key_arn=storage_customer_managed_key.arn,\n        key_alias=storage_customer_managed_key_alias.name,\n    ),\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // AWS ARN for the Databricks cross account role\n    var databricksCrossAccountRole = config.RequireObject\u003cdynamic\u003e(\"databricksCrossAccountRole\");\n    var databricksStorageCmk = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Version = \"2012-10-17\",\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Enable IAM User Permissions\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            current.AccountId,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:*\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Encrypt\",\n                    \"kms:Decrypt\",\n                    \"kms:ReEncrypt*\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for DBFS (Grants)\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:root\",\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:CreateGrant\",\n                    \"kms:ListGrants\",\n                    \"kms:RevokeGrant\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"Bool\",\n                        Variable = \"kms:GrantIsForAWSResource\",\n                        Values = new[]\n                        {\n                            \"true\",\n                        },\n                    },\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"Allow Databricks to use KMS key for EBS\",\n                Effect = \"Allow\",\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            databricksCrossAccountRole,\n                        },\n                    },\n                },\n                Actions = new[]\n                {\n                    \"kms:Decrypt\",\n                    \"kms:GenerateDataKey*\",\n                    \"kms:CreateGrant\",\n                    \"kms:DescribeKey\",\n                },\n                Resources = new[]\n                {\n                    \"*\",\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"ForAnyValue:StringLike\",\n                        Variable = \"kms:ViaService\",\n                        Values = new[]\n                        {\n                            \"ec2.*.amazonaws.com\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var storageCustomerManagedKey = new Aws.Kms.Key(\"storage_customer_managed_key\", new()\n    {\n        Policy = databricksStorageCmk.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n    });\n\n    var storageCustomerManagedKeyAlias = new Aws.Kms.Alias(\"storage_customer_managed_key_alias\", new()\n    {\n        Name = \"alias/storage-customer-managed-key-alias\",\n        TargetKeyId = storageCustomerManagedKey.KeyId,\n    });\n\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        AwsKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysAwsKeyInfoArgs\n        {\n            KeyArn = storageCustomerManagedKey.Arn,\n            KeyAlias = storageCustomerManagedKeyAlias.Name,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/kms\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\nfunc main() {\npulumi.Run(func(ctx *pulumi.Context) error {\ncfg := config.New(ctx, \"\")\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n// AWS ARN for the Databricks cross account role\ndatabricksCrossAccountRole := cfg.RequireObject(\"databricksCrossAccountRole\")\ndatabricksStorageCmk, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\nVersion: pulumi.StringRef(\"2012-10-17\"),\nStatements: []iam.GetPolicyDocumentStatement{\n{\nSid: pulumi.StringRef(\"Enable IAM User Permissions\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ncurrent.AccountId,\n},\n},\n},\nActions: []string{\n\"kms:*\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for DBFS\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:Encrypt\",\n\"kms:Decrypt\",\n\"kms:ReEncrypt*\",\n\"kms:GenerateDataKey*\",\n\"kms:DescribeKey\",\n},\nResources: []string{\n\"*\",\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for DBFS (Grants)\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:root\",\n},\n},\n},\nActions: []string{\n\"kms:CreateGrant\",\n\"kms:ListGrants\",\n\"kms:RevokeGrant\",\n},\nResources: []string{\n\"*\",\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"Bool\",\nVariable: \"kms:GrantIsForAWSResource\",\nValues: []string{\n\"true\",\n},\n},\n},\n},\n{\nSid: pulumi.StringRef(\"Allow Databricks to use KMS key for EBS\"),\nEffect: pulumi.StringRef(\"Allow\"),\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: interface{}{\ndatabricksCrossAccountRole,\n},\n},\n},\nActions: []string{\n\"kms:Decrypt\",\n\"kms:GenerateDataKey*\",\n\"kms:CreateGrant\",\n\"kms:DescribeKey\",\n},\nResources: []string{\n\"*\",\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"ForAnyValue:StringLike\",\nVariable: \"kms:ViaService\",\nValues: []string{\n\"ec2.*.amazonaws.com\",\n},\n},\n},\n},\n},\n}, nil);\nif err != nil {\nreturn err\n}\nstorageCustomerManagedKey, err := kms.NewKey(ctx, \"storage_customer_managed_key\", \u0026kms.KeyArgs{\nPolicy: pulumi.String(databricksStorageCmk.Json),\n})\nif err != nil {\nreturn err\n}\nstorageCustomerManagedKeyAlias, err := kms.NewAlias(ctx, \"storage_customer_managed_key_alias\", \u0026kms.AliasArgs{\nName: pulumi.String(\"alias/storage-customer-managed-key-alias\"),\nTargetKeyId: storageCustomerManagedKey.KeyId,\n})\nif err != nil {\nreturn err\n}\n_, err = databricks.NewMwsCustomerManagedKeys(ctx, \"storage\", \u0026databricks.MwsCustomerManagedKeysArgs{\nAccountId: pulumi.Any(databricksAccountId),\nAwsKeyInfo: \u0026databricks.MwsCustomerManagedKeysAwsKeyInfoArgs{\nKeyArn: storageCustomerManagedKey.Arn,\nKeyAlias: storageCustomerManagedKeyAlias.Name,\n},\nUseCases: pulumi.StringArray{\npulumi.String(\"STORAGE\"),\n},\n})\nif err != nil {\nreturn err\n}\nreturn nil\n})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.kms.Key;\nimport com.pulumi.aws.kms.KeyArgs;\nimport com.pulumi.aws.kms.Alias;\nimport com.pulumi.aws.kms.AliasArgs;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysAwsKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksCrossAccountRole = config.get(\"databricksCrossAccountRole\");\n        final var databricksStorageCmk = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .version(\"2012-10-17\")\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Enable IAM User Permissions\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(current.accountId())\n                        .build())\n                    .actions(\"kms:*\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:Encrypt\",\n                        \"kms:Decrypt\",\n                        \"kms:ReEncrypt*\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for DBFS (Grants)\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(\"arn:aws:iam::414351767826:root\")\n                        .build())\n                    .actions(                    \n                        \"kms:CreateGrant\",\n                        \"kms:ListGrants\",\n                        \"kms:RevokeGrant\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"Bool\")\n                        .variable(\"kms:GrantIsForAWSResource\")\n                        .values(\"true\")\n                        .build())\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"Allow Databricks to use KMS key for EBS\")\n                    .effect(\"Allow\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(databricksCrossAccountRole)\n                        .build())\n                    .actions(                    \n                        \"kms:Decrypt\",\n                        \"kms:GenerateDataKey*\",\n                        \"kms:CreateGrant\",\n                        \"kms:DescribeKey\")\n                    .resources(\"*\")\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"ForAnyValue:StringLike\")\n                        .variable(\"kms:ViaService\")\n                        .values(\"ec2.*.amazonaws.com\")\n                        .build())\n                    .build())\n            .build());\n\n        var storageCustomerManagedKey = new Key(\"storageCustomerManagedKey\", KeyArgs.builder()        \n            .policy(databricksStorageCmk.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .build());\n\n        var storageCustomerManagedKeyAlias = new Alias(\"storageCustomerManagedKeyAlias\", AliasArgs.builder()        \n            .name(\"alias/storage-customer-managed-key-alias\")\n            .targetKeyId(storageCustomerManagedKey.keyId())\n            .build());\n\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .awsKeyInfo(MwsCustomerManagedKeysAwsKeyInfoArgs.builder()\n                .keyArn(storageCustomerManagedKey.arn())\n                .keyAlias(storageCustomerManagedKeyAlias.name())\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksCrossAccountRole:\n    type: dynamic\nresources:\n  storageCustomerManagedKey:\n    type: aws:kms:Key\n    name: storage_customer_managed_key\n    properties:\n      policy: ${databricksStorageCmk.json}\n  storageCustomerManagedKeyAlias:\n    type: aws:kms:Alias\n    name: storage_customer_managed_key_alias\n    properties:\n      name: alias/storage-customer-managed-key-alias\n      targetKeyId: ${storageCustomerManagedKey.keyId}\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      awsKeyInfo:\n        keyArn: ${storageCustomerManagedKey.arn}\n        keyAlias: ${storageCustomerManagedKeyAlias.name}\n      useCases:\n        - STORAGE\nvariables:\n  databricksStorageCmk:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        version: 2012-10-17\n        statements:\n          - sid: Enable IAM User Permissions\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${current.accountId}\n            actions:\n              - kms:*\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:Encrypt\n              - kms:Decrypt\n              - kms:ReEncrypt*\n              - kms:GenerateDataKey*\n              - kms:DescribeKey\n            resources:\n              - '*'\n          - sid: Allow Databricks to use KMS key for DBFS (Grants)\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::414351767826:root\n            actions:\n              - kms:CreateGrant\n              - kms:ListGrants\n              - kms:RevokeGrant\n            resources:\n              - '*'\n            conditions:\n              - test: Bool\n                variable: kms:GrantIsForAWSResource\n                values:\n                  - 'true'\n          - sid: Allow Databricks to use KMS key for EBS\n            effect: Allow\n            principals:\n              - type: AWS\n                identifiers:\n                  - ${databricksCrossAccountRole}\n            actions:\n              - kms:Decrypt\n              - kms:GenerateDataKey*\n              - kms:CreateGrant\n              - kms:DescribeKey\n            resources:\n              - '*'\n            conditions:\n              - test: ForAnyValue:StringLike\n                variable: kms:ViaService\n                values:\n                  - ec2.*.amazonaws.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### For GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\n// Id of a google_kms_crypto_key\nconst cmekResourceId = config.requireObject(\"cmekResourceId\");\nconst storage = new databricks.MwsCustomerManagedKeys(\"storage\", {\n    accountId: databricksAccountId,\n    gcpKeyInfo: {\n        kmsKeyId: cmekResourceId,\n    },\n    useCases: [\"STORAGE\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# Id of a google_kms_crypto_key\ncmek_resource_id = config.require_object(\"cmekResourceId\")\nstorage = databricks.MwsCustomerManagedKeys(\"storage\",\n    account_id=databricks_account_id,\n    gcp_key_info=databricks.MwsCustomerManagedKeysGcpKeyInfoArgs(\n        kms_key_id=cmek_resource_id,\n    ),\n    use_cases=[\"STORAGE\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // Id of a google_kms_crypto_key\n    var cmekResourceId = config.RequireObject\u003cdynamic\u003e(\"cmekResourceId\");\n    var storage = new Databricks.MwsCustomerManagedKeys(\"storage\", new()\n    {\n        AccountId = databricksAccountId,\n        GcpKeyInfo = new Databricks.Inputs.MwsCustomerManagedKeysGcpKeyInfoArgs\n        {\n            KmsKeyId = cmekResourceId,\n        },\n        UseCases = new[]\n        {\n            \"STORAGE\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.gcp.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// Id of a google_kms_crypto_key\n\t\tcmekResourceId := cfg.RequireObject(\"cmekResourceId\")\n\t\t_, err := databricks.NewMwsCustomerManagedKeys(ctx, \"storage\", \u0026databricks.MwsCustomerManagedKeysArgs{\n\t\t\tAccountId: pulumi.Any(databricksAccountId),\n\t\t\tGcpKeyInfo: \u0026databricks.MwsCustomerManagedKeysGcpKeyInfoArgs{\n\t\t\t\tKmsKeyId: pulumi.Any(cmekResourceId),\n\t\t\t},\n\t\t\tUseCases: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"STORAGE\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCustomerManagedKeys;\nimport com.pulumi.databricks.MwsCustomerManagedKeysArgs;\nimport com.pulumi.databricks.inputs.MwsCustomerManagedKeysGcpKeyInfoArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var cmekResourceId = config.get(\"cmekResourceId\");\n        var storage = new MwsCustomerManagedKeys(\"storage\", MwsCustomerManagedKeysArgs.builder()        \n            .accountId(databricksAccountId)\n            .gcpKeyInfo(MwsCustomerManagedKeysGcpKeyInfoArgs.builder()\n                .kmsKeyId(cmekResourceId)\n                .build())\n            .useCases(\"STORAGE\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  cmekResourceId:\n    type: dynamic\nresources:\n  storage:\n    type: databricks:MwsCustomerManagedKeys\n    properties:\n      accountId: ${databricksAccountId}\n      gcpKeyInfo:\n        kmsKeyId: ${cmekResourceId}\n      useCases:\n        - STORAGE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n"
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "gcpKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n"
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "customerManagedKeyId",
                "useCases"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "description": "(String) ID of the encryption key configuration object.\n"
                },
                "gcpKeyInfo": {
                    "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                    "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n",
                    "willReplaceOnChanges": true
                },
                "useCases": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "useCases"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsCustomerManagedKeys resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysAwsKeyInfo:MwsCustomerManagedKeysAwsKeyInfo",
                        "description": "This field is a block and is documented below. This conflicts with `gcp_key_info`\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) Time in epoch milliseconds when the customer key was created.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "description": "(String) ID of the encryption key configuration object.\n"
                    },
                    "gcpKeyInfo": {
                        "$ref": "#/types/databricks:index/MwsCustomerManagedKeysGcpKeyInfo:MwsCustomerManagedKeysGcpKeyInfo",
                        "description": "This field is a block and is documented below. This conflicts with `aws_key_info`\n",
                        "willReplaceOnChanges": true
                    },
                    "useCases": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "*(since v0.3.4)* List of use cases for which this key will be used. *If you've used the resource before, please add `use_cases = [\"MANAGED_SERVICES\"]` to keep the previous behaviour.* Possible values are:\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsLogDelivery:MwsLogDelivery": {
            "description": "\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nThis resource configures the delivery of the two supported log types from Databricks workspaces: [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n\nYou cannot delete a log delivery configuration, but you can disable it when you no longer need it. This fact is important because there is a limit to the number of enabled log delivery configurations that you can create for an account. You can create a maximum of two enabled configurations that use the account level (no workspace filter) and two enabled configurations for every specific workspace (a workspaceId can occur in the workspace filter for two configurations). You can re-enable a disabled configuration, but the request fails if it violates the limits previously described.\n\n## Billable Usage\n\nCSV files are delivered to `\u003cdelivery_path_prefix\u003e/billable-usage/csv/` and are named `workspaceId=\u003cworkspace-id\u003e-usageMonth=\u003cmonth\u003e.csv`, which are delivered daily by overwriting the month's CSV file for each workspace. Format of CSV file, as well as some usage examples, can be found [here](https://docs.databricks.com/administration-guide/account-settings/usage.html#download-usage-as-a-csv-file).\n\nCommon processing scenario is to apply [cost allocation tags](https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/cost-alloc-tags.html), that could be enforced by setting custom_tags on a cluster or through cluster policy. Report contains `clusterId` field, that could be joined with data from AWS [cost and usage reports](https://docs.aws.amazon.com/cur/latest/userguide/cur-create.html), that can be joined with `user:ClusterId` tag from AWS usage report.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst usageLogs = new databricks.MwsLogDelivery(\"usage_logs\", {\n    accountId: databricksAccountId,\n    credentialsId: logWriter.credentialsId,\n    storageConfigurationId: logBucket.storageConfigurationId,\n    deliveryPathPrefix: \"billable-usage\",\n    configName: \"Usage Logs\",\n    logType: \"BILLABLE_USAGE\",\n    outputFormat: \"CSV\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nusage_logs = databricks.MwsLogDelivery(\"usage_logs\",\n    account_id=databricks_account_id,\n    credentials_id=log_writer[\"credentialsId\"],\n    storage_configuration_id=log_bucket[\"storageConfigurationId\"],\n    delivery_path_prefix=\"billable-usage\",\n    config_name=\"Usage Logs\",\n    log_type=\"BILLABLE_USAGE\",\n    output_format=\"CSV\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var usageLogs = new Databricks.MwsLogDelivery(\"usage_logs\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsId = logWriter.CredentialsId,\n        StorageConfigurationId = logBucket.StorageConfigurationId,\n        DeliveryPathPrefix = \"billable-usage\",\n        ConfigName = \"Usage Logs\",\n        LogType = \"BILLABLE_USAGE\",\n        OutputFormat = \"CSV\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"usage_logs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tCredentialsId:          pulumi.Any(logWriter.CredentialsId),\n\t\t\tStorageConfigurationId: pulumi.Any(logBucket.StorageConfigurationId),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"billable-usage\"),\n\t\t\tConfigName:             pulumi.String(\"Usage Logs\"),\n\t\t\tLogType:                pulumi.String(\"BILLABLE_USAGE\"),\n\t\t\tOutputFormat:           pulumi.String(\"CSV\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var usageLogs = new MwsLogDelivery(\"usageLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsId(logWriter.credentialsId())\n            .storageConfigurationId(logBucket.storageConfigurationId())\n            .deliveryPathPrefix(\"billable-usage\")\n            .configName(\"Usage Logs\")\n            .logType(\"BILLABLE_USAGE\")\n            .outputFormat(\"CSV\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  usageLogs:\n    type: databricks:MwsLogDelivery\n    name: usage_logs\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsId: ${logWriter.credentialsId}\n      storageConfigurationId: ${logBucket.storageConfigurationId}\n      deliveryPathPrefix: billable-usage\n      configName: Usage Logs\n      logType: BILLABLE_USAGE\n      outputFormat: CSV\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Audit Logs\n\nJSON files with [static schema](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html#audit-log-schema) are delivered to `\u003cdelivery_path_prefix\u003e/workspaceId=\u003cworkspaceId\u003e/date=\u003cyyyy-mm-dd\u003e/auditlogs_\u003cinternal-id\u003e.json`. Logs are available within 15 minutes of activation for audit logs. New JSON files are delivered every few minutes, potentially overwriting existing files for each workspace. Sometimes data may arrive later than 15 minutes. Databricks can overwrite the delivered log files in your bucket at any time. If a file is overwritten, the existing content remains, but there may be additional lines for more auditable events. Overwriting ensures exactly-once semantics without requiring read or delete access to your account.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst auditLogs = new databricks.MwsLogDelivery(\"audit_logs\", {\n    accountId: databricksAccountId,\n    credentialsId: logWriter.credentialsId,\n    storageConfigurationId: logBucket.storageConfigurationId,\n    deliveryPathPrefix: \"audit-logs\",\n    configName: \"Audit Logs\",\n    logType: \"AUDIT_LOGS\",\n    outputFormat: \"JSON\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naudit_logs = databricks.MwsLogDelivery(\"audit_logs\",\n    account_id=databricks_account_id,\n    credentials_id=log_writer[\"credentialsId\"],\n    storage_configuration_id=log_bucket[\"storageConfigurationId\"],\n    delivery_path_prefix=\"audit-logs\",\n    config_name=\"Audit Logs\",\n    log_type=\"AUDIT_LOGS\",\n    output_format=\"JSON\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var auditLogs = new Databricks.MwsLogDelivery(\"audit_logs\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsId = logWriter.CredentialsId,\n        StorageConfigurationId = logBucket.StorageConfigurationId,\n        DeliveryPathPrefix = \"audit-logs\",\n        ConfigName = \"Audit Logs\",\n        LogType = \"AUDIT_LOGS\",\n        OutputFormat = \"JSON\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsLogDelivery(ctx, \"audit_logs\", \u0026databricks.MwsLogDeliveryArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tCredentialsId:          pulumi.Any(logWriter.CredentialsId),\n\t\t\tStorageConfigurationId: pulumi.Any(logBucket.StorageConfigurationId),\n\t\t\tDeliveryPathPrefix:     pulumi.String(\"audit-logs\"),\n\t\t\tConfigName:             pulumi.String(\"Audit Logs\"),\n\t\t\tLogType:                pulumi.String(\"AUDIT_LOGS\"),\n\t\t\tOutputFormat:           pulumi.String(\"JSON\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsLogDelivery;\nimport com.pulumi.databricks.MwsLogDeliveryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var auditLogs = new MwsLogDelivery(\"auditLogs\", MwsLogDeliveryArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsId(logWriter.credentialsId())\n            .storageConfigurationId(logBucket.storageConfigurationId())\n            .deliveryPathPrefix(\"audit-logs\")\n            .configName(\"Audit Logs\")\n            .logType(\"AUDIT_LOGS\")\n            .outputFormat(\"JSON\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  auditLogs:\n    type: databricks:MwsLogDelivery\n    name: audit_logs\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsId: ${logWriter.credentialsId}\n      storageConfigurationId: ${logBucket.storageConfigurationId}\n      deliveryPathPrefix: audit-logs\n      configName: Audit Logs\n      logType: AUDIT_LOGS\n      outputFormat: JSON\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n"
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n"
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n"
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n"
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n"
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n"
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n"
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n"
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n"
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n"
                }
            },
            "required": [
                "accountId",
                "configId",
                "credentialsId",
                "deliveryStartTime",
                "logType",
                "outputFormat",
                "status",
                "storageConfigurationId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "willReplaceOnChanges": true
                },
                "configId": {
                    "type": "string",
                    "description": "Databricks log delivery configuration ID.\n",
                    "willReplaceOnChanges": true
                },
                "configName": {
                    "type": "string",
                    "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                    "willReplaceOnChanges": true
                },
                "credentialsId": {
                    "type": "string",
                    "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryPathPrefix": {
                    "type": "string",
                    "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                    "willReplaceOnChanges": true
                },
                "deliveryStartTime": {
                    "type": "string",
                    "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                    "willReplaceOnChanges": true
                },
                "logType": {
                    "type": "string",
                    "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                    "willReplaceOnChanges": true
                },
                "outputFormat": {
                    "type": "string",
                    "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string",
                    "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceIdsFilters": {
                    "type": "array",
                    "items": {
                        "type": "integer"
                    },
                    "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "credentialsId",
                "logType",
                "outputFormat",
                "storageConfigurationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsLogDelivery resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "configId": {
                        "type": "string",
                        "description": "Databricks log delivery configuration ID.\n",
                        "willReplaceOnChanges": true
                    },
                    "configName": {
                        "type": "string",
                        "description": "The optional human-readable name of the log delivery configuration. Defaults to empty.\n",
                        "willReplaceOnChanges": true
                    },
                    "credentialsId": {
                        "type": "string",
                        "description": "The ID for a Databricks credential configuration that represents the AWS IAM role with policy and trust relationship as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryPathPrefix": {
                        "type": "string",
                        "description": "Defaults to empty, which means that logs are delivered to the root of the bucket. The value must be a valid S3 object key. It must not start or end with a slash character.\n",
                        "willReplaceOnChanges": true
                    },
                    "deliveryStartTime": {
                        "type": "string",
                        "description": "The optional start month and year for delivery, specified in YYYY-MM format. Defaults to current year and month. Usage is not available before 2019-03.\n",
                        "willReplaceOnChanges": true
                    },
                    "logType": {
                        "type": "string",
                        "description": "The type of log delivery. `BILLABLE_USAGE` and `AUDIT_LOGS` are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "outputFormat": {
                        "type": "string",
                        "description": "The file type of log delivery. Currently `CSV` (for `BILLABLE_USAGE`) and `JSON` (for `AUDIT_LOGS`) are supported.\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string",
                        "description": "Status of log delivery configuration. Set to ENABLED or DISABLED. Defaults to ENABLED. This is the only field you can update.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "The ID for a Databricks storage configuration that represents the S3 bucket with bucket policy as described in the main billable usage documentation page.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceIdsFilters": {
                        "type": "array",
                        "items": {
                            "type": "integer"
                        },
                        "description": "By default, this log configuration applies to all workspaces associated with your account ID. If your account is on the E2 version of the platform or on a select custom plan that allows multiple workspaces per account, you may have multiple workspaces associated with your account ID. You can optionally set the field as mentioned earlier to an array of workspace IDs. If you plan to use different log delivery configurations for several workspaces, set this explicitly rather than leaving it blank. If you leave this blank and your account ID gets additional workspaces in the future, this configuration will also apply to the new workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNccBinding:MwsNccBinding": {
            "description": "\u003e **Note** Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e **Public Preview** This feature is available for AWS \u0026 Azure only, and is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html) in AWS.\n\nAllows you to attach a Network Connectivity Config object to a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages serverless network connectivity configs](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/serverless-firewall).\n\nThe NCC and workspace must be in the same region.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject(\"region\");\nconst prefix = config.requireObject(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `Network Connectivity Config for ${prefix}`,\n    region: region,\n});\nconst nccBinding = new databricks.MwsNccBinding(\"ncc_binding\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    workspaceId: databricksWorkspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"Network Connectivity Config for {prefix}\",\n    region=region)\nncc_binding = databricks.MwsNccBinding(\"ncc_binding\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    workspace_id=databricks_workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"Network Connectivity Config for {prefix}\",\n        Region = region,\n    });\n\n    var nccBinding = new Databricks.MwsNccBinding(\"ncc_binding\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        WorkspaceId = databricksWorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.String(fmt.Sprintf(\"Network Connectivity Config for %v\", prefix)),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccBinding(ctx, \"ncc_binding\", \u0026databricks.MwsNccBindingArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tWorkspaceId:                 pulumi.Any(databricksWorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccBinding;\nimport com.pulumi.databricks.MwsNccBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()        \n            .name(String.format(\"Network Connectivity Config for %s\", prefix))\n            .region(region)\n            .build());\n\n        var nccBinding = new MwsNccBinding(\"nccBinding\", MwsNccBindingArgs.builder()        \n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .workspaceId(databricksWorkspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: Network Connectivity Config for ${prefix}\n      region: ${region}\n  nccBinding:\n    type: databricks:MwsNccBinding\n    name: ncc_binding\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      workspaceId: ${databricksWorkspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n* databricks.MwsNetworkConnectivityConfig to create Network Connectivity Config objects.\n",
            "properties": {
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "networkConnectivityConfigId",
                "workspaceId"
            ],
            "inputProperties": {
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "networkConnectivityConfigId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNccBinding resources.\n",
                "properties": {
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account.\n"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "Identifier of the workspace to attach the NCC to. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNccPrivateEndpointRule:MwsNccPrivateEndpointRule": {
            "description": "\u003e **Note** Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e **Note** This feature is only available in Azure.\n\nAllows you to create a private endpoint in a Network Connectivity Config that can be used to [configure private connectivity from serverless compute](https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/serverless-private-link).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject(\"region\");\nconst prefix = config.requireObject(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `Network Connectivity Config for ${prefix}`,\n    region: region,\n});\nconst storage = new databricks.MwsNccPrivateEndpointRule(\"storage\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    resourceId: \"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n    groupId: \"blob\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"Network Connectivity Config for {prefix}\",\n    region=region)\nstorage = databricks.MwsNccPrivateEndpointRule(\"storage\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    resource_id=\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n    group_id=\"blob\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"Network Connectivity Config for {prefix}\",\n        Region = region,\n    });\n\n    var storage = new Databricks.MwsNccPrivateEndpointRule(\"storage\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        ResourceId = \"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\",\n        GroupId = \"blob\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.String(fmt.Sprintf(\"Network Connectivity Config for %v\", prefix)),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccPrivateEndpointRule(ctx, \"storage\", \u0026databricks.MwsNccPrivateEndpointRuleArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tResourceId:                  pulumi.String(\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\"),\n\t\t\tGroupId:                     pulumi.String(\"blob\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccPrivateEndpointRule;\nimport com.pulumi.databricks.MwsNccPrivateEndpointRuleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()        \n            .name(String.format(\"Network Connectivity Config for %s\", prefix))\n            .region(region)\n            .build());\n\n        var storage = new MwsNccPrivateEndpointRule(\"storage\", MwsNccPrivateEndpointRuleArgs.builder()        \n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .resourceId(\"/subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\")\n            .groupId(\"blob\")\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: Network Connectivity Config for ${prefix}\n      region: ${region}\n  storage:\n    type: databricks:MwsNccPrivateEndpointRule\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      resourceId: /subscriptions/653bb673-1234-abcd-a90b-d064d5d53ca4/resourcegroups/example-resource-group/providers/Microsoft.Storage/storageAccounts/examplesa\n      groupId: blob\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsNetworkConnectivityConfig to create Network Connectivity Config objects.\n* databricks.MwsNccBinding to attach an NCC to a workspace.\n\n## Import\n\nThis resource can be imported by Databricks account ID and Network Connectivity Config ID.\n\n```sh\n$ pulumi import databricks:index/mwsNccPrivateEndpointRule:MwsNccPrivateEndpointRule rule \u003cnetwork_connectivity_config_id\u003e/\u003crule_id\u003e\n```\n\n",
            "properties": {
                "connectionState": {
                    "type": "string",
                    "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was created.\n"
                },
                "deactivated": {
                    "type": "boolean",
                    "description": "Whether this private endpoint is deactivated.\n"
                },
                "deactivatedAt": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was deactivated.\n"
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "The sub-resource type (group ID) of the target resource. Must be one of `blob`, `dfs`, `sqlServer` or `mysqlServer`. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource.\n"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n"
                },
                "resourceId": {
                    "type": "string",
                    "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n"
                },
                "ruleId": {
                    "type": "string",
                    "description": "the ID of a private endpoint rule.\n"
                },
                "updatedTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was updated.\n"
                }
            },
            "required": [
                "connectionState",
                "creationTime",
                "endpointName",
                "groupId",
                "networkConnectivityConfigId",
                "resourceId",
                "ruleId",
                "updatedTime"
            ],
            "inputProperties": {
                "connectionState": {
                    "type": "string",
                    "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was created.\n"
                },
                "deactivated": {
                    "type": "boolean",
                    "description": "Whether this private endpoint is deactivated.\n",
                    "willReplaceOnChanges": true
                },
                "deactivatedAt": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was deactivated.\n",
                    "willReplaceOnChanges": true
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                },
                "groupId": {
                    "type": "string",
                    "description": "The sub-resource type (group ID) of the target resource. Must be one of `blob`, `dfs`, `sqlServer` or `mysqlServer`. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "resourceId": {
                    "type": "string",
                    "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "ruleId": {
                    "type": "string",
                    "description": "the ID of a private endpoint rule.\n"
                },
                "updatedTime": {
                    "type": "integer",
                    "description": "Time in epoch milliseconds when this object was updated.\n"
                }
            },
            "requiredInputs": [
                "groupId",
                "networkConnectivityConfigId",
                "resourceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNccPrivateEndpointRule resources.\n",
                "properties": {
                    "connectionState": {
                        "type": "string",
                        "description": "The current status of this private endpoint. The private endpoint rules are effective only if the connection state is ESTABLISHED. Remember that you must approve new endpoints on your resources in the Azure portal before they take effect.\nThe possible values are:\n"
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was created.\n"
                    },
                    "deactivated": {
                        "type": "boolean",
                        "description": "Whether this private endpoint is deactivated.\n",
                        "willReplaceOnChanges": true
                    },
                    "deactivatedAt": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was deactivated.\n",
                        "willReplaceOnChanges": true
                    },
                    "endpointName": {
                        "type": "string",
                        "description": "The name of the Azure private endpoint resource, e.g. \"databricks-088781b3-77fa-4132-b429-1af0d91bc593-pe-3cb31234\"\n"
                    },
                    "groupId": {
                        "type": "string",
                        "description": "The sub-resource type (group ID) of the target resource. Must be one of `blob`, `dfs`, `sqlServer` or `mysqlServer`. Note that to connect to workspace root storage (root DBFS), you need two endpoints, one for blob and one for dfs. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "resourceId": {
                        "type": "string",
                        "description": "The Azure resource ID of the target resource. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "ruleId": {
                        "type": "string",
                        "description": "the ID of a private endpoint rule.\n"
                    },
                    "updatedTime": {
                        "type": "integer",
                        "description": "Time in epoch milliseconds when this object was updated.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworkConnectivityConfig:MwsNetworkConnectivityConfig": {
            "description": "\u003e **Note** Initialize provider with `alias = \"account\"`, `host = \"https://accounts.azuredatabricks.net\"` and use `provider = databricks.account` for all `databricks_mws_*` resources.\n\n\u003e **Public Preview** This feature is available for AWS \u0026 Azure only, and is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html) in AWS.\n\nAllows you to create a [Network Connectivity Config] that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages serverless network connectivity configs](https://learn.microsoft.com/en-us/azure/databricks/security/network/serverless-network-security/serverless-firewall).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\nconst region = config.requireObject(\"region\");\nconst prefix = config.requireObject(\"prefix\");\nconst ncc = new databricks.MwsNetworkConnectivityConfig(\"ncc\", {\n    name: `Network Connectivity Config for ${prefix}`,\n    region: region,\n});\nconst nccBinding = new databricks.MwsNccBinding(\"ncc_binding\", {\n    networkConnectivityConfigId: ncc.networkConnectivityConfigId,\n    workspaceId: databricksWorkspaceId,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\nregion = config.require_object(\"region\")\nprefix = config.require_object(\"prefix\")\nncc = databricks.MwsNetworkConnectivityConfig(\"ncc\",\n    name=f\"Network Connectivity Config for {prefix}\",\n    region=region)\nncc_binding = databricks.MwsNccBinding(\"ncc_binding\",\n    network_connectivity_config_id=ncc.network_connectivity_config_id,\n    workspace_id=databricks_workspace_id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    var region = config.RequireObject\u003cdynamic\u003e(\"region\");\n    var prefix = config.RequireObject\u003cdynamic\u003e(\"prefix\");\n    var ncc = new Databricks.MwsNetworkConnectivityConfig(\"ncc\", new()\n    {\n        Name = $\"Network Connectivity Config for {prefix}\",\n        Region = region,\n    });\n\n    var nccBinding = new Databricks.MwsNccBinding(\"ncc_binding\", new()\n    {\n        NetworkConnectivityConfigId = ncc.NetworkConnectivityConfigId,\n        WorkspaceId = databricksWorkspaceId,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\tregion := cfg.RequireObject(\"region\")\n\t\tprefix := cfg.RequireObject(\"prefix\")\n\t\tncc, err := databricks.NewMwsNetworkConnectivityConfig(ctx, \"ncc\", \u0026databricks.MwsNetworkConnectivityConfigArgs{\n\t\t\tName:   pulumi.String(fmt.Sprintf(\"Network Connectivity Config for %v\", prefix)),\n\t\t\tRegion: pulumi.Any(region),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsNccBinding(ctx, \"ncc_binding\", \u0026databricks.MwsNccBindingArgs{\n\t\t\tNetworkConnectivityConfigId: ncc.NetworkConnectivityConfigId,\n\t\t\tWorkspaceId:                 pulumi.Any(databricksWorkspaceId),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfig;\nimport com.pulumi.databricks.MwsNetworkConnectivityConfigArgs;\nimport com.pulumi.databricks.MwsNccBinding;\nimport com.pulumi.databricks.MwsNccBindingArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var region = config.get(\"region\");\n        final var prefix = config.get(\"prefix\");\n        var ncc = new MwsNetworkConnectivityConfig(\"ncc\", MwsNetworkConnectivityConfigArgs.builder()        \n            .name(String.format(\"Network Connectivity Config for %s\", prefix))\n            .region(region)\n            .build());\n\n        var nccBinding = new MwsNccBinding(\"nccBinding\", MwsNccBindingArgs.builder()        \n            .networkConnectivityConfigId(ncc.networkConnectivityConfigId())\n            .workspaceId(databricksWorkspaceId)\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  region:\n    type: dynamic\n  prefix:\n    type: dynamic\nresources:\n  ncc:\n    type: databricks:MwsNetworkConnectivityConfig\n    properties:\n      name: Network Connectivity Config for ${prefix}\n      region: ${region}\n  nccBinding:\n    type: databricks:MwsNccBinding\n    name: ncc_binding\n    properties:\n      networkConnectivityConfigId: ${ncc.networkConnectivityConfigId}\n      workspaceId: ${databricksWorkspaceId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the context:\n\n* databricks.MwsWorkspaces to set up Databricks workspaces.\n* databricks.MwsNccBinding to attach an NCC to a workspace.\n* databricks.MwsNccPrivateEndpointRule to create a private endpoint rule.\n\n## Import\n\nThis resource can be imported by Databricks account ID and Network Connectivity Config ID.\n\n```sh\n$ pulumi import databricks:index/mwsNetworkConnectivityConfig:MwsNetworkConnectivityConfig ncc \u003caccount_id\u003e/\u003cnetwork_connectivity_config_id\u003e\n```\n\n",
            "properties": {
                "accountId": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "egressConfig": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n"
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n"
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "egressConfig",
                "name",
                "networkConnectivityConfigId",
                "region",
                "updatedTime"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "egressConfig": {
                    "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "networkConnectivityConfigId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "updatedTime": {
                    "type": "integer"
                }
            },
            "requiredInputs": [
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworkConnectivityConfig resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "egressConfig": {
                        "$ref": "#/types/databricks:index/MwsNetworkConnectivityConfigEgressConfig:MwsNetworkConnectivityConfigEgressConfig"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Network Connectivity Config in Databricks Account. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "networkConnectivityConfigId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Network Connectivity Config in Databricks Account\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of the Network Connectivity Config. NCCs can only be referenced by your workspaces in the same region. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "updatedTime": {
                        "type": "integer"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsNetworks:MwsNetworks": {
            "description": "## Databricks on AWS usage\n\n\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nUse this resource to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS. It is essential to understand that this will require you to configure your provider separately for the multiple workspaces resources.\n\n* Databricks must have access to at least two subnets for each workspace, with each subnet in a different Availability Zone. You cannot specify more than one Databricks workspace subnet per Availability Zone in the Create network configuration API call. You can have more than one subnet per Availability Zone as part of your network setup, but you can choose only one subnet per Availability Zone for the Databricks workspace.\n* Databricks assigns two IP addresses per node, one for management traffic and one for Spark applications. The total number of instances for each subnet is equal to half of the available IP addresses.\n* Each subnet must have a netmask between /17 and /25.\n* Subnets must be private.\n* Subnets must have outbound access to the public network using a aws_nat_gateway, or other similar customer-managed appliance infrastructure.\n* The NAT gateway must be set up in its subnet (public_subnets in the example below) that routes quad-zero (0.0.0.0/0) traffic to an internet gateway or other customer-managed appliance infrastructure.\n\n\u003e **Note** The NAT gateway needs only one IP address per AZ. Hence, the public subnet only needs two IP addresses. In order to limit the number of IP addresses in the public subnet, you can specify a secondary CIDR block (cidr_block_public) using the argument secondary_cidr_blocks then pass it to the public_subnets argument. Please review the [IPv4 CIDR block association restrictions](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html) when choosing the secondary cidr block.\n\nPlease follow this complete runnable example \u0026 subnet for new workspaces within GCP. It is essential to understand that this will require you to configure your provider separately for the multiple workspaces resources.\n\n* Databricks must have access to a subnet in the same region as the workspace, of which IP range will be used to allocate your workspace’s GKE cluster nodes.\n* The subnet must have a netmask between /29 and /9.\n* Databricks must have access to 2 secondary IP ranges, one between /21 to /9 for workspace’s GKE cluster pods, and one between /27 to /16 for workspace’s GKE cluster services.\n* Subnet must have outbound access to the public network using a gcp_compute_router_nat or other similar customer-managed appliance infrastructure.\n\nPlease follow this complete runnable example]\n  private_subnets = [cidrsubnet(var.cidr_block, 3, 1),\n  cidrsubnet(var.cidr_block, 3, 2)]\n\n  default_security_group_egress = [{\n    cidr_blocks = \"0.0.0.0/0\"\n  }]\n\n  default_security_group_ingress = [{\n    description = \"Allow all internal TCP and UDP\"\n    self        = true\n  }]\n}\n\nresource \"databricks.MwsNetworks\" \"this\" {\n  provider           = databricks.mws\n  account_id         = var.databricks_account_id\n  network_name       = \"${local.prefix}-network\"\n  security_group_ids = [module.vpc.default_security_group_id]\n  subnet_ids         = module.vpc.private_subnets\n  vpc_id             = module.vpc.vpc_id\n}\n```\n\nIn order to create a VPC [that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) you would need to add the `vpc_endpoint_id` Attributes from [mws_vpc_endpoint](mws_vpc_endpoint.md) resources into the [databricks_mws_networks](databricks_mws_networks.md) resource. For example:\n\n```hcl\nresource \"databricks_mws_networks\" \"this\" {\n  provider           = databricks.mws\n  account_id         = var.databricks_account_id\n  network_name       = \"${local.prefix}-network\"\n  security_group_ids = [module.vpc.default_security_group_id]\n  subnet_ids         = module.vpc.private_subnets\n  vpc_id             = module.vpc.vpc_id\n  vpc_endpoints {\n    dataplane_relay = [databricks_mws_vpc_endpoint.relay.vpc_endpoint_id]\n    rest_api        = [databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id]\n  }\n  depends_on = [aws_vpc_endpoint.workspace, aws_vpc_endpoint.relay]\n}\n```\n\n### Creating a Databricks on GCP workspace\n\n```hcl\nvariable \"databricks_account_id\" {\n  description = \"Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\"\n}\n\nresource \"google_compute_network\" \"dbx_private_vpc\" {\n  project                 = var.google_project\n  name                    = \"tf-network-${random_string.suffix.result}\"\n  auto_create_subnetworks = false\n}\n\nresource \"google_compute_subnetwork\" \"network-with-private-secondary-ip-ranges\" {\n  name          = \"test-dbx-${random_string.suffix.result}\"\n  ip_cidr_range = \"10.0.0.0/16\"\n  region        = \"us-central1\"\n  network       = google_compute_network.dbx_private_vpc.id\n  secondary_ip_range {\n    range_name    = \"pods\"\n    ip_cidr_range = \"10.1.0.0/16\"\n  }\n  secondary_ip_range {\n    range_name    = \"svc\"\n    ip_cidr_range = \"10.2.0.0/20\"\n  }\n  private_ip_google_access = true\n}\n\nresource \"google_compute_router\" \"router\" {\n  name    = \"my-router-${random_string.suffix.result}\"\n  region  = google_compute_subnetwork.network-with-private-secondary-ip-ranges.region\n  network = google_compute_network.dbx_private_vpc.id\n}\n\nresource \"google_compute_router_nat\" \"nat\" {\n  name                               = \"my-router-nat-${random_string.suffix.result}\"\n  router                             = google_compute_router.router.name\n  region                             = google_compute_router.router.region\n  nat_ip_allocate_option             = \"AUTO_ONLY\"\n  source_subnetwork_ip_ranges_to_nat = \"ALL_SUBNETWORKS_ALL_IP_RANGES\"\n}\n\nresource \"databricks_mws_networks\" \"this\" {\n  account_id   = var.databricks_account_id\n  network_name = \"test-demo-${random_string.suffix.result}\"\n  gcp_network_info {\n    network_project_id    = var.google_project\n    vpc_id                = google_compute_network.dbx_private_vpc.name\n    subnet_id             = google_compute_subnetwork.network_with_private_secondary_ip_ranges.name\n    subnet_region         = google_compute_subnetwork.network_with_private_secondary_ip_ranges.region\n    pod_ip_range_name     = \"pods\"\n    service_ip_range_name = \"svc\"\n  }\n}\n```\n\nIn order to create a VPC [that leverages GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) you would need to add the `vpc_endpoint_id` Attributes from mws_vpc_endpoint resources into the databricks.MwsNetworks resource. For example:\n\n```hcl\nresource \"databricks_mws_networks\" \"this\" {\n  account_id   = var.databricks_account_id\n  network_name = \"test-demo-${random_string.suffix.result}\"\n  gcp_network_info {\n    network_project_id    = var.google_project\n    vpc_id                = google_compute_network.dbx_private_vpc.name\n    subnet_id             = google_compute_subnetwork.network_with_private_secondary_ip_ranges.name\n    subnet_region         = google_compute_subnetwork.network_with_private_secondary_ip_ranges.region\n    pod_ip_range_name     = \"pods\"\n    service_ip_range_name = \"svc\"\n  }\n  vpc_endpoints {\n    dataplane_relay = [databricks_mws_vpc_endpoint.relay.vpc_endpoint_id]\n    rest_api        = [databricks_mws_vpc_endpoint.workspace.vpc_endpoint_id]\n  }\n}\n```\n\n## Modifying networks on running workspaces (AWS only)\n\nDue to specifics of platform APIs, changing any attribute of network configuration would cause `databricks.MwsNetworks` to be re-created - deleted \u0026 added again with special case for running workspaces. Once network configuration is attached to a running databricks_mws_workspaces, you cannot delete it and `pulumi up` would result in `INVALID_STATE: Unable to delete, Network is being used by active workspace X` error. In order to modify any attributes of a network, you have to perform three different `pulumi up` steps:\n\n1. Create a new `databricks.MwsNetworks` resource.\n2. Update the `databricks.MwsWorkspaces` to point to the new `network_id`.\n3. Delete the old `databricks.MwsNetworks` resource.\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with PrivateLink guide.\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* Provisioning Databricks on GCP guide.\n* Provisioning Databricks workspaces on GCP with Private Service Connect guide.\n* databricks.MwsVpcEndpoint resources with Databricks such that they can be used as part of a databricks.MwsNetworks configuration.\n* databricks.MwsPrivateAccessSettings to create a Private Access Setting that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) or [GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html).\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n"
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_security_group\n"
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_subnet\n"
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n"
                },
                "vpcId": {
                    "type": "string",
                    "description": "aws_vpc id\n"
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "required": [
                "accountId",
                "creationTime",
                "errorMessages",
                "networkId",
                "networkName",
                "vpcEndpoints",
                "vpcStatus",
                "workspaceId"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "errorMessages": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                    }
                },
                "gcpNetworkInfo": {
                    "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                    "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n",
                    "willReplaceOnChanges": true
                },
                "networkId": {
                    "type": "string",
                    "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                },
                "networkName": {
                    "type": "string",
                    "description": "name under which this network is registered\n",
                    "willReplaceOnChanges": true
                },
                "securityGroupIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_security_group\n",
                    "willReplaceOnChanges": true
                },
                "subnetIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "ids of aws_subnet\n",
                    "willReplaceOnChanges": true
                },
                "vpcEndpoints": {
                    "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                    "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n",
                    "willReplaceOnChanges": true
                },
                "vpcId": {
                    "type": "string",
                    "description": "aws_vpc id\n",
                    "willReplaceOnChanges": true
                },
                "vpcStatus": {
                    "type": "string",
                    "description": "(String) VPC attachment status\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(Integer) id of associated workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "networkName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsNetworks resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/)\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "errorMessages": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/MwsNetworksErrorMessage:MwsNetworksErrorMessage"
                        }
                    },
                    "gcpNetworkInfo": {
                        "$ref": "#/types/databricks:index/MwsNetworksGcpNetworkInfo:MwsNetworksGcpNetworkInfo",
                        "description": "a block consists of Google Cloud specific information for this network, for example the VPC ID, subnet ID, and secondary IP ranges. It has the following fields:\n",
                        "willReplaceOnChanges": true
                    },
                    "networkId": {
                        "type": "string",
                        "description": "(String) id of network to be used for databricks.MwsWorkspaces resource.\n"
                    },
                    "networkName": {
                        "type": "string",
                        "description": "name under which this network is registered\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "ids of aws_security_group\n",
                        "willReplaceOnChanges": true
                    },
                    "subnetIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "ids of aws_subnet\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcEndpoints": {
                        "$ref": "#/types/databricks:index/MwsNetworksVpcEndpoints:MwsNetworksVpcEndpoints",
                        "description": "mapping of databricks.MwsVpcEndpoint for PrivateLink or Private Service Connect connections\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "description": "aws_vpc id\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcStatus": {
                        "type": "string",
                        "description": "(String) VPC attachment status\n"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "(Integer) id of associated workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPermissionAssignment:MwsPermissionAssignment": {
            "description": "These resources are invoked in the account context. Permission Assignment Account API endpoints are restricted to account admins. Provider must have `account_id` attribute configured. Account Id that could be found in the top right corner of Accounts Console\n\n## Example Usage\n\nIn account context, adding account-level group to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dataEng = new databricks.Group(\"data_eng\", {displayName: \"Data Engineering\"});\nconst addAdminGroup = new databricks.MwsPermissionAssignment(\"add_admin_group\", {\n    workspaceId: _this.workspaceId,\n    principalId: dataEng.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndata_eng = databricks.Group(\"data_eng\", display_name=\"Data Engineering\")\nadd_admin_group = databricks.MwsPermissionAssignment(\"add_admin_group\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=data_eng.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dataEng = new Databricks.Group(\"data_eng\", new()\n    {\n        DisplayName = \"Data Engineering\",\n    });\n\n    var addAdminGroup = new Databricks.MwsPermissionAssignment(\"add_admin_group\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = dataEng.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdataEng, err := databricks.NewGroup(ctx, \"data_eng\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"Data Engineering\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_admin_group\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: dataEng.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dataEng = new Group(\"dataEng\", GroupArgs.builder()        \n            .displayName(\"Data Engineering\")\n            .build());\n\n        var addAdminGroup = new MwsPermissionAssignment(\"addAdminGroup\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(this_.workspaceId())\n            .principalId(dataEng.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dataEng:\n    type: databricks:Group\n    name: data_eng\n    properties:\n      displayName: Data Engineering\n  addAdminGroup:\n    type: databricks:MwsPermissionAssignment\n    name: add_admin_group\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${dataEng.id}\n      permissions:\n        - ADMIN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn account context, adding account-level user to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst addUser = new databricks.MwsPermissionAssignment(\"add_user\", {\n    workspaceId: _this.workspaceId,\n    principalId: me.id,\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nadd_user = databricks.MwsPermissionAssignment(\"add_user\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.MwsPermissionAssignment(\"add_user\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = me.Id,\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_user\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: me.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new MwsPermissionAssignment(\"addUser\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(this_.workspaceId())\n            .principalId(me.id())\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  addUser:\n    type: databricks:MwsPermissionAssignment\n    name: add_user\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${me.id}\n      permissions:\n        - USER\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn account context, adding account-level service principal to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\nconst addAdminSpn = new databricks.MwsPermissionAssignment(\"add_admin_spn\", {\n    workspaceId: _this.workspaceId,\n    principalId: sp.id,\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.MwsPermissionAssignment(\"add_admin_spn\",\n    workspace_id=this[\"workspaceId\"],\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.MwsPermissionAssignment(\"add_admin_spn\", new()\n    {\n        WorkspaceId = @this.WorkspaceId,\n        PrincipalId = sp.Id,\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewMwsPermissionAssignment(ctx, \"add_admin_spn\", \u0026databricks.MwsPermissionAssignmentArgs{\n\t\t\tWorkspaceId: pulumi.Any(this.WorkspaceId),\n\t\t\tPrincipalId: sp.ID(),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.MwsPermissionAssignment;\nimport com.pulumi.databricks.MwsPermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new MwsPermissionAssignment(\"addAdminSpn\", MwsPermissionAssignmentArgs.builder()        \n            .workspaceId(this_.workspaceId())\n            .principalId(sp.id())\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n  addAdminSpn:\n    type: databricks:MwsPermissionAssignment\n    name: add_admin_spn\n    properties:\n      workspaceId: ${this.workspaceId}\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.PermissionAssignment to manage permission assignment from a workspace context\n\n## Import\n\nThe resource `databricks_mws_permission_assignment` can be imported using the workspace id and principal id\n\nbash\n\n```sh\n$ pulumi import databricks:index/mwsPermissionAssignment:MwsPermissionAssignment this \"workspace_id|principal_id\"\n```\n\n",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n"
                },
                "principalId": {
                    "type": "integer",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Databricks workspace ID.\n"
                }
            },
            "required": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceId": {
                    "type": "string",
                    "description": "Databricks workspace ID.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId",
                "workspaceId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "description": "Databricks ID of the user, service principal, or group. The principal ID can be retrieved using the SCIM API, or using databricks_user, databricks.ServicePrincipal or databricks.Group data sources.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "Databricks workspace ID.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsPrivateAccessSettings:MwsPrivateAccessSettings": {
            "description": "Allows you to create a Private Access Setting resource that can be used as part of a databricks.MwsWorkspaces resource to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) or [GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html)\n\nIt is strongly recommended that customers read the [Enable AWS Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) [Enable GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) documentation before trying to leverage this resource.\n\n## Databricks on AWS usage\n\n\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst pas = new databricks.MwsPrivateAccessSettings(\"pas\", {\n    accountId: databricksAccountId,\n    privateAccessSettingsName: `Private Access Settings for ${prefix}`,\n    region: region,\n    publicAccessEnabled: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npas = databricks.MwsPrivateAccessSettings(\"pas\",\n    account_id=databricks_account_id,\n    private_access_settings_name=f\"Private Access Settings for {prefix}\",\n    region=region,\n    public_access_enabled=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var pas = new Databricks.MwsPrivateAccessSettings(\"pas\", new()\n    {\n        AccountId = databricksAccountId,\n        PrivateAccessSettingsName = $\"Private Access Settings for {prefix}\",\n        Region = region,\n        PublicAccessEnabled = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsPrivateAccessSettings(ctx, \"pas\", \u0026databricks.MwsPrivateAccessSettingsArgs{\n\t\t\tAccountId:                 pulumi.Any(databricksAccountId),\n\t\t\tPrivateAccessSettingsName: pulumi.String(fmt.Sprintf(\"Private Access Settings for %v\", prefix)),\n\t\t\tRegion:                    pulumi.Any(region),\n\t\t\tPublicAccessEnabled:       pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsPrivateAccessSettings;\nimport com.pulumi.databricks.MwsPrivateAccessSettingsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var pas = new MwsPrivateAccessSettings(\"pas\", MwsPrivateAccessSettingsArgs.builder()        \n            .accountId(databricksAccountId)\n            .privateAccessSettingsName(String.format(\"Private Access Settings for %s\", prefix))\n            .region(region)\n            .publicAccessEnabled(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  pas:\n    type: databricks:MwsPrivateAccessSettings\n    properties:\n      accountId: ${databricksAccountId}\n      privateAccessSettingsName: Private Access Settings for ${prefix}\n      region: ${region}\n      publicAccessEnabled: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nThe `databricks_mws_private_access_settings.pas.private_access_settings_id` can then be used as part of a databricks.MwsWorkspaces resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    awsRegion: region,\n    workspaceName: prefix,\n    credentialsId: thisDatabricksMwsCredentials.credentialsId,\n    storageConfigurationId: thisDatabricksMwsStorageConfigurations.storageConfigurationId,\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"ENTERPRISE\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    aws_region=region,\n    workspace_name=prefix,\n    credentials_id=this_databricks_mws_credentials[\"credentialsId\"],\n    storage_configuration_id=this_databricks_mws_storage_configurations[\"storageConfigurationId\"],\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"ENTERPRISE\",\n    opts=pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AwsRegion = region,\n        WorkspaceName = prefix,\n        CredentialsId = thisDatabricksMwsCredentials.CredentialsId,\n        StorageConfigurationId = thisDatabricksMwsStorageConfigurations.StorageConfigurationId,\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"ENTERPRISE\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAwsRegion:               pulumi.Any(region),\n\t\t\tWorkspaceName:           pulumi.Any(prefix),\n\t\t\tCredentialsId:           pulumi.Any(thisDatabricksMwsCredentials.CredentialsId),\n\t\t\tStorageConfigurationId:  pulumi.Any(thisDatabricksMwsStorageConfigurations.StorageConfigurationId),\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"ENTERPRISE\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()        \n            .awsRegion(region)\n            .workspaceName(prefix)\n            .credentialsId(thisDatabricksMwsCredentials.credentialsId())\n            .storageConfigurationId(thisDatabricksMwsStorageConfigurations.storageConfigurationId())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"ENTERPRISE\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      awsRegion: ${region}\n      workspaceName: ${prefix}\n      credentialsId: ${thisDatabricksMwsCredentials.credentialsId}\n      storageConfigurationId: ${thisDatabricksMwsStorageConfigurations.storageConfigurationId}\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: ENTERPRISE\n    options:\n      dependson:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Databricks on GCP usage\n\n\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.gcp.databricks.com\"` and use `provider = databricks.mws`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.MwsWorkspaces(\"this\", {\n    workspaceName: \"gcp-workspace\",\n    location: subnetRegion,\n    cloudResourceContainer: {\n        gcp: {\n            projectId: googleProject,\n        },\n    },\n    gkeConfig: {\n        connectivityType: \"PRIVATE_NODE_PUBLIC_MASTER\",\n        masterIpRange: \"10.3.0.0/28\",\n    },\n    networkId: thisDatabricksMwsNetworks.networkId,\n    privateAccessSettingsId: pas.privateAccessSettingsId,\n    pricingTier: \"PREMIUM\",\n}, {\n    dependsOn: [thisDatabricksMwsNetworks],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.MwsWorkspaces(\"this\",\n    workspace_name=\"gcp-workspace\",\n    location=subnet_region,\n    cloud_resource_container=databricks.MwsWorkspacesCloudResourceContainerArgs(\n        gcp=databricks.MwsWorkspacesCloudResourceContainerGcpArgs(\n            project_id=google_project,\n        ),\n    ),\n    gke_config=databricks.MwsWorkspacesGkeConfigArgs(\n        connectivity_type=\"PRIVATE_NODE_PUBLIC_MASTER\",\n        master_ip_range=\"10.3.0.0/28\",\n    ),\n    network_id=this_databricks_mws_networks[\"networkId\"],\n    private_access_settings_id=pas[\"privateAccessSettingsId\"],\n    pricing_tier=\"PREMIUM\",\n    opts=pulumi.ResourceOptions(depends_on=[this_databricks_mws_networks]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        WorkspaceName = \"gcp-workspace\",\n        Location = subnetRegion,\n        CloudResourceContainer = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerArgs\n        {\n            Gcp = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerGcpArgs\n            {\n                ProjectId = googleProject,\n            },\n        },\n        GkeConfig = new Databricks.Inputs.MwsWorkspacesGkeConfigArgs\n        {\n            ConnectivityType = \"PRIVATE_NODE_PUBLIC_MASTER\",\n            MasterIpRange = \"10.3.0.0/28\",\n        },\n        NetworkId = thisDatabricksMwsNetworks.NetworkId,\n        PrivateAccessSettingsId = pas.PrivateAccessSettingsId,\n        PricingTier = \"PREMIUM\",\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            thisDatabricksMwsNetworks,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tWorkspaceName: pulumi.String(\"gcp-workspace\"),\n\t\t\tLocation:      pulumi.Any(subnetRegion),\n\t\t\tCloudResourceContainer: \u0026databricks.MwsWorkspacesCloudResourceContainerArgs{\n\t\t\t\tGcp: \u0026databricks.MwsWorkspacesCloudResourceContainerGcpArgs{\n\t\t\t\t\tProjectId: pulumi.Any(googleProject),\n\t\t\t\t},\n\t\t\t},\n\t\t\tGkeConfig: \u0026databricks.MwsWorkspacesGkeConfigArgs{\n\t\t\t\tConnectivityType: pulumi.String(\"PRIVATE_NODE_PUBLIC_MASTER\"),\n\t\t\t\tMasterIpRange:    pulumi.String(\"10.3.0.0/28\"),\n\t\t\t},\n\t\t\tNetworkId:               pulumi.Any(thisDatabricksMwsNetworks.NetworkId),\n\t\t\tPrivateAccessSettingsId: pulumi.Any(pas.PrivateAccessSettingsId),\n\t\t\tPricingTier:             pulumi.String(\"PREMIUM\"),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\tthisDatabricksMwsNetworks,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerGcpArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesGkeConfigArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new MwsWorkspaces(\"this\", MwsWorkspacesArgs.builder()        \n            .workspaceName(\"gcp-workspace\")\n            .location(subnetRegion)\n            .cloudResourceContainer(MwsWorkspacesCloudResourceContainerArgs.builder()\n                .gcp(MwsWorkspacesCloudResourceContainerGcpArgs.builder()\n                    .projectId(googleProject)\n                    .build())\n                .build())\n            .gkeConfig(MwsWorkspacesGkeConfigArgs.builder()\n                .connectivityType(\"PRIVATE_NODE_PUBLIC_MASTER\")\n                .masterIpRange(\"10.3.0.0/28\")\n                .build())\n            .networkId(thisDatabricksMwsNetworks.networkId())\n            .privateAccessSettingsId(pas.privateAccessSettingsId())\n            .pricingTier(\"PREMIUM\")\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(thisDatabricksMwsNetworks)\n                .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:MwsWorkspaces\n    properties:\n      workspaceName: gcp-workspace\n      location: ${subnetRegion}\n      cloudResourceContainer:\n        gcp:\n          projectId: ${googleProject}\n      gkeConfig:\n        connectivityType: PRIVATE_NODE_PUBLIC_MASTER\n        masterIpRange: 10.3.0.0/28\n      networkId: ${thisDatabricksMwsNetworks.networkId}\n      privateAccessSettingsId: ${pas.privateAccessSettingsId}\n      pricingTier: PREMIUM\n    options:\n      dependson:\n        - ${thisDatabricksMwsNetworks}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with PrivateLink guide.\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide.\n* Provisioning Databricks workspaces on GCP with Private Service Connect guide.\n* databricks.MwsVpcEndpoint resources with Databricks such that they can be used as part of a databricks.MwsNetworks configuration.\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                }
            },
            "required": [
                "accountId",
                "privateAccessSettingsId",
                "privateAccessSettingsName",
                "region"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                },
                "allowedVpcEndpointIds": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                },
                "privateAccessLevel": {
                    "type": "string",
                    "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                },
                "privateAccessSettingsName": {
                    "type": "string",
                    "description": "Name of Private Access Settings in Databricks Account\n"
                },
                "publicAccessEnabled": {
                    "type": "boolean",
                    "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                }
            },
            "requiredInputs": [
                "privateAccessSettingsName",
                "region"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsPrivateAccessSettings resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "deprecationMessage": "Configuring `account_id` at the resource-level is deprecated; please specify it in the `provider {}` configuration block instead"
                    },
                    "allowedVpcEndpointIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "An array of databricks.MwsVpcEndpoint `vpc_endpoint_id` (not `id`). Only used when `private_access_level` is set to `ENDPOINT`. This is an allow list of databricks.MwsVpcEndpoint that in your account that can connect to your databricks.MwsWorkspaces over AWS PrivateLink. If hybrid access to your workspace is enabled by setting `public_access_enabled` to true, then this control only works for PrivateLink connections. To control how your workspace is accessed via public internet, see the article for databricks_ip_access_list.\n"
                    },
                    "privateAccessLevel": {
                        "type": "string",
                        "description": "The private access level controls which VPC endpoints can connect to the UI or API of any workspace that attaches this private access settings object. `ACCOUNT` level access _(default)_ lets only databricks.MwsVpcEndpoint that are registered in your Databricks account connect to your databricks_mws_workspaces. `ENDPOINT` level access lets only specified databricks.MwsVpcEndpoint connect to your workspace. Please see the `allowed_vpc_endpoint_ids` documentation for more details.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of Private Access Settings in Databricks Account\n"
                    },
                    "privateAccessSettingsName": {
                        "type": "string",
                        "description": "Name of Private Access Settings in Databricks Account\n"
                    },
                    "publicAccessEnabled": {
                        "type": "boolean",
                        "description": "If `true`, the databricks.MwsWorkspaces can be accessed over the databricks.MwsVpcEndpoint as well as over the public network. In such a case, you could also configure an databricks.IpAccessList for the workspace, to restrict the source networks that could be used to access it over the public network. If `false`, the workspace can be accessed only over VPC endpoints, and not over the public network. Once explicitly set, this field becomes mandatory.\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC or the Google Cloud VPC network\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsStorageConfigurations:MwsStorageConfigurations": {
            "description": "\u003e **Note** Initialize provider with `alias = \"mws\"`, `host  = \"https://accounts.cloud.databricks.com\"` and use `provider = databricks.mws`\n\nThis resource to configure root bucket new workspaces within AWS.\n\nIt is important to understand that this will require you to configure your provider separately for the multiple workspaces resources. This will point to \u003chttps://accounts.cloud.databricks.com\u003e for the HOST and it will use basic auth as that is the only authentication method available for multiple workspaces api.\n\nPlease follow this complete runnable example\n* `storage_configuration_name` - name under which this storage configuration is stored\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* Provisioning Databricks on AWS with PrivateLink guide.\n* databricks.MwsCredentials to configure the cross-account role for creation of new workspaces within AWS.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "secret": true
                },
                "bucketName": {
                    "type": "string"
                },
                "creationTime": {
                    "type": "integer"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                },
                "storageConfigurationName": {
                    "type": "string"
                }
            },
            "required": [
                "accountId",
                "bucketName",
                "creationTime",
                "storageConfigurationId",
                "storageConfigurationName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "bucketName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageConfigurationName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accountId",
                "bucketName",
                "storageConfigurationName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsStorageConfigurations resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "(String) id of storage config to be used for `databricks_mws_workspace` resource.\n"
                    },
                    "storageConfigurationName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsVpcEndpoint:MwsVpcEndpoint": {
            "description": "\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n"
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "description": "ID of configured aws_vpc_endpoint\n"
                },
                "gcpVpcEndpointInfo": {
                    "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                    "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n"
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n"
                },
                "state": {
                    "type": "string",
                    "description": "(AWS Only) State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n"
                }
            },
            "required": [
                "awsAccountId",
                "awsEndpointServiceId",
                "state",
                "useCase",
                "vpcEndpointId",
                "vpcEndpointName"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n",
                    "willReplaceOnChanges": true
                },
                "awsAccountId": {
                    "type": "string"
                },
                "awsEndpointServiceId": {
                    "type": "string",
                    "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                },
                "awsVpcEndpointId": {
                    "type": "string",
                    "description": "ID of configured aws_vpc_endpoint\n",
                    "willReplaceOnChanges": true
                },
                "gcpVpcEndpointInfo": {
                    "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                    "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n",
                    "willReplaceOnChanges": true
                },
                "region": {
                    "type": "string",
                    "description": "Region of AWS VPC\n",
                    "willReplaceOnChanges": true
                },
                "state": {
                    "type": "string",
                    "description": "(AWS Only) State of VPC Endpoint\n"
                },
                "useCase": {
                    "type": "string"
                },
                "vpcEndpointId": {
                    "type": "string",
                    "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                },
                "vpcEndpointName": {
                    "type": "string",
                    "description": "Name of VPC Endpoint in Databricks Account\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "vpcEndpointName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsVpcEndpoint resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the Accounts Console for [AWS](https://accounts.cloud.databricks.com/) or [GCP](https://accounts.gcp.databricks.com/)\n",
                        "willReplaceOnChanges": true
                    },
                    "awsAccountId": {
                        "type": "string"
                    },
                    "awsEndpointServiceId": {
                        "type": "string",
                        "description": "(AWS Only) The ID of the Databricks endpoint service that this VPC endpoint is connected to. Please find the list of endpoint service IDs for each supported region in the [Databricks PrivateLink documentation](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html)\n"
                    },
                    "awsVpcEndpointId": {
                        "type": "string",
                        "description": "ID of configured aws_vpc_endpoint\n",
                        "willReplaceOnChanges": true
                    },
                    "gcpVpcEndpointInfo": {
                        "$ref": "#/types/databricks:index/MwsVpcEndpointGcpVpcEndpointInfo:MwsVpcEndpointGcpVpcEndpointInfo",
                        "description": "a block consists of Google Cloud specific information for this PSC endpoint. It has the following fields:\n",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of AWS VPC\n",
                        "willReplaceOnChanges": true
                    },
                    "state": {
                        "type": "string",
                        "description": "(AWS Only) State of VPC Endpoint\n"
                    },
                    "useCase": {
                        "type": "string"
                    },
                    "vpcEndpointId": {
                        "type": "string",
                        "description": "Canonical unique identifier of VPC Endpoint in Databricks Account\n"
                    },
                    "vpcEndpointName": {
                        "type": "string",
                        "description": "Name of VPC Endpoint in Databricks Account\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/mwsWorkspaces:MwsWorkspaces": {
            "description": "## Example Usage\n\n### Creating a Databricks on AWS workspace\n\n!Simplest multiworkspace\n\nTo get workspace running, you have to configure a couple of things:\n\n* databricks.MwsCredentials - You can share a credentials (cross-account IAM role) configuration ID with multiple workspaces. It is not required to create a new one for each workspace.\n* databricks.MwsStorageConfigurations - You can share a root S3 bucket with multiple workspaces in a single account. You do not have to create new ones for each workspace. If you share a root S3 bucket for multiple workspaces in an account, data on the root S3 bucket is partitioned into separate directories by workspace.\n* databricks.MwsNetworks - (optional, but recommended) You can share one [customer-managed VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you cannot reuse subnets or security groups with other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks databricks.MwsNetworks encapsulates this information, you cannot reuse it across workspaces.\n* databricks.MwsCustomerManagedKeys - You can share a customer-managed key across workspaces.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\n// register cross-account ARN\nconst _this = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossaccountArn,\n});\n// register root bucket\nconst thisMwsStorageConfigurations = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${prefix}-storage`,\n    bucketName: rootBucket,\n});\n// register VPC\nconst thisMwsNetworks = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `${prefix}-network`,\n    vpcId: vpcId,\n    subnetIds: subnetsPrivate,\n    securityGroupIds: [securityGroup],\n});\n// create workspace in given VPC with DBFS on root bucket\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    awsRegion: region,\n    credentialsId: _this.credentialsId,\n    storageConfigurationId: thisMwsStorageConfigurations.storageConfigurationId,\n    networkId: thisMwsNetworks.networkId,\n    token: {},\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\n# register cross-account ARN\nthis = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=crossaccount_arn)\n# register root bucket\nthis_mws_storage_configurations = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{prefix}-storage\",\n    bucket_name=root_bucket)\n# register VPC\nthis_mws_networks = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"{prefix}-network\",\n    vpc_id=vpc_id,\n    subnet_ids=subnets_private,\n    security_group_ids=[security_group])\n# create workspace in given VPC with DBFS on root bucket\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    aws_region=region,\n    credentials_id=this.credentials_id,\n    storage_configuration_id=this_mws_storage_configurations.storage_configuration_id,\n    network_id=this_mws_networks.network_id,\n    token=databricks.MwsWorkspacesTokenArgs())\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    // register cross-account ARN\n    var @this = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossaccountArn,\n    });\n\n    // register root bucket\n    var thisMwsStorageConfigurations = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{prefix}-storage\",\n        BucketName = rootBucket,\n    });\n\n    // register VPC\n    var thisMwsNetworks = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"{prefix}-network\",\n        VpcId = vpcId,\n        SubnetIds = subnetsPrivate,\n        SecurityGroupIds = new[]\n        {\n            securityGroup,\n        },\n    });\n\n    // create workspace in given VPC with DBFS on root bucket\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        AwsRegion = region,\n        CredentialsId = @this.CredentialsId,\n        StorageConfigurationId = thisMwsStorageConfigurations.StorageConfigurationId,\n        NetworkId = thisMwsNetworks.NetworkId,\n        Token = null,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account ID that can be found in the dropdown under the email address in the upper-right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\t// register cross-account ARN\n\t\tthis, err := databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.String(fmt.Sprintf(\"%v-creds\", prefix)),\n\t\t\tRoleArn:         pulumi.Any(crossaccountArn),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// register root bucket\n\t\tthisMwsStorageConfigurations, err := databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.String(fmt.Sprintf(\"%v-storage\", prefix)),\n\t\t\tBucketName:               pulumi.Any(rootBucket),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// register VPC\n\t\tthisMwsNetworks, err := databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.String(fmt.Sprintf(\"%v-network\", prefix)),\n\t\t\tVpcId:       pulumi.Any(vpcId),\n\t\t\tSubnetIds:   pulumi.Any(subnetsPrivate),\n\t\t\tSecurityGroupIds: pulumi.StringArray{\n\t\t\t\tsecurityGroup,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// create workspace in given VPC with DBFS on root bucket\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName:          pulumi.Any(prefix),\n\t\t\tAwsRegion:              pulumi.Any(region),\n\t\t\tCredentialsId:          this.CredentialsId,\n\t\t\tStorageConfigurationId: thisMwsStorageConfigurations.StorageConfigurationId,\n\t\t\tNetworkId:              thisMwsNetworks.NetworkId,\n\t\t\tToken:                  nil,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        // register cross-account ARN\n        var this_ = new MwsCredentials(\"this\", MwsCredentialsArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossaccountArn)\n            .build());\n\n        // register root bucket\n        var thisMwsStorageConfigurations = new MwsStorageConfigurations(\"thisMwsStorageConfigurations\", MwsStorageConfigurationsArgs.builder()        \n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", prefix))\n            .bucketName(rootBucket)\n            .build());\n\n        // register VPC\n        var thisMwsNetworks = new MwsNetworks(\"thisMwsNetworks\", MwsNetworksArgs.builder()        \n            .accountId(databricksAccountId)\n            .networkName(String.format(\"%s-network\", prefix))\n            .vpcId(vpcId)\n            .subnetIds(subnetsPrivate)\n            .securityGroupIds(securityGroup)\n            .build());\n\n        // create workspace in given VPC with DBFS on root bucket\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()        \n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .awsRegion(region)\n            .credentialsId(this_.credentialsId())\n            .storageConfigurationId(thisMwsStorageConfigurations.storageConfigurationId())\n            .networkId(thisMwsNetworks.networkId())\n            .token()\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(token -\u003e token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  # register cross-account ARN\n  this:\n    type: databricks:MwsCredentials\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossaccountArn}\n  # register root bucket\n  thisMwsStorageConfigurations:\n    type: databricks:MwsStorageConfigurations\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${prefix}-storage\n      bucketName: ${rootBucket}\n  # register VPC\n  thisMwsNetworks:\n    type: databricks:MwsNetworks\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: ${prefix}-network\n      vpcId: ${vpcId}\n      subnetIds: ${subnetsPrivate}\n      securityGroupIds:\n        - ${securityGroup}\n  # create workspace in given VPC with DBFS on root bucket\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      awsRegion: ${region}\n      credentialsId: ${this.credentialsId}\n      storageConfigurationId: ${thisMwsStorageConfigurations.storageConfigurationId}\n      networkId: ${thisMwsNetworks.networkId}\n      token: {}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Creating a Databricks on AWS workspace with Databricks-Managed VPC\n\n![VPCs](https://docs.databricks.com/_images/customer-managed-vpc.png)\n\nBy default, Databricks creates a VPC in your AWS account for each workspace. Databricks uses it for running clusters in the workspace. Optionally, you can use your VPC for the workspace, using the feature customer-managed VPC. Databricks recommends that you provide your VPC with databricks.MwsNetworks so that you can configure it according to your organization’s enterprise cloud standards while still conforming to Databricks requirements. You cannot migrate an existing workspace to your VPC. Please see the difference described through IAM policy actions [on this page](https://docs.databricks.com/administration-guide/account-api/iam-role.html).\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as random from \"@pulumi/random\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst naming = new random.index.String(\"naming\", {\n    special: false,\n    upper: false,\n    length: 6,\n});\nconst prefix = `dltp${naming.result}`;\nconst this = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccountRole = new aws.iam.Role(\"cross_account_role\", {\n    name: `${prefix}-crossaccount`,\n    assumeRolePolicy: _this.then(_this =\u003e _this.json),\n    tags: tags,\n});\nconst thisGetAwsCrossAccountPolicy = databricks.getAwsCrossAccountPolicy({});\nconst thisRolePolicy = new aws.iam.RolePolicy(\"this\", {\n    name: `${prefix}-policy`,\n    role: crossAccountRole.id,\n    policy: thisGetAwsCrossAccountPolicy.then(thisGetAwsCrossAccountPolicy =\u003e thisGetAwsCrossAccountPolicy.json),\n});\nconst thisMwsCredentials = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossAccountRole.arn,\n});\nconst rootStorageBucket = new aws.s3.BucketV2(\"root_storage_bucket\", {\n    bucket: `${prefix}-rootbucket`,\n    acl: \"private\",\n    forceDestroy: true,\n    tags: tags,\n});\nconst rootVersioning = new aws.s3.BucketVersioningV2(\"root_versioning\", {\n    bucket: rootStorageBucket.id,\n    versioningConfiguration: {\n        status: \"Disabled\",\n    },\n});\nconst rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new aws.s3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\", {\n    bucket: rootStorageBucket.bucket,\n    rules: [{\n        applyServerSideEncryptionByDefault: {\n            sseAlgorithm: \"AES256\",\n        },\n    }],\n});\nconst rootStorageBucketBucketPublicAccessBlock = new aws.s3.BucketPublicAccessBlock(\"root_storage_bucket\", {\n    bucket: rootStorageBucket.id,\n    blockPublicAcls: true,\n    blockPublicPolicy: true,\n    ignorePublicAcls: true,\n    restrictPublicBuckets: true,\n}, {\n    dependsOn: [rootStorageBucket],\n});\nconst thisGetAwsBucketPolicy = databricks.getAwsBucketPolicyOutput({\n    bucket: rootStorageBucket.bucket,\n});\nconst rootBucketPolicy = new aws.s3.BucketPolicy(\"root_bucket_policy\", {\n    bucket: rootStorageBucket.id,\n    policy: thisGetAwsBucketPolicy.apply(thisGetAwsBucketPolicy =\u003e thisGetAwsBucketPolicy.json),\n}, {\n    dependsOn: [rootStorageBucketBucketPublicAccessBlock],\n});\nconst thisMwsStorageConfigurations = new databricks.MwsStorageConfigurations(\"this\", {\n    accountId: databricksAccountId,\n    storageConfigurationName: `${prefix}-storage`,\n    bucketName: rootStorageBucket.bucket,\n});\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    awsRegion: \"us-east-1\",\n    credentialsId: thisMwsCredentials.credentialsId,\n    storageConfigurationId: thisMwsStorageConfigurations.storageConfigurationId,\n    token: {},\n    customTags: {\n        SoldToCode: \"1234\",\n    },\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\nimport pulumi_random as random\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nnaming = random.index.String(\"naming\",\n    special=False,\n    upper=False,\n    length=6)\nprefix = f\"dltp{naming['result']}\"\nthis = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account_role = aws.iam.Role(\"cross_account_role\",\n    name=f\"{prefix}-crossaccount\",\n    assume_role_policy=this.json,\n    tags=tags)\nthis_get_aws_cross_account_policy = databricks.get_aws_cross_account_policy()\nthis_role_policy = aws.iam.RolePolicy(\"this\",\n    name=f\"{prefix}-policy\",\n    role=cross_account_role.id,\n    policy=this_get_aws_cross_account_policy.json)\nthis_mws_credentials = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=cross_account_role.arn)\nroot_storage_bucket = aws.s3.BucketV2(\"root_storage_bucket\",\n    bucket=f\"{prefix}-rootbucket\",\n    acl=\"private\",\n    force_destroy=True,\n    tags=tags)\nroot_versioning = aws.s3.BucketVersioningV2(\"root_versioning\",\n    bucket=root_storage_bucket.id,\n    versioning_configuration=aws.s3.BucketVersioningV2VersioningConfigurationArgs(\n        status=\"Disabled\",\n    ))\nroot_storage_bucket_bucket_server_side_encryption_configuration_v2 = aws.s3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\",\n    bucket=root_storage_bucket.bucket,\n    rules=[aws.s3.BucketServerSideEncryptionConfigurationV2RuleArgs(\n        apply_server_side_encryption_by_default=aws.s3.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs(\n            sse_algorithm=\"AES256\",\n        ),\n    )])\nroot_storage_bucket_bucket_public_access_block = aws.s3.BucketPublicAccessBlock(\"root_storage_bucket\",\n    bucket=root_storage_bucket.id,\n    block_public_acls=True,\n    block_public_policy=True,\n    ignore_public_acls=True,\n    restrict_public_buckets=True,\n    opts=pulumi.ResourceOptions(depends_on=[root_storage_bucket]))\nthis_get_aws_bucket_policy = databricks.get_aws_bucket_policy_output(bucket=root_storage_bucket.bucket)\nroot_bucket_policy = aws.s3.BucketPolicy(\"root_bucket_policy\",\n    bucket=root_storage_bucket.id,\n    policy=this_get_aws_bucket_policy.json,\n    opts=pulumi.ResourceOptions(depends_on=[root_storage_bucket_bucket_public_access_block]))\nthis_mws_storage_configurations = databricks.MwsStorageConfigurations(\"this\",\n    account_id=databricks_account_id,\n    storage_configuration_name=f\"{prefix}-storage\",\n    bucket_name=root_storage_bucket.bucket)\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    aws_region=\"us-east-1\",\n    credentials_id=this_mws_credentials.credentials_id,\n    storage_configuration_id=this_mws_storage_configurations.storage_configuration_id,\n    token=databricks.MwsWorkspacesTokenArgs(),\n    custom_tags={\n        \"SoldToCode\": \"1234\",\n    })\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\nusing Random = Pulumi.Random;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var naming = new Random.Index.String(\"naming\", new()\n    {\n        Special = false,\n        Upper = false,\n        Length = 6,\n    });\n\n    var prefix = $\"dltp{naming.Result}\";\n\n    var @this = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccountRole = new Aws.Iam.Role(\"cross_account_role\", new()\n    {\n        Name = $\"{prefix}-crossaccount\",\n        AssumeRolePolicy = @this.Apply(@this =\u003e @this.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json)),\n        Tags = tags,\n    });\n\n    var thisGetAwsCrossAccountPolicy = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var thisRolePolicy = new Aws.Iam.RolePolicy(\"this\", new()\n    {\n        Name = $\"{prefix}-policy\",\n        Role = crossAccountRole.Id,\n        Policy = thisGetAwsCrossAccountPolicy.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json),\n    });\n\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossAccountRole.Arn,\n    });\n\n    var rootStorageBucket = new Aws.S3.BucketV2(\"root_storage_bucket\", new()\n    {\n        Bucket = $\"{prefix}-rootbucket\",\n        Acl = \"private\",\n        ForceDestroy = true,\n        Tags = tags,\n    });\n\n    var rootVersioning = new Aws.S3.BucketVersioningV2(\"root_versioning\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        VersioningConfiguration = new Aws.S3.Inputs.BucketVersioningV2VersioningConfigurationArgs\n        {\n            Status = \"Disabled\",\n        },\n    });\n\n    var rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new Aws.S3.BucketServerSideEncryptionConfigurationV2(\"root_storage_bucket\", new()\n    {\n        Bucket = rootStorageBucket.Bucket,\n        Rules = new[]\n        {\n            new Aws.S3.Inputs.BucketServerSideEncryptionConfigurationV2RuleArgs\n            {\n                ApplyServerSideEncryptionByDefault = new Aws.S3.Inputs.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs\n                {\n                    SseAlgorithm = \"AES256\",\n                },\n            },\n        },\n    });\n\n    var rootStorageBucketBucketPublicAccessBlock = new Aws.S3.BucketPublicAccessBlock(\"root_storage_bucket\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        BlockPublicAcls = true,\n        BlockPublicPolicy = true,\n        IgnorePublicAcls = true,\n        RestrictPublicBuckets = true,\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            rootStorageBucket,\n        },\n    });\n\n    var thisGetAwsBucketPolicy = Databricks.GetAwsBucketPolicy.Invoke(new()\n    {\n        Bucket = rootStorageBucket.Bucket,\n    });\n\n    var rootBucketPolicy = new Aws.S3.BucketPolicy(\"root_bucket_policy\", new()\n    {\n        Bucket = rootStorageBucket.Id,\n        Policy = thisGetAwsBucketPolicy.Apply(getAwsBucketPolicyResult =\u003e getAwsBucketPolicyResult.Json),\n    }, new CustomResourceOptions\n    {\n        DependsOn =\n        {\n            rootStorageBucketBucketPublicAccessBlock,\n        },\n    });\n\n    var thisMwsStorageConfigurations = new Databricks.MwsStorageConfigurations(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        StorageConfigurationName = $\"{prefix}-storage\",\n        BucketName = rootStorageBucket.Bucket,\n    });\n\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        AwsRegion = \"us-east-1\",\n        CredentialsId = thisMwsCredentials.CredentialsId,\n        StorageConfigurationId = thisMwsStorageConfigurations.StorageConfigurationId,\n        Token = null,\n        CustomTags = \n        {\n            { \"SoldToCode\", \"1234\" },\n        },\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/s3\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-random/sdk/v4/go/random\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tnaming, err := random.NewString(ctx, \"naming\", \u0026random.StringArgs{\n\t\t\tSpecial: false,\n\t\t\tUpper:   false,\n\t\t\tLength:  6,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tprefix := fmt.Sprintf(\"dltp%v\", naming.Result)\n\t\tthis, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountRole, err := iam.NewRole(ctx, \"cross_account_role\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(fmt.Sprintf(\"%v-crossaccount\", prefix)),\n\t\t\tAssumeRolePolicy: pulumi.String(this.Json),\n\t\t\tTags:             pulumi.Any(tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsCrossAccountPolicy, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicy(ctx, \"this\", \u0026iam.RolePolicyArgs{\n\t\t\tName:   pulumi.String(fmt.Sprintf(\"%v-policy\", prefix)),\n\t\t\tRole:   crossAccountRole.ID(),\n\t\t\tPolicy: pulumi.String(thisGetAwsCrossAccountPolicy.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsCredentials, err := databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.String(fmt.Sprintf(\"%v-creds\", prefix)),\n\t\t\tRoleArn:         crossAccountRole.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trootStorageBucket, err := s3.NewBucketV2(ctx, \"root_storage_bucket\", \u0026s3.BucketV2Args{\n\t\t\tBucket:       pulumi.String(fmt.Sprintf(\"%v-rootbucket\", prefix)),\n\t\t\tAcl:          pulumi.String(\"private\"),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t\tTags:         pulumi.Any(tags),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = s3.NewBucketVersioningV2(ctx, \"root_versioning\", \u0026s3.BucketVersioningV2Args{\n\t\t\tBucket: rootStorageBucket.ID(),\n\t\t\tVersioningConfiguration: \u0026s3.BucketVersioningV2VersioningConfigurationArgs{\n\t\t\t\tStatus: pulumi.String(\"Disabled\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = s3.NewBucketServerSideEncryptionConfigurationV2(ctx, \"root_storage_bucket\", \u0026s3.BucketServerSideEncryptionConfigurationV2Args{\n\t\t\tBucket: rootStorageBucket.Bucket,\n\t\t\tRules: s3.BucketServerSideEncryptionConfigurationV2RuleArray{\n\t\t\t\t\u0026s3.BucketServerSideEncryptionConfigurationV2RuleArgs{\n\t\t\t\t\tApplyServerSideEncryptionByDefault: \u0026s3.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs{\n\t\t\t\t\t\tSseAlgorithm: pulumi.String(\"AES256\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\trootStorageBucketBucketPublicAccessBlock, err := s3.NewBucketPublicAccessBlock(ctx, \"root_storage_bucket\", \u0026s3.BucketPublicAccessBlockArgs{\n\t\t\tBucket:                rootStorageBucket.ID(),\n\t\t\tBlockPublicAcls:       pulumi.Bool(true),\n\t\t\tBlockPublicPolicy:     pulumi.Bool(true),\n\t\t\tIgnorePublicAcls:      pulumi.Bool(true),\n\t\t\tRestrictPublicBuckets: pulumi.Bool(true),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\trootStorageBucket,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsBucketPolicy := databricks.GetAwsBucketPolicyOutput(ctx, databricks.GetAwsBucketPolicyOutputArgs{\n\t\t\tBucket: rootStorageBucket.Bucket,\n\t\t}, nil)\n\t\t_, err = s3.NewBucketPolicy(ctx, \"root_bucket_policy\", \u0026s3.BucketPolicyArgs{\n\t\t\tBucket: rootStorageBucket.ID(),\n\t\t\tPolicy: thisGetAwsBucketPolicy.ApplyT(func(thisGetAwsBucketPolicy databricks.GetAwsBucketPolicyResult) (*string, error) {\n\t\t\t\treturn \u0026thisGetAwsBucketPolicy.Json, nil\n\t\t\t}).(pulumi.StringPtrOutput),\n\t\t}, pulumi.DependsOn([]pulumi.Resource{\n\t\t\trootStorageBucketBucketPublicAccessBlock,\n\t\t}))\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsStorageConfigurations, err := databricks.NewMwsStorageConfigurations(ctx, \"this\", \u0026databricks.MwsStorageConfigurationsArgs{\n\t\t\tAccountId:                pulumi.Any(databricksAccountId),\n\t\t\tStorageConfigurationName: pulumi.String(fmt.Sprintf(\"%v-storage\", prefix)),\n\t\t\tBucketName:               rootStorageBucket.Bucket,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:              pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName:          pulumi.String(prefix),\n\t\t\tAwsRegion:              pulumi.String(\"us-east-1\"),\n\t\t\tCredentialsId:          thisMwsCredentials.CredentialsId,\n\t\t\tStorageConfigurationId: thisMwsStorageConfigurations.StorageConfigurationId,\n\t\t\tToken:                  nil,\n\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\"SoldToCode\": pulumi.Any(\"1234\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.random.string;\nimport com.pulumi.random.StringArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.RolePolicy;\nimport com.pulumi.aws.iam.RolePolicyArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.aws.s3.BucketVersioningV2;\nimport com.pulumi.aws.s3.BucketVersioningV2Args;\nimport com.pulumi.aws.s3.inputs.BucketVersioningV2VersioningConfigurationArgs;\nimport com.pulumi.aws.s3.BucketServerSideEncryptionConfigurationV2;\nimport com.pulumi.aws.s3.BucketServerSideEncryptionConfigurationV2Args;\nimport com.pulumi.aws.s3.inputs.BucketServerSideEncryptionConfigurationV2RuleArgs;\nimport com.pulumi.aws.s3.inputs.BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs;\nimport com.pulumi.aws.s3.BucketPublicAccessBlock;\nimport com.pulumi.aws.s3.BucketPublicAccessBlockArgs;\nimport com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;\nimport com.pulumi.aws.s3.BucketPolicy;\nimport com.pulumi.aws.s3.BucketPolicyArgs;\nimport com.pulumi.databricks.MwsStorageConfigurations;\nimport com.pulumi.databricks.MwsStorageConfigurationsArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport com.pulumi.resources.CustomResourceOptions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        var naming = new String(\"naming\", StringArgs.builder()        \n            .special(false)\n            .upper(false)\n            .length(6)\n            .build());\n\n        final var prefix = String.format(\"dltp%s\", naming.result());\n\n        final var this = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccountRole = new Role(\"crossAccountRole\", RoleArgs.builder()        \n            .name(String.format(\"%s-crossaccount\", prefix))\n            .assumeRolePolicy(this_.json())\n            .tags(tags)\n            .build());\n\n        final var thisGetAwsCrossAccountPolicy = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n        var thisRolePolicy = new RolePolicy(\"thisRolePolicy\", RolePolicyArgs.builder()        \n            .name(String.format(\"%s-policy\", prefix))\n            .role(crossAccountRole.id())\n            .policy(thisGetAwsCrossAccountPolicy.applyValue(getAwsCrossAccountPolicyResult -\u003e getAwsCrossAccountPolicyResult.json()))\n            .build());\n\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossAccountRole.arn())\n            .build());\n\n        var rootStorageBucket = new BucketV2(\"rootStorageBucket\", BucketV2Args.builder()        \n            .bucket(String.format(\"%s-rootbucket\", prefix))\n            .acl(\"private\")\n            .forceDestroy(true)\n            .tags(tags)\n            .build());\n\n        var rootVersioning = new BucketVersioningV2(\"rootVersioning\", BucketVersioningV2Args.builder()        \n            .bucket(rootStorageBucket.id())\n            .versioningConfiguration(BucketVersioningV2VersioningConfigurationArgs.builder()\n                .status(\"Disabled\")\n                .build())\n            .build());\n\n        var rootStorageBucketBucketServerSideEncryptionConfigurationV2 = new BucketServerSideEncryptionConfigurationV2(\"rootStorageBucketBucketServerSideEncryptionConfigurationV2\", BucketServerSideEncryptionConfigurationV2Args.builder()        \n            .bucket(rootStorageBucket.bucket())\n            .rules(BucketServerSideEncryptionConfigurationV2RuleArgs.builder()\n                .applyServerSideEncryptionByDefault(BucketServerSideEncryptionConfigurationV2RuleApplyServerSideEncryptionByDefaultArgs.builder()\n                    .sseAlgorithm(\"AES256\")\n                    .build())\n                .build())\n            .build());\n\n        var rootStorageBucketBucketPublicAccessBlock = new BucketPublicAccessBlock(\"rootStorageBucketBucketPublicAccessBlock\", BucketPublicAccessBlockArgs.builder()        \n            .bucket(rootStorageBucket.id())\n            .blockPublicAcls(true)\n            .blockPublicPolicy(true)\n            .ignorePublicAcls(true)\n            .restrictPublicBuckets(true)\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(rootStorageBucket)\n                .build());\n\n        final var thisGetAwsBucketPolicy = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()\n            .bucket(rootStorageBucket.bucket())\n            .build());\n\n        var rootBucketPolicy = new BucketPolicy(\"rootBucketPolicy\", BucketPolicyArgs.builder()        \n            .bucket(rootStorageBucket.id())\n            .policy(thisGetAwsBucketPolicy.applyValue(getAwsBucketPolicyResult -\u003e getAwsBucketPolicyResult).applyValue(thisGetAwsBucketPolicy -\u003e thisGetAwsBucketPolicy.applyValue(getAwsBucketPolicyResult -\u003e getAwsBucketPolicyResult.json())))\n            .build(), CustomResourceOptions.builder()\n                .dependsOn(rootStorageBucketBucketPublicAccessBlock)\n                .build());\n\n        var thisMwsStorageConfigurations = new MwsStorageConfigurations(\"thisMwsStorageConfigurations\", MwsStorageConfigurationsArgs.builder()        \n            .accountId(databricksAccountId)\n            .storageConfigurationName(String.format(\"%s-storage\", prefix))\n            .bucketName(rootStorageBucket.bucket())\n            .build());\n\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()        \n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .awsRegion(\"us-east-1\")\n            .credentialsId(thisMwsCredentials.credentialsId())\n            .storageConfigurationId(thisMwsStorageConfigurations.storageConfigurationId())\n            .token()\n            .customTags(Map.of(\"SoldToCode\", \"1234\"))\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(token -\u003e token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  naming:\n    type: random:string\n    properties:\n      special: false\n      upper: false\n      length: 6\n  crossAccountRole:\n    type: aws:iam:Role\n    name: cross_account_role\n    properties:\n      name: ${prefix}-crossaccount\n      assumeRolePolicy: ${this.json}\n      tags: ${tags}\n  thisRolePolicy:\n    type: aws:iam:RolePolicy\n    name: this\n    properties:\n      name: ${prefix}-policy\n      role: ${crossAccountRole.id}\n      policy: ${thisGetAwsCrossAccountPolicy.json}\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossAccountRole.arn}\n  rootStorageBucket:\n    type: aws:s3:BucketV2\n    name: root_storage_bucket\n    properties:\n      bucket: ${prefix}-rootbucket\n      acl: private\n      forceDestroy: true\n      tags: ${tags}\n  rootVersioning:\n    type: aws:s3:BucketVersioningV2\n    name: root_versioning\n    properties:\n      bucket: ${rootStorageBucket.id}\n      versioningConfiguration:\n        status: Disabled\n  rootStorageBucketBucketServerSideEncryptionConfigurationV2:\n    type: aws:s3:BucketServerSideEncryptionConfigurationV2\n    name: root_storage_bucket\n    properties:\n      bucket: ${rootStorageBucket.bucket}\n      rules:\n        - applyServerSideEncryptionByDefault:\n            sseAlgorithm: AES256\n  rootStorageBucketBucketPublicAccessBlock:\n    type: aws:s3:BucketPublicAccessBlock\n    name: root_storage_bucket\n    properties:\n      bucket: ${rootStorageBucket.id}\n      blockPublicAcls: true\n      blockPublicPolicy: true\n      ignorePublicAcls: true\n      restrictPublicBuckets: true\n    options:\n      dependson:\n        - ${rootStorageBucket}\n  rootBucketPolicy:\n    type: aws:s3:BucketPolicy\n    name: root_bucket_policy\n    properties:\n      bucket: ${rootStorageBucket.id}\n      policy: ${thisGetAwsBucketPolicy.json}\n    options:\n      dependson:\n        - ${rootStorageBucketBucketPublicAccessBlock}\n  thisMwsStorageConfigurations:\n    type: databricks:MwsStorageConfigurations\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      storageConfigurationName: ${prefix}-storage\n      bucketName: ${rootStorageBucket.bucket}\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      awsRegion: us-east-1\n      credentialsId: ${thisMwsCredentials.credentialsId}\n      storageConfigurationId: ${thisMwsStorageConfigurations.storageConfigurationId}\n      token: {}\n      customTags:\n        SoldToCode: '1234'\nvariables:\n  prefix: dltp${naming.result}\n  this:\n    fn::invoke:\n      Function: databricks:getAwsAssumeRolePolicy\n      Arguments:\n        externalId: ${databricksAccountId}\n  thisGetAwsCrossAccountPolicy:\n    fn::invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n  thisGetAwsBucketPolicy:\n    fn::invoke:\n      Function: databricks:getAwsBucketPolicy\n      Arguments:\n        bucket: ${rootStorageBucket.bucket}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn order to create a [Databricks Workspace that leverages AWS PrivateLink](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) please ensure that you have read and understood the [Enable Private Link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/privatelink.html) documentation and then customise the example above with the relevant examples from mws_vpc_endpoint, mws_private_access_settings and mws_networks.\n\n### Creating a Databricks on GCP workspace\n\nTo get workspace running, you have to configure a network object:\n\n* databricks.MwsNetworks - (optional, but recommended) You can share one [customer-managed VPC](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/customer-managed-vpc.html) with multiple workspaces in a single account. You do not have to create a new VPC for each workspace. However, you cannot reuse subnets with other resources, including other workspaces or non-Databricks resources. If you plan to share one VPC with multiple workspaces, be sure to size your VPC and subnets accordingly. Because a Databricks databricks.MwsNetworks encapsulates this information, you cannot reuse it across workspaces.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst databricksGoogleServiceAccount = config.requireObject(\"databricksGoogleServiceAccount\");\nconst googleProject = config.requireObject(\"googleProject\");\n// register VPC\nconst _this = new databricks.MwsNetworks(\"this\", {\n    accountId: databricksAccountId,\n    networkName: `${prefix}-network`,\n    gcpNetworkInfo: {\n        networkProjectId: googleProject,\n        vpcId: vpcId,\n        subnetId: subnetId,\n        subnetRegion: subnetRegion,\n        podIpRangeName: \"pods\",\n        serviceIpRangeName: \"svc\",\n    },\n});\n// create workspace in given VPC\nconst thisMwsWorkspaces = new databricks.MwsWorkspaces(\"this\", {\n    accountId: databricksAccountId,\n    workspaceName: prefix,\n    location: subnetRegion,\n    cloudResourceContainer: {\n        gcp: {\n            projectId: googleProject,\n        },\n    },\n    networkId: _this.networkId,\n    gkeConfig: {\n        connectivityType: \"PRIVATE_NODE_PUBLIC_MASTER\",\n        masterIpRange: \"10.3.0.0/28\",\n    },\n    token: {},\n});\nexport const databricksToken = thisMwsWorkspaces.token.apply(token =\u003e token?.tokenValue);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\ndatabricks_google_service_account = config.require_object(\"databricksGoogleServiceAccount\")\ngoogle_project = config.require_object(\"googleProject\")\n# register VPC\nthis = databricks.MwsNetworks(\"this\",\n    account_id=databricks_account_id,\n    network_name=f\"{prefix}-network\",\n    gcp_network_info=databricks.MwsNetworksGcpNetworkInfoArgs(\n        network_project_id=google_project,\n        vpc_id=vpc_id,\n        subnet_id=subnet_id,\n        subnet_region=subnet_region,\n        pod_ip_range_name=\"pods\",\n        service_ip_range_name=\"svc\",\n    ))\n# create workspace in given VPC\nthis_mws_workspaces = databricks.MwsWorkspaces(\"this\",\n    account_id=databricks_account_id,\n    workspace_name=prefix,\n    location=subnet_region,\n    cloud_resource_container=databricks.MwsWorkspacesCloudResourceContainerArgs(\n        gcp=databricks.MwsWorkspacesCloudResourceContainerGcpArgs(\n            project_id=google_project,\n        ),\n    ),\n    network_id=this.network_id,\n    gke_config=databricks.MwsWorkspacesGkeConfigArgs(\n        connectivity_type=\"PRIVATE_NODE_PUBLIC_MASTER\",\n        master_ip_range=\"10.3.0.0/28\",\n    ),\n    token=databricks.MwsWorkspacesTokenArgs())\npulumi.export(\"databricksToken\", this_mws_workspaces.token.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var databricksGoogleServiceAccount = config.RequireObject\u003cdynamic\u003e(\"databricksGoogleServiceAccount\");\n    var googleProject = config.RequireObject\u003cdynamic\u003e(\"googleProject\");\n    // register VPC\n    var @this = new Databricks.MwsNetworks(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        NetworkName = $\"{prefix}-network\",\n        GcpNetworkInfo = new Databricks.Inputs.MwsNetworksGcpNetworkInfoArgs\n        {\n            NetworkProjectId = googleProject,\n            VpcId = vpcId,\n            SubnetId = subnetId,\n            SubnetRegion = subnetRegion,\n            PodIpRangeName = \"pods\",\n            ServiceIpRangeName = \"svc\",\n        },\n    });\n\n    // create workspace in given VPC\n    var thisMwsWorkspaces = new Databricks.MwsWorkspaces(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        WorkspaceName = prefix,\n        Location = subnetRegion,\n        CloudResourceContainer = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerArgs\n        {\n            Gcp = new Databricks.Inputs.MwsWorkspacesCloudResourceContainerGcpArgs\n            {\n                ProjectId = googleProject,\n            },\n        },\n        NetworkId = @this.NetworkId,\n        GkeConfig = new Databricks.Inputs.MwsWorkspacesGkeConfigArgs\n        {\n            ConnectivityType = \"PRIVATE_NODE_PUBLIC_MASTER\",\n            MasterIpRange = \"10.3.0.0/28\",\n        },\n        Token = null,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = thisMwsWorkspaces.Token.Apply(token =\u003e token?.TokenValue),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tdatabricksGoogleServiceAccount := cfg.RequireObject(\"databricksGoogleServiceAccount\")\n\t\tgoogleProject := cfg.RequireObject(\"googleProject\")\n\t\t// register VPC\n\t\tthis, err := databricks.NewMwsNetworks(ctx, \"this\", \u0026databricks.MwsNetworksArgs{\n\t\t\tAccountId:   pulumi.Any(databricksAccountId),\n\t\t\tNetworkName: pulumi.String(fmt.Sprintf(\"%v-network\", prefix)),\n\t\t\tGcpNetworkInfo: \u0026databricks.MwsNetworksGcpNetworkInfoArgs{\n\t\t\t\tNetworkProjectId:   pulumi.Any(googleProject),\n\t\t\t\tVpcId:              pulumi.Any(vpcId),\n\t\t\t\tSubnetId:           pulumi.Any(subnetId),\n\t\t\t\tSubnetRegion:       pulumi.Any(subnetRegion),\n\t\t\t\tPodIpRangeName:     pulumi.String(\"pods\"),\n\t\t\t\tServiceIpRangeName: pulumi.String(\"svc\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// create workspace in given VPC\n\t\tthisMwsWorkspaces, err := databricks.NewMwsWorkspaces(ctx, \"this\", \u0026databricks.MwsWorkspacesArgs{\n\t\t\tAccountId:     pulumi.Any(databricksAccountId),\n\t\t\tWorkspaceName: pulumi.Any(prefix),\n\t\t\tLocation:      pulumi.Any(subnetRegion),\n\t\t\tCloudResourceContainer: \u0026databricks.MwsWorkspacesCloudResourceContainerArgs{\n\t\t\t\tGcp: \u0026databricks.MwsWorkspacesCloudResourceContainerGcpArgs{\n\t\t\t\t\tProjectId: pulumi.Any(googleProject),\n\t\t\t\t},\n\t\t\t},\n\t\t\tNetworkId: this.NetworkId,\n\t\t\tGkeConfig: \u0026databricks.MwsWorkspacesGkeConfigArgs{\n\t\t\t\tConnectivityType: pulumi.String(\"PRIVATE_NODE_PUBLIC_MASTER\"),\n\t\t\t\tMasterIpRange:    pulumi.String(\"10.3.0.0/28\"),\n\t\t\t},\n\t\t\tToken: nil,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", thisMwsWorkspaces.Token.ApplyT(func(token databricks.MwsWorkspacesToken) (*string, error) {\n\t\t\treturn \u0026token.TokenValue, nil\n\t\t}).(pulumi.StringPtrOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MwsNetworks;\nimport com.pulumi.databricks.MwsNetworksArgs;\nimport com.pulumi.databricks.inputs.MwsNetworksGcpNetworkInfoArgs;\nimport com.pulumi.databricks.MwsWorkspaces;\nimport com.pulumi.databricks.MwsWorkspacesArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesCloudResourceContainerGcpArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesGkeConfigArgs;\nimport com.pulumi.databricks.inputs.MwsWorkspacesTokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var databricksGoogleServiceAccount = config.get(\"databricksGoogleServiceAccount\");\n        final var googleProject = config.get(\"googleProject\");\n        // register VPC\n        var this_ = new MwsNetworks(\"this\", MwsNetworksArgs.builder()        \n            .accountId(databricksAccountId)\n            .networkName(String.format(\"%s-network\", prefix))\n            .gcpNetworkInfo(MwsNetworksGcpNetworkInfoArgs.builder()\n                .networkProjectId(googleProject)\n                .vpcId(vpcId)\n                .subnetId(subnetId)\n                .subnetRegion(subnetRegion)\n                .podIpRangeName(\"pods\")\n                .serviceIpRangeName(\"svc\")\n                .build())\n            .build());\n\n        // create workspace in given VPC\n        var thisMwsWorkspaces = new MwsWorkspaces(\"thisMwsWorkspaces\", MwsWorkspacesArgs.builder()        \n            .accountId(databricksAccountId)\n            .workspaceName(prefix)\n            .location(subnetRegion)\n            .cloudResourceContainer(MwsWorkspacesCloudResourceContainerArgs.builder()\n                .gcp(MwsWorkspacesCloudResourceContainerGcpArgs.builder()\n                    .projectId(googleProject)\n                    .build())\n                .build())\n            .networkId(this_.networkId())\n            .gkeConfig(MwsWorkspacesGkeConfigArgs.builder()\n                .connectivityType(\"PRIVATE_NODE_PUBLIC_MASTER\")\n                .masterIpRange(\"10.3.0.0/28\")\n                .build())\n            .token()\n            .build());\n\n        ctx.export(\"databricksToken\", thisMwsWorkspaces.token().applyValue(token -\u003e token.tokenValue()));\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\n  databricksGoogleServiceAccount:\n    type: dynamic\n  googleProject:\n    type: dynamic\nresources:\n  # register VPC\n  this:\n    type: databricks:MwsNetworks\n    properties:\n      accountId: ${databricksAccountId}\n      networkName: ${prefix}-network\n      gcpNetworkInfo:\n        networkProjectId: ${googleProject}\n        vpcId: ${vpcId}\n        subnetId: ${subnetId}\n        subnetRegion: ${subnetRegion}\n        podIpRangeName: pods\n        serviceIpRangeName: svc\n  # create workspace in given VPC\n  thisMwsWorkspaces:\n    type: databricks:MwsWorkspaces\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      workspaceName: ${prefix}\n      location: ${subnetRegion}\n      cloudResourceContainer:\n        gcp:\n          projectId: ${googleProject}\n      networkId: ${this.networkId}\n      gkeConfig:\n        connectivityType: PRIVATE_NODE_PUBLIC_MASTER\n        masterIpRange: 10.3.0.0/28\n      token: {}\noutputs:\n  databricksToken: ${thisMwsWorkspaces.token.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn order to create a [Databricks Workspace that leverages GCP Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) please ensure that you have read and understood the [Enable Private Service Connect](https://docs.gcp.databricks.com/administration-guide/cloud-configurations/gcp/private-service-connect.html) documentation and then customise the example above with the relevant examples from mws_vpc_endpoint, mws_private_access_settings and mws_networks.\n\n",
            "properties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC.\n"
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n"
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead"
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n"
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo"
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig"
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n"
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean"
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet.\n"
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string",
                    "description": "The pricing tier of the workspace.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration.\n"
                },
                "storageCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(String) workspace id\n"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI.\n"
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "required": [
                "accountId",
                "cloud",
                "creationTime",
                "pricingTier",
                "workspaceId",
                "workspaceName",
                "workspaceStatus",
                "workspaceStatusMessage",
                "workspaceUrl"
            ],
            "inputProperties": {
                "accountId": {
                    "type": "string",
                    "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "awsRegion": {
                    "type": "string",
                    "description": "region of VPC.\n",
                    "willReplaceOnChanges": true
                },
                "cloud": {
                    "type": "string"
                },
                "cloudResourceContainer": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                    "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer",
                    "description": "(Integer) time when workspace was created\n"
                },
                "credentialsId": {
                    "type": "string"
                },
                "customTags": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                },
                "customerManagedKeyId": {
                    "type": "string",
                    "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                    "willReplaceOnChanges": true
                },
                "deploymentName": {
                    "type": "string",
                    "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                    "willReplaceOnChanges": true
                },
                "externalCustomerInfo": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                    "willReplaceOnChanges": true
                },
                "gcpManagedNetworkConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                    "willReplaceOnChanges": true
                },
                "gkeConfig": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                    "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                    "willReplaceOnChanges": true
                },
                "isNoPublicIpEnabled": {
                    "type": "boolean",
                    "willReplaceOnChanges": true
                },
                "location": {
                    "type": "string",
                    "description": "region of the subnet.\n",
                    "willReplaceOnChanges": true
                },
                "managedServicesCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                },
                "networkId": {
                    "type": "string",
                    "description": "`network_id` from networks.\n"
                },
                "pricingTier": {
                    "type": "string",
                    "description": "The pricing tier of the workspace.\n"
                },
                "privateAccessSettingsId": {
                    "type": "string",
                    "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                },
                "storageConfigurationId": {
                    "type": "string",
                    "description": "`storage_configuration_id` from storage configuration.\n",
                    "willReplaceOnChanges": true
                },
                "storageCustomerManagedKeyId": {
                    "type": "string",
                    "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                },
                "token": {
                    "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                },
                "workspaceId": {
                    "type": "string",
                    "description": "(String) workspace id\n"
                },
                "workspaceName": {
                    "type": "string",
                    "description": "name of the workspace, will appear on UI.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceStatus": {
                    "type": "string",
                    "description": "(String) workspace status\n"
                },
                "workspaceStatusMessage": {
                    "type": "string",
                    "description": "(String) updates on workspace status\n"
                },
                "workspaceUrl": {
                    "type": "string",
                    "description": "(String) URL of the workspace\n"
                }
            },
            "requiredInputs": [
                "accountId",
                "workspaceName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering MwsWorkspaces resources.\n",
                "properties": {
                    "accountId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "awsRegion": {
                        "type": "string",
                        "description": "region of VPC.\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string"
                    },
                    "cloudResourceContainer": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesCloudResourceContainer:MwsWorkspacesCloudResourceContainer",
                        "description": "A block that specifies GCP workspace configurations, consisting of following blocks:\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer",
                        "description": "(Integer) time when workspace was created\n"
                    },
                    "credentialsId": {
                        "type": "string"
                    },
                    "customTags": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "The custom tags key-value pairing that is attached to this workspace. These tags will be applied to clusters automatically in addition to any `default_tags` or `custom_tags` on a cluster level. Please note it can take up to an hour for custom_tags to be set due to scheduling on Control Plane. After custom tags are applied, they can be modified however they can never be completely removed.\n"
                    },
                    "customerManagedKeyId": {
                        "type": "string",
                        "deprecationMessage": "Use managed_services_customer_managed_key_id instead",
                        "willReplaceOnChanges": true
                    },
                    "deploymentName": {
                        "type": "string",
                        "description": "part of URL as in `https://\u003cprefix\u003e-\u003cdeployment-name\u003e.cloud.databricks.com`. Deployment name cannot be used until a deployment name prefix is defined. Please contact your Databricks representative. Once a new deployment prefix is added/updated, it only will affect the new workspaces created.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalCustomerInfo": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesExternalCustomerInfo:MwsWorkspacesExternalCustomerInfo",
                        "willReplaceOnChanges": true
                    },
                    "gcpManagedNetworkConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGcpManagedNetworkConfig:MwsWorkspacesGcpManagedNetworkConfig",
                        "willReplaceOnChanges": true
                    },
                    "gkeConfig": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesGkeConfig:MwsWorkspacesGkeConfig",
                        "description": "A block that specifies GKE configuration for the Databricks workspace:\n",
                        "willReplaceOnChanges": true
                    },
                    "isNoPublicIpEnabled": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "location": {
                        "type": "string",
                        "description": "region of the subnet.\n",
                        "willReplaceOnChanges": true
                    },
                    "managedServicesCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `MANAGED_SERVICES`. This is used to encrypt the workspace's notebook and secret data in the control plane.\n"
                    },
                    "networkId": {
                        "type": "string",
                        "description": "`network_id` from networks.\n"
                    },
                    "pricingTier": {
                        "type": "string",
                        "description": "The pricing tier of the workspace.\n"
                    },
                    "privateAccessSettingsId": {
                        "type": "string",
                        "description": "Canonical unique identifier of databricks.MwsPrivateAccessSettings in Databricks Account.\n"
                    },
                    "storageConfigurationId": {
                        "type": "string",
                        "description": "`storage_configuration_id` from storage configuration.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCustomerManagedKeyId": {
                        "type": "string",
                        "description": "`customer_managed_key_id` from customer managed keys with `use_cases` set to `STORAGE`. This is used to encrypt the DBFS Storage \u0026 Cluster Volumes.\n"
                    },
                    "token": {
                        "$ref": "#/types/databricks:index/MwsWorkspacesToken:MwsWorkspacesToken"
                    },
                    "workspaceId": {
                        "type": "string",
                        "description": "(String) workspace id\n"
                    },
                    "workspaceName": {
                        "type": "string",
                        "description": "name of the workspace, will appear on UI.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceStatus": {
                        "type": "string",
                        "description": "(String) workspace status\n"
                    },
                    "workspaceStatusMessage": {
                        "type": "string",
                        "description": "(String) updates on workspace status\n"
                    },
                    "workspaceUrl": {
                        "type": "string",
                        "description": "(String) URL of the workspace\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/notebook:Notebook": {
            "description": "\n\n## Import\n\nThe resource notebook can be imported using notebook path\n\nbash\n\n```sh\n$ pulumi import databricks:index/notebook:Notebook this /path/to/notebook\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the notebook\n"
                }
            },
            "required": [
                "objectId",
                "objectType",
                "path",
                "url"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "format": {
                    "type": "string"
                },
                "language": {
                    "type": "string",
                    "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a NOTEBOOK\n"
                },
                "objectType": {
                    "type": "string",
                    "deprecationMessage": "Always is a notebook"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Notebook resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "language": {
                        "type": "string",
                        "description": "One of `SCALA`, `PYTHON`, `SQL`, `R`.\n"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a NOTEBOOK\n"
                    },
                    "objectType": {
                        "type": "string",
                        "deprecationMessage": "Always is a notebook"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the notebook or directory, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to notebook in source code format on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the notebook\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/oboToken:OboToken": {
            "description": "\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "applicationId",
                "tokenValue"
            ],
            "inputProperties": {
                "applicationId": {
                    "type": "string",
                    "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Comment that describes the purpose of the token.\n",
                    "willReplaceOnChanges": true
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "applicationId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OboToken resources.\n",
                "properties": {
                    "applicationId": {
                        "type": "string",
                        "description": "Application ID of databricks.ServicePrincipal to create a PAT token for.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Comment that describes the purpose of the token.\n",
                        "willReplaceOnChanges": true
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "The number of seconds before the token expires. Token resource is re-created when it expires. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/onlineTable:OnlineTable": {
            "description": "\u003e **Note** This resource could be only used on Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Online Table](https://docs.databricks.com/en/machine-learning/feature-store/online-tables.html) in Databricks.  An online table is a read-only copy of a Delta Table that is stored in row-oriented format optimized for online access. Online tables are fully serverless tables that auto-scale throughput capacity with the request load and provide low latency and high throughput access to data of any scale. Online tables are designed to work with Databricks Model Serving, Feature Serving, and retrieval-augmented generation (RAG) applications where they are used for fast data lookups.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.OnlineTable(\"this\", {\n    name: \"main.default.online_table\",\n    spec: {\n        sourceTableFullName: \"main.default.source_table\",\n        primaryKeyColumns: [\"id\"],\n        runTriggered: {},\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.OnlineTable(\"this\",\n    name=\"main.default.online_table\",\n    spec=databricks.OnlineTableSpecArgs(\n        source_table_full_name=\"main.default.source_table\",\n        primary_key_columns=[\"id\"],\n        run_triggered=databricks.OnlineTableSpecRunTriggeredArgs(),\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.OnlineTable(\"this\", new()\n    {\n        Name = \"main.default.online_table\",\n        Spec = new Databricks.Inputs.OnlineTableSpecArgs\n        {\n            SourceTableFullName = \"main.default.source_table\",\n            PrimaryKeyColumns = new[]\n            {\n                \"id\",\n            },\n            RunTriggered = null,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewOnlineTable(ctx, \"this\", \u0026databricks.OnlineTableArgs{\n\t\t\tName: pulumi.String(\"main.default.online_table\"),\n\t\t\tSpec: \u0026databricks.OnlineTableSpecArgs{\n\t\t\t\tSourceTableFullName: pulumi.String(\"main.default.source_table\"),\n\t\t\t\tPrimaryKeyColumns: pulumi.StringArray{\n\t\t\t\t\tpulumi.String(\"id\"),\n\t\t\t\t},\n\t\t\t\tRunTriggered: nil,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.OnlineTable;\nimport com.pulumi.databricks.OnlineTableArgs;\nimport com.pulumi.databricks.inputs.OnlineTableSpecArgs;\nimport com.pulumi.databricks.inputs.OnlineTableSpecRunTriggeredArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new OnlineTable(\"this\", OnlineTableArgs.builder()        \n            .name(\"main.default.online_table\")\n            .spec(OnlineTableSpecArgs.builder()\n                .sourceTableFullName(\"main.default.source_table\")\n                .primaryKeyColumns(\"id\")\n                .runTriggered()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:OnlineTable\n    properties:\n      name: main.default.online_table\n      spec:\n        sourceTableFullName: main.default.source_table\n        primaryKeyColumns:\n          - id\n        runTriggered: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Online Table:\n\nbash\n\n```sh\n$ pulumi import databricks:index/onlineTable:OnlineTable this \u003cendpoint-name\u003e\n```\n\n",
            "properties": {
                "name": {
                    "type": "string",
                    "description": "3-level name of the Online Table to create.\n"
                },
                "spec": {
                    "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                    "description": "object containing specification of the online table:\n"
                },
                "statuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/OnlineTableStatus:OnlineTableStatus"
                    },
                    "description": "object describing status of the online table:\n"
                }
            },
            "required": [
                "name",
                "statuses"
            ],
            "inputProperties": {
                "name": {
                    "type": "string",
                    "description": "3-level name of the Online Table to create.\n",
                    "willReplaceOnChanges": true
                },
                "spec": {
                    "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                    "description": "object containing specification of the online table:\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering OnlineTable resources.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "3-level name of the Online Table to create.\n",
                        "willReplaceOnChanges": true
                    },
                    "spec": {
                        "$ref": "#/types/databricks:index/OnlineTableSpec:OnlineTableSpec",
                        "description": "object containing specification of the online table:\n",
                        "willReplaceOnChanges": true
                    },
                    "statuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/OnlineTableStatus:OnlineTableStatus"
                        },
                        "description": "object describing status of the online table:\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissionAssignment:PermissionAssignment": {
            "description": "These resources are invoked in the workspace context.\n\n## Example Usage\n\nIn workspace context, adding account-level user to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst addUser = new databricks.PermissionAssignment(\"add_user\", {\n    principalId: me.then(me =\u003e me.id),\n    permissions: [\"USER\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\nme = databricks.get_user(user_name=\"me@example.com\")\nadd_user = databricks.PermissionAssignment(\"add_user\",\n    principal_id=me.id,\n    permissions=[\"USER\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var addUser = new Databricks.PermissionAssignment(\"add_user\", new()\n    {\n        PrincipalId = me.Apply(getUserResult =\u003e getUserResult.Id),\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"add_user\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(me.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var addUser = new PermissionAssignment(\"addUser\", PermissionAssignmentArgs.builder()        \n            .principalId(me.applyValue(getUserResult -\u003e getUserResult.id()))\n            .permissions(\"USER\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  addUser:\n    type: databricks:PermissionAssignment\n    name: add_user\n    properties:\n      principalId: ${me.id}\n      permissions:\n        - USER\nvariables:\n  # Use the account provider\n  me:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn workspace context, adding account-level service principal to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst sp = databricks.getServicePrincipal({\n    displayName: \"Automation-only SP\",\n});\nconst addAdminSpn = new databricks.PermissionAssignment(\"add_admin_spn\", {\n    principalId: sp.then(sp =\u003e sp.id),\n    permissions: [\"ADMIN\"],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\nsp = databricks.get_service_principal(display_name=\"Automation-only SP\")\nadd_admin_spn = databricks.PermissionAssignment(\"add_admin_spn\",\n    principal_id=sp.id,\n    permissions=[\"ADMIN\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var sp = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n    var addAdminSpn = new Databricks.PermissionAssignment(\"add_admin_spn\", new()\n    {\n        PrincipalId = sp.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.Id),\n        Permissions = new[]\n        {\n            \"ADMIN\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\tsp, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.StringRef(\"Automation-only SP\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"add_admin_spn\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(sp.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"ADMIN\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var sp = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .displayName(\"Automation-only SP\")\n            .build());\n\n        var addAdminSpn = new PermissionAssignment(\"addAdminSpn\", PermissionAssignmentArgs.builder()        \n            .principalId(sp.applyValue(getServicePrincipalResult -\u003e getServicePrincipalResult.id()))\n            .permissions(\"ADMIN\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  addAdminSpn:\n    type: databricks:PermissionAssignment\n    name: add_admin_spn\n    properties:\n      principalId: ${sp.id}\n      permissions:\n        - ADMIN\nvariables:\n  # Use the account provider\n  sp:\n    fn::invoke:\n      Function: databricks:getServicePrincipal\n      Arguments:\n        displayName: Automation-only SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nIn workspace context, adding account-level group to a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// Use the account provider\nconst accountLevel = databricks.getGroup({\n    displayName: \"example-group\",\n});\n// Use the workspace provider\nconst _this = new databricks.PermissionAssignment(\"this\", {\n    principalId: accountLevel.then(accountLevel =\u003e accountLevel.id),\n    permissions: [\"USER\"],\n});\nconst workspaceLevel = databricks.getGroup({\n    displayName: \"example-group\",\n});\nexport const databricksGroupId = workspaceLevel.then(workspaceLevel =\u003e workspaceLevel.id);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# Use the account provider\naccount_level = databricks.get_group(display_name=\"example-group\")\n# Use the workspace provider\nthis = databricks.PermissionAssignment(\"this\",\n    principal_id=account_level.id,\n    permissions=[\"USER\"])\nworkspace_level = databricks.get_group(display_name=\"example-group\")\npulumi.export(\"databricksGroupId\", workspace_level.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // Use the account provider\n    var accountLevel = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"example-group\",\n    });\n\n    // Use the workspace provider\n    var @this = new Databricks.PermissionAssignment(\"this\", new()\n    {\n        PrincipalId = accountLevel.Apply(getGroupResult =\u003e getGroupResult.Id),\n        Permissions = new[]\n        {\n            \"USER\",\n        },\n    });\n\n    var workspaceLevel = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"example-group\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksGroupId\"] = workspaceLevel.Apply(getGroupResult =\u003e getGroupResult.Id),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// Use the account provider\n\t\taccountLevel, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"example-group\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// Use the workspace provider\n\t\t_, err = databricks.NewPermissionAssignment(ctx, \"this\", \u0026databricks.PermissionAssignmentArgs{\n\t\t\tPrincipalId: pulumi.String(accountLevel.Id),\n\t\t\tPermissions: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"USER\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tworkspaceLevel, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"example-group\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksGroupId\", workspaceLevel.Id)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.PermissionAssignment;\nimport com.pulumi.databricks.PermissionAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // Use the account provider\n        final var accountLevel = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"example-group\")\n            .build());\n\n        // Use the workspace provider\n        var this_ = new PermissionAssignment(\"this\", PermissionAssignmentArgs.builder()        \n            .principalId(accountLevel.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .permissions(\"USER\")\n            .build());\n\n        final var workspaceLevel = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"example-group\")\n            .build());\n\n        ctx.export(\"databricksGroupId\", workspaceLevel.applyValue(getGroupResult -\u003e getGroupResult.id()));\n    }\n}\n```\n```yaml\nresources:\n  # Use the workspace provider\n  this:\n    type: databricks:PermissionAssignment\n    properties:\n      principalId: ${accountLevel.id}\n      permissions:\n        - USER\nvariables:\n  # Use the account provider\n  accountLevel:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: example-group\n  workspaceLevel:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: example-group\noutputs:\n  databricksGroupId: ${workspaceLevel.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.MwsPermissionAssignment to manage permission assignment from an account context\n\n## Import\n\nThe resource `databricks_permission_assignment` can be imported using the principal id\n\nbash\n\n```sh\n$ pulumi import databricks:index/permissionAssignment:PermissionAssignment this principal_id\n```\n\n",
            "properties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n"
                },
                "principalId": {
                    "type": "integer"
                }
            },
            "required": [
                "permissions",
                "principalId"
            ],
            "inputProperties": {
                "permissions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                    "willReplaceOnChanges": true
                },
                "principalId": {
                    "type": "integer",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permissions",
                "principalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering PermissionAssignment resources.\n",
                "properties": {
                    "permissions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "The list of workspace permissions to assign to the principal:\n* `\"USER\"` - Can access the workspace with basic privileges.\n* `\"ADMIN\"` - Can access the workspace and has workspace admin privileges to manage users and groups, workspace configurations, and more.\n",
                        "willReplaceOnChanges": true
                    },
                    "principalId": {
                        "type": "integer",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/permissions:Permissions": {
            "description": "\n\n## Import\n\n### Import Example\n\nConfiguration file:\n\nhcl\n\nresource \"databricks_mlflow_model\" \"model\" {\n\n  name        = \"example_model\"\n\n  description = \"MLflow registered model\"\n\n}\n\nresource \"databricks_permissions\" \"model_usage\" {\n\n  registered_model_id = databricks_mlflow_model.model.registered_model_id\n\n  access_control {\n\n    group_name       = \"users\"\n\n    permission_level = \"CAN_READ\"\n\n  }\n\n}\n\nImport command:\n\nbash\n\n```sh\n$ pulumi import databricks:index/permissions:Permissions model_usage /registered-models/\u003cregistered_model_id\u003e\n```\n\n",
            "properties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterPolicyId": {
                    "type": "string"
                },
                "directoryId": {
                    "type": "string"
                },
                "directoryPath": {
                    "type": "string"
                },
                "experimentId": {
                    "type": "string"
                },
                "instancePoolId": {
                    "type": "string"
                },
                "jobId": {
                    "type": "string"
                },
                "notebookId": {
                    "type": "string"
                },
                "notebookPath": {
                    "type": "string"
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string"
                },
                "registeredModelId": {
                    "type": "string"
                },
                "repoId": {
                    "type": "string"
                },
                "repoPath": {
                    "type": "string"
                },
                "servingEndpointId": {
                    "type": "string"
                },
                "sqlAlertId": {
                    "type": "string"
                },
                "sqlDashboardId": {
                    "type": "string"
                },
                "sqlEndpointId": {
                    "type": "string"
                },
                "sqlQueryId": {
                    "type": "string"
                },
                "workspaceFileId": {
                    "type": "string"
                },
                "workspaceFilePath": {
                    "type": "string"
                }
            },
            "required": [
                "accessControls",
                "objectType"
            ],
            "inputProperties": {
                "accessControls": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                    }
                },
                "authorization": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "clusterPolicyId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directoryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "directoryPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "experimentId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "instancePoolId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "jobId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "notebookId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "notebookPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "objectType": {
                    "type": "string",
                    "description": "type of permissions.\n"
                },
                "pipelineId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "registeredModelId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repoId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "repoPath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "servingEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlAlertId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlDashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlEndpointId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "sqlQueryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "workspaceFileId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "workspaceFilePath": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "accessControls"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Permissions resources.\n",
                "properties": {
                    "accessControls": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PermissionsAccessControl:PermissionsAccessControl"
                        }
                    },
                    "authorization": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "clusterPolicyId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directoryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "directoryPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "experimentId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "instancePoolId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "jobId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "notebookId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "notebookPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "objectType": {
                        "type": "string",
                        "description": "type of permissions.\n"
                    },
                    "pipelineId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "registeredModelId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "repoId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "repoPath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "servingEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlAlertId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlDashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlEndpointId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "sqlQueryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "workspaceFileId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "workspaceFilePath": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/pipeline:Pipeline": {
            "description": "Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst dltDemo = new databricks.Notebook(\"dlt_demo\", {});\nconst dltDemoRepo = new databricks.Repo(\"dlt_demo\", {});\nconst _this = new databricks.Pipeline(\"this\", {\n    name: \"Pipeline Name\",\n    storage: \"/test/first-pipeline\",\n    configuration: {\n        key1: \"value1\",\n        key2: \"value2\",\n    },\n    clusters: [\n        {\n            label: \"default\",\n            numWorkers: 2,\n            customTags: {\n                cluster_type: \"default\",\n            },\n        },\n        {\n            label: \"maintenance\",\n            numWorkers: 1,\n            customTags: {\n                cluster_type: \"maintenance\",\n            },\n        },\n    ],\n    libraries: [\n        {\n            notebook: {\n                path: dltDemo.id,\n            },\n        },\n        {\n            file: {\n                path: pulumi.interpolate`${dltDemoRepo.path}/pipeline.sql`,\n            },\n        },\n    ],\n    continuous: false,\n    notifications: [{\n        emailRecipients: [\n            \"user@domain.com\",\n            \"user1@domain.com\",\n        ],\n        alerts: [\n            \"on-update-failure\",\n            \"on-update-fatal-failure\",\n            \"on-update-success\",\n            \"on-flow-failure\",\n        ],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndlt_demo = databricks.Notebook(\"dlt_demo\")\ndlt_demo_repo = databricks.Repo(\"dlt_demo\")\nthis = databricks.Pipeline(\"this\",\n    name=\"Pipeline Name\",\n    storage=\"/test/first-pipeline\",\n    configuration={\n        \"key1\": \"value1\",\n        \"key2\": \"value2\",\n    },\n    clusters=[\n        databricks.PipelineClusterArgs(\n            label=\"default\",\n            num_workers=2,\n            custom_tags={\n                \"cluster_type\": \"default\",\n            },\n        ),\n        databricks.PipelineClusterArgs(\n            label=\"maintenance\",\n            num_workers=1,\n            custom_tags={\n                \"cluster_type\": \"maintenance\",\n            },\n        ),\n    ],\n    libraries=[\n        databricks.PipelineLibraryArgs(\n            notebook=databricks.PipelineLibraryNotebookArgs(\n                path=dlt_demo.id,\n            ),\n        ),\n        databricks.PipelineLibraryArgs(\n            file=databricks.PipelineLibraryFileArgs(\n                path=dlt_demo_repo.path.apply(lambda path: f\"{path}/pipeline.sql\"),\n            ),\n        ),\n    ],\n    continuous=False,\n    notifications=[databricks.PipelineNotificationArgs(\n        email_recipients=[\n            \"user@domain.com\",\n            \"user1@domain.com\",\n        ],\n        alerts=[\n            \"on-update-failure\",\n            \"on-update-fatal-failure\",\n            \"on-update-success\",\n            \"on-flow-failure\",\n        ],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var dltDemo = new Databricks.Notebook(\"dlt_demo\");\n\n    var dltDemoRepo = new Databricks.Repo(\"dlt_demo\");\n\n    var @this = new Databricks.Pipeline(\"this\", new()\n    {\n        Name = \"Pipeline Name\",\n        Storage = \"/test/first-pipeline\",\n        Configuration = \n        {\n            { \"key1\", \"value1\" },\n            { \"key2\", \"value2\" },\n        },\n        Clusters = new[]\n        {\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"default\",\n                NumWorkers = 2,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"default\" },\n                },\n            },\n            new Databricks.Inputs.PipelineClusterArgs\n            {\n                Label = \"maintenance\",\n                NumWorkers = 1,\n                CustomTags = \n                {\n                    { \"cluster_type\", \"maintenance\" },\n                },\n            },\n        },\n        Libraries = new[]\n        {\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                Notebook = new Databricks.Inputs.PipelineLibraryNotebookArgs\n                {\n                    Path = dltDemo.Id,\n                },\n            },\n            new Databricks.Inputs.PipelineLibraryArgs\n            {\n                File = new Databricks.Inputs.PipelineLibraryFileArgs\n                {\n                    Path = dltDemoRepo.Path.Apply(path =\u003e $\"{path}/pipeline.sql\"),\n                },\n            },\n        },\n        Continuous = false,\n        Notifications = new[]\n        {\n            new Databricks.Inputs.PipelineNotificationArgs\n            {\n                EmailRecipients = new[]\n                {\n                    \"user@domain.com\",\n                    \"user1@domain.com\",\n                },\n                Alerts = new[]\n                {\n                    \"on-update-failure\",\n                    \"on-update-fatal-failure\",\n                    \"on-update-success\",\n                    \"on-flow-failure\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdltDemo, err := databricks.NewNotebook(ctx, \"dlt_demo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tdltDemoRepo, err := databricks.NewRepo(ctx, \"dlt_demo\", nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewPipeline(ctx, \"this\", \u0026databricks.PipelineArgs{\n\t\t\tName:    pulumi.String(\"Pipeline Name\"),\n\t\t\tStorage: pulumi.String(\"/test/first-pipeline\"),\n\t\t\tConfiguration: pulumi.Map{\n\t\t\t\t\"key1\": pulumi.Any(\"value1\"),\n\t\t\t\t\"key2\": pulumi.Any(\"value2\"),\n\t\t\t},\n\t\t\tClusters: databricks.PipelineClusterArray{\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"default\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(2),\n\t\t\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PipelineClusterArgs{\n\t\t\t\t\tLabel:      pulumi.String(\"maintenance\"),\n\t\t\t\t\tNumWorkers: pulumi.Int(1),\n\t\t\t\t\tCustomTags: pulumi.Map{\n\t\t\t\t\t\t\"cluster_type\": pulumi.Any(\"maintenance\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tLibraries: databricks.PipelineLibraryArray{\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tNotebook: \u0026databricks.PipelineLibraryNotebookArgs{\n\t\t\t\t\t\tPath: dltDemo.ID(),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PipelineLibraryArgs{\n\t\t\t\t\tFile: \u0026databricks.PipelineLibraryFileArgs{\n\t\t\t\t\t\tPath: dltDemoRepo.Path.ApplyT(func(path string) (string, error) {\n\t\t\t\t\t\t\treturn fmt.Sprintf(\"%v/pipeline.sql\", path), nil\n\t\t\t\t\t\t}).(pulumi.StringOutput),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tContinuous: pulumi.Bool(false),\n\t\t\tNotifications: databricks.PipelineNotificationArray{\n\t\t\t\t\u0026databricks.PipelineNotificationArgs{\n\t\t\t\t\tEmailRecipients: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"user@domain.com\"),\n\t\t\t\t\t\tpulumi.String(\"user1@domain.com\"),\n\t\t\t\t\t},\n\t\t\t\t\tAlerts: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"on-update-failure\"),\n\t\t\t\t\t\tpulumi.String(\"on-update-fatal-failure\"),\n\t\t\t\t\t\tpulumi.String(\"on-update-success\"),\n\t\t\t\t\t\tpulumi.String(\"on-flow-failure\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Notebook;\nimport com.pulumi.databricks.Repo;\nimport com.pulumi.databricks.Pipeline;\nimport com.pulumi.databricks.PipelineArgs;\nimport com.pulumi.databricks.inputs.PipelineClusterArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;\nimport com.pulumi.databricks.inputs.PipelineLibraryFileArgs;\nimport com.pulumi.databricks.inputs.PipelineNotificationArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var dltDemo = new Notebook(\"dltDemo\");\n\n        var dltDemoRepo = new Repo(\"dltDemoRepo\");\n\n        var this_ = new Pipeline(\"this\", PipelineArgs.builder()        \n            .name(\"Pipeline Name\")\n            .storage(\"/test/first-pipeline\")\n            .configuration(Map.ofEntries(\n                Map.entry(\"key1\", \"value1\"),\n                Map.entry(\"key2\", \"value2\")\n            ))\n            .clusters(            \n                PipelineClusterArgs.builder()\n                    .label(\"default\")\n                    .numWorkers(2)\n                    .customTags(Map.of(\"cluster_type\", \"default\"))\n                    .build(),\n                PipelineClusterArgs.builder()\n                    .label(\"maintenance\")\n                    .numWorkers(1)\n                    .customTags(Map.of(\"cluster_type\", \"maintenance\"))\n                    .build())\n            .libraries(            \n                PipelineLibraryArgs.builder()\n                    .notebook(PipelineLibraryNotebookArgs.builder()\n                        .path(dltDemo.id())\n                        .build())\n                    .build(),\n                PipelineLibraryArgs.builder()\n                    .file(PipelineLibraryFileArgs.builder()\n                        .path(dltDemoRepo.path().applyValue(path -\u003e String.format(\"%s/pipeline.sql\", path)))\n                        .build())\n                    .build())\n            .continuous(false)\n            .notifications(PipelineNotificationArgs.builder()\n                .emailRecipients(                \n                    \"user@domain.com\",\n                    \"user1@domain.com\")\n                .alerts(                \n                    \"on-update-failure\",\n                    \"on-update-fatal-failure\",\n                    \"on-update-success\",\n                    \"on-flow-failure\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  dltDemo:\n    type: databricks:Notebook\n    name: dlt_demo\n  dltDemoRepo:\n    type: databricks:Repo\n    name: dlt_demo\n  this:\n    type: databricks:Pipeline\n    properties:\n      name: Pipeline Name\n      storage: /test/first-pipeline\n      configuration:\n        key1: value1\n        key2: value2\n      clusters:\n        - label: default\n          numWorkers: 2\n          customTags:\n            cluster_type: default\n        - label: maintenance\n          numWorkers: 1\n          customTags:\n            cluster_type: maintenance\n      libraries:\n        - notebook:\n            path: ${dltDemo.id}\n        - file:\n            path: ${dltDemoRepo.path}/pipeline.sql\n      continuous: false\n      notifications:\n        - emailRecipients:\n            - user@domain.com\n            - user1@domain.com\n          alerts:\n            - on-update-failure\n            - on-update-fatal-failure\n            - on-update-success\n            - on-flow-failure\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.getPipelines to retrieve [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipeline data.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n\n## Import\n\nThe resource job can be imported using the id of the pipeline\n\nbash\n\n```sh\n$ pulumi import databricks:index/pipeline:Pipeline this \u003cpipeline-id\u003e\n```\n\n",
            "properties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "catalog": {
                    "type": "string",
                    "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n"
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "notifications": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                    }
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "serverless": {
                    "type": "boolean"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n"
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                },
                "url": {
                    "type": "string",
                    "description": "URL of the DLT pipeline on the given workspace.\n"
                }
            },
            "required": [
                "name",
                "url"
            ],
            "inputProperties": {
                "allowDuplicateNames": {
                    "type": "boolean"
                },
                "catalog": {
                    "type": "string",
                    "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n",
                    "willReplaceOnChanges": true
                },
                "channel": {
                    "type": "string",
                    "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                },
                "clusters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                    },
                    "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                },
                "configuration": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                },
                "continuous": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                },
                "deployment": {
                    "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment"
                },
                "development": {
                    "type": "boolean",
                    "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                },
                "edition": {
                    "type": "string",
                    "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).\n"
                },
                "filters": {
                    "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                },
                "libraries": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                    },
                    "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                },
                "name": {
                    "type": "string",
                    "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                },
                "notifications": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                    }
                },
                "photon": {
                    "type": "boolean",
                    "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                },
                "serverless": {
                    "type": "boolean"
                },
                "storage": {
                    "type": "string",
                    "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n",
                    "willReplaceOnChanges": true
                },
                "target": {
                    "type": "string",
                    "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Pipeline resources.\n",
                "properties": {
                    "allowDuplicateNames": {
                        "type": "boolean"
                    },
                    "catalog": {
                        "type": "string",
                        "description": "The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).\n",
                        "willReplaceOnChanges": true
                    },
                    "channel": {
                        "type": "string",
                        "description": "optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.\n"
                    },
                    "clusters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineCluster:PipelineCluster"
                        },
                        "description": "blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).\n"
                    },
                    "configuration": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.\n"
                    },
                    "continuous": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline continuously. The default value is `false`.\n"
                    },
                    "deployment": {
                        "$ref": "#/types/databricks:index/PipelineDeployment:PipelineDeployment"
                    },
                    "development": {
                        "type": "boolean",
                        "description": "A flag indicating whether to run the pipeline in development mode. The default value is `true`.\n"
                    },
                    "edition": {
                        "type": "string",
                        "description": "optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).\n"
                    },
                    "filters": {
                        "$ref": "#/types/databricks:index/PipelineFilters:PipelineFilters"
                    },
                    "libraries": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineLibrary:PipelineLibrary"
                        },
                        "description": "blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` \u0026 `file` library types that should have the `path` attribute. *Right now only the `notebook` \u0026 `file` types are supported.*\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.\n"
                    },
                    "notifications": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/PipelineNotification:PipelineNotification"
                        }
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "A flag indicating whether to use Photon engine. The default value is `false`.\n"
                    },
                    "serverless": {
                        "type": "boolean"
                    },
                    "storage": {
                        "type": "string",
                        "description": "A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).\n",
                        "willReplaceOnChanges": true
                    },
                    "target": {
                        "type": "string",
                        "description": "The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "URL of the DLT pipeline on the given workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/recipient:Recipient": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nIn Delta Sharing, a recipient is an entity that receives shares from a provider. In Unity Catalog, a share is a securable object that represents an organization and associates it with a credential or secure sharing identifier that allows that organization to access one or more shares.\n\nAs a data provider (sharer), you can define multiple recipients for any given Unity Catalog metastore, but if you want to share data from multiple metastores with a particular user or group of users, you must define the recipient separately for each metastore. A recipient can have access to multiple shares.\n\nA `databricks.Recipient` is contained within databricks.Metastore and can have permissions to `SELECT` from a list of shares.\n\n## Example Usage\n\n### Databricks Sharing with non databricks recipient\n\nSetting `authentication_type` type to `TOKEN` creates a temporary url to download a credentials file. This is used to\nauthenticate to the sharing server to access data. This is for when the recipient is not using Databricks.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as random from \"@pulumi/random\";\n\nconst db2opensharecode = new random.index.Password(\"db2opensharecode\", {\n    length: 16,\n    special: true,\n});\nconst current = databricks.getCurrentUser({});\nconst db2open = new databricks.Recipient(\"db2open\", {\n    name: current.then(current =\u003e `${current.alphanumeric}-recipient`),\n    comment: \"made by terraform\",\n    authenticationType: \"TOKEN\",\n    sharingCode: db2opensharecode.result,\n    ipAccessList: {\n        allowedIpAddresses: [],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumi_random as random\n\ndb2opensharecode = random.index.Password(\"db2opensharecode\",\n    length=16,\n    special=True)\ncurrent = databricks.get_current_user()\ndb2open = databricks.Recipient(\"db2open\",\n    name=f\"{current.alphanumeric}-recipient\",\n    comment=\"made by terraform\",\n    authentication_type=\"TOKEN\",\n    sharing_code=db2opensharecode[\"result\"],\n    ip_access_list=databricks.RecipientIpAccessListArgs(\n        allowed_ip_addresses=[],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Random = Pulumi.Random;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var db2opensharecode = new Random.Index.Password(\"db2opensharecode\", new()\n    {\n        Length = 16,\n        Special = true,\n    });\n\n    var current = Databricks.GetCurrentUser.Invoke();\n\n    var db2open = new Databricks.Recipient(\"db2open\", new()\n    {\n        Name = $\"{current.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)}-recipient\",\n        Comment = \"made by terraform\",\n        AuthenticationType = \"TOKEN\",\n        SharingCode = db2opensharecode.Result,\n        IpAccessList = new Databricks.Inputs.RecipientIpAccessListArgs\n        {\n            AllowedIpAddresses = new() { },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-random/sdk/v4/go/random\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tdb2opensharecode, err := random.NewPassword(ctx, \"db2opensharecode\", \u0026random.PasswordArgs{\n\t\t\tLength:  16,\n\t\t\tSpecial: true,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcurrent, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewRecipient(ctx, \"db2open\", \u0026databricks.RecipientArgs{\n\t\t\tName:               pulumi.String(fmt.Sprintf(\"%v-recipient\", current.Alphanumeric)),\n\t\t\tComment:            pulumi.String(\"made by terraform\"),\n\t\t\tAuthenticationType: pulumi.String(\"TOKEN\"),\n\t\t\tSharingCode:        db2opensharecode.Result,\n\t\t\tIpAccessList: \u0026databricks.RecipientIpAccessListArgs{\n\t\t\t\tAllowedIpAddresses: pulumi.StringArray{},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.random.password;\nimport com.pulumi.random.PasswordArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.Recipient;\nimport com.pulumi.databricks.RecipientArgs;\nimport com.pulumi.databricks.inputs.RecipientIpAccessListArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var db2opensharecode = new Password(\"db2opensharecode\", PasswordArgs.builder()        \n            .length(16)\n            .special(true)\n            .build());\n\n        final var current = DatabricksFunctions.getCurrentUser();\n\n        var db2open = new Recipient(\"db2open\", RecipientArgs.builder()        \n            .name(String.format(\"%s-recipient\", current.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.alphanumeric())))\n            .comment(\"made by terraform\")\n            .authenticationType(\"TOKEN\")\n            .sharingCode(db2opensharecode.result())\n            .ipAccessList(RecipientIpAccessListArgs.builder()\n                .allowedIpAddresses()\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  db2opensharecode:\n    type: random:password\n    properties:\n      length: 16\n      special: true\n  db2open:\n    type: databricks:Recipient\n    properties:\n      name: ${current.alphanumeric}-recipient\n      comment: made by terraform\n      authenticationType: TOKEN\n      sharingCode: ${db2opensharecode.result}\n      ipAccessList:\n        allowedIpAddresses: []\nvariables:\n  current:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n",
            "properties": {
                "activated": {
                    "type": "boolean"
                },
                "activationUrl": {
                    "type": "string",
                    "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                },
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n"
                },
                "cloud": {
                    "type": "string",
                    "description": "Cloud vendor of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "createdAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "Username of recipient creator.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when `authentication_type` is `DATABRICKS`.\n"
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "Recipient IP access list.\n"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of recipient's Unity Catalog metastore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the recipient owner.\n"
                },
                "propertiesKvpairs": {
                    "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                    "description": "Recipient properties - object consisting of following fields:\n"
                },
                "region": {
                    "type": "string",
                    "description": "Cloud region of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                },
                "updatedAt": {
                    "type": "integer",
                    "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                },
                "updatedBy": {
                    "type": "string",
                    "description": "Username of recipient Token updater.\n"
                }
            },
            "required": [
                "activated",
                "activationUrl",
                "authenticationType",
                "cloud",
                "createdAt",
                "createdBy",
                "metastoreId",
                "name",
                "region",
                "tokens",
                "updatedAt",
                "updatedBy"
            ],
            "inputProperties": {
                "authenticationType": {
                    "type": "string",
                    "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Description about the recipient.\n"
                },
                "dataRecipientGlobalMetastoreId": {
                    "type": "string",
                    "description": "Required when `authentication_type` is `DATABRICKS`.\n",
                    "willReplaceOnChanges": true
                },
                "ipAccessList": {
                    "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                    "description": "Recipient IP access list.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of recipient. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the recipient owner.\n"
                },
                "propertiesKvpairs": {
                    "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                    "description": "Recipient properties - object consisting of following fields:\n"
                },
                "sharingCode": {
                    "type": "string",
                    "description": "The one-time sharing code provided by the data recipient.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                },
                "tokens": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                    },
                    "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                }
            },
            "requiredInputs": [
                "authenticationType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Recipient resources.\n",
                "properties": {
                    "activated": {
                        "type": "boolean"
                    },
                    "activationUrl": {
                        "type": "string",
                        "description": "Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.\n"
                    },
                    "authenticationType": {
                        "type": "string",
                        "description": "The delta sharing authentication type. Valid values are `TOKEN` and `DATABRICKS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "cloud": {
                        "type": "string",
                        "description": "Cloud vendor of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "comment": {
                        "type": "string",
                        "description": "Description about the recipient.\n"
                    },
                    "createdAt": {
                        "type": "integer",
                        "description": "Time at which this recipient was created, in epoch milliseconds.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "Username of recipient creator.\n"
                    },
                    "dataRecipientGlobalMetastoreId": {
                        "type": "string",
                        "description": "Required when `authentication_type` is `DATABRICKS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "ipAccessList": {
                        "$ref": "#/types/databricks:index/RecipientIpAccessList:RecipientIpAccessList",
                        "description": "Recipient IP access list.\n"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of recipient's Unity Catalog metastore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of recipient. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the recipient owner.\n"
                    },
                    "propertiesKvpairs": {
                        "$ref": "#/types/databricks:index/RecipientPropertiesKvpairs:RecipientPropertiesKvpairs",
                        "description": "Recipient properties - object consisting of following fields:\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Cloud region of the recipient's Unity Catalog Metstore. This field is only present when the authentication_type is `DATABRICKS`.\n"
                    },
                    "sharingCode": {
                        "type": "string",
                        "description": "The one-time sharing code provided by the data recipient.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    },
                    "tokens": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/RecipientToken:RecipientToken"
                        },
                        "description": "List of Recipient Tokens. This field is only present when the authentication_type is TOKEN. Each list element is an object with following attributes:\n"
                    },
                    "updatedAt": {
                        "type": "integer",
                        "description": "Time at which this recipient was updated, in epoch milliseconds.\n"
                    },
                    "updatedBy": {
                        "type": "string",
                        "description": "Username of recipient Token updater.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/registeredModel:RegisteredModel": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nThis resource allows you to create [Models in Unity Catalog](https://docs.databricks.com/en/mlflow/models-in-uc.html) in Databricks.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.RegisteredModel(\"this\", {\n    name: \"my_model\",\n    catalogName: \"main\",\n    schemaName: \"default\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.RegisteredModel(\"this\",\n    name=\"my_model\",\n    catalog_name=\"main\",\n    schema_name=\"default\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.RegisteredModel(\"this\", new()\n    {\n        Name = \"my_model\",\n        CatalogName = \"main\",\n        SchemaName = \"default\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewRegisteredModel(ctx, \"this\", \u0026databricks.RegisteredModelArgs{\n\t\t\tName:        pulumi.String(\"my_model\"),\n\t\t\tCatalogName: pulumi.String(\"main\"),\n\t\t\tSchemaName:  pulumi.String(\"default\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.RegisteredModel;\nimport com.pulumi.databricks.RegisteredModelArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RegisteredModel(\"this\", RegisteredModelArgs.builder()        \n            .name(\"my_model\")\n            .catalogName(\"main\")\n            .schemaName(\"default\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:RegisteredModel\n    properties:\n      name: my_model\n      catalogName: main\n      schemaName: default\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access Control\n\n* databricks.Grants can be used to grant principals `ALL_PRIVILEGES`, `APPLY_TAG`, and `EXECUTE` privileges.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.ModelServing to serve this model on a Databricks serving endpoint.\n* databricks.MlflowExperiment to manage [MLflow experiments](https://docs.databricks.com/data/data-sources/mlflow-experiment.html) in Databricks.\n* databricks.Table data to manage tables within Unity Catalog.\n* databricks.Schema data to manage schemas within Unity Catalog.\n* databricks.Catalog data to manage catalogs within Unity Catalog.\n\n## Import\n\nThe registered model resource can be imported using the full (3-level) name of the model.\n\nbash\n\n```sh\n$ pulumi import databricks:index/registeredModel:RegisteredModel this \u003ccatalog_name.schema_name.model_name\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n"
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the registered model owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n"
                }
            },
            "required": [
                "catalogName",
                "name",
                "owner",
                "schemaName",
                "storageLocation"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "The comment attached to the registered model.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the registered model owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering RegisteredModel resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "The name of the catalog where the schema and the registered model reside. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "The comment attached to the registered model.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the registered model.  *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the registered model owner.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "The name of the schema where the registered model resides. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "The storage location under which model version data files are stored. *Change of this parameter forces recreation of the resource.*\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/repo:Repo": {
            "description": "\n\n## Import\n\nThe resource Repo can be imported using the Repo ID (obtained via UI or using API)\n\nbash\n\n```sh\n$ pulumi import databricks:index/repo:Repo this repo_id\n```\n\n",
            "properties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n"
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n"
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout"
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "branch",
                "commitHash",
                "gitProvider",
                "path",
                "url",
                "workspacePath"
            ],
            "inputProperties": {
                "branch": {
                    "type": "string",
                    "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                },
                "commitHash": {
                    "type": "string",
                    "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                },
                "gitProvider": {
                    "type": "string",
                    "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n",
                    "willReplaceOnChanges": true
                },
                "path": {
                    "type": "string",
                    "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                },
                "sparseCheckout": {
                    "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                    "willReplaceOnChanges": true
                },
                "tag": {
                    "type": "string",
                    "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "url"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Repo resources.\n",
                "properties": {
                    "branch": {
                        "type": "string",
                        "description": "name of the branch for initial checkout. If not specified, the default branch of the repository will be used.  Conflicts with `tag`.  If `branch` is removed, and `tag` isn't specified, then the repository will stay at the previously checked out state.\n"
                    },
                    "commitHash": {
                        "type": "string",
                        "description": "Hash of the HEAD commit at time of the last executed operation. It won't change if you manually perform pull operation via UI or API\n"
                    },
                    "gitProvider": {
                        "type": "string",
                        "description": "case insensitive name of the Git provider.  Following values are supported right now (could be a subject for a change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`, `awsCodeCommit`.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "path to put the checked out Repo. If not specified, then repo will be created in the user's repo directory (`/Repos/\u003cusername\u003e/...`).  If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "sparseCheckout": {
                        "$ref": "#/types/databricks:index/RepoSparseCheckout:RepoSparseCheckout",
                        "willReplaceOnChanges": true
                    },
                    "tag": {
                        "type": "string",
                        "description": "name of the tag for initial checkout.  Conflicts with `branch`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "The URL of the Git Repository to clone from. If the value changes, repo is re-created.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/restrictWorkspaceAdminsSetting:RestrictWorkspaceAdminsSetting": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nThe `databricks.RestrictWorkspaceAdminsSetting` resource lets you control the capabilities of workspace admins.\n\nWith the status set to `ALLOW_ALL`, workspace admins can:\n\n1. Create service principal personal access tokens on behalf of any service principal in their workspace.\n2. Change a job owner to any user in the workspace.\n3. Change the job run_as setting to any user in their workspace or a service principal on which they have the Service Principal User role.\n\nWith the status set to `RESTRICT_TOKENS_AND_JOB_RUN_AS`, workspace admins can:\n\n1. Only create personal access tokens on behalf of service principals on which they have the Service Principal User role.\n2. Only change a job owner to themselves.\n3. Only change the job run_as setting to themselves a service principal on which they have the Service Principal User role.\n\n\u003e **Note** Only account admins can update the setting. And the account admin must be part of the workspace to change the setting status.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.RestrictWorkspaceAdminsSetting(\"this\", {restrictWorkspaceAdmins: {\n    status: \"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.RestrictWorkspaceAdminsSetting(\"this\", restrict_workspace_admins=databricks.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs(\n    status=\"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.RestrictWorkspaceAdminsSetting(\"this\", new()\n    {\n        RestrictWorkspaceAdmins = new Databricks.Inputs.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs\n        {\n            Status = \"RESTRICT_TOKENS_AND_JOB_RUN_AS\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewRestrictWorkspaceAdminsSetting(ctx, \"this\", \u0026databricks.RestrictWorkspaceAdminsSettingArgs{\n\t\t\tRestrictWorkspaceAdmins: \u0026databricks.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs{\n\t\t\t\tStatus: pulumi.String(\"RESTRICT_TOKENS_AND_JOB_RUN_AS\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.RestrictWorkspaceAdminsSetting;\nimport com.pulumi.databricks.RestrictWorkspaceAdminsSettingArgs;\nimport com.pulumi.databricks.inputs.RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new RestrictWorkspaceAdminsSetting(\"this\", RestrictWorkspaceAdminsSettingArgs.builder()        \n            .restrictWorkspaceAdmins(RestrictWorkspaceAdminsSettingRestrictWorkspaceAdminsArgs.builder()\n                .status(\"RESTRICT_TOKENS_AND_JOB_RUN_AS\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:RestrictWorkspaceAdminsSetting\n    properties:\n      restrictWorkspaceAdmins:\n        status: RESTRICT_TOKENS_AND_JOB_RUN_AS\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by predefined name `global`:\n\nbash\n\n```sh\n$ pulumi import databricks:index/restrictWorkspaceAdminsSetting:RestrictWorkspaceAdminsSetting this global\n```\n\n",
            "properties": {
                "etag": {
                    "type": "string"
                },
                "restrictWorkspaceAdmins": {
                    "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "required": [
                "etag",
                "restrictWorkspaceAdmins",
                "settingName"
            ],
            "inputProperties": {
                "etag": {
                    "type": "string"
                },
                "restrictWorkspaceAdmins": {
                    "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                    "description": "The configuration details.\n"
                },
                "settingName": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "restrictWorkspaceAdmins"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering RestrictWorkspaceAdminsSetting resources.\n",
                "properties": {
                    "etag": {
                        "type": "string"
                    },
                    "restrictWorkspaceAdmins": {
                        "$ref": "#/types/databricks:index/RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins:RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins",
                        "description": "The configuration details.\n"
                    },
                    "settingName": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/schema:Schema": {
            "description": "\u003e **Note** This resource could be only used with workspace-level provider!\n\nWithin a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, Databases (also called Schemas), and Tables / Views.\n\nA `databricks.Schema` is contained within databricks.Catalog and can contain tables \u0026 views.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.id,\n    name: \"things\",\n    comment: \"this database is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.id,\n    name=\"things\",\n    comment=\"this database is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Id,\n        Name = \"things\",\n        Comment = \"this database is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.ID(),\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this database is managed by terraform\"),\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"kind\": pulumi.Any(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.id())\n            .name(\"things\")\n            .comment(\"this database is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.id}\n      name: things\n      comment: this database is managed by terraform\n      properties:\n        kind: various\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getTables data to list tables within Unity Catalog.\n* databricks.getSchemas data to list schemas within Unity Catalog.\n* databricks.getCatalogs data to list catalogs within Unity Catalog.\n\n## Import\n\nThis resource can be imported by its full name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/schema:Schema this \u003ccatalog_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "catalogName",
                "enablePredictiveOptimization",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text.\n"
                },
                "enablePredictiveOptimization": {
                    "type": "string",
                    "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete schema regardless of its contents.\n"
                },
                "metastoreId": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the schema owner.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Extensible Schema properties.\n"
                },
                "storageRoot": {
                    "type": "string",
                    "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Schema resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text.\n"
                    },
                    "enablePredictiveOptimization": {
                        "type": "string",
                        "description": "Whether predictive optimization should be enabled for this object and objects under it. Can be `ENABLE`, `DISABLE` or `INHERIT`\n"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete schema regardless of its contents.\n"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Schema relative to parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the schema owner.\n"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Extensible Schema properties.\n"
                    },
                    "storageRoot": {
                        "type": "string",
                        "description": "Managed location of the schema. Location in cloud storage where data for managed tables will be stored. If not specified, the location will default to the catalog root location. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secret:Secret": {
            "description": "With this resource you can insert a secret under the provided scope with the given name. If a secret already exists with the same name, this command overwrites the existing secret’s value. The server encrypts the secret using the secret scope’s encryption settings before storing it. You must have WRITE or MANAGE permission on the secret scope. The secret key must consist of alphanumeric characters, dashes, underscores, and periods, and cannot exceed 128 characters. The maximum allowed secret value size is 128 KB. The maximum number of secrets in a given scope is 1000. You can read a secret value only from within a command on a cluster (for example, through a notebook); there is no API to read a secret value outside of a cluster. The permission applied is based on who is invoking the command and you must have at least READ permission. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst app = new databricks.SecretScope(\"app\", {name: \"application-secret-scope\"});\nconst publishingApi = new databricks.Secret(\"publishing_api\", {\n    key: \"publishing_api\",\n    stringValue: example.value,\n    scope: app.id,\n});\nconst _this = new databricks.Cluster(\"this\", {sparkConf: {\n    \"fs.azure.account.oauth2.client.secret\": publishingApi.configReference,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\napp = databricks.SecretScope(\"app\", name=\"application-secret-scope\")\npublishing_api = databricks.Secret(\"publishing_api\",\n    key=\"publishing_api\",\n    string_value=example[\"value\"],\n    scope=app.id)\nthis = databricks.Cluster(\"this\", spark_conf={\n    \"fs.azure.account.oauth2.client.secret\": publishing_api.config_reference,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var app = new Databricks.SecretScope(\"app\", new()\n    {\n        Name = \"application-secret-scope\",\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishing_api\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = example.Value,\n        Scope = app.Id,\n    });\n\n    var @this = new Databricks.Cluster(\"this\", new()\n    {\n        SparkConf = \n        {\n            { \"fs.azure.account.oauth2.client.secret\", publishingApi.ConfigReference },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"application-secret-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tpublishingApi, err := databricks.NewSecret(ctx, \"publishing_api\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(example.Value),\n\t\t\tScope:       app.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"this\", \u0026databricks.ClusterArgs{\n\t\t\tSparkConf: pulumi.Map{\n\t\t\t\t\"fs.azure.account.oauth2.client.secret\": publishingApi.ConfigReference,\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var app = new SecretScope(\"app\", SecretScopeArgs.builder()        \n            .name(\"application-secret-scope\")\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(example.value())\n            .scope(app.id())\n            .build());\n\n        var this_ = new Cluster(\"this\", ClusterArgs.builder()        \n            .sparkConf(Map.of(\"fs.azure.account.oauth2.client.secret\", publishingApi.configReference()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  app:\n    type: databricks:SecretScope\n    properties:\n      name: application-secret-scope\n  publishingApi:\n    type: databricks:Secret\n    name: publishing_api\n    properties:\n      key: publishing_api\n      stringValue: ${example.value}\n      scope: ${app.id}\n  this:\n    type: databricks:Cluster\n    properties:\n      sparkConf:\n        fs.azure.account.oauth2.client.secret: ${publishingApi.configReference}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe resource secret can be imported using `scopeName|||secretKey` combination. **This may change in future versions.**\n\nbash\n\n```sh\n$ pulumi import databricks:index/secret:Secret app `scopeName|||secretKey`\n```\n\n",
            "properties": {
                "configReference": {
                    "type": "string",
                    "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                },
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "(Integer) time secret was updated\n"
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true
                }
            },
            "required": [
                "configReference",
                "key",
                "lastUpdatedTimestamp",
                "scope",
                "stringValue"
            ],
            "inputProperties": {
                "key": {
                    "type": "string",
                    "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                },
                "stringValue": {
                    "type": "string",
                    "description": "(String) super secret sensitive value.\n",
                    "secret": true,
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "key",
                "scope",
                "stringValue"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Secret resources.\n",
                "properties": {
                    "configReference": {
                        "type": "string",
                        "description": "(String) value to use as a secret reference in [Spark configuration and environment variables](https://docs.databricks.com/security/secrets/secrets.html#use-a-secret-in-a-spark-configuration-property-or-environment-variable): `{{secrets/scope/key}}`.\n"
                    },
                    "key": {
                        "type": "string",
                        "description": "(String) key within secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "(Integer) time secret was updated\n"
                    },
                    "scope": {
                        "type": "string",
                        "description": "(String) name of databricks secret scope. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "stringValue": {
                        "type": "string",
                        "description": "(String) super secret sensitive value.\n",
                        "secret": true,
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretAcl:SecretAcl": {
            "description": "Create or overwrite the ACL associated with the given principal (user or group) on the specified databricks_secret_scope. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\nThis way, data scientists can read the Publishing API key that is synchronized from, for example, Azure Key Vault.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst ds = new databricks.Group(\"ds\", {displayName: \"data-scientists\"});\nconst app = new databricks.SecretScope(\"app\", {name: \"app-secret-scope\"});\nconst mySecretAcl = new databricks.SecretAcl(\"my_secret_acl\", {\n    principal: ds.displayName,\n    permission: \"READ\",\n    scope: app.name,\n});\nconst publishingApi = new databricks.Secret(\"publishing_api\", {\n    key: \"publishing_api\",\n    stringValue: example.value,\n    scope: app.name,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nds = databricks.Group(\"ds\", display_name=\"data-scientists\")\napp = databricks.SecretScope(\"app\", name=\"app-secret-scope\")\nmy_secret_acl = databricks.SecretAcl(\"my_secret_acl\",\n    principal=ds.display_name,\n    permission=\"READ\",\n    scope=app.name)\npublishing_api = databricks.Secret(\"publishing_api\",\n    key=\"publishing_api\",\n    string_value=example[\"value\"],\n    scope=app.name)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var ds = new Databricks.Group(\"ds\", new()\n    {\n        DisplayName = \"data-scientists\",\n    });\n\n    var app = new Databricks.SecretScope(\"app\", new()\n    {\n        Name = \"app-secret-scope\",\n    });\n\n    var mySecretAcl = new Databricks.SecretAcl(\"my_secret_acl\", new()\n    {\n        Principal = ds.DisplayName,\n        Permission = \"READ\",\n        Scope = app.Name,\n    });\n\n    var publishingApi = new Databricks.Secret(\"publishing_api\", new()\n    {\n        Key = \"publishing_api\",\n        StringValue = example.Value,\n        Scope = app.Name,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tds, err := databricks.NewGroup(ctx, \"ds\", \u0026databricks.GroupArgs{\n\t\t\tDisplayName: pulumi.String(\"data-scientists\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tapp, err := databricks.NewSecretScope(ctx, \"app\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"app-secret-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecretAcl(ctx, \"my_secret_acl\", \u0026databricks.SecretAclArgs{\n\t\t\tPrincipal:  ds.DisplayName,\n\t\t\tPermission: pulumi.String(\"READ\"),\n\t\t\tScope:      app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSecret(ctx, \"publishing_api\", \u0026databricks.SecretArgs{\n\t\t\tKey:         pulumi.String(\"publishing_api\"),\n\t\t\tStringValue: pulumi.Any(example.Value),\n\t\t\tScope:       app.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Group;\nimport com.pulumi.databricks.GroupArgs;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport com.pulumi.databricks.SecretAcl;\nimport com.pulumi.databricks.SecretAclArgs;\nimport com.pulumi.databricks.Secret;\nimport com.pulumi.databricks.SecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var ds = new Group(\"ds\", GroupArgs.builder()        \n            .displayName(\"data-scientists\")\n            .build());\n\n        var app = new SecretScope(\"app\", SecretScopeArgs.builder()        \n            .name(\"app-secret-scope\")\n            .build());\n\n        var mySecretAcl = new SecretAcl(\"mySecretAcl\", SecretAclArgs.builder()        \n            .principal(ds.displayName())\n            .permission(\"READ\")\n            .scope(app.name())\n            .build());\n\n        var publishingApi = new Secret(\"publishingApi\", SecretArgs.builder()        \n            .key(\"publishing_api\")\n            .stringValue(example.value())\n            .scope(app.name())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  ds:\n    type: databricks:Group\n    properties:\n      displayName: data-scientists\n  app:\n    type: databricks:SecretScope\n    properties:\n      name: app-secret-scope\n  mySecretAcl:\n    type: databricks:SecretAcl\n    name: my_secret_acl\n    properties:\n      principal: ${ds.displayName}\n      permission: READ\n      scope: ${app.name}\n  publishingApi:\n    type: databricks:Secret\n    name: publishing_api\n    properties:\n      key: publishing_api\n      stringValue: ${example.value}\n      scope: ${app.name}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretScope to create [secret scopes](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe resource secret acl can be imported using `scopeName|||principalName` combination.\n\nbash\n\n```sh\n$ pulumi import databricks:index/secretAcl:SecretAcl object `scopeName|||principalName`\n```\n\n",
            "properties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n"
                },
                "principal": {
                    "type": "string",
                    "description": "principal's identifier. It can be:\n"
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n"
                }
            },
            "required": [
                "permission",
                "principal",
                "scope"
            ],
            "inputProperties": {
                "permission": {
                    "type": "string",
                    "description": "`READ`, `WRITE` or `MANAGE`.\n",
                    "willReplaceOnChanges": true
                },
                "principal": {
                    "type": "string",
                    "description": "principal's identifier. It can be:\n",
                    "willReplaceOnChanges": true
                },
                "scope": {
                    "type": "string",
                    "description": "name of the scope\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "permission",
                "principal",
                "scope"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretAcl resources.\n",
                "properties": {
                    "permission": {
                        "type": "string",
                        "description": "`READ`, `WRITE` or `MANAGE`.\n",
                        "willReplaceOnChanges": true
                    },
                    "principal": {
                        "type": "string",
                        "description": "principal's identifier. It can be:\n",
                        "willReplaceOnChanges": true
                    },
                    "scope": {
                        "type": "string",
                        "description": "name of the scope\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/secretScope:SecretScope": {
            "description": "Sometimes accessing data requires that you authenticate to external data sources through JDBC. Instead of directly entering your credentials into a notebook, use Databricks secrets to store your credentials and reference them in notebooks and jobs. Please consult [Secrets User Guide](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) for more details.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SecretScope(\"this\", {name: \"terraform-demo-scope\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SecretScope(\"this\", name=\"terraform-demo-scope\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SecretScope(\"this\", new()\n    {\n        Name = \"terraform-demo-scope\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSecretScope(ctx, \"this\", \u0026databricks.SecretScopeArgs{\n\t\t\tName: pulumi.String(\"terraform-demo-scope\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SecretScope;\nimport com.pulumi.databricks.SecretScopeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SecretScope(\"this\", SecretScopeArgs.builder()        \n            .name(\"terraform-demo-scope\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SecretScope\n    properties:\n      name: terraform-demo-scope\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n* databricks.Secret to manage [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n* databricks.SecretAcl to manage access to [secrets](https://docs.databricks.com/security/secrets/index.html#secrets-user-guide) in Databricks workspace.\n\n## Import\n\nThe secret resource scope can be imported using the scope name. `initial_manage_principal` state won't be imported, because the underlying API doesn't include it in the response.\n\nbash\n\n```sh\n$ pulumi import databricks:index/secretScope:SecretScope object \u003cscopeName\u003e\n```\n\n",
            "properties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n"
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata"
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n"
                }
            },
            "required": [
                "backendType",
                "name"
            ],
            "inputProperties": {
                "backendType": {
                    "type": "string",
                    "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                },
                "initialManagePrincipal": {
                    "type": "string",
                    "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                    "willReplaceOnChanges": true
                },
                "keyvaultMetadata": {
                    "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SecretScope resources.\n",
                "properties": {
                    "backendType": {
                        "type": "string",
                        "description": "Either `DATABRICKS` or `AZURE_KEYVAULT`\n"
                    },
                    "initialManagePrincipal": {
                        "type": "string",
                        "description": "The principal with the only possible value `users` that is initially granted `MANAGE` permission to the created scope.  If it's omitted, then the databricks.SecretAcl with `MANAGE` permission applied to the scope is assigned to the API request issuer's user identity (see [documentation](https://docs.databricks.com/dev-tools/api/latest/secrets.html#create-secret-scope)). This part of the state cannot be imported.\n",
                        "willReplaceOnChanges": true
                    },
                    "keyvaultMetadata": {
                        "$ref": "#/types/databricks:index/SecretScopeKeyvaultMetadata:SecretScopeKeyvaultMetadata",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Scope name requested by the user. Must be unique within a workspace. Must consist of alphanumeric characters, dashes, underscores, and periods, and may not exceed 128 characters.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipal:ServicePrincipal": {
            "description": "Directly manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) that could be added to databricks.Group in Databricks account or workspace.\n\nThere are different types of service principals:\n\n* Databricks-managed - exists only inside the Databricks platform (all clouds) and couldn't be used for accessing non-Databricks services.\n* Azure-managed - existing Azure service principal (enterprise application) is registered inside Databricks.  It could be used to work with other Azure services.\n\n\u003e **Note** To assign account level service principals to workspace use databricks_mws_permission_assignment.\n\n\u003e **Note** Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level service principals. Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level service principals.\n\nTo create service principals in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using the supported authentication method for account operations.\n\nThe default behavior when deleting a `databricks.ServicePrincipal` resource depends on whether the provider is configured at the workspace-level or account-level. When the provider is configured at the workspace-level, the service principal will be deleted from the workspace. When the provider is configured at the account-level, the service principal will be deactivated but not deleted. When the provider is configured at the account level, to delete the service principal from the account when the resource is deleted, set `disable_as_user_deletion = false`. Conversely, when the provider is configured at the account-level, to deactivate the service principal when the resource is deleted, set `disable_as_user_deletion = true`.\n\n## Example Usage\n\nCreating regular Databricks-managed service principal:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Admin SP\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Admin SP\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Admin SP\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Admin SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Admin SP\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Admin SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating service principal with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Admin SP\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: sp.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Admin SP\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=sp.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Admin SP\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = sp.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsp, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Admin SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: sp.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Admin SP\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(sp.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Admin SP\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${sp.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Azure-managed service principal with cluster create permissions:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {\n    applicationId: \"00000000-0000-0000-0000-000000000000\",\n    displayName: \"Example service principal\",\n    allowClusterCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\",\n    application_id=\"00000000-0000-0000-0000-000000000000\",\n    display_name=\"Example service principal\",\n    allow_cluster_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n        DisplayName = \"Example service principal\",\n        AllowClusterCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId:      pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t\tDisplayName:        pulumi.String(\"Example service principal\"),\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .displayName(\"Example service principal\")\n            .allowClusterCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n      displayName: Example service principal\n      allowClusterCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Databricks-managed service principal in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {displayName: \"Automation-only SP\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", display_name=\"Automation-only SP\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        DisplayName = \"Automation-only SP\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"Automation-only SP\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .displayName(\"Automation-only SP\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: Automation-only SP\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating Azure-managed service principal in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sp = new databricks.ServicePrincipal(\"sp\", {applicationId: \"00000000-0000-0000-0000-000000000000\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsp = databricks.ServicePrincipal(\"sp\", application_id=\"00000000-0000-0000-0000-000000000000\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sp = new Databricks.ServicePrincipal(\"sp\", new()\n    {\n        ApplicationId = \"00000000-0000-0000-0000-000000000000\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipal(ctx, \"sp\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.String(\"00000000-0000-0000-0000-000000000000\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sp = new ServicePrincipal(\"sp\", ServicePrincipalArgs.builder()        \n            .applicationId(\"00000000-0000-0000-0000-000000000000\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sp:\n    type: databricks:ServicePrincipal\n    properties:\n      applicationId: 00000000-0000-0000-0000-000000000000\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n- End to end workspace management guide.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and more to manage secrets for the service principal (only for AWS deployments)\n\n## Import\n\nThe resource scim service principal can be imported using its id, for example `2345678901234567`. To get the service principal ID, call [Get service principals](https://docs.databricks.com/dev-tools/api/latest/scim/scim-sp.html#get-service-principals).\n\nbash\n\n```sh\n$ pulumi import databricks:index/servicePrincipal:ServicePrincipal me \u003cservice-principal-id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "required": [
                "aclPrincipalId",
                "applicationId",
                "displayName",
                "home",
                "repos"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "applicationId": {
                    "type": "string",
                    "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n",
                    "willReplaceOnChanges": true
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the service principal and can be the full name of the service principal.\n",
                    "willReplaceOnChanges": true
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the service principal in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                },
                "workspaceAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipal resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Either service principal is active or not. True by default, but can be set to false in case of service principal deactivation with preserving service principal assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within the boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the service principal to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "This is the Azure Application ID of the given Azure service principal and will be their form of access and identity. For Databricks-managed service principals this value is auto-generated.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature through databricks_sql_endpoint.\n"
                    },
                    "disableAsUserDeletion": {
                        "type": "boolean",
                        "description": "Deactivate the service principal when deleting the resource, rather than deleting the service principal entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the service principal and can be the full name of the service principal.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "forceDeleteHomeDir": {
                        "type": "boolean",
                        "description": "This flag determines whether the service principal's home directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "forceDeleteRepos": {
                        "type": "boolean",
                        "description": "This flag determines whether the service principal's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the service principal, e.g. `/Repos/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to Databricks Workspace.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalRole:ServicePrincipalRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to a databricks_service_principal.\n\n## Example Usage\n\nGranting a service principal access to an instance profile\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst _this = new databricks.ServicePrincipal(\"this\", {displayName: \"My Service Principal\"});\nconst myServicePrincipalInstanceProfile = new databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\", {\n    servicePrincipalId: _this.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nthis = databricks.ServicePrincipal(\"this\", display_name=\"My Service Principal\")\nmy_service_principal_instance_profile = databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\",\n    service_principal_id=this.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var @this = new Databricks.ServicePrincipal(\"this\", new()\n    {\n        DisplayName = \"My Service Principal\",\n    });\n\n    var myServicePrincipalInstanceProfile = new Databricks.ServicePrincipalRole(\"my_service_principal_instance_profile\", new()\n    {\n        ServicePrincipalId = @this.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewServicePrincipal(ctx, \"this\", \u0026databricks.ServicePrincipalArgs{\n\t\t\tDisplayName: pulumi.String(\"My Service Principal\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewServicePrincipalRole(ctx, \"my_service_principal_instance_profile\", \u0026databricks.ServicePrincipalRoleArgs{\n\t\t\tServicePrincipalId: this.ID(),\n\t\t\tRole:               instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.ServicePrincipal;\nimport com.pulumi.databricks.ServicePrincipalArgs;\nimport com.pulumi.databricks.ServicePrincipalRole;\nimport com.pulumi.databricks.ServicePrincipalRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var this_ = new ServicePrincipal(\"this\", ServicePrincipalArgs.builder()        \n            .displayName(\"My Service Principal\")\n            .build());\n\n        var myServicePrincipalInstanceProfile = new ServicePrincipalRole(\"myServicePrincipalInstanceProfile\", ServicePrincipalRoleArgs.builder()        \n            .servicePrincipalId(this_.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  this:\n    type: databricks:ServicePrincipal\n    properties:\n      displayName: My Service Principal\n  myServicePrincipalInstanceProfile:\n    type: databricks:ServicePrincipalRole\n    name: my_service_principal_instance_profile\n    properties:\n      servicePrincipalId: ${this.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.UserRole to attach role or databricks.InstanceProfile (AWS) to databricks_user.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n"
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n"
                }
            },
            "required": [
                "role",
                "servicePrincipalId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "This is the id of the role or instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "This is the id of the service principal resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "This is the id of the role or instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "This is the id of the service principal resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/servicePrincipalSecret:ServicePrincipalSecret": {
            "description": "## Example Usage\n\nCreate service principal secret\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst terraformSp = new databricks.ServicePrincipalSecret(\"terraform_sp\", {servicePrincipalId: _this.id});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nterraform_sp = databricks.ServicePrincipalSecret(\"terraform_sp\", service_principal_id=this[\"id\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var terraformSp = new Databricks.ServicePrincipalSecret(\"terraform_sp\", new()\n    {\n        ServicePrincipalId = @this.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewServicePrincipalSecret(ctx, \"terraform_sp\", \u0026databricks.ServicePrincipalSecretArgs{\n\t\t\tServicePrincipalId: pulumi.Any(this.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.ServicePrincipalSecret;\nimport com.pulumi.databricks.ServicePrincipalSecretArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var terraformSp = new ServicePrincipalSecret(\"terraformSp\", ServicePrincipalSecretArgs.builder()        \n            .servicePrincipalId(this_.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  terraformSp:\n    type: databricks:ServicePrincipalSecret\n    name: terraform_sp\n    properties:\n      servicePrincipalId: ${this.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* databricks.ServicePrincipal to manage [Service Principals](https://docs.databricks.com/administration-guide/users-groups/service-principals.html) in Databricks\n",
            "properties": {
                "secret": {
                    "type": "string",
                    "description": "Generated secret for the service principal\n",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "ID of the databricks.ServicePrincipal (not application ID).\n"
                },
                "status": {
                    "type": "string"
                }
            },
            "required": [
                "secret",
                "servicePrincipalId",
                "status"
            ],
            "inputProperties": {
                "secret": {
                    "type": "string",
                    "description": "Generated secret for the service principal\n",
                    "secret": true
                },
                "servicePrincipalId": {
                    "type": "string",
                    "description": "ID of the databricks.ServicePrincipal (not application ID).\n",
                    "willReplaceOnChanges": true
                },
                "status": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "servicePrincipalId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering ServicePrincipalSecret resources.\n",
                "properties": {
                    "secret": {
                        "type": "string",
                        "description": "Generated secret for the service principal\n",
                        "secret": true
                    },
                    "servicePrincipalId": {
                        "type": "string",
                        "description": "ID of the databricks.ServicePrincipal (not application ID).\n",
                        "willReplaceOnChanges": true
                    },
                    "status": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/share:Share": {
            "properties": {
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n"
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the share owner.\n"
                }
            },
            "required": [
                "createdAt",
                "createdBy",
                "name"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "integer",
                    "description": "Time when the share was created.\n"
                },
                "createdBy": {
                    "type": "string",
                    "description": "The principal that created the share.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of share. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "objects": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                    }
                },
                "owner": {
                    "type": "string",
                    "description": "User name/group name/sp application_id of the share owner.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Share resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of share. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/ShareObject:ShareObject"
                        }
                    },
                    "owner": {
                        "type": "string",
                        "description": "User name/group name/sp application_id of the share owner.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlAlert:SqlAlert": {
            "description": "This resource allows you to manage [Databricks SQL Alerts](https://docs.databricks.com/sql/user/queries/index.html).\n\n**Note:** To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Queries\"});\nconst _this = new databricks.SqlQuery(\"this\", {\n    dataSourceId: example.dataSourceId,\n    name: \"My Query Name\",\n    query: \"SELECT 1 AS p1, 2 as p2\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n});\nconst alert = new databricks.SqlAlert(\"alert\", {\n    queryId: _this.id,\n    name: \"My Alert\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    rearm: 1,\n    options: {\n        column: \"p1\",\n        op: \"==\",\n        value: \"2\",\n        muted: false,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Queries\")\nthis = databricks.SqlQuery(\"this\",\n    data_source_id=example[\"dataSourceId\"],\n    name=\"My Query Name\",\n    query=\"SELECT 1 AS p1, 2 as p2\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"))\nalert = databricks.SqlAlert(\"alert\",\n    query_id=this.id,\n    name=\"My Alert\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    rearm=1,\n    options=databricks.SqlAlertOptionsArgs(\n        column=\"p1\",\n        op=\"==\",\n        value=\"2\",\n        muted=False,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Queries\",\n    });\n\n    var @this = new Databricks.SqlQuery(\"this\", new()\n    {\n        DataSourceId = example.DataSourceId,\n        Name = \"My Query Name\",\n        Query = \"SELECT 1 AS p1, 2 as p2\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n    });\n\n    var alert = new Databricks.SqlAlert(\"alert\", new()\n    {\n        QueryId = @this.Id,\n        Name = \"My Alert\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        Rearm = 1,\n        Options = new Databricks.Inputs.SqlAlertOptionsArgs\n        {\n            Column = \"p1\",\n            Op = \"==\",\n            Value = \"2\",\n            Muted = false,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Queries\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.NewSqlQuery(ctx, \"this\", \u0026databricks.SqlQueryArgs{\n\t\t\tDataSourceId: pulumi.Any(example.DataSourceId),\n\t\t\tName:         pulumi.String(\"My Query Name\"),\n\t\t\tQuery:        pulumi.String(\"SELECT 1 AS p1, 2 as p2\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlAlert(ctx, \"alert\", \u0026databricks.SqlAlertArgs{\n\t\t\tQueryId: this.ID(),\n\t\t\tName:    pulumi.String(\"My Alert\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tRearm: pulumi.Int(1),\n\t\t\tOptions: \u0026databricks.SqlAlertOptionsArgs{\n\t\t\t\tColumn: pulumi.String(\"p1\"),\n\t\t\t\tOp:     pulumi.String(\"==\"),\n\t\t\t\tValue:  pulumi.String(\"2\"),\n\t\t\t\tMuted:  pulumi.Bool(false),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlQuery;\nimport com.pulumi.databricks.SqlQueryArgs;\nimport com.pulumi.databricks.SqlAlert;\nimport com.pulumi.databricks.SqlAlertArgs;\nimport com.pulumi.databricks.inputs.SqlAlertOptionsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()        \n            .path(\"/Shared/Queries\")\n            .build());\n\n        var this_ = new SqlQuery(\"this\", SqlQueryArgs.builder()        \n            .dataSourceId(example.dataSourceId())\n            .name(\"My Query Name\")\n            .query(\"SELECT 1 AS p1, 2 as p2\")\n            .parent(sharedDir.objectId().applyValue(objectId -\u003e String.format(\"folders/%s\", objectId)))\n            .build());\n\n        var alert = new SqlAlert(\"alert\", SqlAlertArgs.builder()        \n            .queryId(this_.id())\n            .name(\"My Alert\")\n            .parent(sharedDir.objectId().applyValue(objectId -\u003e String.format(\"folders/%s\", objectId)))\n            .rearm(1)\n            .options(SqlAlertOptionsArgs.builder()\n                .column(\"p1\")\n                .op(\"==\")\n                .value(\"2\")\n                .muted(false)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Queries\n  this:\n    type: databricks:SqlQuery\n    properties:\n      dataSourceId: ${example.dataSourceId}\n      name: My Query Name\n      query: SELECT 1 AS p1, 2 as p2\n      parent: folders/${sharedDir.objectId}\n  alert:\n    type: databricks:SqlAlert\n    properties:\n      queryId: ${this.id}\n      name: My Alert\n      parent: folders/${sharedDir.objectId}\n      rearm: 1\n      options:\n        column: p1\n        op: ==\n        value: '2'\n        muted: false\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlQuery to manage Databricks SQL [Queries](https://docs.databricks.com/sql/user/queries/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "options": {
                    "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                    "description": "Alert configuration options.\n"
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n"
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "rearm": {
                    "type": "integer",
                    "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "name",
                "options",
                "queryId",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the alert.\n"
                },
                "options": {
                    "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                    "description": "Alert configuration options.\n"
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n",
                    "willReplaceOnChanges": true
                },
                "queryId": {
                    "type": "string",
                    "description": "ID of the query evaluated by the alert.\n"
                },
                "rearm": {
                    "type": "integer",
                    "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "options",
                "queryId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlAlert resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the alert.\n"
                    },
                    "options": {
                        "$ref": "#/types/databricks:index/SqlAlertOptions:SqlAlertOptions",
                        "description": "Alert configuration options.\n"
                    },
                    "parent": {
                        "type": "string",
                        "description": "The identifier of the workspace folder containing the alert. The default is ther user's home folder. The folder identifier is formatted as `folder/\u003cfolder_id\u003e`.\n",
                        "willReplaceOnChanges": true
                    },
                    "queryId": {
                        "type": "string",
                        "description": "ID of the query evaluated by the alert.\n"
                    },
                    "rearm": {
                        "type": "integer",
                        "description": "Number of seconds after being triggered before the alert rearms itself and can be triggered again. If not defined, alert will never be triggered again.\n"
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlDashboard:SqlDashboard": {
            "description": "This resource is used to manage [Databricks SQL Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html). To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA dashboard may have one or more widgets.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Dashboards\"});\nconst d1 = new databricks.SqlDashboard(\"d1\", {\n    name: \"My Dashboard Name\",\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    tags: [\n        \"some-tag\",\n        \"another-tag\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Dashboards\")\nd1 = databricks.SqlDashboard(\"d1\",\n    name=\"My Dashboard Name\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    tags=[\n        \"some-tag\",\n        \"another-tag\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Dashboards\",\n    });\n\n    var d1 = new Databricks.SqlDashboard(\"d1\", new()\n    {\n        Name = \"My Dashboard Name\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        Tags = new[]\n        {\n            \"some-tag\",\n            \"another-tag\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Dashboards\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlDashboard(ctx, \"d1\", \u0026databricks.SqlDashboardArgs{\n\t\t\tName: pulumi.String(\"My Dashboard Name\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"some-tag\"),\n\t\t\t\tpulumi.String(\"another-tag\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlDashboard;\nimport com.pulumi.databricks.SqlDashboardArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()        \n            .path(\"/Shared/Dashboards\")\n            .build());\n\n        var d1 = new SqlDashboard(\"d1\", SqlDashboardArgs.builder()        \n            .name(\"My Dashboard Name\")\n            .parent(sharedDir.objectId().applyValue(objectId -\u003e String.format(\"folders/%s\", objectId)))\n            .tags(            \n                \"some-tag\",\n                \"another-tag\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Dashboards\n  d1:\n    type: databricks:SqlDashboard\n    properties:\n      name: My Dashboard Name\n      parent: folders/${sharedDir.objectId}\n      tags:\n        - some-tag\n        - another-tag\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nExample permission to share dashboard with all users:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1 = new databricks.Permissions(\"d1\", {\n    sqlDashboardId: d1DatabricksSqlDashboard.id,\n    accessControls: [{\n        groupName: users.displayName,\n        permissionLevel: \"CAN_RUN\",\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1 = databricks.Permissions(\"d1\",\n    sql_dashboard_id=d1_databricks_sql_dashboard[\"id\"],\n    access_controls=[databricks.PermissionsAccessControlArgs(\n        group_name=users[\"displayName\"],\n        permission_level=\"CAN_RUN\",\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1 = new Databricks.Permissions(\"d1\", new()\n    {\n        SqlDashboardId = d1DatabricksSqlDashboard.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = users.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"d1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlDashboardId: pulumi.Any(d1DatabricksSqlDashboard.Id),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(users.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1 = new Permissions(\"d1\", PermissionsArgs.builder()        \n            .sqlDashboardId(d1DatabricksSqlDashboard.id())\n            .accessControls(PermissionsAccessControlArgs.builder()\n                .groupName(users.displayName())\n                .permissionLevel(\"CAN_RUN\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1:\n    type: databricks:Permissions\n    properties:\n      sqlDashboardId: ${d1DatabricksSqlDashboard.id}\n      accessControls:\n        - groupName: ${users.displayName}\n          permissionLevel: CAN_RUN\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_dashboard` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlDashboard:SqlDashboard this \u003cdashboard-id\u003e\n```\n\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "dashboardFiltersEnabled": {
                    "type": "boolean"
                },
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string"
                },
                "runAsRole": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "name",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "dashboardFiltersEnabled": {
                    "type": "boolean"
                },
                "name": {
                    "type": "string"
                },
                "parent": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "runAsRole": {
                    "type": "string"
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlDashboard resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "dashboardFiltersEnabled": {
                        "type": "boolean"
                    },
                    "name": {
                        "type": "string"
                    },
                    "parent": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "runAsRole": {
                        "type": "string"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlEndpoint:SqlEndpoint": {
            "description": "This resource is used to manage [Databricks SQL warehouses](https://docs.databricks.com/sql/admin/sql-endpoints.html). To create [SQL warehouses](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = databricks.getCurrentUser({});\nconst _this = new databricks.SqlEndpoint(\"this\", {\n    name: me.then(me =\u003e `Endpoint of ${me.alphanumeric}`),\n    clusterSize: \"Small\",\n    maxNumClusters: 1,\n    tags: {\n        customTags: [{\n            key: \"City\",\n            value: \"Amsterdam\",\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.get_current_user()\nthis = databricks.SqlEndpoint(\"this\",\n    name=f\"Endpoint of {me.alphanumeric}\",\n    cluster_size=\"Small\",\n    max_num_clusters=1,\n    tags=databricks.SqlEndpointTagsArgs(\n        custom_tags=[databricks.SqlEndpointTagsCustomTagArgs(\n            key=\"City\",\n            value=\"Amsterdam\",\n        )],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = Databricks.GetCurrentUser.Invoke();\n\n    var @this = new Databricks.SqlEndpoint(\"this\", new()\n    {\n        Name = $\"Endpoint of {me.Apply(getCurrentUserResult =\u003e getCurrentUserResult.Alphanumeric)}\",\n        ClusterSize = \"Small\",\n        MaxNumClusters = 1,\n        Tags = new Databricks.Inputs.SqlEndpointTagsArgs\n        {\n            CustomTags = new[]\n            {\n                new Databricks.Inputs.SqlEndpointTagsCustomTagArgs\n                {\n                    Key = \"City\",\n                    Value = \"Amsterdam\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tme, err := databricks.GetCurrentUser(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlEndpoint(ctx, \"this\", \u0026databricks.SqlEndpointArgs{\n\t\t\tName:           pulumi.String(fmt.Sprintf(\"Endpoint of %v\", me.Alphanumeric)),\n\t\t\tClusterSize:    pulumi.String(\"Small\"),\n\t\t\tMaxNumClusters: pulumi.Int(1),\n\t\t\tTags: \u0026databricks.SqlEndpointTagsArgs{\n\t\t\t\tCustomTags: databricks.SqlEndpointTagsCustomTagArray{\n\t\t\t\t\t\u0026databricks.SqlEndpointTagsCustomTagArgs{\n\t\t\t\t\t\tKey:   pulumi.String(\"City\"),\n\t\t\t\t\t\tValue: pulumi.String(\"Amsterdam\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.SqlEndpoint;\nimport com.pulumi.databricks.SqlEndpointArgs;\nimport com.pulumi.databricks.inputs.SqlEndpointTagsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var me = DatabricksFunctions.getCurrentUser();\n\n        var this_ = new SqlEndpoint(\"this\", SqlEndpointArgs.builder()        \n            .name(String.format(\"Endpoint of %s\", me.applyValue(getCurrentUserResult -\u003e getCurrentUserResult.alphanumeric())))\n            .clusterSize(\"Small\")\n            .maxNumClusters(1)\n            .tags(SqlEndpointTagsArgs.builder()\n                .customTags(SqlEndpointTagsCustomTagArgs.builder()\n                    .key(\"City\")\n                    .value(\"Amsterdam\")\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlEndpoint\n    properties:\n      name: Endpoint of ${me.alphanumeric}\n      clusterSize: Small\n      maxNumClusters: 1\n      tags:\n        customTags:\n          - key: City\n            value: Amsterdam\nvariables:\n  me:\n    fn::invoke:\n      Function: databricks:getCurrentUser\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Access control\n\n* databricks.Permissions can control which groups or individual users can *Can Use* or *Can Manage* SQL warehouses.\n* `databricks_sql_access` on databricks.Group or databricks_user.\n\n## Related resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_endpoint` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlEndpoint:SqlEndpoint this \u003cendpoint-id\u003e\n```\n\n",
            "properties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "creatorName": {
                    "type": "string",
                    "description": "The username of the user who created the endpoint.\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                },
                "healths": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlEndpointHealth:SqlEndpointHealth"
                    },
                    "description": "Health status of the endpoint.\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "jdbcUrl": {
                    "type": "string",
                    "description": "JDBC connection string.\n"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse. Must be unique.\n"
                },
                "numActiveSessions": {
                    "type": "integer",
                    "description": "The current number of clusters used by the endpoint.\n"
                },
                "numClusters": {
                    "type": "integer",
                    "description": "The current number of clusters used by the endpoint.\n"
                },
                "odbcParams": {
                    "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                    "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of the endpoint.\n"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                }
            },
            "required": [
                "clusterSize",
                "creatorName",
                "dataSourceId",
                "enableServerlessCompute",
                "healths",
                "jdbcUrl",
                "name",
                "numActiveSessions",
                "numClusters",
                "odbcParams",
                "state"
            ],
            "inputProperties": {
                "autoStopMins": {
                    "type": "integer",
                    "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                },
                "channel": {
                    "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                    "description": "block, consisting of following fields:\n"
                },
                "clusterSize": {
                    "type": "string",
                    "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                },
                "enablePhoton": {
                    "type": "boolean",
                    "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                },
                "instanceProfileArn": {
                    "type": "string"
                },
                "maxNumClusters": {
                    "type": "integer",
                    "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                },
                "minNumClusters": {
                    "type": "integer",
                    "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the SQL warehouse. Must be unique.\n"
                },
                "spotInstancePolicy": {
                    "type": "string",
                    "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                },
                "tags": {
                    "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                    "description": "Databricks tags all endpoint resources with these tags.\n"
                },
                "warehouseType": {
                    "type": "string",
                    "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                }
            },
            "requiredInputs": [
                "clusterSize"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlEndpoint resources.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops. This field is optional. The default is 120, set to 0 to disable the auto stop.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/SqlEndpointChannel:SqlEndpointChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the endpoint: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "creatorName": {
                        "type": "string",
                        "description": "The username of the user who created the endpoint.\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this endpoint. This is used to bind an Databricks SQL query to an endpoint.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether to enable [Photon](https://databricks.com/product/delta-engine). This field is optional and is enabled by default.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a serverless endpoint. See below for details about the default values. To avoid ambiguity, especially for organizations with many workspaces, Databricks recommends that you always set this field explicitly.\n\n* **For AWS**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between September 1, 2022 and April 30, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. If your account needs updated [terms of use](https://docs.databricks.com/sql/admin/serverless.html#accept-terms), workspace admins are prompted in the Databricks SQL UI. A workspace must meet the [requirements](https://docs.databricks.com/sql/admin/serverless.html#requirements) and might require an update to its instance profile role to [add a trust relationship](https://docs.databricks.com/sql/admin/serverless.html#aws-instance-profile-setup).\n\n* **For Azure**, If omitted, the default is `false` for most workspaces. However, if this workspace used the SQL Warehouses API to create a warehouse between November 1, 2022 and May 19, 2023, the default remains the previous behavior which is default to `true` if the workspace is enabled for serverless and fits the requirements for serverless SQL warehouses. A workspace must meet the [requirements](https://learn.microsoft.com/azure/databricks/sql/admin/serverless) and might require an update to its [Azure storage firewall](https://learn.microsoft.com/azure/databricks/sql/admin/serverless-firewall).\n"
                    },
                    "healths": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlEndpointHealth:SqlEndpointHealth"
                        },
                        "description": "Health status of the endpoint.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running. This field is required. If multi-cluster load balancing is not enabled, this is default to `1`.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running. The default is `1`.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the SQL warehouse. Must be unique.\n"
                    },
                    "numActiveSessions": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "numClusters": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/SqlEndpointOdbcParams:SqlEndpointOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`. This field is optional. Default is `COST_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of the endpoint.\n"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/SqlEndpointTags:SqlEndpointTags",
                        "description": "Databricks tags all endpoint resources with these tags.\n"
                    },
                    "warehouseType": {
                        "type": "string",
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/admin/sql-endpoints.html#switch-the-sql-warehouse-type-pro-classic-or-serverless) or [Azure](https://learn.microsoft.com/en-us/azure/databricks/sql/admin/create-sql-warehouse#--upgrade-a-pro-or-classic-sql-warehouse-to-a-serverless-sql-warehouse). Set to `PRO` or `CLASSIC`. If the field `enable_serverless_compute` has the value `true` either explicitly or through the default logic (see that field above for details), the default is `PRO`, which is required for serverless SQL warehouses. Otherwise, the default is `CLASSIC`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlGlobalConfig:SqlGlobalConfig": {
            "description": "This resource configures the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace. *Please note that changing parameters of this resource will restart all running databricks_sql_endpoint.*  To use this resource you need to be an administrator.\n\n## Example Usage\n\n### AWS example\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    instanceProfileArn: \"arn:....\",\n    dataAccessConfig: {\n        \"spark.sql.session.timeZone\": \"UTC\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    instance_profile_arn=\"arn:....\",\n    data_access_config={\n        \"spark.sql.session.timeZone\": \"UTC\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        InstanceProfileArn = \"arn:....\",\n        DataAccessConfig = \n        {\n            { \"spark.sql.session.timeZone\", \"UTC\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy:     pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tInstanceProfileArn: pulumi.String(\"arn:....\"),\n\t\t\tDataAccessConfig: pulumi.Map{\n\t\t\t\t\"spark.sql.session.timeZone\": pulumi.Any(\"UTC\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .instanceProfileArn(\"arn:....\")\n            .dataAccessConfig(Map.of(\"spark.sql.session.timeZone\", \"UTC\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      instanceProfileArn: arn:....\n      dataAccessConfig:\n        spark.sql.session.timeZone: UTC\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n### Azure example\n\nFor Azure you should use the `data_access_config` to provide the service principal configuration. You can use the Databricks SQL Admin Console UI to help you generate the right configuration values.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SqlGlobalConfig(\"this\", {\n    securityPolicy: \"DATA_ACCESS_CONTROL\",\n    dataAccessConfig: {\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": applicationId,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": `{{secrets/${secretScope}/${secretKey}}}`,\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": `https://login.microsoftonline.com/${tenantId}/oauth2/token`,\n    },\n    sqlConfigParams: {\n        ANSI_MODE: \"true\",\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SqlGlobalConfig(\"this\",\n    security_policy=\"DATA_ACCESS_CONTROL\",\n    data_access_config={\n        \"spark.hadoop.fs.azure.account.auth.type\": \"OAuth\",\n        \"spark.hadoop.fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.id\": application_id,\n        \"spark.hadoop.fs.azure.account.oauth2.client.secret\": f\"{{{{secrets/{secret_scope}/{secret_key}}}}}\",\n        \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": f\"https://login.microsoftonline.com/{tenant_id}/oauth2/token\",\n    },\n    sql_config_params={\n        \"ANSI_MODE\": \"true\",\n    })\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SqlGlobalConfig(\"this\", new()\n    {\n        SecurityPolicy = \"DATA_ACCESS_CONTROL\",\n        DataAccessConfig = \n        {\n            { \"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\" },\n            { \"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.id\", applicationId },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.secret\", $\"{{{{secrets/{secretScope}/{secretKey}}}}}\" },\n            { \"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", $\"https://login.microsoftonline.com/{tenantId}/oauth2/token\" },\n        },\n        SqlConfigParams = \n        {\n            { \"ANSI_MODE\", \"true\" },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlGlobalConfig(ctx, \"this\", \u0026databricks.SqlGlobalConfigArgs{\n\t\t\tSecurityPolicy: pulumi.String(\"DATA_ACCESS_CONTROL\"),\n\t\t\tDataAccessConfig: pulumi.Map{\n\t\t\t\t\"spark.hadoop.fs.azure.account.auth.type\":              pulumi.Any(\"OAuth\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth.provider.type\":    pulumi.Any(\"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.id\":       pulumi.Any(applicationId),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.secret\":   pulumi.Any(fmt.Sprintf(\"{{secrets/%v/%v}}\", secretScope, secretKey)),\n\t\t\t\t\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\": pulumi.Any(fmt.Sprintf(\"https://login.microsoftonline.com/%v/oauth2/token\", tenantId)),\n\t\t\t},\n\t\t\tSqlConfigParams: pulumi.Map{\n\t\t\t\t\"ANSI_MODE\": pulumi.Any(\"true\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlGlobalConfig;\nimport com.pulumi.databricks.SqlGlobalConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SqlGlobalConfig(\"this\", SqlGlobalConfigArgs.builder()        \n            .securityPolicy(\"DATA_ACCESS_CONTROL\")\n            .dataAccessConfig(Map.ofEntries(\n                Map.entry(\"spark.hadoop.fs.azure.account.auth.type\", \"OAuth\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth.provider.type\", \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\"),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.id\", applicationId),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.secret\", String.format(\"{{{{secrets/%s/%s}}}}\", secretScope,secretKey)),\n                Map.entry(\"spark.hadoop.fs.azure.account.oauth2.client.endpoint\", String.format(\"https://login.microsoftonline.com/%s/oauth2/token\", tenantId))\n            ))\n            .sqlConfigParams(Map.of(\"ANSI_MODE\", \"true\"))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SqlGlobalConfig\n    properties:\n      securityPolicy: DATA_ACCESS_CONTROL\n      dataAccessConfig:\n        spark.hadoop.fs.azure.account.auth.type: OAuth\n        spark.hadoop.fs.azure.account.oauth.provider.type: org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\n        spark.hadoop.fs.azure.account.oauth2.client.id: ${applicationId}\n        spark.hadoop.fs.azure.account.oauth2.client.secret: '{{secrets/${secretScope}/${secretKey}}}'\n        spark.hadoop.fs.azure.account.oauth2.client.endpoint: https://login.microsoftonline.com/${tenantId}/oauth2/token\n      sqlConfigParams:\n        ANSI_MODE: 'true'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_global_config` resource with command like the following (you need to use `global` as ID):\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlGlobalConfig:SqlGlobalConfig this global\n```\n\n",
            "properties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "required": [
                "enableServerlessCompute"
            ],
            "inputProperties": {
                "dataAccessConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                },
                "enableServerlessCompute": {
                    "type": "boolean",
                    "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                },
                "googleServiceAccount": {
                    "type": "string",
                    "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                },
                "instanceProfileArn": {
                    "type": "string",
                    "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                },
                "securityPolicy": {
                    "type": "string",
                    "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                },
                "sqlConfigParams": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlGlobalConfig resources.\n",
                "properties": {
                    "dataAccessConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you're specifying not permitted configuration.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "deprecationMessage": "This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future"
                    },
                    "googleServiceAccount": {
                        "type": "string",
                        "description": "used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string",
                        "description": "databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.\n"
                    },
                    "securityPolicy": {
                        "type": "string",
                        "description": "The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values\n"
                    },
                    "sqlConfigParams": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlPermissions:SqlPermissions": {
            "description": "## Example Usage\n\nThe following resource definition will enforce access control on a table by executing the following SQL queries on a special auto-terminating cluster it would create for this operation:\n\n* ```SHOW GRANT ON TABLE `default`.`foo` ```\n* ```REVOKE ALL PRIVILEGES ON TABLE `default`.`foo` FROM ... every group and user that has access to it ...```\n* ```GRANT MODIFY, SELECT ON TABLE `default`.`foo` TO `serge@example.com` ```\n* ```GRANT SELECT ON TABLE `default`.`foo` TO `special group` ```\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst fooTable = new databricks.SqlPermissions(\"foo_table\", {\n    table: \"foo\",\n    privilegeAssignments: [\n        {\n            principal: \"serge@example.com\",\n            privileges: [\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        },\n        {\n            principal: \"special group\",\n            privileges: [\"SELECT\"],\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfoo_table = databricks.SqlPermissions(\"foo_table\",\n    table=\"foo\",\n    privilege_assignments=[\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"serge@example.com\",\n            privileges=[\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        ),\n        databricks.SqlPermissionsPrivilegeAssignmentArgs(\n            principal=\"special group\",\n            privileges=[\"SELECT\"],\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var fooTable = new Databricks.SqlPermissions(\"foo_table\", new()\n    {\n        Table = \"foo\",\n        PrivilegeAssignments = new[]\n        {\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"serge@example.com\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                    \"MODIFY\",\n                },\n            },\n            new Databricks.Inputs.SqlPermissionsPrivilegeAssignmentArgs\n            {\n                Principal = \"special group\",\n                Privileges = new[]\n                {\n                    \"SELECT\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlPermissions(ctx, \"foo_table\", \u0026databricks.SqlPermissionsArgs{\n\t\t\tTable: pulumi.String(\"foo\"),\n\t\t\tPrivilegeAssignments: databricks.SqlPermissionsPrivilegeAssignmentArray{\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"serge@example.com\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlPermissionsPrivilegeAssignmentArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"special group\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlPermissions;\nimport com.pulumi.databricks.SqlPermissionsArgs;\nimport com.pulumi.databricks.inputs.SqlPermissionsPrivilegeAssignmentArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var fooTable = new SqlPermissions(\"fooTable\", SqlPermissionsArgs.builder()        \n            .table(\"foo\")\n            .privilegeAssignments(            \n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"serge@example.com\")\n                    .privileges(                    \n                        \"SELECT\",\n                        \"MODIFY\")\n                    .build(),\n                SqlPermissionsPrivilegeAssignmentArgs.builder()\n                    .principal(\"special group\")\n                    .privileges(\"SELECT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  fooTable:\n    type: databricks:SqlPermissions\n    name: foo_table\n    properties:\n      table: foo\n      privilegeAssignments:\n        - principal: serge@example.com\n          privileges:\n            - SELECT\n            - MODIFY\n        - principal: special group\n          privileges:\n            - SELECT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Grants to manage data access in Unity Catalog.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n\n## Import\n\nThe resource can be imported using a synthetic identifier. Examples of valid synthetic identifiers are:\n\n* `table/default.foo` - table `foo` in a `default` database. Database is always mandatory.\n\n* `view/bar.foo` - view `foo` in `bar` database.\n\n* `database/bar` - `bar` database.\n\n* `catalog/` - entire catalog. `/` suffix is mandatory.\n\n* `any file/` - direct access to any file. `/` suffix is mandatory.\n\n* `anonymous function/` - anonymous function. `/` suffix is mandatory.\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlPermissions:SqlPermissions foo /\u003cobject-type\u003e/\u003cobject-name\u003e\n```\n\n",
            "properties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n"
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading/writing any file. Defaults to `false`.\n"
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n"
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n"
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n"
                }
            },
            "required": [
                "clusterId"
            ],
            "inputProperties": {
                "anonymousFunction": {
                    "type": "boolean",
                    "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "anyFile": {
                    "type": "boolean",
                    "description": "If this access control for reading/writing any file. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "catalog": {
                    "type": "boolean",
                    "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "database": {
                    "type": "string",
                    "description": "Name of the database. Has default value of `default`.\n",
                    "willReplaceOnChanges": true
                },
                "privilegeAssignments": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                    }
                },
                "table": {
                    "type": "string",
                    "description": "Name of the table. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                },
                "view": {
                    "type": "string",
                    "description": "Name of the view. Can be combined with `database`.\n",
                    "willReplaceOnChanges": true
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlPermissions resources.\n",
                "properties": {
                    "anonymousFunction": {
                        "type": "boolean",
                        "description": "If this access control for using anonymous function. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "anyFile": {
                        "type": "boolean",
                        "description": "If this access control for reading/writing any file. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "catalog": {
                        "type": "boolean",
                        "description": "If this access control for the entire catalog. Defaults to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "database": {
                        "type": "string",
                        "description": "Name of the database. Has default value of `default`.\n",
                        "willReplaceOnChanges": true
                    },
                    "privilegeAssignments": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlPermissionsPrivilegeAssignment:SqlPermissionsPrivilegeAssignment"
                        }
                    },
                    "table": {
                        "type": "string",
                        "description": "Name of the table. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    },
                    "view": {
                        "type": "string",
                        "description": "Name of the view. Can be combined with `database`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlQuery:SqlQuery": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA query may have one or more visualizations.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sharedDir = new databricks.Directory(\"shared_dir\", {path: \"/Shared/Queries\"});\nconst q1 = new databricks.SqlQuery(\"q1\", {\n    dataSourceId: example.dataSourceId,\n    name: \"My Query Name\",\n    query: `                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n`,\n    parent: pulumi.interpolate`folders/${sharedDir.objectId}`,\n    runAsRole: \"viewer\",\n    parameters: [\n        {\n            name: \"p1\",\n            title: \"Title for p1\",\n            text: {\n                value: \"default\",\n            },\n        },\n        {\n            name: \"p2\",\n            title: \"Title for p2\",\n            \"enum\": {\n                options: [\n                    \"default\",\n                    \"foo\",\n                    \"bar\",\n                ],\n                value: \"default\",\n                multiple: {\n                    prefix: \"\\\"\",\n                    suffix: \"\\\"\",\n                    separator: \",\",\n                },\n            },\n        },\n        {\n            name: \"p3\",\n            title: \"Title for p3\",\n            date: {\n                value: \"2022-01-01\",\n            },\n        },\n    ],\n    tags: [\n        \"t1\",\n        \"t2\",\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nshared_dir = databricks.Directory(\"shared_dir\", path=\"/Shared/Queries\")\nq1 = databricks.SqlQuery(\"q1\",\n    data_source_id=example[\"dataSourceId\"],\n    name=\"My Query Name\",\n    query=\"\"\"                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n\"\"\",\n    parent=shared_dir.object_id.apply(lambda object_id: f\"folders/{object_id}\"),\n    run_as_role=\"viewer\",\n    parameters=[\n        databricks.SqlQueryParameterArgs(\n            name=\"p1\",\n            title=\"Title for p1\",\n            text=databricks.SqlQueryParameterTextArgs(\n                value=\"default\",\n            ),\n        ),\n        databricks.SqlQueryParameterArgs(\n            name=\"p2\",\n            title=\"Title for p2\",\n            enum=databricks.SqlQueryParameterEnumArgs(\n                options=[\n                    \"default\",\n                    \"foo\",\n                    \"bar\",\n                ],\n                value=\"default\",\n                multiple=databricks.SqlQueryParameterEnumMultipleArgs(\n                    prefix=\"\\\"\",\n                    suffix=\"\\\"\",\n                    separator=\",\",\n                ),\n            ),\n        ),\n        databricks.SqlQueryParameterArgs(\n            name=\"p3\",\n            title=\"Title for p3\",\n            date=databricks.SqlQueryParameterDateArgs(\n                value=\"2022-01-01\",\n            ),\n        ),\n    ],\n    tags=[\n        \"t1\",\n        \"t2\",\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sharedDir = new Databricks.Directory(\"shared_dir\", new()\n    {\n        Path = \"/Shared/Queries\",\n    });\n\n    var q1 = new Databricks.SqlQuery(\"q1\", new()\n    {\n        DataSourceId = example.DataSourceId,\n        Name = \"My Query Name\",\n        Query = @\"                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n\",\n        Parent = sharedDir.ObjectId.Apply(objectId =\u003e $\"folders/{objectId}\"),\n        RunAsRole = \"viewer\",\n        Parameters = new[]\n        {\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p1\",\n                Title = \"Title for p1\",\n                Text = new Databricks.Inputs.SqlQueryParameterTextArgs\n                {\n                    Value = \"default\",\n                },\n            },\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p2\",\n                Title = \"Title for p2\",\n                Enum = new Databricks.Inputs.SqlQueryParameterEnumArgs\n                {\n                    Options = new[]\n                    {\n                        \"default\",\n                        \"foo\",\n                        \"bar\",\n                    },\n                    Value = \"default\",\n                    Multiple = new Databricks.Inputs.SqlQueryParameterEnumMultipleArgs\n                    {\n                        Prefix = \"\\\"\",\n                        Suffix = \"\\\"\",\n                        Separator = \",\",\n                    },\n                },\n            },\n            new Databricks.Inputs.SqlQueryParameterArgs\n            {\n                Name = \"p3\",\n                Title = \"Title for p3\",\n                Date = new Databricks.Inputs.SqlQueryParameterDateArgs\n                {\n                    Value = \"2022-01-01\",\n                },\n            },\n        },\n        Tags = new[]\n        {\n            \"t1\",\n            \"t2\",\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsharedDir, err := databricks.NewDirectory(ctx, \"shared_dir\", \u0026databricks.DirectoryArgs{\n\t\t\tPath: pulumi.String(\"/Shared/Queries\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlQuery(ctx, \"q1\", \u0026databricks.SqlQueryArgs{\n\t\t\tDataSourceId: pulumi.Any(example.DataSourceId),\n\t\t\tName:         pulumi.String(\"My Query Name\"),\n\t\t\tQuery:        pulumi.String(\"                        SELECT {{ p1 }} AS p1\\n                        WHERE 1=1\\n                        AND p2 in ({{ p2 }})\\n                        AND event_date \u003e date '{{ p3 }}'\\n\"),\n\t\t\tParent: sharedDir.ObjectId.ApplyT(func(objectId int) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"folders/%v\", objectId), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tRunAsRole: pulumi.String(\"viewer\"),\n\t\t\tParameters: databricks.SqlQueryParameterArray{\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p1\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p1\"),\n\t\t\t\t\tText: \u0026databricks.SqlQueryParameterTextArgs{\n\t\t\t\t\t\tValue: pulumi.String(\"default\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p2\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p2\"),\n\t\t\t\t\tEnum: \u0026databricks.SqlQueryParameterEnumArgs{\n\t\t\t\t\t\tOptions: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"default\"),\n\t\t\t\t\t\t\tpulumi.String(\"foo\"),\n\t\t\t\t\t\t\tpulumi.String(\"bar\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t\tValue: pulumi.String(\"default\"),\n\t\t\t\t\t\tMultiple: \u0026databricks.SqlQueryParameterEnumMultipleArgs{\n\t\t\t\t\t\t\tPrefix:    pulumi.String(\"\\\"\"),\n\t\t\t\t\t\t\tSuffix:    pulumi.String(\"\\\"\"),\n\t\t\t\t\t\t\tSeparator: pulumi.String(\",\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t\t\u0026databricks.SqlQueryParameterArgs{\n\t\t\t\t\tName:  pulumi.String(\"p3\"),\n\t\t\t\t\tTitle: pulumi.String(\"Title for p3\"),\n\t\t\t\t\tDate: \u0026databricks.SqlQueryParameterDateArgs{\n\t\t\t\t\t\tValue: pulumi.String(\"2022-01-01\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t\tTags: pulumi.StringArray{\n\t\t\t\tpulumi.String(\"t1\"),\n\t\t\t\tpulumi.String(\"t2\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Directory;\nimport com.pulumi.databricks.DirectoryArgs;\nimport com.pulumi.databricks.SqlQuery;\nimport com.pulumi.databricks.SqlQueryArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterTextArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterEnumArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterEnumMultipleArgs;\nimport com.pulumi.databricks.inputs.SqlQueryParameterDateArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sharedDir = new Directory(\"sharedDir\", DirectoryArgs.builder()        \n            .path(\"/Shared/Queries\")\n            .build());\n\n        var q1 = new SqlQuery(\"q1\", SqlQueryArgs.builder()        \n            .dataSourceId(example.dataSourceId())\n            .name(\"My Query Name\")\n            .query(\"\"\"\n                        SELECT {{ p1 }} AS p1\n                        WHERE 1=1\n                        AND p2 in ({{ p2 }})\n                        AND event_date \u003e date '{{ p3 }}'\n            \"\"\")\n            .parent(sharedDir.objectId().applyValue(objectId -\u003e String.format(\"folders/%s\", objectId)))\n            .runAsRole(\"viewer\")\n            .parameters(            \n                SqlQueryParameterArgs.builder()\n                    .name(\"p1\")\n                    .title(\"Title for p1\")\n                    .text(SqlQueryParameterTextArgs.builder()\n                        .value(\"default\")\n                        .build())\n                    .build(),\n                SqlQueryParameterArgs.builder()\n                    .name(\"p2\")\n                    .title(\"Title for p2\")\n                    .enum_(SqlQueryParameterEnumArgs.builder()\n                        .options(                        \n                            \"default\",\n                            \"foo\",\n                            \"bar\")\n                        .value(\"default\")\n                        .multiple(SqlQueryParameterEnumMultipleArgs.builder()\n                            .prefix(\"\\\"\")\n                            .suffix(\"\\\"\")\n                            .separator(\",\")\n                            .build())\n                        .build())\n                    .build(),\n                SqlQueryParameterArgs.builder()\n                    .name(\"p3\")\n                    .title(\"Title for p3\")\n                    .date(SqlQueryParameterDateArgs.builder()\n                        .value(\"2022-01-01\")\n                        .build())\n                    .build())\n            .tags(            \n                \"t1\",\n                \"t2\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sharedDir:\n    type: databricks:Directory\n    name: shared_dir\n    properties:\n      path: /Shared/Queries\n  q1:\n    type: databricks:SqlQuery\n    properties:\n      dataSourceId: ${example.dataSourceId}\n      name: My Query Name\n      query: |2\n                                SELECT {{ p1 }} AS p1\n                                WHERE 1=1\n                                AND p2 in ({{ p2 }})\n                                AND event_date \u003e date '{{ p3 }}'\n      parent: folders/${sharedDir.objectId}\n      runAsRole: viewer\n      parameters:\n        - name: p1\n          title: Title for p1\n          text:\n            value: default\n        - name: p2\n          title: Title for p2\n          enum:\n            options:\n              - default\n              - foo\n              - bar\n            value: default\n            multiple:\n              prefix: '\"'\n              suffix: '\"'\n              separator: ','\n        - name: p3\n          title: Title for p3\n          date:\n            value: 2022-01-01\n      tags:\n        - t1\n        - t2\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nExample permission to share query with all users:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst q1 = new databricks.Permissions(\"q1\", {\n    sqlQueryId: q1DatabricksSqlQuery.id,\n    accessControls: [\n        {\n            groupName: users.displayName,\n            permissionLevel: \"CAN_RUN\",\n        },\n        {\n            groupName: team.displayName,\n            permissionLevel: \"CAN_EDIT\",\n        },\n    ],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nq1 = databricks.Permissions(\"q1\",\n    sql_query_id=q1_databricks_sql_query[\"id\"],\n    access_controls=[\n        databricks.PermissionsAccessControlArgs(\n            group_name=users[\"displayName\"],\n            permission_level=\"CAN_RUN\",\n        ),\n        databricks.PermissionsAccessControlArgs(\n            group_name=team[\"displayName\"],\n            permission_level=\"CAN_EDIT\",\n        ),\n    ])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var q1 = new Databricks.Permissions(\"q1\", new()\n    {\n        SqlQueryId = q1DatabricksSqlQuery.Id,\n        AccessControls = new[]\n        {\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = users.DisplayName,\n                PermissionLevel = \"CAN_RUN\",\n            },\n            new Databricks.Inputs.PermissionsAccessControlArgs\n            {\n                GroupName = team.DisplayName,\n                PermissionLevel = \"CAN_EDIT\",\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewPermissions(ctx, \"q1\", \u0026databricks.PermissionsArgs{\n\t\t\tSqlQueryId: pulumi.Any(q1DatabricksSqlQuery.Id),\n\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(users.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_RUN\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\tGroupName:       pulumi.Any(team.DisplayName),\n\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_EDIT\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var q1 = new Permissions(\"q1\", PermissionsArgs.builder()        \n            .sqlQueryId(q1DatabricksSqlQuery.id())\n            .accessControls(            \n                PermissionsAccessControlArgs.builder()\n                    .groupName(users.displayName())\n                    .permissionLevel(\"CAN_RUN\")\n                    .build(),\n                PermissionsAccessControlArgs.builder()\n                    .groupName(team.displayName())\n                    .permissionLevel(\"CAN_EDIT\")\n                    .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  q1:\n    type: databricks:Permissions\n    properties:\n      sqlQueryId: ${q1DatabricksSqlQuery.id}\n      accessControls:\n        - groupName: ${users.displayName}\n          permissionLevel: CAN_RUN\n        - groupName: ${team.displayName}\n          permissionLevel: CAN_EDIT\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Troubleshooting\n\nIn case you see `Error: cannot create sql query: Internal Server Error` during `pulumi up`; double check that you are using the correct `data_source_id`\n\nOperations on `databricks.SqlQuery` schedules are ⛔️ deprecated. You can create, update or delete a schedule for SQLA and other Databricks resources using the databricks.Job resource.\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n* databricks.Job to schedule Databricks SQL queries (as well as dashboards and alerts) using Databricks Jobs.\n\n## Import\n\nYou can import a `databricks_sql_query` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlQuery:SqlQuery this \u003cquery-id\u003e\n```\n\n",
            "properties": {
                "createdAt": {
                    "type": "string"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "Data source ID of a SQL warehouse\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the object.\n"
                },
                "query": {
                    "type": "string",
                    "description": "The text of the query to be run.\n"
                },
                "runAsRole": {
                    "type": "string",
                    "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                    "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "required": [
                "createdAt",
                "dataSourceId",
                "name",
                "query",
                "updatedAt"
            ],
            "inputProperties": {
                "createdAt": {
                    "type": "string"
                },
                "dataSourceId": {
                    "type": "string",
                    "description": "Data source ID of a SQL warehouse\n"
                },
                "description": {
                    "type": "string",
                    "description": "General description that conveys additional information about this query such as usage notes.\n"
                },
                "name": {
                    "type": "string",
                    "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                    }
                },
                "parent": {
                    "type": "string",
                    "description": "The identifier of the workspace folder containing the object.\n",
                    "willReplaceOnChanges": true
                },
                "query": {
                    "type": "string",
                    "description": "The text of the query to be run.\n"
                },
                "runAsRole": {
                    "type": "string",
                    "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                },
                "schedule": {
                    "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                    "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                },
                "tags": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    }
                },
                "updatedAt": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "dataSourceId",
                "query"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlQuery resources.\n",
                "properties": {
                    "createdAt": {
                        "type": "string"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "Data source ID of a SQL warehouse\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "General description that conveys additional information about this query such as usage notes.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The title of this query that appears in list views, widget headings, and on the query page.\n"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlQueryParameter:SqlQueryParameter"
                        }
                    },
                    "parent": {
                        "type": "string",
                        "description": "The identifier of the workspace folder containing the object.\n",
                        "willReplaceOnChanges": true
                    },
                    "query": {
                        "type": "string",
                        "description": "The text of the query to be run.\n"
                    },
                    "runAsRole": {
                        "type": "string",
                        "description": "Run as role. Possible values are `viewer`, `owner`.\n"
                    },
                    "schedule": {
                        "$ref": "#/types/databricks:index/SqlQuerySchedule:SqlQuerySchedule",
                        "deprecationMessage": "Operations on `databricks.SqlQuery` schedules are deprecated. Please use `databricks.Job` resource to schedule a `sql_task`."
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        }
                    },
                    "updatedAt": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlTable:SqlTable": {
            "description": "Within a metastore, Unity Catalog provides a 3-level namespace for organizing data: Catalogs, databases (also called schemas), and tables / views.\n\nA `databricks.SqlTable` is contained within databricks_schema, and can represent either a managed table, an external table or a view.\n\nThis resource creates and updates the Unity Catalog table/view by executing the necessary SQL queries on a special auto-terminating cluster it would create for this operation. You could also specify a SQL warehouse or cluster for the queries to be executed on.\n\n## Import\n\nThis resource can be imported by its full name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlTable:SqlTable this \u003ccatalog_name\u003e.\u003cschema_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n"
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to liquid cluster the table by. Conflicts with `partitions`.\n"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text. Changing comment is not currently supported on `VIEW` table_type.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`. Change forces creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of table relative to parent catalog and schema. Change forces creation of a new resource.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map of user defined table options. Change forces creation of a new resource.\n"
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with `cluster_keys`.\n"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map of table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n"
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`. Change forces creation of a new resource.\n"
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                }
            },
            "required": [
                "catalogName",
                "clusterId",
                "columns",
                "name",
                "properties",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "clusterId": {
                    "type": "string"
                },
                "clusterKeys": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to liquid cluster the table by. Conflicts with `partitions`.\n",
                    "willReplaceOnChanges": true
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                    }
                },
                "comment": {
                    "type": "string",
                    "description": "User-supplied free-form text. Changing comment is not currently supported on `VIEW` table_type.\n"
                },
                "dataSourceFormat": {
                    "type": "string",
                    "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`. Change forces creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of table relative to parent catalog and schema. Change forces creation of a new resource.\n"
                },
                "options": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map of user defined table options. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "partitions": {
                    "type": "array",
                    "items": {
                        "type": "string"
                    },
                    "description": "a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with `cluster_keys`.\n",
                    "willReplaceOnChanges": true
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Map of table properties.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                },
                "tableType": {
                    "type": "string",
                    "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string",
                    "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                },
                "warehouseId": {
                    "type": "string",
                    "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlTable resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterKeys": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a subset of columns to liquid cluster the table by. Conflicts with `partitions`.\n",
                        "willReplaceOnChanges": true
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlTableColumn:SqlTableColumn"
                        }
                    },
                    "comment": {
                        "type": "string",
                        "description": "User-supplied free-form text. Changing comment is not currently supported on `VIEW` table_type.\n"
                    },
                    "dataSourceFormat": {
                        "type": "string",
                        "description": "External tables are supported in multiple data source formats. The string constants identifying these formats are `DELTA`, `CSV`, `JSON`, `AVRO`, `PARQUET`, `ORC`, `TEXT`. Change forces creation of a new resource. Not supported for `MANAGED` tables or `VIEW`.\n",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of table relative to parent catalog and schema. Change forces creation of a new resource.\n"
                    },
                    "options": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map of user defined table options. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "partitions": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a subset of columns to partition the table by. Change forces creation of a new resource. Conflicts with `cluster_keys`.\n",
                        "willReplaceOnChanges": true
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Map of table properties.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "description": "For EXTERNAL Tables only: the name of storage credential to use. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "URL of storage location for Table data (required for EXTERNAL Tables). Not supported for `VIEW` or `MANAGED` table_type.\n"
                    },
                    "tableType": {
                        "type": "string",
                        "description": "Distinguishes a view vs. managed/external Table. `MANAGED`, `EXTERNAL` or `VIEW`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string",
                        "description": "SQL text defining the view (for `table_type == \"VIEW\"`). Not supported for `MANAGED` or `EXTERNAL` table_type.\n"
                    },
                    "warehouseId": {
                        "type": "string",
                        "description": "All table CRUD operations must be executed on a running cluster or SQL warehouse. If a `warehouse_id` is specified, that SQL warehouse will be used to execute SQL commands to manage this table. Conflicts with `cluster_id`.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlVisualization:SqlVisualization": {
            "description": "\n\n## Import\n\nYou can import a `databricks_sql_visualization` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlVisualization:SqlVisualization this \u003cquery-id\u003e/\u003cvisualization-id\u003e\n```\n\n",
            "properties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string"
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                }
            },
            "required": [
                "name",
                "options",
                "queryId",
                "type",
                "visualizationId"
            ],
            "inputProperties": {
                "description": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "options": {
                    "type": "string"
                },
                "queryId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "queryPlan": {
                    "type": "string"
                },
                "type": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "options",
                "queryId",
                "type"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlVisualization resources.\n",
                "properties": {
                    "description": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "options": {
                        "type": "string"
                    },
                    "queryId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "queryPlan": {
                        "type": "string"
                    },
                    "type": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/sqlWidget:SqlWidget": {
            "description": "To manage [SQLA resources](https://docs.databricks.com/sql/get-started/concepts.html) you must have `databricks_sql_access` on your databricks.Group or databricks_user.\n\n**Note:** documentation for this resource is a work in progress.\n\nA widget is always tied to a dashboard. Every dashboard may have one or more widgets.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst d1w1 = new databricks.SqlWidget(\"d1w1\", {\n    dashboardId: d1.id,\n    text: \"Hello! I'm a **text widget**!\",\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 0,\n        posY: 0,\n    },\n});\nconst d1w2 = new databricks.SqlWidget(\"d1w2\", {\n    dashboardId: d1.id,\n    visualizationId: q1v1.id,\n    position: {\n        sizeX: 3,\n        sizeY: 4,\n        posX: 3,\n        posY: 0,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nd1w1 = databricks.SqlWidget(\"d1w1\",\n    dashboard_id=d1[\"id\"],\n    text=\"Hello! I'm a **text widget**!\",\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=0,\n        pos_y=0,\n    ))\nd1w2 = databricks.SqlWidget(\"d1w2\",\n    dashboard_id=d1[\"id\"],\n    visualization_id=q1v1[\"id\"],\n    position=databricks.SqlWidgetPositionArgs(\n        size_x=3,\n        size_y=4,\n        pos_x=3,\n        pos_y=0,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var d1w1 = new Databricks.SqlWidget(\"d1w1\", new()\n    {\n        DashboardId = d1.Id,\n        Text = \"Hello! I'm a **text widget**!\",\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 0,\n            PosY = 0,\n        },\n    });\n\n    var d1w2 = new Databricks.SqlWidget(\"d1w2\", new()\n    {\n        DashboardId = d1.Id,\n        VisualizationId = q1v1.Id,\n        Position = new Databricks.Inputs.SqlWidgetPositionArgs\n        {\n            SizeX = 3,\n            SizeY = 4,\n            PosX = 3,\n            PosY = 0,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSqlWidget(ctx, \"d1w1\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId: pulumi.Any(d1.Id),\n\t\t\tText:        pulumi.String(\"Hello! I'm a **text widget**!\"),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(0),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewSqlWidget(ctx, \"d1w2\", \u0026databricks.SqlWidgetArgs{\n\t\t\tDashboardId:     pulumi.Any(d1.Id),\n\t\t\tVisualizationId: pulumi.Any(q1v1.Id),\n\t\t\tPosition: \u0026databricks.SqlWidgetPositionArgs{\n\t\t\t\tSizeX: pulumi.Int(3),\n\t\t\t\tSizeY: pulumi.Int(4),\n\t\t\t\tPosX:  pulumi.Int(3),\n\t\t\t\tPosY:  pulumi.Int(0),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SqlWidget;\nimport com.pulumi.databricks.SqlWidgetArgs;\nimport com.pulumi.databricks.inputs.SqlWidgetPositionArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var d1w1 = new SqlWidget(\"d1w1\", SqlWidgetArgs.builder()        \n            .dashboardId(d1.id())\n            .text(\"Hello! I'm a **text widget**!\")\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(0)\n                .posY(0)\n                .build())\n            .build());\n\n        var d1w2 = new SqlWidget(\"d1w2\", SqlWidgetArgs.builder()        \n            .dashboardId(d1.id())\n            .visualizationId(q1v1.id())\n            .position(SqlWidgetPositionArgs.builder()\n                .sizeX(3)\n                .sizeY(4)\n                .posX(3)\n                .posY(0)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  d1w1:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${d1.id}\n      text: Hello! I'm a **text widget**!\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 0\n        posY: 0\n  d1w2:\n    type: databricks:SqlWidget\n    properties:\n      dashboardId: ${d1.id}\n      visualizationId: ${q1v1.id}\n      position:\n        sizeX: 3\n        sizeY: 4\n        posX: 3\n        posY: 0\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n\n## Import\n\nYou can import a `databricks_sql_widget` resource with ID like the following:\n\nbash\n\n```sh\n$ pulumi import databricks:index/sqlWidget:SqlWidget this \u003cdashboard-id\u003e/\u003cwidget-id\u003e\n```\n\n",
            "properties": {
                "dashboardId": {
                    "type": "string"
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string"
                },
                "widgetId": {
                    "type": "string"
                }
            },
            "required": [
                "dashboardId",
                "widgetId"
            ],
            "inputProperties": {
                "dashboardId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "description": {
                    "type": "string"
                },
                "parameters": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                    }
                },
                "position": {
                    "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                },
                "text": {
                    "type": "string"
                },
                "title": {
                    "type": "string"
                },
                "visualizationId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "widgetId": {
                    "type": "string",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "dashboardId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SqlWidget resources.\n",
                "properties": {
                    "dashboardId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "description": {
                        "type": "string"
                    },
                    "parameters": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/SqlWidgetParameter:SqlWidgetParameter"
                        }
                    },
                    "position": {
                        "$ref": "#/types/databricks:index/SqlWidgetPosition:SqlWidgetPosition"
                    },
                    "text": {
                        "type": "string"
                    },
                    "title": {
                        "type": "string"
                    },
                    "visualizationId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "widgetId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/storageCredential:StorageCredential": {
            "description": "\u003e **Note** This resource could be used with account or workspace-level provider.\n\nTo work with external tables, Unity Catalog introduces two new objects to access and work with external cloud storage:\n\n- `databricks.StorageCredential` represents authentication methods to access cloud storage (e.g. an IAM role for Amazon S3 or a service principal/managed identity for Azure Storage). Storage credentials are access-controlled to determine which users can use the credential.\n- databricks.ExternalLocation are objects that combine a cloud storage path with a Storage Credential that can be used to access the location.\n\n## Example Usage\n\nFor AWS\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: externalDataAccess.name,\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n    comment: \"Managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=external_data_access[\"name\"],\n    aws_iam_role=databricks.StorageCredentialAwsIamRoleArgs(\n        role_arn=external_data_access[\"arn\"],\n    ),\n    comment=\"Managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external.id,\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\"CREATE_EXTERNAL_TABLE\"],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = externalDataAccess.Name,\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n        Comment = \"Managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.Any(externalDataAccess.Name),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()        \n            .name(externalDataAccess.name())\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .comment(\"Managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()        \n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: ${externalDataAccess.name}\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n      comment: Managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor Azure\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst externalMi = new databricks.StorageCredential(\"external_mi\", {\n    name: \"mi_credential\",\n    azureManagedIdentity: {\n        accessConnectorId: example.id,\n    },\n    comment: \"Managed identity credential managed by TF\",\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal_mi = databricks.StorageCredential(\"external_mi\",\n    name=\"mi_credential\",\n    azure_managed_identity=databricks.StorageCredentialAzureManagedIdentityArgs(\n        access_connector_id=example[\"id\"],\n    ),\n    comment=\"Managed identity credential managed by TF\")\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external[\"id\"],\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\"CREATE_EXTERNAL_TABLE\"],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var externalMi = new Databricks.StorageCredential(\"external_mi\", new()\n    {\n        Name = \"mi_credential\",\n        AzureManagedIdentity = new Databricks.Inputs.StorageCredentialAzureManagedIdentityArgs\n        {\n            AccessConnectorId = example.Id,\n        },\n        Comment = \"Managed identity credential managed by TF\",\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewStorageCredential(ctx, \"external_mi\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.String(\"mi_credential\"),\n\t\t\tAzureManagedIdentity: \u0026databricks.StorageCredentialAzureManagedIdentityArgs{\n\t\t\t\tAccessConnectorId: pulumi.Any(example.Id),\n\t\t\t},\n\t\t\tComment: pulumi.String(\"Managed identity credential managed by TF\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: pulumi.Any(external.Id),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAzureManagedIdentityArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var externalMi = new StorageCredential(\"externalMi\", StorageCredentialArgs.builder()        \n            .name(\"mi_credential\")\n            .azureManagedIdentity(StorageCredentialAzureManagedIdentityArgs.builder()\n                .accessConnectorId(example.id())\n                .build())\n            .comment(\"Managed identity credential managed by TF\")\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()        \n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  externalMi:\n    type: databricks:StorageCredential\n    name: external_mi\n    properties:\n      name: mi_credential\n      azureManagedIdentity:\n        accessConnectorId: ${example.id}\n      comment: Managed identity credential managed by TF\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFor GCP\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst external = new databricks.StorageCredential(\"external\", {\n    name: \"the-creds\",\n    databricksGcpServiceAccount: {},\n});\nconst externalCreds = new databricks.Grants(\"external_creds\", {\n    storageCredential: external.id,\n    grants: [{\n        principal: \"Data Engineers\",\n        privileges: [\"CREATE_EXTERNAL_TABLE\"],\n    }],\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nexternal = databricks.StorageCredential(\"external\",\n    name=\"the-creds\",\n    databricks_gcp_service_account=databricks.StorageCredentialDatabricksGcpServiceAccountArgs())\nexternal_creds = databricks.Grants(\"external_creds\",\n    storage_credential=external.id,\n    grants=[databricks.GrantsGrantArgs(\n        principal=\"Data Engineers\",\n        privileges=[\"CREATE_EXTERNAL_TABLE\"],\n    )])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = \"the-creds\",\n        DatabricksGcpServiceAccount = null,\n    });\n\n    var externalCreds = new Databricks.Grants(\"external_creds\", new()\n    {\n        StorageCredential = external.Id,\n        GrantDetails = new[]\n        {\n            new Databricks.Inputs.GrantsGrantArgs\n            {\n                Principal = \"Data Engineers\",\n                Privileges = new[]\n                {\n                    \"CREATE_EXTERNAL_TABLE\",\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName:                        pulumi.String(\"the-creds\"),\n\t\t\tDatabricksGcpServiceAccount: nil,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGrants(ctx, \"external_creds\", \u0026databricks.GrantsArgs{\n\t\t\tStorageCredential: external.ID(),\n\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\tPrincipal: pulumi.String(\"Data Engineers\"),\n\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\tpulumi.String(\"CREATE_EXTERNAL_TABLE\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialDatabricksGcpServiceAccountArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()        \n            .name(\"the-creds\")\n            .databricksGcpServiceAccount()\n            .build());\n\n        var externalCreds = new Grants(\"externalCreds\", GrantsArgs.builder()        \n            .storageCredential(external.id())\n            .grants(GrantsGrantArgs.builder()\n                .principal(\"Data Engineers\")\n                .privileges(\"CREATE_EXTERNAL_TABLE\")\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: the-creds\n      databricksGcpServiceAccount: {}\n  externalCreds:\n    type: databricks:Grants\n    name: external_creds\n    properties:\n      storageCredential: ${external.id}\n      grants:\n        - principal: Data Engineers\n          privileges:\n            - CREATE_EXTERNAL_TABLE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by name:\n\nbash\n\n```sh\n$ pulumi import databricks:index/storageCredential:StorageCredential this \u003cname\u003e\n```\n\n",
            "properties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete storage credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update storage credential regardless of its dependents.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                }
            },
            "required": [
                "databricksGcpServiceAccount",
                "metastoreId",
                "name",
                "owner"
            ],
            "inputProperties": {
                "awsIamRole": {
                    "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                },
                "azureManagedIdentity": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                },
                "azureServicePrincipal": {
                    "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                },
                "comment": {
                    "type": "string"
                },
                "databricksGcpServiceAccount": {
                    "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                },
                "forceDestroy": {
                    "type": "boolean",
                    "description": "Delete storage credential regardless of its dependencies.\n"
                },
                "forceUpdate": {
                    "type": "boolean",
                    "description": "Update storage credential regardless of its dependents.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                },
                "gcpServiceAccountKey": {
                    "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                },
                "metastoreId": {
                    "type": "string",
                    "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "owner": {
                    "type": "string",
                    "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                },
                "readOnly": {
                    "type": "boolean",
                    "description": "Indicates whether the storage credential is only usable for read operations.\n"
                },
                "skipValidation": {
                    "type": "boolean",
                    "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering StorageCredential resources.\n",
                "properties": {
                    "awsIamRole": {
                        "$ref": "#/types/databricks:index/StorageCredentialAwsIamRole:StorageCredentialAwsIamRole"
                    },
                    "azureManagedIdentity": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureManagedIdentity:StorageCredentialAzureManagedIdentity"
                    },
                    "azureServicePrincipal": {
                        "$ref": "#/types/databricks:index/StorageCredentialAzureServicePrincipal:StorageCredentialAzureServicePrincipal"
                    },
                    "comment": {
                        "type": "string"
                    },
                    "databricksGcpServiceAccount": {
                        "$ref": "#/types/databricks:index/StorageCredentialDatabricksGcpServiceAccount:StorageCredentialDatabricksGcpServiceAccount"
                    },
                    "forceDestroy": {
                        "type": "boolean",
                        "description": "Delete storage credential regardless of its dependencies.\n"
                    },
                    "forceUpdate": {
                        "type": "boolean",
                        "description": "Update storage credential regardless of its dependents.\n\n`aws_iam_role` optional configuration block for credential details for AWS:\n"
                    },
                    "gcpServiceAccountKey": {
                        "$ref": "#/types/databricks:index/StorageCredentialGcpServiceAccountKey:StorageCredentialGcpServiceAccountKey"
                    },
                    "metastoreId": {
                        "type": "string",
                        "description": "Unique identifier of the parent Metastore. If set for workspace-level, it must match the ID of the metastore assigned to the worspace. When changing the metastore assigned to a workspace, this field becomes required.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of Storage Credentials, which must be unique within the databricks_metastore. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "owner": {
                        "type": "string",
                        "description": "Username/groupname/sp application_id of the storage credential owner.\n"
                    },
                    "readOnly": {
                        "type": "boolean",
                        "description": "Indicates whether the storage credential is only usable for read operations.\n"
                    },
                    "skipValidation": {
                        "type": "boolean",
                        "description": "Suppress validation errors if any \u0026 force save the storage credential.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/systemSchema:SystemSchema": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n\u003e **Note** This resource could be only used with workspace-level provider!\n\nManages system tables enablement. System tables are a Databricks-hosted analytical store of your account’s operational data. System tables can be used for historical observability across your account. System tables must be enabled by an account admin.\n\n## Example Usage\n\nEnable the system schema `access`\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.SystemSchema(\"this\", {schema: \"access\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.SystemSchema(\"this\", schema=\"access\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.SystemSchema(\"this\", new()\n    {\n        Schema = \"access\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewSystemSchema(ctx, \"this\", \u0026databricks.SystemSchemaArgs{\n\t\t\tSchema: pulumi.String(\"access\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.SystemSchema;\nimport com.pulumi.databricks.SystemSchemaArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new SystemSchema(\"this\", SystemSchemaArgs.builder()        \n            .schema(\"access\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:SystemSchema\n    properties:\n      schema: access\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by the metastore id and schema name\n\nbash\n\n```sh\n$ pulumi import databricks:index/systemSchema:SystemSchema this \u003cmetastore_id\u003e|\u003cschema_name\u003e\n```\n\n",
            "properties": {
                "metastoreId": {
                    "type": "string"
                },
                "schema": {
                    "type": "string",
                    "description": "Full name of the system schema.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of enablement for the system schema.\n"
                }
            },
            "required": [
                "metastoreId",
                "state"
            ],
            "inputProperties": {
                "schema": {
                    "type": "string",
                    "description": "Full name of the system schema.\n"
                },
                "state": {
                    "type": "string",
                    "description": "The current state of enablement for the system schema.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering SystemSchema resources.\n",
                "properties": {
                    "metastoreId": {
                        "type": "string"
                    },
                    "schema": {
                        "type": "string",
                        "description": "Full name of the system schema.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of enablement for the system schema.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/table:Table": {
            "properties": {
                "catalogName": {
                    "type": "string"
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "schemaName": {
                    "type": "string"
                },
                "storageCredentialName": {
                    "type": "string"
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string"
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "required": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "name",
                "owner",
                "schemaName",
                "tableType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "columns": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                    }
                },
                "comment": {
                    "type": "string"
                },
                "dataSourceFormat": {
                    "type": "string"
                },
                "name": {
                    "type": "string"
                },
                "owner": {
                    "type": "string"
                },
                "properties": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    }
                },
                "schemaName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageCredentialName": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string"
                },
                "tableType": {
                    "type": "string",
                    "willReplaceOnChanges": true
                },
                "viewDefinition": {
                    "type": "string"
                }
            },
            "requiredInputs": [
                "catalogName",
                "columns",
                "dataSourceFormat",
                "schemaName",
                "tableType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Table resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "columns": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/TableColumn:TableColumn"
                        }
                    },
                    "comment": {
                        "type": "string"
                    },
                    "dataSourceFormat": {
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "owner": {
                        "type": "string"
                    },
                    "properties": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        }
                    },
                    "schemaName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialName": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string"
                    },
                    "tableType": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "viewDefinition": {
                        "type": "string"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/token:Token": {
            "description": "This resource creates [Personal Access Tokens](https://docs.databricks.com/sql/user/security/personal-access-tokens.html) for the same user that is authenticated with the provider. Most likely you should use databricks.OboToken to create [On-Behalf-Of tokens](https://docs.databricks.com/administration-guide/users-groups/service-principals.html#manage-personal-access-tokens-for-a-service-principal) for a databricks.ServicePrincipal in Databricks workspaces on AWS. Databricks workspaces on other clouds use their own native OAuth token flows.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\n// create PAT token to provision entities within workspace\nconst pat = new databricks.Token(\"pat\", {\n    comment: \"Terraform Provisioning\",\n    lifetimeSeconds: 8640000,\n});\nexport const databricksToken = pat.tokenValue;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\n# create PAT token to provision entities within workspace\npat = databricks.Token(\"pat\",\n    comment=\"Terraform Provisioning\",\n    lifetime_seconds=8640000)\npulumi.export(\"databricksToken\", pat.token_value)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    // create PAT token to provision entities within workspace\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = \"Terraform Provisioning\",\n        LifetimeSeconds = 8640000,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"databricksToken\"] = pat.TokenValue,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t// create PAT token to provision entities within workspace\n\t\tpat, err := databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment:         pulumi.String(\"Terraform Provisioning\"),\n\t\t\tLifetimeSeconds: pulumi.Int(8640000),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"databricksToken\", pat.TokenValue)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        // create PAT token to provision entities within workspace\n        var pat = new Token(\"pat\", TokenArgs.builder()        \n            .comment(\"Terraform Provisioning\")\n            .lifetimeSeconds(8640000)\n            .build());\n\n        ctx.export(\"databricksToken\", pat.tokenValue());\n    }\n}\n```\n```yaml\nresources:\n  # create PAT token to provision entities within workspace\n  pat:\n    type: databricks:Token\n    properties:\n      comment: Terraform Provisioning\n      lifetimeSeconds: 8.64e+06\noutputs:\n  # output token for other modules\n  databricksToken: ${pat.tokenValue}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nA token can be automatically rotated by taking a dependency on the `time_rotating` resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\nimport * as time from \"@pulumiverse/time\";\n\nconst _this = new time.Rotating(\"this\", {rotationDays: 30});\nconst pat = new databricks.Token(\"pat\", {\n    comment: pulumi.interpolate`Terraform (created: ${_this.rfc3339})`,\n    lifetimeSeconds: 60 * 24 * 60 * 60,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\nimport pulumiverse_time as time\n\nthis = time.Rotating(\"this\", rotation_days=30)\npat = databricks.Token(\"pat\",\n    comment=this.rfc3339.apply(lambda rfc3339: f\"Terraform (created: {rfc3339})\"),\n    lifetime_seconds=60 * 24 * 60 * 60)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\nusing Time = Pulumiverse.Time;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Time.Rotating(\"this\", new()\n    {\n        RotationDays = 30,\n    });\n\n    var pat = new Databricks.Token(\"pat\", new()\n    {\n        Comment = @this.Rfc3339.Apply(rfc3339 =\u003e $\"Terraform (created: {rfc3339})\"),\n        LifetimeSeconds = 60 * 24 * 60 * 60,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi-time/sdk/go/time\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := time.NewRotating(ctx, \"this\", \u0026time.RotatingArgs{\n\t\t\tRotationDays: pulumi.Int(30),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewToken(ctx, \"pat\", \u0026databricks.TokenArgs{\n\t\t\tComment: this.Rfc3339.ApplyT(func(rfc3339 string) (string, error) {\n\t\t\t\treturn fmt.Sprintf(\"Terraform (created: %v)\", rfc3339), nil\n\t\t\t}).(pulumi.StringOutput),\n\t\t\tLifetimeSeconds: 60 * 24 * 60 * 60,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.time.Rotating;\nimport com.pulumi.time.RotatingArgs;\nimport com.pulumi.databricks.Token;\nimport com.pulumi.databricks.TokenArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new Rotating(\"this\", RotatingArgs.builder()        \n            .rotationDays(30)\n            .build());\n\n        var pat = new Token(\"pat\", TokenArgs.builder()        \n            .comment(this_.rfc3339().applyValue(rfc3339 -\u003e String.format(\"Terraform (created: %s)\", rfc3339)))\n            .lifetimeSeconds(60 * 24 * 60 * 60)\n            .build());\n\n    }\n}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n"
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n"
                },
                "tokenId": {
                    "type": "string"
                },
                "tokenValue": {
                    "type": "string",
                    "description": "**Sensitive** value of the newly-created token.\n",
                    "secret": true
                }
            },
            "required": [
                "creationTime",
                "expiryTime",
                "tokenId",
                "tokenValue"
            ],
            "inputProperties": {
                "comment": {
                    "type": "string",
                    "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                    "willReplaceOnChanges": true
                },
                "creationTime": {
                    "type": "integer"
                },
                "expiryTime": {
                    "type": "integer"
                },
                "lifetimeSeconds": {
                    "type": "integer",
                    "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                    "willReplaceOnChanges": true
                },
                "tokenId": {
                    "type": "string"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Token resources.\n",
                "properties": {
                    "comment": {
                        "type": "string",
                        "description": "(String) Comment that will appear on the user’s settings page for this token.\n",
                        "willReplaceOnChanges": true
                    },
                    "creationTime": {
                        "type": "integer"
                    },
                    "expiryTime": {
                        "type": "integer"
                    },
                    "lifetimeSeconds": {
                        "type": "integer",
                        "description": "(Integer) The lifetime of the token, in seconds. If no lifetime is specified, the token remains valid indefinitely.\n",
                        "willReplaceOnChanges": true
                    },
                    "tokenId": {
                        "type": "string"
                    },
                    "tokenValue": {
                        "type": "string",
                        "description": "**Sensitive** value of the newly-created token.\n",
                        "secret": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/user:User": {
            "description": "This resource allows you to manage [users in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/users.html), [Databricks Account Console](https://accounts.cloud.databricks.com/) or [Azure Databricks Account Console](https://accounts.azuredatabricks.net). You can also associate Databricks users to databricks_group. Upon user creation the user will receive a password reset email. You can also get information about caller identity using databricks.getCurrentUser data source.\n\n\u003e **Note** To assign account level users to workspace use databricks_mws_permission_assignment.\n\n\u003e **Note** Entitlements, like, `allow_cluster_create`, `allow_instance_pool_create`, `databricks_sql_access`, `workspace_access` applicable only for workspace-level users.  Use databricks.Entitlements resource to assign entitlements inside a workspace to account-level users.\n\nTo create users in the Databricks account, the provider must be configured with `host = \"https://accounts.cloud.databricks.com\"` on AWS deployments or `host = \"https://accounts.azuredatabricks.net\"` and authenticate using AAD tokens on Azure deployments.\n\nThe default behavior when deleting a `databricks.User` resource depends on whether the provider is configured at the workspace-level or account-level. When the provider is configured at the workspace-level, the user will be deleted from the workspace. When the provider is configured at the account-level, the user will be deactivated but not deleted. When the provider is configured at the account level, to delete the user from the account when the resource is deleted, set `disable_as_user_deletion = false`. Conversely, when the provider is configured at the account-level, to deactivate the user when the resource is deleted, set `disable_as_user_deletion = true`.\n\n## Example Usage\n\nCreating regular user:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\", user_name=\"me@example.com\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user with administrative permissions - referencing special `admins` databricks.Group in databricks.GroupMember resource:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst i_am_admin = new databricks.GroupMember(\"i-am-admin\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\ni_am_admin = databricks.GroupMember(\"i-am-admin\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var i_am_admin = new Databricks.GroupMember(\"i-am-admin\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"i-am-admin\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var i_am_admin = new GroupMember(\"i-am-admin\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  i-am-admin:\n    type: databricks:GroupMember\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user with cluster create permissions:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst me = new databricks.User(\"me\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n    allowClusterCreate: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nme = databricks.User(\"me\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\",\n    allow_cluster_create=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n        AllowClusterCreate = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName:           pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName:        pulumi.String(\"Example user\"),\n\t\t\tAllowClusterCreate: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .allowClusterCreate(true)\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n      displayName: Example user\n      allowClusterCreate: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user in AWS Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountUser = new databricks.User(\"account_user\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_user = databricks.User(\"account_user\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountUser = new Databricks.User(\"account_user\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"account_user\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var accountUser = new User(\"accountUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountUser:\n    type: databricks:User\n    name: account_user\n    properties:\n      userName: me@example.com\n      displayName: Example user\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nCreating user in Azure Databricks account:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst accountUser = new databricks.User(\"account_user\", {\n    userName: \"me@example.com\",\n    displayName: \"Example user\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\naccount_user = databricks.User(\"account_user\",\n    user_name=\"me@example.com\",\n    display_name=\"Example user\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var accountUser = new Databricks.User(\"account_user\", new()\n    {\n        UserName = \"me@example.com\",\n        DisplayName = \"Example user\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewUser(ctx, \"account_user\", \u0026databricks.UserArgs{\n\t\t\tUserName:    pulumi.String(\"me@example.com\"),\n\t\t\tDisplayName: pulumi.String(\"Example user\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var accountUser = new User(\"accountUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .displayName(\"Example user\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  accountUser:\n    type: databricks:User\n    name: account_user\n    properties:\n      userName: me@example.com\n      displayName: Example user\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n* databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\nThe resource scim user can be imported using id:\n\nbash\n\n```sh\n$ pulumi import databricks:index/user:User me \u003cuser-id\u003e\n```\n\n",
            "properties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n"
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "required": [
                "aclPrincipalId",
                "disableAsUserDeletion",
                "displayName",
                "home",
                "repos",
                "userName"
            ],
            "inputProperties": {
                "aclPrincipalId": {
                    "type": "string",
                    "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                },
                "active": {
                    "type": "boolean",
                    "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                },
                "allowClusterCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                },
                "allowInstancePoolCreate": {
                    "type": "boolean",
                    "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                },
                "databricksSqlAccess": {
                    "type": "boolean",
                    "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                },
                "disableAsUserDeletion": {
                    "type": "boolean",
                    "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                },
                "displayName": {
                    "type": "string",
                    "description": "This is an alias for the username that can be the full name of the user.\n"
                },
                "externalId": {
                    "type": "string",
                    "description": "ID of the user in an external identity provider.\n"
                },
                "force": {
                    "type": "boolean"
                },
                "forceDeleteHomeDir": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                },
                "forceDeleteRepos": {
                    "type": "boolean",
                    "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                },
                "home": {
                    "type": "string",
                    "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                },
                "repos": {
                    "type": "string",
                    "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                },
                "userName": {
                    "type": "string",
                    "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n",
                    "willReplaceOnChanges": true
                },
                "workspaceAccess": {
                    "type": "boolean"
                }
            },
            "requiredInputs": [
                "userName"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering User resources.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Either user is active or not. True by default, but can be set to false in case of user deactivation with preserving user assets.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have cluster create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and `cluster_id` argument. Everyone without `allow_cluster_create` argument set, but with permission to use Cluster Policy would be able to create clusters, but within boundaries of that specific policy.\n"
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "Allow the user to have instance pool create privileges. Defaults to false. More fine grained permissions could be assigned with databricks.Permissions and instance_pool_id argument.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "description": "This is a field to allow the group to have access to [Databricks SQL](https://databricks.com/product/databricks-sql) feature in User Interface and through databricks_sql_endpoint.\n"
                    },
                    "disableAsUserDeletion": {
                        "type": "boolean",
                        "description": "Deactivate the user when deleting the resource, rather than deleting the user entirely. Defaults to `true` when the provider is configured at the account-level and `false` when configured at the workspace-level. This flag is exclusive to force_delete_repos and force_delete_home_dir flags.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "This is an alias for the username that can be the full name of the user.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the user in an external identity provider.\n"
                    },
                    "force": {
                        "type": "boolean"
                    },
                    "forceDeleteHomeDir": {
                        "type": "boolean",
                        "description": "This flag determines whether the user's home directory is deleted when the user is deleted. It will have not impact when in the accounts SCIM API. False by default.\n"
                    },
                    "forceDeleteRepos": {
                        "type": "boolean",
                        "description": "This flag determines whether the user's repo directory is deleted when the user is deleted. It will have no impact when in the accounts SCIM API. False by default.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n"
                    },
                    "userName": {
                        "type": "string",
                        "description": "This is the username of the given user and will be their form of access and identity.  Provided username will be converted to lower case if it contains upper case characters.\n",
                        "willReplaceOnChanges": true
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userInstanceProfile:UserInstanceProfile": {
            "description": "\u003e **Deprecated** Please rewrite with databricks_user_role. This resource will be removed in v0.5.x\n\nThis resource allows you to attach databricks.InstanceProfile (AWS) to databricks_user.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserInstanceProfile = new databricks.UserInstanceProfile(\"my_user_instance_profile\", {\n    userId: myUser.id,\n    instanceProfileId: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_instance_profile = databricks.UserInstanceProfile(\"my_user_instance_profile\",\n    user_id=my_user.id,\n    instance_profile_id=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserInstanceProfile = new Databricks.UserInstanceProfile(\"my_user_instance_profile\", new()\n    {\n        UserId = myUser.Id,\n        InstanceProfileId = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserInstanceProfile(ctx, \"my_user_instance_profile\", \u0026databricks.UserInstanceProfileArgs{\n\t\t\tUserId:            myUser.ID(),\n\t\t\tInstanceProfileId: instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserInstanceProfile;\nimport com.pulumi.databricks.UserInstanceProfileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserInstanceProfile = new UserInstanceProfile(\"myUserInstanceProfile\", UserInstanceProfileArgs.builder()        \n            .userId(myUser.id())\n            .instanceProfileId(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserInstanceProfile:\n    type: databricks:UserInstanceProfile\n    name: my_user_instance_profile\n    properties:\n      userId: ${myUser.id}\n      instanceProfileId: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "instanceProfileId",
                "userId"
            ],
            "inputProperties": {
                "instanceProfileId": {
                    "type": "string",
                    "description": "This is the id of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "instanceProfileId",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserInstanceProfile resources.\n",
                "properties": {
                    "instanceProfileId": {
                        "type": "string",
                        "description": "This is the id of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/userRole:UserRole": {
            "description": "This resource allows you to attach a role or databricks.InstanceProfile (AWS) to databricks_user.\n\n## Example Usage\n\nAdding AWS instance profile to a user\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst instanceProfile = new databricks.InstanceProfile(\"instance_profile\", {instanceProfileArn: \"my_instance_profile_arn\"});\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserRole = new databricks.UserRole(\"my_user_role\", {\n    userId: myUser.id,\n    role: instanceProfile.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ninstance_profile = databricks.InstanceProfile(\"instance_profile\", instance_profile_arn=\"my_instance_profile_arn\")\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_role = databricks.UserRole(\"my_user_role\",\n    user_id=my_user.id,\n    role=instance_profile.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var instanceProfile = new Databricks.InstanceProfile(\"instance_profile\", new()\n    {\n        InstanceProfileArn = \"my_instance_profile_arn\",\n    });\n\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserRole = new Databricks.UserRole(\"my_user_role\", new()\n    {\n        UserId = myUser.Id,\n        Role = instanceProfile.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tinstanceProfile, err := databricks.NewInstanceProfile(ctx, \"instance_profile\", \u0026databricks.InstanceProfileArgs{\n\t\t\tInstanceProfileArn: pulumi.String(\"my_instance_profile_arn\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"my_user_role\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   instanceProfile.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.InstanceProfile;\nimport com.pulumi.databricks.InstanceProfileArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var instanceProfile = new InstanceProfile(\"instanceProfile\", InstanceProfileArgs.builder()        \n            .instanceProfileArn(\"my_instance_profile_arn\")\n            .build());\n\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserRole = new UserRole(\"myUserRole\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(instanceProfile.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  instanceProfile:\n    type: databricks:InstanceProfile\n    name: instance_profile\n    properties:\n      instanceProfileArn: my_instance_profile_arn\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserRole:\n    type: databricks:UserRole\n    name: my_user_role\n    properties:\n      userId: ${myUser.id}\n      role: ${instanceProfile.id}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nAdding user as administrator to Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst myUser = new databricks.User(\"my_user\", {userName: \"me@example.com\"});\nconst myUserAccountAdmin = new databricks.UserRole(\"my_user_account_admin\", {\n    userId: myUser.id,\n    role: \"account_admin\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nmy_user = databricks.User(\"my_user\", user_name=\"me@example.com\")\nmy_user_account_admin = databricks.UserRole(\"my_user_account_admin\",\n    user_id=my_user.id,\n    role=\"account_admin\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var myUser = new Databricks.User(\"my_user\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myUserAccountAdmin = new Databricks.UserRole(\"my_user_account_admin\", new()\n    {\n        UserId = myUser.Id,\n        Role = \"account_admin\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tmyUser, err := databricks.NewUser(ctx, \"my_user\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewUserRole(ctx, \"my_user_account_admin\", \u0026databricks.UserRoleArgs{\n\t\t\tUserId: myUser.ID(),\n\t\t\tRole:   pulumi.String(\"account_admin\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.UserRole;\nimport com.pulumi.databricks.UserRoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var myUser = new User(\"myUser\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myUserAccountAdmin = new UserRole(\"myUserAccountAdmin\", UserRoleArgs.builder()        \n            .userId(myUser.id())\n            .role(\"account_admin\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myUser:\n    type: databricks:User\n    name: my_user\n    properties:\n      userName: me@example.com\n  myUserAccountAdmin:\n    type: databricks:UserRole\n    name: my_user_account_admin\n    properties:\n      userId: ${myUser.id}\n      role: account_admin\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n* databricks.User data to retrieve information about databricks_user.\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n"
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n"
                }
            },
            "required": [
                "role",
                "userId"
            ],
            "inputProperties": {
                "role": {
                    "type": "string",
                    "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                    "willReplaceOnChanges": true
                },
                "userId": {
                    "type": "string",
                    "description": "This is the id of the user resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "role",
                "userId"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering UserRole resources.\n",
                "properties": {
                    "role": {
                        "type": "string",
                        "description": "Either a role name or the ARN/ID of the instance profile resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "userId": {
                        "type": "string",
                        "description": "This is the id of the user resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/vectorSearchEndpoint:VectorSearchEndpoint": {
            "description": "\u003e **Note** This resource could be only used on Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Vector Search Endpoint](https://docs.databricks.com/en/generative-ai/vector-search.html) in Databricks.  Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database.  The Vector Search Endpoint is used to create and access vector search indexes.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.VectorSearchEndpoint(\"this\", {\n    name: \"vector-search-test\",\n    endpointType: \"STANDARD\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.VectorSearchEndpoint(\"this\",\n    name=\"vector-search-test\",\n    endpoint_type=\"STANDARD\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.VectorSearchEndpoint(\"this\", new()\n    {\n        Name = \"vector-search-test\",\n        EndpointType = \"STANDARD\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewVectorSearchEndpoint(ctx, \"this\", \u0026databricks.VectorSearchEndpointArgs{\n\t\t\tName:         pulumi.String(\"vector-search-test\"),\n\t\t\tEndpointType: pulumi.String(\"STANDARD\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.VectorSearchEndpoint;\nimport com.pulumi.databricks.VectorSearchEndpointArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new VectorSearchEndpoint(\"this\", VectorSearchEndpointArgs.builder()        \n            .name(\"vector-search-test\")\n            .endpointType(\"STANDARD\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:VectorSearchEndpoint\n    properties:\n      name: vector-search-test\n      endpointType: STANDARD\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Vector Search Endpoint\n\nbash\n\n```sh\n$ pulumi import databricks:index/vectorSearchEndpoint:VectorSearchEndpoint this \u003cendpoint-name\u003e\n```\n\n",
            "properties": {
                "creationTimestamp": {
                    "type": "integer",
                    "description": "Timestamp of endpoint creation (milliseconds).\n"
                },
                "creator": {
                    "type": "string",
                    "description": "Creator of the endpoint.\n"
                },
                "endpointId": {
                    "type": "string",
                    "description": "Unique internal identifier of the endpoint (UUID).\n"
                },
                "endpointStatuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus"
                    },
                    "description": "Object describing the current status of the endpoint consisting of the following fields:\n"
                },
                "endpointType": {
                    "type": "string",
                    "description": "Type of Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n"
                },
                "lastUpdatedTimestamp": {
                    "type": "integer",
                    "description": "Timestamp of the last update to the endpoint (milliseconds).\n"
                },
                "lastUpdatedUser": {
                    "type": "string",
                    "description": "User who last updated the endpoint.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Vector Search Endpoint to create.\n"
                },
                "numIndexes": {
                    "type": "integer",
                    "description": "Number of indexes on the endpoint.\n"
                }
            },
            "required": [
                "creationTimestamp",
                "creator",
                "endpointId",
                "endpointStatuses",
                "endpointType",
                "lastUpdatedTimestamp",
                "lastUpdatedUser",
                "name",
                "numIndexes"
            ],
            "inputProperties": {
                "endpointType": {
                    "type": "string",
                    "description": "Type of Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Vector Search Endpoint to create.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "endpointType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering VectorSearchEndpoint resources.\n",
                "properties": {
                    "creationTimestamp": {
                        "type": "integer",
                        "description": "Timestamp of endpoint creation (milliseconds).\n"
                    },
                    "creator": {
                        "type": "string",
                        "description": "Creator of the endpoint.\n"
                    },
                    "endpointId": {
                        "type": "string",
                        "description": "Unique internal identifier of the endpoint (UUID).\n"
                    },
                    "endpointStatuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/VectorSearchEndpointEndpointStatus:VectorSearchEndpointEndpointStatus"
                        },
                        "description": "Object describing the current status of the endpoint consisting of the following fields:\n"
                    },
                    "endpointType": {
                        "type": "string",
                        "description": "Type of Vector Search Endpoint.  Currently only accepting single value: `STANDARD` (See [documentation](https://docs.databricks.com/api/workspace/vectorsearchendpoints/createendpoint) for the list of currently supported values).\n",
                        "willReplaceOnChanges": true
                    },
                    "lastUpdatedTimestamp": {
                        "type": "integer",
                        "description": "Timestamp of the last update to the endpoint (milliseconds).\n"
                    },
                    "lastUpdatedUser": {
                        "type": "string",
                        "description": "User who last updated the endpoint.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Vector Search Endpoint to create.\n",
                        "willReplaceOnChanges": true
                    },
                    "numIndexes": {
                        "type": "integer",
                        "description": "Number of indexes on the endpoint.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/vectorSearchIndex:VectorSearchIndex": {
            "description": "\u003e **Note** This resource could be only used on Unity Catalog-enabled workspace!\n\nThis resource allows you to create [Vector Search Index](https://docs.databricks.com/en/generative-ai/create-query-vector-search.html) in Databricks.  Vector Search is a serverless similarity search engine that allows you to store a vector representation of your data, including metadata, in a vector database.  The Vector Search Index provides the ability to search data in the linked Delta Table.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sync = new databricks.VectorSearchIndex(\"sync\", {\n    name: \"main.default.vector_search_index\",\n    endpointName: thisDatabricksVectorSearchEndpoint.name,\n    primaryKey: \"id\",\n    indexType: \"DELTA_SYNC\",\n    deltaSyncIndexSpec: {\n        sourceTable: \"main.default.source_table\",\n        pipelineType: \"TRIGGERED\",\n        embeddingSourceColumns: [{\n            name: \"text\",\n            embeddingModelEndpointName: _this.name,\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsync = databricks.VectorSearchIndex(\"sync\",\n    name=\"main.default.vector_search_index\",\n    endpoint_name=this_databricks_vector_search_endpoint[\"name\"],\n    primary_key=\"id\",\n    index_type=\"DELTA_SYNC\",\n    delta_sync_index_spec=databricks.VectorSearchIndexDeltaSyncIndexSpecArgs(\n        source_table=\"main.default.source_table\",\n        pipeline_type=\"TRIGGERED\",\n        embedding_source_columns=[databricks.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs(\n            name=\"text\",\n            embedding_model_endpoint_name=this[\"name\"],\n        )],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sync = new Databricks.VectorSearchIndex(\"sync\", new()\n    {\n        Name = \"main.default.vector_search_index\",\n        EndpointName = thisDatabricksVectorSearchEndpoint.Name,\n        PrimaryKey = \"id\",\n        IndexType = \"DELTA_SYNC\",\n        DeltaSyncIndexSpec = new Databricks.Inputs.VectorSearchIndexDeltaSyncIndexSpecArgs\n        {\n            SourceTable = \"main.default.source_table\",\n            PipelineType = \"TRIGGERED\",\n            EmbeddingSourceColumns = new[]\n            {\n                new Databricks.Inputs.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs\n                {\n                    Name = \"text\",\n                    EmbeddingModelEndpointName = @this.Name,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewVectorSearchIndex(ctx, \"sync\", \u0026databricks.VectorSearchIndexArgs{\n\t\t\tName:         pulumi.String(\"main.default.vector_search_index\"),\n\t\t\tEndpointName: pulumi.Any(thisDatabricksVectorSearchEndpoint.Name),\n\t\t\tPrimaryKey:   pulumi.String(\"id\"),\n\t\t\tIndexType:    pulumi.String(\"DELTA_SYNC\"),\n\t\t\tDeltaSyncIndexSpec: \u0026databricks.VectorSearchIndexDeltaSyncIndexSpecArgs{\n\t\t\t\tSourceTable:  pulumi.String(\"main.default.source_table\"),\n\t\t\t\tPipelineType: pulumi.String(\"TRIGGERED\"),\n\t\t\t\tEmbeddingSourceColumns: databricks.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArray{\n\t\t\t\t\t\u0026databricks.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs{\n\t\t\t\t\t\tName:                       pulumi.String(\"text\"),\n\t\t\t\t\t\tEmbeddingModelEndpointName: pulumi.Any(this.Name),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.VectorSearchIndex;\nimport com.pulumi.databricks.VectorSearchIndexArgs;\nimport com.pulumi.databricks.inputs.VectorSearchIndexDeltaSyncIndexSpecArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sync = new VectorSearchIndex(\"sync\", VectorSearchIndexArgs.builder()        \n            .name(\"main.default.vector_search_index\")\n            .endpointName(thisDatabricksVectorSearchEndpoint.name())\n            .primaryKey(\"id\")\n            .indexType(\"DELTA_SYNC\")\n            .deltaSyncIndexSpec(VectorSearchIndexDeltaSyncIndexSpecArgs.builder()\n                .sourceTable(\"main.default.source_table\")\n                .pipelineType(\"TRIGGERED\")\n                .embeddingSourceColumns(VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumnArgs.builder()\n                    .name(\"text\")\n                    .embeddingModelEndpointName(this_.name())\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sync:\n    type: databricks:VectorSearchIndex\n    properties:\n      name: main.default.vector_search_index\n      endpointName: ${thisDatabricksVectorSearchEndpoint.name}\n      primaryKey: id\n      indexType: DELTA_SYNC\n      deltaSyncIndexSpec:\n        sourceTable: main.default.source_table\n        pipelineType: TRIGGERED\n        embeddingSourceColumns:\n          - name: text\n            embeddingModelEndpointName: ${this.name}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThe resource can be imported using the name of the Vector Search Index\n\nbash\n\n```sh\n$ pulumi import databricks:index/vectorSearchIndex:VectorSearchIndex this \u003cindex-name\u003e\n```\n\n",
            "properties": {
                "creator": {
                    "type": "string",
                    "description": "Creator of the endpoint.\n"
                },
                "deltaSyncIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                    "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`.\n"
                },
                "directAccessIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                    "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`.\n"
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Vector Search Endpoint that will be used for indexing the data.\n"
                },
                "indexType": {
                    "type": "string",
                    "description": "Vector Search index type. Currently supported values are:\n"
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n"
                },
                "primaryKey": {
                    "type": "string",
                    "description": "The column name that will be used as a primary key.\n"
                },
                "statuses": {
                    "type": "array",
                    "items": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus"
                    },
                    "description": "Object describing the current status of the index consisting of the following fields:\n"
                }
            },
            "required": [
                "creator",
                "endpointName",
                "indexType",
                "name",
                "primaryKey",
                "statuses"
            ],
            "inputProperties": {
                "deltaSyncIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                    "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`.\n",
                    "willReplaceOnChanges": true
                },
                "directAccessIndexSpec": {
                    "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                    "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`.\n",
                    "willReplaceOnChanges": true
                },
                "endpointName": {
                    "type": "string",
                    "description": "The name of the Vector Search Endpoint that will be used for indexing the data.\n",
                    "willReplaceOnChanges": true
                },
                "indexType": {
                    "type": "string",
                    "description": "Vector Search index type. Currently supported values are:\n",
                    "willReplaceOnChanges": true
                },
                "name": {
                    "type": "string",
                    "description": "The name of the column.\n",
                    "willReplaceOnChanges": true
                },
                "primaryKey": {
                    "type": "string",
                    "description": "The column name that will be used as a primary key.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "endpointName",
                "indexType",
                "primaryKey"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering VectorSearchIndex resources.\n",
                "properties": {
                    "creator": {
                        "type": "string",
                        "description": "Creator of the endpoint.\n"
                    },
                    "deltaSyncIndexSpec": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDeltaSyncIndexSpec:VectorSearchIndexDeltaSyncIndexSpec",
                        "description": "Specification for Delta Sync Index. Required if `index_type` is `DELTA_SYNC`.\n",
                        "willReplaceOnChanges": true
                    },
                    "directAccessIndexSpec": {
                        "$ref": "#/types/databricks:index/VectorSearchIndexDirectAccessIndexSpec:VectorSearchIndexDirectAccessIndexSpec",
                        "description": "Specification for Direct Vector Access Index. Required if `index_type` is `DIRECT_ACCESS`.\n",
                        "willReplaceOnChanges": true
                    },
                    "endpointName": {
                        "type": "string",
                        "description": "The name of the Vector Search Endpoint that will be used for indexing the data.\n",
                        "willReplaceOnChanges": true
                    },
                    "indexType": {
                        "type": "string",
                        "description": "Vector Search index type. Currently supported values are:\n",
                        "willReplaceOnChanges": true
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the column.\n",
                        "willReplaceOnChanges": true
                    },
                    "primaryKey": {
                        "type": "string",
                        "description": "The column name that will be used as a primary key.\n",
                        "willReplaceOnChanges": true
                    },
                    "statuses": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/VectorSearchIndexStatus:VectorSearchIndexStatus"
                        },
                        "description": "Object describing the current status of the index consisting of the following fields:\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/volume:Volume": {
            "description": "\u003e **Public Preview** This feature is in [Public Preview](https://docs.databricks.com/release-notes/release-types.html).\n\n\u003e **Note** This resource could be only used with workspace-level provider!\n\nVolumes are Unity Catalog objects representing a logical volume of storage in a cloud object storage location. Volumes provide capabilities for accessing, storing, governing, and organizing files. While tables provide governance over tabular datasets, volumes add governance over non-tabular datasets. You can use volumes to store and access files in any format, including structured, semi-structured, and unstructured data.\n\nA volume resides in the third layer of Unity Catalog’s three-level namespace. Volumes are siblings to tables, views, and other objects organized under a schema in Unity Catalog.\n\nA volume can be **managed** or **external**.\n\nA **managed volume** is a Unity Catalog-governed storage volume created within the default storage location of the containing schema. Managed volumes allow the creation of governed storage for working with files without the overhead of external locations and storage credentials. You do not need to specify a location when creating a managed volume, and all file access for data in managed volumes is through paths managed by Unity Catalog.\n\nAn **external volume** is a Unity Catalog-governed storage volume registered against a directory within an external location.\n\nA volume can be referenced using its identifier: ```\u003ccatalogName\u003e.\u003cschemaName\u003e.\u003cvolumeName\u003e```, where:\n\n* ```\u003ccatalogName\u003e```: The name of the catalog containing the Volume.\n* ```\u003cschemaName\u003e```: The name of the schema containing the Volume.\n* ```\u003cvolumeName\u003e```: The name of the Volume. It identifies the volume object.\n\nThe path to access files in volumes uses the following format:\n\n```/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/\u003cpath\u003e/\u003cfile_name\u003e```\n\nDatabricks also supports an optional ```dbfs:/``` scheme, so the following path also works:\n\n```dbfs:/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cvolume\u003e/\u003cpath\u003e/\u003cfile_name\u003e```\n\nThis resource manages Volumes in Unity Catalog.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = new databricks.Catalog(\"sandbox\", {\n    name: \"sandbox\",\n    comment: \"this catalog is managed by terraform\",\n    properties: {\n        purpose: \"testing\",\n    },\n});\nconst things = new databricks.Schema(\"things\", {\n    catalogName: sandbox.name,\n    name: \"things\",\n    comment: \"this schema is managed by terraform\",\n    properties: {\n        kind: \"various\",\n    },\n});\nconst external = new databricks.StorageCredential(\"external\", {\n    name: \"creds\",\n    awsIamRole: {\n        roleArn: externalDataAccess.arn,\n    },\n});\nconst some = new databricks.ExternalLocation(\"some\", {\n    name: \"external-location\",\n    url: `s3://${externalAwsS3Bucket.id}/some`,\n    credentialName: external.name,\n});\nconst _this = new databricks.Volume(\"this\", {\n    name: \"quickstart_volume\",\n    catalogName: sandbox.name,\n    schemaName: things.name,\n    volumeType: \"EXTERNAL\",\n    storageLocation: some.url,\n    comment: \"this volume is managed by terraform\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.Catalog(\"sandbox\",\n    name=\"sandbox\",\n    comment=\"this catalog is managed by terraform\",\n    properties={\n        \"purpose\": \"testing\",\n    })\nthings = databricks.Schema(\"things\",\n    catalog_name=sandbox.name,\n    name=\"things\",\n    comment=\"this schema is managed by terraform\",\n    properties={\n        \"kind\": \"various\",\n    })\nexternal = databricks.StorageCredential(\"external\",\n    name=\"creds\",\n    aws_iam_role=databricks.StorageCredentialAwsIamRoleArgs(\n        role_arn=external_data_access[\"arn\"],\n    ))\nsome = databricks.ExternalLocation(\"some\",\n    name=\"external-location\",\n    url=f\"s3://{external_aws_s3_bucket['id']}/some\",\n    credential_name=external.name)\nthis = databricks.Volume(\"this\",\n    name=\"quickstart_volume\",\n    catalog_name=sandbox.name,\n    schema_name=things.name,\n    volume_type=\"EXTERNAL\",\n    storage_location=some.url,\n    comment=\"this volume is managed by terraform\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = new Databricks.Catalog(\"sandbox\", new()\n    {\n        Name = \"sandbox\",\n        Comment = \"this catalog is managed by terraform\",\n        Properties = \n        {\n            { \"purpose\", \"testing\" },\n        },\n    });\n\n    var things = new Databricks.Schema(\"things\", new()\n    {\n        CatalogName = sandbox.Name,\n        Name = \"things\",\n        Comment = \"this schema is managed by terraform\",\n        Properties = \n        {\n            { \"kind\", \"various\" },\n        },\n    });\n\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        Name = \"creds\",\n        AwsIamRole = new Databricks.Inputs.StorageCredentialAwsIamRoleArgs\n        {\n            RoleArn = externalDataAccess.Arn,\n        },\n    });\n\n    var some = new Databricks.ExternalLocation(\"some\", new()\n    {\n        Name = \"external-location\",\n        Url = $\"s3://{externalAwsS3Bucket.Id}/some\",\n        CredentialName = external.Name,\n    });\n\n    var @this = new Databricks.Volume(\"this\", new()\n    {\n        Name = \"quickstart_volume\",\n        CatalogName = sandbox.Name,\n        SchemaName = things.Name,\n        VolumeType = \"EXTERNAL\",\n        StorageLocation = some.Url,\n        Comment = \"this volume is managed by terraform\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.NewCatalog(ctx, \"sandbox\", \u0026databricks.CatalogArgs{\n\t\t\tName:    pulumi.String(\"sandbox\"),\n\t\t\tComment: pulumi.String(\"this catalog is managed by terraform\"),\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"purpose\": pulumi.Any(\"testing\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthings, err := databricks.NewSchema(ctx, \"things\", \u0026databricks.SchemaArgs{\n\t\t\tCatalogName: sandbox.Name,\n\t\t\tName:        pulumi.String(\"things\"),\n\t\t\tComment:     pulumi.String(\"this schema is managed by terraform\"),\n\t\t\tProperties: pulumi.Map{\n\t\t\t\t\"kind\": pulumi.Any(\"various\"),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\texternal, err := databricks.NewStorageCredential(ctx, \"external\", \u0026databricks.StorageCredentialArgs{\n\t\t\tName: pulumi.String(\"creds\"),\n\t\t\tAwsIamRole: \u0026databricks.StorageCredentialAwsIamRoleArgs{\n\t\t\t\tRoleArn: pulumi.Any(externalDataAccess.Arn),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tsome, err := databricks.NewExternalLocation(ctx, \"some\", \u0026databricks.ExternalLocationArgs{\n\t\t\tName:           pulumi.String(\"external-location\"),\n\t\t\tUrl:            pulumi.String(fmt.Sprintf(\"s3://%v/some\", externalAwsS3Bucket.Id)),\n\t\t\tCredentialName: external.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewVolume(ctx, \"this\", \u0026databricks.VolumeArgs{\n\t\t\tName:            pulumi.String(\"quickstart_volume\"),\n\t\t\tCatalogName:     sandbox.Name,\n\t\t\tSchemaName:      things.Name,\n\t\t\tVolumeType:      pulumi.String(\"EXTERNAL\"),\n\t\t\tStorageLocation: some.Url,\n\t\t\tComment:         pulumi.String(\"this volume is managed by terraform\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Catalog;\nimport com.pulumi.databricks.CatalogArgs;\nimport com.pulumi.databricks.Schema;\nimport com.pulumi.databricks.SchemaArgs;\nimport com.pulumi.databricks.StorageCredential;\nimport com.pulumi.databricks.StorageCredentialArgs;\nimport com.pulumi.databricks.inputs.StorageCredentialAwsIamRoleArgs;\nimport com.pulumi.databricks.ExternalLocation;\nimport com.pulumi.databricks.ExternalLocationArgs;\nimport com.pulumi.databricks.Volume;\nimport com.pulumi.databricks.VolumeArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var sandbox = new Catalog(\"sandbox\", CatalogArgs.builder()        \n            .name(\"sandbox\")\n            .comment(\"this catalog is managed by terraform\")\n            .properties(Map.of(\"purpose\", \"testing\"))\n            .build());\n\n        var things = new Schema(\"things\", SchemaArgs.builder()        \n            .catalogName(sandbox.name())\n            .name(\"things\")\n            .comment(\"this schema is managed by terraform\")\n            .properties(Map.of(\"kind\", \"various\"))\n            .build());\n\n        var external = new StorageCredential(\"external\", StorageCredentialArgs.builder()        \n            .name(\"creds\")\n            .awsIamRole(StorageCredentialAwsIamRoleArgs.builder()\n                .roleArn(externalDataAccess.arn())\n                .build())\n            .build());\n\n        var some = new ExternalLocation(\"some\", ExternalLocationArgs.builder()        \n            .name(\"external-location\")\n            .url(String.format(\"s3://%s/some\", externalAwsS3Bucket.id()))\n            .credentialName(external.name())\n            .build());\n\n        var this_ = new Volume(\"this\", VolumeArgs.builder()        \n            .name(\"quickstart_volume\")\n            .catalogName(sandbox.name())\n            .schemaName(things.name())\n            .volumeType(\"EXTERNAL\")\n            .storageLocation(some.url())\n            .comment(\"this volume is managed by terraform\")\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  sandbox:\n    type: databricks:Catalog\n    properties:\n      name: sandbox\n      comment: this catalog is managed by terraform\n      properties:\n        purpose: testing\n  things:\n    type: databricks:Schema\n    properties:\n      catalogName: ${sandbox.name}\n      name: things\n      comment: this schema is managed by terraform\n      properties:\n        kind: various\n  external:\n    type: databricks:StorageCredential\n    properties:\n      name: creds\n      awsIamRole:\n        roleArn: ${externalDataAccess.arn}\n  some:\n    type: databricks:ExternalLocation\n    properties:\n      name: external-location\n      url: s3://${externalAwsS3Bucket.id}/some\n      credentialName: ${external.name}\n  this:\n    type: databricks:Volume\n    properties:\n      name: quickstart_volume\n      catalogName: ${sandbox.name}\n      schemaName: ${things.name}\n      volumeType: EXTERNAL\n      storageLocation: ${some.url}\n      comment: this volume is managed by terraform\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\nThis resource can be imported by `full_name` which is the 3-level Volume identifier: `\u003ccatalog\u003e.\u003cschema\u003e.\u003cname\u003e`\n\nbash\n\n```sh\n$ pulumi import databricks:index/volume:Volume this \u003ccatalog_name\u003e.\u003cschema_name\u003e.\u003cname\u003e\n```\n\n",
            "properties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent Catalog. Change forces creation of a new resource.\n"
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Volume\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the volume owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n"
                },
                "storageLocation": {
                    "type": "string",
                    "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n"
                },
                "volumePath": {
                    "type": "string",
                    "description": "base file path for this Unity Catalog Volume in form of `/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cname\u003e`.\n"
                },
                "volumeType": {
                    "type": "string",
                    "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n"
                }
            },
            "required": [
                "catalogName",
                "name",
                "owner",
                "schemaName",
                "volumePath",
                "volumeType"
            ],
            "inputProperties": {
                "catalogName": {
                    "type": "string",
                    "description": "Name of parent Catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "comment": {
                    "type": "string",
                    "description": "Free-form text.\n"
                },
                "name": {
                    "type": "string",
                    "description": "Name of the Volume\n"
                },
                "owner": {
                    "type": "string",
                    "description": "Name of the volume owner.\n"
                },
                "schemaName": {
                    "type": "string",
                    "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "storageLocation": {
                    "type": "string",
                    "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                },
                "volumeType": {
                    "type": "string",
                    "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n",
                    "willReplaceOnChanges": true
                }
            },
            "requiredInputs": [
                "catalogName",
                "schemaName",
                "volumeType"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering Volume resources.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of parent Catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "comment": {
                        "type": "string",
                        "description": "Free-form text.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the Volume\n"
                    },
                    "owner": {
                        "type": "string",
                        "description": "Name of the volume owner.\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of parent Schema relative to parent Catalog. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "storageLocation": {
                        "type": "string",
                        "description": "Path inside an External Location. Only used for `EXTERNAL` Volumes. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    },
                    "volumePath": {
                        "type": "string",
                        "description": "base file path for this Unity Catalog Volume in form of `/Volumes/\u003ccatalog\u003e/\u003cschema\u003e/\u003cname\u003e`.\n"
                    },
                    "volumeType": {
                        "type": "string",
                        "description": "Volume type. `EXTERNAL` or `MANAGED`. Change forces creation of a new resource.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceConf:WorkspaceConf": {
            "description": "## Example Usage\n\nAllows specification of custom configuration properties for expert usage:\n\n* `enableIpAccessLists` - enables the use of databricks.IpAccessList resources\n* `maxTokenLifetimeDays` - (string) Maximum token lifetime of new tokens in days, as an integer. If zero, new tokens are permitted to have no lifetime limit. Negative numbers are unsupported. **WARNING:** This limit only applies to new tokens, so there may be tokens with lifetimes longer than this value, including unlimited lifetime. Such tokens may have been created before the current maximum token lifetime was set.\n* `enableTokensConfig` - (boolean) Enable or disable personal access tokens for this workspace.\n* `enableDeprecatedClusterNamedInitScripts` - (boolean) Enable or disable [legacy cluster-named init scripts](https://docs.databricks.com/clusters/init-scripts.html#disable-legacy-cluster-named-init-scripts-for-a-workspace) for this workspace.\n* `enableDeprecatedGlobalInitScripts` - (boolean) Enable or disable [legacy global init scripts](https://docs.databricks.com/clusters/init-scripts.html#migrate-legacy-scripts) for this workspace.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst _this = new databricks.WorkspaceConf(\"this\", {customConfig: {\n    enableIpAccessLists: true,\n}});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.WorkspaceConf(\"this\", custom_config={\n    \"enableIpAccessLists\": True,\n})\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = new Databricks.WorkspaceConf(\"this\", new()\n    {\n        CustomConfig = \n        {\n            { \"enableIpAccessLists\", true },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewWorkspaceConf(ctx, \"this\", \u0026databricks.WorkspaceConfArgs{\n\t\t\tCustomConfig: pulumi.Map{\n\t\t\t\t\"enableIpAccessLists\": pulumi.Any(true),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.WorkspaceConf;\nimport com.pulumi.databricks.WorkspaceConfArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new WorkspaceConf(\"this\", WorkspaceConfArgs.builder()        \n            .customConfig(Map.of(\"enableIpAccessLists\", true))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: databricks:WorkspaceConf\n    properties:\n      customConfig:\n        enableIpAccessLists: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Import\n\n-\u003e **Note** Importing this resource is not currently supported.\n\n",
            "properties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "inputProperties": {
                "customConfig": {
                    "type": "object",
                    "additionalProperties": {
                        "$ref": "pulumi.json#/Any"
                    },
                    "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                }
            },
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceConf resources.\n",
                "properties": {
                    "customConfig": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Key-value map of strings that represent workspace configuration. Upon resource deletion, properties that start with `enable` or `enforce` will be reset to `false` value, regardless of initial default one.\n"
                    }
                },
                "type": "object"
            }
        },
        "databricks:index/workspaceFile:WorkspaceFile": {
            "description": "\n\n## Import\n\nThe workspace file resource can be imported using workspace file path\n\nbash\n\n```sh\n$ pulumi import databricks:index/workspaceFile:WorkspaceFile this /path/to/file\n```\n\n",
            "properties": {
                "contentBase64": {
                    "type": "string"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a workspace file\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n"
                },
                "source": {
                    "type": "string",
                    "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                },
                "url": {
                    "type": "string",
                    "description": "Routable URL of the workspace file\n"
                },
                "workspacePath": {
                    "type": "string",
                    "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                }
            },
            "required": [
                "objectId",
                "path",
                "url",
                "workspacePath"
            ],
            "inputProperties": {
                "contentBase64": {
                    "type": "string"
                },
                "md5": {
                    "type": "string"
                },
                "objectId": {
                    "type": "integer",
                    "description": "Unique identifier for a workspace file\n"
                },
                "path": {
                    "type": "string",
                    "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n",
                    "willReplaceOnChanges": true
                },
                "source": {
                    "type": "string",
                    "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                }
            },
            "requiredInputs": [
                "path"
            ],
            "stateInputs": {
                "description": "Input properties used for looking up and filtering WorkspaceFile resources.\n",
                "properties": {
                    "contentBase64": {
                        "type": "string"
                    },
                    "md5": {
                        "type": "string"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "Unique identifier for a workspace file\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "The absolute path of the workspace file, beginning with \"/\", e.g. \"/Demo\".\n",
                        "willReplaceOnChanges": true
                    },
                    "source": {
                        "type": "string",
                        "description": "Path to file on local filesystem. Conflicts with `content_base64`.\n"
                    },
                    "url": {
                        "type": "string",
                        "description": "Routable URL of the workspace file\n"
                    },
                    "workspacePath": {
                        "type": "string",
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n"
                    }
                },
                "type": "object"
            }
        }
    },
    "functions": {
        "databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy": {
            "description": "This data source constructs necessary AWS STS assume role policy for you.\n\n## Example Usage\n\nEnd-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst config = new pulumi.Config();\n// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\nconst databricksAccountId = config.requireObject(\"databricksAccountId\");\nconst this = databricks.getAwsCrossAccountPolicy({});\nconst crossAccountPolicy = new aws.iam.Policy(\"cross_account_policy\", {\n    name: `${prefix}-crossaccount-iam-policy`,\n    policy: _this.then(_this =\u003e _this.json),\n});\nconst thisGetAwsAssumeRolePolicy = databricks.getAwsAssumeRolePolicy({\n    externalId: databricksAccountId,\n});\nconst crossAccount = new aws.iam.Role(\"cross_account\", {\n    name: `${prefix}-crossaccount-iam-role`,\n    assumeRolePolicy: thisGetAwsAssumeRolePolicy.then(thisGetAwsAssumeRolePolicy =\u003e thisGetAwsAssumeRolePolicy.json),\n    description: \"Grants Databricks full access to VPC resources\",\n});\nconst crossAccountRolePolicyAttachment = new aws.iam.RolePolicyAttachment(\"cross_account\", {\n    policyArn: crossAccountPolicy.arn,\n    role: crossAccount.name,\n});\n// required only in case of multi-workspace setup\nconst thisMwsCredentials = new databricks.MwsCredentials(\"this\", {\n    accountId: databricksAccountId,\n    credentialsName: `${prefix}-creds`,\n    roleArn: crossAccount.arn,\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nconfig = pulumi.Config()\n# Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\ndatabricks_account_id = config.require_object(\"databricksAccountId\")\nthis = databricks.get_aws_cross_account_policy()\ncross_account_policy = aws.iam.Policy(\"cross_account_policy\",\n    name=f\"{prefix}-crossaccount-iam-policy\",\n    policy=this.json)\nthis_get_aws_assume_role_policy = databricks.get_aws_assume_role_policy(external_id=databricks_account_id)\ncross_account = aws.iam.Role(\"cross_account\",\n    name=f\"{prefix}-crossaccount-iam-role\",\n    assume_role_policy=this_get_aws_assume_role_policy.json,\n    description=\"Grants Databricks full access to VPC resources\")\ncross_account_role_policy_attachment = aws.iam.RolePolicyAttachment(\"cross_account\",\n    policy_arn=cross_account_policy.arn,\n    role=cross_account.name)\n# required only in case of multi-workspace setup\nthis_mws_credentials = databricks.MwsCredentials(\"this\",\n    account_id=databricks_account_id,\n    credentials_name=f\"{prefix}-creds\",\n    role_arn=cross_account.arn)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var config = new Config();\n    // Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n    var databricksAccountId = config.RequireObject\u003cdynamic\u003e(\"databricksAccountId\");\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n    var crossAccountPolicy = new Aws.Iam.Policy(\"cross_account_policy\", new()\n    {\n        Name = $\"{prefix}-crossaccount-iam-policy\",\n        PolicyDocument = @this.Apply(@this =\u003e @this.Apply(getAwsCrossAccountPolicyResult =\u003e getAwsCrossAccountPolicyResult.Json)),\n    });\n\n    var thisGetAwsAssumeRolePolicy = Databricks.GetAwsAssumeRolePolicy.Invoke(new()\n    {\n        ExternalId = databricksAccountId,\n    });\n\n    var crossAccount = new Aws.Iam.Role(\"cross_account\", new()\n    {\n        Name = $\"{prefix}-crossaccount-iam-role\",\n        AssumeRolePolicy = thisGetAwsAssumeRolePolicy.Apply(getAwsAssumeRolePolicyResult =\u003e getAwsAssumeRolePolicyResult.Json),\n        Description = \"Grants Databricks full access to VPC resources\",\n    });\n\n    var crossAccountRolePolicyAttachment = new Aws.Iam.RolePolicyAttachment(\"cross_account\", new()\n    {\n        PolicyArn = crossAccountPolicy.Arn,\n        Role = crossAccount.Name,\n    });\n\n    // required only in case of multi-workspace setup\n    var thisMwsCredentials = new Databricks.MwsCredentials(\"this\", new()\n    {\n        AccountId = databricksAccountId,\n        CredentialsName = $\"{prefix}-creds\",\n        RoleArn = crossAccount.Arn,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi/config\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tcfg := config.New(ctx, \"\")\n\t\t// Account Id that could be found in the top right corner of https://accounts.cloud.databricks.com/\n\t\tdatabricksAccountId := cfg.RequireObject(\"databricksAccountId\")\n\t\tthis, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccountPolicy, err := iam.NewPolicy(ctx, \"cross_account_policy\", \u0026iam.PolicyArgs{\n\t\t\tName:   pulumi.String(fmt.Sprintf(\"%v-crossaccount-iam-policy\", prefix)),\n\t\t\tPolicy: pulumi.String(this.Json),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthisGetAwsAssumeRolePolicy, err := databricks.GetAwsAssumeRolePolicy(ctx, \u0026databricks.GetAwsAssumeRolePolicyArgs{\n\t\t\tExternalId: databricksAccountId,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tcrossAccount, err := iam.NewRole(ctx, \"cross_account\", \u0026iam.RoleArgs{\n\t\t\tName:             pulumi.String(fmt.Sprintf(\"%v-crossaccount-iam-role\", prefix)),\n\t\t\tAssumeRolePolicy: pulumi.String(thisGetAwsAssumeRolePolicy.Json),\n\t\t\tDescription:      pulumi.String(\"Grants Databricks full access to VPC resources\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = iam.NewRolePolicyAttachment(ctx, \"cross_account\", \u0026iam.RolePolicyAttachmentArgs{\n\t\t\tPolicyArn: crossAccountPolicy.Arn,\n\t\t\tRole:      crossAccount.Name,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t// required only in case of multi-workspace setup\n\t\t_, err = databricks.NewMwsCredentials(ctx, \"this\", \u0026databricks.MwsCredentialsArgs{\n\t\t\tAccountId:       pulumi.Any(databricksAccountId),\n\t\t\tCredentialsName: pulumi.String(fmt.Sprintf(\"%v-creds\", prefix)),\n\t\t\tRoleArn:         crossAccount.Arn,\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport com.pulumi.aws.iam.RolePolicyAttachment;\nimport com.pulumi.aws.iam.RolePolicyAttachmentArgs;\nimport com.pulumi.databricks.MwsCredentials;\nimport com.pulumi.databricks.MwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var config = ctx.config();\n        final var databricksAccountId = config.get(\"databricksAccountId\");\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n        var crossAccountPolicy = new Policy(\"crossAccountPolicy\", PolicyArgs.builder()        \n            .name(String.format(\"%s-crossaccount-iam-policy\", prefix))\n            .policy(this_.json())\n            .build());\n\n        final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()\n            .externalId(databricksAccountId)\n            .build());\n\n        var crossAccount = new Role(\"crossAccount\", RoleArgs.builder()        \n            .name(String.format(\"%s-crossaccount-iam-role\", prefix))\n            .assumeRolePolicy(thisGetAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -\u003e getAwsAssumeRolePolicyResult.json()))\n            .description(\"Grants Databricks full access to VPC resources\")\n            .build());\n\n        var crossAccountRolePolicyAttachment = new RolePolicyAttachment(\"crossAccountRolePolicyAttachment\", RolePolicyAttachmentArgs.builder()        \n            .policyArn(crossAccountPolicy.arn())\n            .role(crossAccount.name())\n            .build());\n\n        // required only in case of multi-workspace setup\n        var thisMwsCredentials = new MwsCredentials(\"thisMwsCredentials\", MwsCredentialsArgs.builder()        \n            .accountId(databricksAccountId)\n            .credentialsName(String.format(\"%s-creds\", prefix))\n            .roleArn(crossAccount.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nconfiguration:\n  databricksAccountId:\n    type: dynamic\nresources:\n  crossAccountPolicy:\n    type: aws:iam:Policy\n    name: cross_account_policy\n    properties:\n      name: ${prefix}-crossaccount-iam-policy\n      policy: ${this.json}\n  crossAccount:\n    type: aws:iam:Role\n    name: cross_account\n    properties:\n      name: ${prefix}-crossaccount-iam-role\n      assumeRolePolicy: ${thisGetAwsAssumeRolePolicy.json}\n      description: Grants Databricks full access to VPC resources\n  crossAccountRolePolicyAttachment:\n    type: aws:iam:RolePolicyAttachment\n    name: cross_account\n    properties:\n      policyArn: ${crossAccountPolicy.arn}\n      role: ${crossAccount.name}\n  # required only in case of multi-workspace setup\n  thisMwsCredentials:\n    type: databricks:MwsCredentials\n    name: this\n    properties:\n      accountId: ${databricksAccountId}\n      credentialsName: ${prefix}-creds\n      roleArn: ${crossAccount.arn}\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n  thisGetAwsAssumeRolePolicy:\n    fn::invoke:\n      Function: databricks:getAwsAssumeRolePolicy\n      Arguments:\n        externalId: ${databricksAccountId}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "Account Id that could be found in the top right corner of [Accounts Console](https://accounts.cloud.databricks.com/).\n",
                        "willReplaceOnChanges": true
                    },
                    "forLogDelivery": {
                        "type": "boolean",
                        "description": "Either or not this assume role policy should be created for usage log delivery. Defaults to false.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "externalId"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsAssumeRolePolicy.\n",
                "properties": {
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "forLogDelivery": {
                        "type": "boolean"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    }
                },
                "required": [
                    "externalId",
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsBucketPolicy:getAwsBucketPolicy": {
            "description": "This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.aws.s3.BucketV2;\nimport com.pulumi.aws.s3.BucketV2Args;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;\nimport com.pulumi.aws.s3.BucketPolicy;\nimport com.pulumi.aws.s3.BucketPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var this_ = new BucketV2(\"this\", BucketV2Args.builder()        \n            .bucket(\"\u003cunique_bucket_name\u003e\")\n            .acl(\"private\")\n            .forceDestroy(true)\n            .build());\n\n        final var stuff = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()\n            .bucketName(this_.bucket())\n            .build());\n\n        var thisBucketPolicy = new BucketPolicy(\"thisBucketPolicy\", BucketPolicyArgs.builder()        \n            .bucket(this_.id())\n            .policy(thisDatabricksAwsBucketPolicy.json())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  this:\n    type: aws:s3:BucketV2\n    properties:\n      bucket: \u003cunique_bucket_name\u003e\n      acl: private\n      forceDestroy: true\n  thisBucketPolicy:\n    type: aws:s3:BucketPolicy\n    name: this\n    properties:\n      bucket: ${this.id}\n      policy: ${thisDatabricksAwsBucketPolicy.json}\nvariables:\n  stuff:\n    fn::invoke:\n      Function: databricks:getAwsBucketPolicy\n      Arguments:\n        bucketName: ${this.bucket}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nBucket policy with full access:\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string",
                        "description": "AWS S3 Bucket name for which to generate the policy document.\n",
                        "willReplaceOnChanges": true
                    },
                    "databricksAccountId": {
                        "type": "string",
                        "willReplaceOnChanges": true
                    },
                    "databricksE2AccountId": {
                        "type": "string",
                        "description": "Your Databricks E2 account ID. Used to generate  restrictive IAM policies that will increase the security of your root bucket\n",
                        "willReplaceOnChanges": true
                    },
                    "fullAccessRole": {
                        "type": "string",
                        "description": "Data access role that can have full access for this bucket\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "bucket"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsBucketPolicy.\n",
                "properties": {
                    "bucket": {
                        "type": "string"
                    },
                    "databricksAccountId": {
                        "type": "string"
                    },
                    "databricksE2AccountId": {
                        "type": "string"
                    },
                    "fullAccessRole": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "(Read-only) AWS IAM Policy JSON document to grant Databricks full access to bucket.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "bucket",
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy": {
            "description": "\u003e **Note** This data source could be only used with account-level provider!\n\nThis data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).\n\n## Example Usage\n\nFor more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getAwsCrossAccountPolicy({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_cross_account_policy()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsCrossAccountPolicy.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetAwsCrossAccountPolicy(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsCrossAccountPolicy();\n\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getAwsCrossAccountPolicy\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning AWS Databricks E2 with a Hub \u0026 Spoke firewall for data exfiltration protection guide\n* databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.\n* databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsCrossAccountPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string",
                        "description": "— Your AWS account ID, which is a number.\n",
                        "willReplaceOnChanges": true
                    },
                    "passRoles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of Data IAM role ARNs that are explicitly granted `iam:PassRole` action.\nThe below arguments are only valid for `restricted` policy type\n",
                        "willReplaceOnChanges": true
                    },
                    "policyType": {
                        "type": "string",
                        "description": "The type of cross account policy to generated: `managed` for Databricks-managed VPC and `customer` for customer-managed VPC, `restricted` for customer-managed VPC with policy restrictions\n",
                        "willReplaceOnChanges": true
                    },
                    "region": {
                        "type": "string",
                        "description": "— AWS Region name for your VPC deployment, for example `us-west-2`.\n",
                        "willReplaceOnChanges": true
                    },
                    "securityGroupId": {
                        "type": "string",
                        "description": "— ID of your AWS security group. When you add a security group restriction, you cannot reuse the cross-account IAM role or reference a credentials ID (`credentials_id`) for any other workspaces. For those other workspaces, you must create separate roles, policies, and credentials objects.\n",
                        "willReplaceOnChanges": true
                    },
                    "vpcId": {
                        "type": "string",
                        "description": "— ID of the AWS VPC where you want to launch workspaces.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getAwsCrossAccountPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    },
                    "passRoles": {
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "policyType": {
                        "type": "string"
                    },
                    "region": {
                        "type": "string"
                    },
                    "securityGroupId": {
                        "type": "string"
                    },
                    "vpcId": {
                        "type": "string"
                    }
                },
                "required": [
                    "json",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy": {
            "description": "\u003e **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default) in case of any questions.\n\nThis data source constructs necessary AWS Unity Catalog policy for you, which is based on [official documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as aws from \"@pulumi/aws\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getAwsUnityCatalogPolicy({\n    awsAccountId: awsAccountId,\n    bucketName: \"databricks-bucket\",\n    roleName: \"databricks-role\",\n    kmsName: \"databricks-kms\",\n});\nconst passroleForUc = aws.iam.getPolicyDocument({\n    statements: [\n        {\n            effect: \"Allow\",\n            actions: [\"sts:AssumeRole\"],\n            principals: [{\n                identifiers: [\"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\"],\n                type: \"AWS\",\n            }],\n            conditions: [{\n                test: \"StringEquals\",\n                variable: \"sts:ExternalId\",\n                values: [databricksAccountId],\n            }],\n        },\n        {\n            sid: \"ExplicitSelfRoleAssumption\",\n            effect: \"Allow\",\n            actions: [\"sts:AssumeRole\"],\n            principals: [{\n                type: \"AWS\",\n                identifiers: [`arn:aws:iam::${awsAccountId}:root`],\n            }],\n            conditions: [{\n                test: \"ArnLike\",\n                variable: \"aws:PrincipalArn\",\n                values: [`arn:aws:iam::${awsAccountId}:role/${prefix}-uc-access`],\n            }],\n        },\n    ],\n});\nconst unityMetastore = new aws.iam.Policy(\"unity_metastore\", {\n    name: `${prefix}-unity-catalog-metastore-access-iam-policy`,\n    policy: _this.then(_this =\u003e _this.json),\n});\nconst metastoreDataAccess = new aws.iam.Role(\"metastore_data_access\", {\n    name: `${prefix}-uc-access`,\n    assumeRolePolicy: passroleForUc.then(passroleForUc =\u003e passroleForUc.json),\n    managedPolicyArns: [unityMetastore.arn],\n});\n```\n```python\nimport pulumi\nimport pulumi_aws as aws\nimport pulumi_databricks as databricks\n\nthis = databricks.get_aws_unity_catalog_policy(aws_account_id=aws_account_id,\n    bucket_name=\"databricks-bucket\",\n    role_name=\"databricks-role\",\n    kms_name=\"databricks-kms\")\npassrole_for_uc = aws.iam.get_policy_document(statements=[\n    aws.iam.GetPolicyDocumentStatementArgs(\n        effect=\"Allow\",\n        actions=[\"sts:AssumeRole\"],\n        principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n            identifiers=[\"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\"],\n            type=\"AWS\",\n        )],\n        conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n            test=\"StringEquals\",\n            variable=\"sts:ExternalId\",\n            values=[databricks_account_id],\n        )],\n    ),\n    aws.iam.GetPolicyDocumentStatementArgs(\n        sid=\"ExplicitSelfRoleAssumption\",\n        effect=\"Allow\",\n        actions=[\"sts:AssumeRole\"],\n        principals=[aws.iam.GetPolicyDocumentStatementPrincipalArgs(\n            type=\"AWS\",\n            identifiers=[f\"arn:aws:iam::{aws_account_id}:root\"],\n        )],\n        conditions=[aws.iam.GetPolicyDocumentStatementConditionArgs(\n            test=\"ArnLike\",\n            variable=\"aws:PrincipalArn\",\n            values=[f\"arn:aws:iam::{aws_account_id}:role/{prefix}-uc-access\"],\n        )],\n    ),\n])\nunity_metastore = aws.iam.Policy(\"unity_metastore\",\n    name=f\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n    policy=this.json)\nmetastore_data_access = aws.iam.Role(\"metastore_data_access\",\n    name=f\"{prefix}-uc-access\",\n    assume_role_policy=passrole_for_uc.json,\n    managed_policy_arns=[unity_metastore.arn])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Aws = Pulumi.Aws;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetAwsUnityCatalogPolicy.Invoke(new()\n    {\n        AwsAccountId = awsAccountId,\n        BucketName = \"databricks-bucket\",\n        RoleName = \"databricks-role\",\n        KmsName = \"databricks-kms\",\n    });\n\n    var passroleForUc = Aws.Iam.GetPolicyDocument.Invoke(new()\n    {\n        Statements = new[]\n        {\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Identifiers = new[]\n                        {\n                            \"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\",\n                        },\n                        Type = \"AWS\",\n                    },\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"StringEquals\",\n                        Variable = \"sts:ExternalId\",\n                        Values = new[]\n                        {\n                            databricksAccountId,\n                        },\n                    },\n                },\n            },\n            new Aws.Iam.Inputs.GetPolicyDocumentStatementInputArgs\n            {\n                Sid = \"ExplicitSelfRoleAssumption\",\n                Effect = \"Allow\",\n                Actions = new[]\n                {\n                    \"sts:AssumeRole\",\n                },\n                Principals = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementPrincipalInputArgs\n                    {\n                        Type = \"AWS\",\n                        Identifiers = new[]\n                        {\n                            $\"arn:aws:iam::{awsAccountId}:root\",\n                        },\n                    },\n                },\n                Conditions = new[]\n                {\n                    new Aws.Iam.Inputs.GetPolicyDocumentStatementConditionInputArgs\n                    {\n                        Test = \"ArnLike\",\n                        Variable = \"aws:PrincipalArn\",\n                        Values = new[]\n                        {\n                            $\"arn:aws:iam::{awsAccountId}:role/{prefix}-uc-access\",\n                        },\n                    },\n                },\n            },\n        },\n    });\n\n    var unityMetastore = new Aws.Iam.Policy(\"unity_metastore\", new()\n    {\n        Name = $\"{prefix}-unity-catalog-metastore-access-iam-policy\",\n        PolicyDocument = @this.Apply(@this =\u003e @this.Apply(getAwsUnityCatalogPolicyResult =\u003e getAwsUnityCatalogPolicyResult.Json)),\n    });\n\n    var metastoreDataAccess = new Aws.Iam.Role(\"metastore_data_access\", new()\n    {\n        Name = $\"{prefix}-uc-access\",\n        AssumeRolePolicy = passroleForUc.Apply(getPolicyDocumentResult =\u003e getPolicyDocumentResult.Json),\n        ManagedPolicyArns = new[]\n        {\n            unityMetastore.Arn,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-aws/sdk/v6/go/aws/iam\"\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\nfunc main() {\npulumi.Run(func(ctx *pulumi.Context) error {\nthis, err := databricks.GetAwsUnityCatalogPolicy(ctx, \u0026databricks.GetAwsUnityCatalogPolicyArgs{\nAwsAccountId: awsAccountId,\nBucketName: \"databricks-bucket\",\nRoleName: \"databricks-role\",\nKmsName: pulumi.StringRef(\"databricks-kms\"),\n}, nil);\nif err != nil {\nreturn err\n}\npassroleForUc, err := iam.GetPolicyDocument(ctx, \u0026iam.GetPolicyDocumentArgs{\nStatements: []iam.GetPolicyDocumentStatement{\n{\nEffect: pulumi.StringRef(\"Allow\"),\nActions: []string{\n\"sts:AssumeRole\",\n},\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nIdentifiers: []string{\n\"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\",\n},\nType: \"AWS\",\n},\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"StringEquals\",\nVariable: \"sts:ExternalId\",\nValues: interface{}{\ndatabricksAccountId,\n},\n},\n},\n},\n{\nSid: pulumi.StringRef(\"ExplicitSelfRoleAssumption\"),\nEffect: pulumi.StringRef(\"Allow\"),\nActions: []string{\n\"sts:AssumeRole\",\n},\nPrincipals: []iam.GetPolicyDocumentStatementPrincipal{\n{\nType: \"AWS\",\nIdentifiers: []string{\nfmt.Sprintf(\"arn:aws:iam::%v:root\", awsAccountId),\n},\n},\n},\nConditions: []iam.GetPolicyDocumentStatementCondition{\n{\nTest: \"ArnLike\",\nVariable: \"aws:PrincipalArn\",\nValues: []string{\nfmt.Sprintf(\"arn:aws:iam::%v:role/%v-uc-access\", awsAccountId, prefix),\n},\n},\n},\n},\n},\n}, nil);\nif err != nil {\nreturn err\n}\nunityMetastore, err := iam.NewPolicy(ctx, \"unity_metastore\", \u0026iam.PolicyArgs{\nName: pulumi.String(fmt.Sprintf(\"%v-unity-catalog-metastore-access-iam-policy\", prefix)),\nPolicy: pulumi.String(this.Json),\n})\nif err != nil {\nreturn err\n}\n_, err = iam.NewRole(ctx, \"metastore_data_access\", \u0026iam.RoleArgs{\nName: pulumi.String(fmt.Sprintf(\"%v-uc-access\", prefix)),\nAssumeRolePolicy: pulumi.String(passroleForUc.Json),\nManagedPolicyArns: pulumi.StringArray{\nunityMetastore.Arn,\n},\n})\nif err != nil {\nreturn err\n}\nreturn nil\n})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;\nimport com.pulumi.aws.iam.IamFunctions;\nimport com.pulumi.aws.iam.inputs.GetPolicyDocumentArgs;\nimport com.pulumi.aws.iam.Policy;\nimport com.pulumi.aws.iam.PolicyArgs;\nimport com.pulumi.aws.iam.Role;\nimport com.pulumi.aws.iam.RoleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()\n            .awsAccountId(awsAccountId)\n            .bucketName(\"databricks-bucket\")\n            .roleName(\"databricks-role\")\n            .kmsName(\"databricks-kms\")\n            .build());\n\n        final var passroleForUc = IamFunctions.getPolicyDocument(GetPolicyDocumentArgs.builder()\n            .statements(            \n                GetPolicyDocumentStatementArgs.builder()\n                    .effect(\"Allow\")\n                    .actions(\"sts:AssumeRole\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .identifiers(\"arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\")\n                        .type(\"AWS\")\n                        .build())\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"StringEquals\")\n                        .variable(\"sts:ExternalId\")\n                        .values(databricksAccountId)\n                        .build())\n                    .build(),\n                GetPolicyDocumentStatementArgs.builder()\n                    .sid(\"ExplicitSelfRoleAssumption\")\n                    .effect(\"Allow\")\n                    .actions(\"sts:AssumeRole\")\n                    .principals(GetPolicyDocumentStatementPrincipalArgs.builder()\n                        .type(\"AWS\")\n                        .identifiers(String.format(\"arn:aws:iam::%s:root\", awsAccountId))\n                        .build())\n                    .conditions(GetPolicyDocumentStatementConditionArgs.builder()\n                        .test(\"ArnLike\")\n                        .variable(\"aws:PrincipalArn\")\n                        .values(String.format(\"arn:aws:iam::%s:role/%s-uc-access\", awsAccountId,prefix))\n                        .build())\n                    .build())\n            .build());\n\n        var unityMetastore = new Policy(\"unityMetastore\", PolicyArgs.builder()        \n            .name(String.format(\"%s-unity-catalog-metastore-access-iam-policy\", prefix))\n            .policy(this_.json())\n            .build());\n\n        var metastoreDataAccess = new Role(\"metastoreDataAccess\", RoleArgs.builder()        \n            .name(String.format(\"%s-uc-access\", prefix))\n            .assumeRolePolicy(passroleForUc.applyValue(getPolicyDocumentResult -\u003e getPolicyDocumentResult.json()))\n            .managedPolicyArns(unityMetastore.arn())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  unityMetastore:\n    type: aws:iam:Policy\n    name: unity_metastore\n    properties:\n      name: ${prefix}-unity-catalog-metastore-access-iam-policy\n      policy: ${this.json}\n  metastoreDataAccess:\n    type: aws:iam:Role\n    name: metastore_data_access\n    properties:\n      name: ${prefix}-uc-access\n      assumeRolePolicy: ${passroleForUc.json}\n      managedPolicyArns:\n        - ${unityMetastore.arn}\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getAwsUnityCatalogPolicy\n      Arguments:\n        awsAccountId: ${awsAccountId}\n        bucketName: databricks-bucket\n        roleName: databricks-role\n        kmsName: databricks-kms\n  passroleForUc:\n    fn::invoke:\n      Function: aws:iam:getPolicyDocument\n      Arguments:\n        statements:\n          - effect: Allow\n            actions:\n              - sts:AssumeRole\n            principals:\n              - identifiers:\n                  - arn:aws:iam::414351767826:role/unity-catalog-prod-UCMasterRole-14S5ZJVKOTYTL\n                type: AWS\n            conditions:\n              - test: StringEquals\n                variable: sts:ExternalId\n                values:\n                  - ${databricksAccountId}\n          - sid: ExplicitSelfRoleAssumption\n            effect: Allow\n            actions:\n              - sts:AssumeRole\n            principals:\n              - type: AWS\n                identifiers:\n                  - arn:aws:iam::${awsAccountId}:root\n            conditions:\n              - test: ArnLike\n                variable: aws:PrincipalArn\n                values:\n                  - arn:aws:iam::${awsAccountId}:role/${prefix}-uc-access\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getAwsUnityCatalogPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string",
                        "description": "The Account ID of the current AWS account (not your Databricks account).\n",
                        "willReplaceOnChanges": true
                    },
                    "bucketName": {
                        "type": "string",
                        "description": "The name of the S3 bucket used as root storage location for [managed tables](https://docs.databricks.com/data-governance/unity-catalog/index.html#managed-table) in Unity Catalog.\n",
                        "willReplaceOnChanges": true
                    },
                    "kmsName": {
                        "type": "string",
                        "description": "If encryption is enabled, provide the name of the KMS key that encrypts the S3 bucket contents. If encryption is disabled, do not provide this argument.\n",
                        "willReplaceOnChanges": true
                    },
                    "roleName": {
                        "type": "string",
                        "description": "The name of the AWS IAM role that you created in the previous step in the [official documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws).\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "awsAccountId",
                    "bucketName",
                    "roleName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getAwsUnityCatalogPolicy.\n",
                "properties": {
                    "awsAccountId": {
                        "type": "string"
                    },
                    "bucketName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "json": {
                        "description": "AWS IAM Policy JSON document\n",
                        "type": "string"
                    },
                    "kmsName": {
                        "type": "string"
                    },
                    "roleName": {
                        "type": "string"
                    }
                },
                "required": [
                    "awsAccountId",
                    "bucketName",
                    "json",
                    "roleName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCatalogs:getCatalogs": {
            "description": "## Example Usage\n\nListing all catalogs:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getCatalogs({});\nexport const allCatalogs = all;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_catalogs()\npulumi.export(\"allCatalogs\", all)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetCatalogs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allCatalogs\"] = all,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetCatalogs(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allCatalogs\", all)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCatalogsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getCatalogs();\n\n        ctx.export(\"allCatalogs\", all.applyValue(getCatalogsResult -\u003e getCatalogsResult));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getCatalogs\n      Arguments: {}\noutputs:\n  allCatalogs: ${all}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCatalogs.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Catalog names\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCatalogs.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Catalog names\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCluster:getCluster": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.\n\n## Example Usage\n\nRetrieve attributes of each SQL warehouses in a workspace\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\nconst allGetCluster = .reduce((__obj, [, ]) =\u003e ({ ...__obj, [__key]: databricks.getCluster({\n    clusterId: __value,\n}) }));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\nall_get_cluster = {__key: databricks.get_cluster(cluster_id=__value) for __key, __value in ids}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n    var allGetCluster = .ToDictionary(item =\u003e {\n        var __key = item.Key;\n        return __key;\n    }, item =\u003e {\n        var __value = item.Value;\n        return Databricks.GetCluster.Invoke(new()\n        {\n            ClusterId = __value,\n        });\n    });\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string",
                        "description": "The id of the cluster\n"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "type": "string",
                        "description": "The exact name of the cluster to search\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "cluster ID\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCluster.\n",
                "properties": {
                    "clusterId": {
                        "type": "string"
                    },
                    "clusterInfo": {
                        "$ref": "#/types/databricks:index/getClusterClusterInfo:getClusterClusterInfo",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterName": {
                        "description": "Cluster name, which doesn’t have to be unique.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "cluster ID\n",
                        "type": "string"
                    }
                },
                "required": [
                    "clusterId",
                    "clusterInfo",
                    "clusterName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getClusterPolicy:getClusterPolicy": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_cluster_policy.\n\n## Example Usage\n\nReferring to a cluster policy by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst personal = databricks.getClusterPolicy({\n    name: \"Personal Compute\",\n});\nconst myCluster = new databricks.Cluster(\"my_cluster\", {policyId: personal.then(personal =\u003e personal.id)});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npersonal = databricks.get_cluster_policy(name=\"Personal Compute\")\nmy_cluster = databricks.Cluster(\"my_cluster\", policy_id=personal.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var personal = Databricks.GetClusterPolicy.Invoke(new()\n    {\n        Name = \"Personal Compute\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"my_cluster\", new()\n    {\n        PolicyId = personal.Apply(getClusterPolicyResult =\u003e getClusterPolicyResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tpersonal, err := databricks.LookupClusterPolicy(ctx, \u0026databricks.LookupClusterPolicyArgs{\n\t\t\tName: pulumi.StringRef(\"Personal Compute\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"my_cluster\", \u0026databricks.ClusterArgs{\n\t\t\tPolicyId: pulumi.String(personal.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClusterPolicyArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()\n            .name(\"Personal Compute\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()        \n            .policyId(personal.applyValue(getClusterPolicyResult -\u003e getClusterPolicyResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    name: my_cluster\n    properties:\n      policyId: ${personal.id}\nvariables:\n  personal:\n    fn::invoke:\n      Function: databricks:getClusterPolicy\n      Arguments:\n        name: Personal Compute\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusterPolicy.\n",
                "properties": {
                    "definition": {
                        "type": "string",
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n"
                    },
                    "description": {
                        "type": "string",
                        "description": "Additional human-readable description of the cluster policy.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the cluster policy.\n"
                    },
                    "isDefault": {
                        "type": "boolean",
                        "description": "If true, policy is a default policy created and managed by Databricks.\n"
                    },
                    "maxClustersPerUser": {
                        "type": "integer",
                        "description": "Max number of clusters per user that can be active using this policy.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the cluster policy. The cluster policy must exist before this resource can be planned.\n"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "type": "string",
                        "description": "Policy definition JSON document expressed in Databricks [Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definitions).\n"
                    },
                    "policyFamilyId": {
                        "type": "string",
                        "description": "ID of the policy family.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusterPolicy.\n",
                "properties": {
                    "definition": {
                        "description": "Policy definition: JSON document expressed in [Databricks Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definition).\n",
                        "type": "string"
                    },
                    "description": {
                        "description": "Additional human-readable description of the cluster policy.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The id of the cluster policy.\n",
                        "type": "string"
                    },
                    "isDefault": {
                        "description": "If true, policy is a default policy created and managed by Databricks.\n",
                        "type": "boolean"
                    },
                    "maxClustersPerUser": {
                        "description": "Max number of clusters per user that can be active using this policy.\n",
                        "type": "integer"
                    },
                    "name": {
                        "type": "string"
                    },
                    "policyFamilyDefinitionOverrides": {
                        "description": "Policy definition JSON document expressed in Databricks [Policy Definition Language](https://docs.databricks.com/administration-guide/clusters/policies.html#cluster-policy-definitions).\n",
                        "type": "string"
                    },
                    "policyFamilyId": {
                        "description": "ID of the policy family.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "definition",
                    "description",
                    "id",
                    "isDefault",
                    "maxClustersPerUser",
                    "name",
                    "policyFamilyDefinitionOverrides",
                    "policyFamilyId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getClusters:getClusters": {
            "description": "## Example Usage\n\nRetrieve all clusters on this workspace on AWS or GCP:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getClusters({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_clusters()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetClusters.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getClusters();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getClusters\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getClusters({\n    clusterNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_clusters(cluster_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetClusters.Invoke(new()\n    {\n        ClusterNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetClusters(ctx, \u0026databricks.GetClustersArgs{\n\t\t\tClusterNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetClustersArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()\n            .clusterNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      Function: databricks:getClusters\n      Arguments:\n        clusterNameContains: shared\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string",
                        "description": "Only return databricks.Cluster ids that match the given name string.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getClusters.\n",
                "properties": {
                    "clusterNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "list of databricks.Cluster ids\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentConfig:getCurrentConfig": {
            "description": "Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.\n\n## Example Usage\n\nCreate cloud-specific databricks_storage_credential:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nfunction singleOrNone\u003cT\u003e(elements: pulumi.Input\u003cT\u003e[]): pulumi.Input\u003cT\u003e {\n    if (elements.length != 1) {\n        throw new Error(\"singleOrNone expected input list to have a single element\");\n    }\n    return elements[0];\n}\n\nconst this = databricks.getCurrentConfig({});\nconst external = new databricks.StorageCredential(\"external\", {\n    awsIamRole: singleOrNone(.map(entry =\u003e ({\n        roleArn: cloudCredentialId,\n    }))),\n    azureManagedIdentity: singleOrNone(.map(entry =\u003e ({\n        accessConnectorId: cloudCredentialId,\n    }))),\n    databricksGcpServiceAccount: singleOrNone(.map(entry =\u003e ({}))),\n    name: \"storage_cred\",\n    comment: \"Managed by TF\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\ndef single_or_none(elements):\n    if len(elements) != 1:\n        raise Exception(\"single_or_none expected input list to have a single element\")\n    return elements[0]\n\n\nthis = databricks.get_current_config()\nexternal = databricks.StorageCredential(\"external\",\n    aws_iam_role=single_or_none([{\n        \"roleArn\": cloud_credential_id,\n    } for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"aws\" else {\n        \"aws\": True,\n    }]]),\n    azure_managed_identity=single_or_none([{\n        \"accessConnectorId\": cloud_credential_id,\n    } for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"azure\" else {\n        \"azure\": True,\n    }]]),\n    databricks_gcp_service_account=single_or_none([{} for entry in [{\"key\": k, \"value\": v} for k, v in {} if this.cloud_type == \"gcp\" else {\n        \"gcp\": True,\n    }]]),\n    name=\"storage_cred\",\n    comment=\"Managed by TF\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetCurrentConfig.Invoke();\n\n    var external = new Databricks.StorageCredential(\"external\", new()\n    {\n        AwsIamRole = Enumerable.Single(),\n        AzureManagedIdentity = Enumerable.Single(),\n        DatabricksGcpServiceAccount = Enumerable.Single(),\n        Name = \"storage_cred\",\n        Comment = \"Managed by TF\",\n    });\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Exported attributes\n\nData source exposes the following attributes:\n\n* `is_account` - Whether the provider is configured at account-level\n* `account_id` - Account Id if provider is configured at account-level\n* `host` - Host of the Databricks workspace or account console\n* `cloud_type` - Cloud type specified in the provider\n* `auth_type` - Auth type used by the provider\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n* databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCurrentConfig.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "authType": {
                        "type": "string"
                    },
                    "cloudType": {
                        "type": "string"
                    },
                    "host": {
                        "type": "string"
                    },
                    "isAccount": {
                        "type": "boolean"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCurrentConfig.\n",
                "properties": {
                    "accountId": {
                        "type": "string"
                    },
                    "authType": {
                        "type": "string"
                    },
                    "cloudType": {
                        "type": "string"
                    },
                    "host": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "isAccount": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "accountId",
                    "authType",
                    "cloudType",
                    "host",
                    "isAccount",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentMetastore:getCurrentMetastore": {
            "description": "Retrieves information about metastore attached to a given workspace.\n\n\u003e **Note** This is the workspace-level data source.\n\n\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.\n\n## Example Usage\n\nMetastoreSummary response for a metastore attached to the current workspace.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getCurrentMetastore({});\nexport const someMetastore = thisDatabricksMetastore.metastoreInfo[0];\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_current_metastore()\npulumi.export(\"someMetastore\", this_databricks_metastore[\"metastoreInfo\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetCurrentMetastore.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"someMetastore\"] = thisDatabricksMetastore.MetastoreInfo[0],\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetCurrentMetastore(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"someMetastore\", thisDatabricksMetastore.MetastoreInfo[0])\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getCurrentMetastore();\n\n        ctx.export(\"someMetastore\", thisDatabricksMetastore.metastoreInfo()[0]);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getCurrentMetastore\n      Arguments: {}\noutputs:\n  someMetastore: ${thisDatabricksMetastore.metastoreInfo[0]}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Metastore to get information for a metastore with a given ID.\n* databricks.getMetastores to get a mapping of name to id of all metastores.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getCurrentMetastore.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "metastore ID. Will be `no_metastore` if there is no metastore assigned for the current workspace\n"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo",
                        "description": "summary about a metastore attached to the current workspace returned by [Get a metastore summary API](https://docs.databricks.com/api/workspace/metastores/summary). This contains the following attributes (check the API page for up-to-date details):\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getCurrentMetastore.\n",
                "properties": {
                    "id": {
                        "description": "metastore ID. Will be `no_metastore` if there is no metastore assigned for the current workspace\n",
                        "type": "string"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getCurrentMetastoreMetastoreInfo:getCurrentMetastoreMetastoreInfo",
                        "description": "summary about a metastore attached to the current workspace returned by [Get a metastore summary API](https://docs.databricks.com/api/workspace/metastores/summary). This contains the following attributes (check the API page for up-to-date details):\n"
                    }
                },
                "required": [
                    "id",
                    "metastoreInfo"
                ],
                "type": "object"
            }
        },
        "databricks:index/getCurrentUser:getCurrentUser": {
            "description": "## Example Usage\n\nCreate personalized databricks.Job and databricks_notebook:\n\n",
            "outputs": {
                "description": "A collection of values returned by getCurrentUser.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string"
                    },
                    "alphanumeric": {
                        "type": "string"
                    },
                    "externalId": {
                        "type": "string"
                    },
                    "home": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "repos": {
                        "type": "string"
                    },
                    "userName": {
                        "type": "string"
                    },
                    "workspaceUrl": {
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "alphanumeric",
                    "externalId",
                    "home",
                    "repos",
                    "userName",
                    "workspaceUrl",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDbfsFile:getDbfsFile": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst report = databricks.getDbfsFile({\n    path: \"dbfs:/reports/some.csv\",\n    limitFileSize: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nreport = databricks.get_dbfs_file(path=\"dbfs:/reports/some.csv\",\n    limit_file_size=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var report = Databricks.GetDbfsFile.Invoke(new()\n    {\n        Path = \"dbfs:/reports/some.csv\",\n        LimitFileSize = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDbfsFile(ctx, \u0026databricks.LookupDbfsFileArgs{\n\t\t\tPath:          \"dbfs:/reports/some.csv\",\n\t\t\tLimitFileSize: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFileArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()\n            .path(\"dbfs:/reports/some.csv\")\n            .limitFileSize(\"true\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  report:\n    fn::invoke:\n      Function: databricks:getDbfsFile\n      Arguments:\n        path: dbfs:/reports/some.csv\n        limitFileSize: 'true'\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFile.\n",
                "properties": {
                    "limitFileSize": {
                        "type": "boolean",
                        "description": "Do not load content for files larger than 4MB.\n",
                        "willReplaceOnChanges": true
                    },
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file from which to get content.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "limitFileSize",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFile.\n",
                "properties": {
                    "content": {
                        "description": "base64-encoded file contents\n",
                        "type": "string"
                    },
                    "fileSize": {
                        "description": "size of the file in bytes\n",
                        "type": "integer"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "limitFileSize": {
                        "type": "boolean"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "required": [
                    "content",
                    "fileSize",
                    "limitFileSize",
                    "path",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDbfsFilePaths:getDbfsFilePaths": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst partitions = databricks.getDbfsFilePaths({\n    path: \"dbfs:/user/hive/default.db/table\",\n    recursive: false,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npartitions = databricks.get_dbfs_file_paths(path=\"dbfs:/user/hive/default.db/table\",\n    recursive=False)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var partitions = Databricks.GetDbfsFilePaths.Invoke(new()\n    {\n        Path = \"dbfs:/user/hive/default.db/table\",\n        Recursive = false,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetDbfsFilePaths(ctx, \u0026databricks.GetDbfsFilePathsArgs{\n\t\t\tPath:      \"dbfs:/user/hive/default.db/table\",\n\t\t\tRecursive: false,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()\n            .path(\"dbfs:/user/hive/default.db/table\")\n            .recursive(false)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  partitions:\n    fn::invoke:\n      Function: databricks:getDbfsFilePaths\n      Arguments:\n        path: dbfs:/user/hive/default.db/table\n        recursive: false\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).\n* databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.\n* databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDbfsFilePaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path on DBFS for the file to perform listing\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or not recursively list all files\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDbfsFilePaths.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "path": {
                        "type": "string"
                    },
                    "pathLists": {
                        "description": "returns list of objects with `path` and `file_size` attributes in each\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getDbfsFilePathsPathList:getDbfsFilePathsPathList"
                        },
                        "type": "array"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "path",
                    "pathLists",
                    "recursive",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getDirectory:getDirectory": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to get information about a directory in a Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getDirectory({\n    path: \"/Production\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_directory(path=\"/Production\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetDirectory.Invoke(new()\n    {\n        Path = \"/Production\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupDirectory(ctx, \u0026databricks.LookupDirectoryArgs{\n\t\t\tPath: \"/Production\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetDirectoryArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()\n            .path(\"/Production\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      Function: databricks:getDirectory\n      Arguments:\n        path: /Production\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getDirectory.\n",
                "properties": {
                    "objectId": {
                        "type": "integer",
                        "description": "directory object ID\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Path to a directory in the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getDirectory.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "objectId": {
                        "description": "directory object ID\n",
                        "type": "integer"
                    },
                    "path": {
                        "type": "string"
                    },
                    "workspacePath": {
                        "description": "path on Workspace File System (WSFS) in form of `/Workspace` + `path`\n",
                        "type": "string"
                    }
                },
                "required": [
                    "objectId",
                    "path",
                    "workspacePath",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getExternalLocation:getExternalLocation": {
            "description": "## Example Usage\n\nGetting details of an existing external location in the metastore\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getExternalLocation.\n",
                "properties": {
                    "externalLocationInfo": {
                        "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the storage credential\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getExternalLocation.\n",
                "properties": {
                    "externalLocationInfo": {
                        "$ref": "#/types/databricks:index/getExternalLocationExternalLocationInfo:getExternalLocationExternalLocationInfo"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    }
                },
                "required": [
                    "externalLocationInfo",
                    "name",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getExternalLocations:getExternalLocations": {
            "description": "## Example Usage\n\nList all external locations in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getExternalLocations({});\nexport const allExternalLocations = all.then(all =\u003e all.names);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_external_locations()\npulumi.export(\"allExternalLocations\", all.names)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetExternalLocations.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allExternalLocations\"] = all.Apply(getExternalLocationsResult =\u003e getExternalLocationsResult.Names),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetExternalLocations(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allExternalLocations\", all.Names)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetExternalLocationsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getExternalLocations();\n\n        ctx.export(\"allExternalLocations\", all.applyValue(getExternalLocationsResult -\u003e getExternalLocationsResult.names()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getExternalLocations\n      Arguments: {}\noutputs:\n  allExternalLocations: ${all.names}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.ExternalLocation to get information about a single external location\n* databricks.ExternalLocation to manage external locations within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getExternalLocations.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks.ExternalLocation in the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getExternalLocations.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks.ExternalLocation in the metastore\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getGroup:getGroup": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks.Group members, entitlements and instance profiles.\n\n## Example Usage\n\nAdding user to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = new databricks.User(\"me\", {userName: \"me@example.com\"});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.id,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.User(\"me\", user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = new Databricks.User(\"me\", new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.NewUser(ctx, \"me\", \u0026databricks.UserArgs{\n\t\t\tUserName: pulumi.String(\"me@example.com\"),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: me.ID(),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.User;\nimport com.pulumi.databricks.UserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        var me = new User(\"me\", UserArgs.builder()        \n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  me:\n    type: databricks:User\n    properties:\n      userName: me@example.com\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).\n* databricks.GroupMember to attach users and groups as group members.\n* databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n* databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getGroup.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n"
                    },
                    "allowClusterCreate": {
                        "type": "boolean",
                        "description": "True if group members can create clusters\n",
                        "willReplaceOnChanges": true
                    },
                    "allowInstancePoolCreate": {
                        "type": "boolean",
                        "description": "True if group members can create instance pools\n",
                        "willReplaceOnChanges": true
                    },
                    "childGroups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Display name of the group. The group must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the group in an external identity provider.\n"
                    },
                    "groups": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n"
                    },
                    "members": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead"
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Collect information for all nested groups. *Defaults to true.*\n",
                        "willReplaceOnChanges": true
                    },
                    "servicePrincipals": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "users": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n"
                    },
                    "workspaceAccess": {
                        "type": "boolean",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "displayName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getGroup.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `groups/Some Group`.\n",
                        "type": "string"
                    },
                    "allowClusterCreate": {
                        "description": "True if group members can create clusters\n",
                        "type": "boolean"
                    },
                    "allowInstancePoolCreate": {
                        "description": "True if group members can create instance pools\n",
                        "type": "boolean"
                    },
                    "childGroups": {
                        "description": "Set of databricks.Group identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "databricksSqlAccess": {
                        "type": "boolean"
                    },
                    "displayName": {
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the group in an external identity provider.\n",
                        "type": "string"
                    },
                    "groups": {
                        "description": "Set of group identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "instanceProfiles": {
                        "description": "Set of instance profile ARNs, that can be modified by databricks.GroupInstanceProfile resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "members": {
                        "deprecationMessage": "Please use `users`, `service_principals`, and `child_groups` instead",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "recursive": {
                        "type": "boolean"
                    },
                    "servicePrincipals": {
                        "description": "Set of databricks.ServicePrincipal identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "users": {
                        "description": "Set of databricks.User identifiers, that can be modified with databricks.GroupMember resource.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "workspaceAccess": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "childGroups",
                    "displayName",
                    "externalId",
                    "groups",
                    "instanceProfiles",
                    "members",
                    "servicePrincipals",
                    "users",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getInstancePool:getInstancePool": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_instance_pool.\n\n## Example Usage\n\nReferring to an instance pool by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst pool = databricks.getInstancePool({\n    name: \"All spot\",\n});\nconst myCluster = new databricks.Cluster(\"my_cluster\", {instancePoolId: poolDatabricksInstancePool.id});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\npool = databricks.get_instance_pool(name=\"All spot\")\nmy_cluster = databricks.Cluster(\"my_cluster\", instance_pool_id=pool_databricks_instance_pool[\"id\"])\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var pool = Databricks.GetInstancePool.Invoke(new()\n    {\n        Name = \"All spot\",\n    });\n\n    var myCluster = new Databricks.Cluster(\"my_cluster\", new()\n    {\n        InstancePoolId = poolDatabricksInstancePool.Id,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupInstancePool(ctx, \u0026databricks.LookupInstancePoolArgs{\n\t\t\tName: \"All spot\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"my_cluster\", \u0026databricks.ClusterArgs{\n\t\t\tInstancePoolId: pulumi.Any(poolDatabricksInstancePool.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetInstancePoolArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()\n            .name(\"All spot\")\n            .build());\n\n        var myCluster = new Cluster(\"myCluster\", ClusterArgs.builder()        \n            .instancePoolId(poolDatabricksInstancePool.id())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myCluster:\n    type: databricks:Cluster\n    name: my_cluster\n    properties:\n      instancePoolId: ${poolDatabricksInstancePool.id}\nvariables:\n  pool:\n    fn::invoke:\n      Function: databricks:getInstancePool\n      Arguments:\n        name: All spot\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getInstancePool.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "Name of the instance pool. The instance pool must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getInstancePool.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "poolInfo": {
                        "$ref": "#/types/databricks:index/getInstancePoolPoolInfo:getInstancePoolPoolInfo",
                        "description": "block describing instance pool and its state. Check documentation for databricks.InstancePool for a list of exposed attributes.\n"
                    }
                },
                "required": [
                    "name",
                    "poolInfo",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getInstanceProfiles:getInstanceProfiles": {
            "description": "Lists all available databricks_instance_profiles.\n\n## Example Usage\n\nGet all instance profiles:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getInstanceProfiles({});\nexport const allInstanceProfiles = all.then(all =\u003e all.instanceProfiles);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_instance_profiles()\npulumi.export(\"allInstanceProfiles\", all.instance_profiles)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetInstanceProfiles.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allInstanceProfiles\"] = all.Apply(getInstanceProfilesResult =\u003e getInstanceProfilesResult.InstanceProfiles),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetInstanceProfiles(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allInstanceProfiles\", all.InstanceProfiles)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetInstanceProfilesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getInstanceProfiles();\n\n        ctx.export(\"allInstanceProfiles\", all.applyValue(getInstanceProfilesResult -\u003e getInstanceProfilesResult.instanceProfiles()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getInstanceProfiles\n      Arguments: {}\noutputs:\n  allInstanceProfiles: ${all.instanceProfiles}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getInstanceProfiles.\n",
                "properties": {
                    "instanceProfiles": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile"
                        },
                        "description": "Set of objects for a databricks_instance_profile. This contains the following attributes:\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getInstanceProfiles.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "instanceProfiles": {
                        "description": "Set of objects for a databricks_instance_profile. This contains the following attributes:\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getInstanceProfilesInstanceProfile:getInstanceProfilesInstanceProfile"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "instanceProfiles",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getJob:getJob": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.\n\n## Example Usage\n\nGetting the existing cluster id of specific databricks.Job by name or by id:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getJob({\n    jobName: \"My job\",\n});\nexport const jobNumWorkers = _this.then(_this =\u003e _this.jobSettings?.settings?.newCluster?.numWorkers);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_job(job_name=\"My job\")\npulumi.export(\"jobNumWorkers\", this.job_settings.settings.new_cluster.num_workers)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetJob.Invoke(new()\n    {\n        JobName = \"My job\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"jobNumWorkers\"] = @this.Apply(@this =\u003e @this.Apply(getJobResult =\u003e getJobResult.JobSettings?.Settings?.NewCluster?.NumWorkers)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupJob(ctx, \u0026databricks.LookupJobArgs{\n\t\t\tJobName: pulumi.StringRef(\"My job\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"jobNumWorkers\", this.JobSettings.Settings.NewCluster.NumWorkers)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJob(GetJobArgs.builder()\n            .jobName(\"My job\")\n            .build());\n\n        ctx.export(\"jobNumWorkers\", this_.jobSettings().settings().newCluster().numWorkers());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getJob\n      Arguments:\n        jobName: My job\noutputs:\n  jobNumWorkers: ${this.jobSettings.settings.newCluster.numWorkers}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getJobs data to get all jobs and their names from a workspace.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJob.\n",
                "properties": {
                    "id": {
                        "type": "string",
                        "description": "the id of databricks.Job if the resource was matched by name.\n"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "the job name of databricks.Job if the resource was matched by id.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJob.\n",
                "properties": {
                    "id": {
                        "description": "the id of databricks.Job if the resource was matched by name.\n",
                        "type": "string"
                    },
                    "jobId": {
                        "type": "string"
                    },
                    "jobName": {
                        "type": "string"
                    },
                    "jobSettings": {
                        "$ref": "#/types/databricks:index/getJobJobSettings:getJobJobSettings",
                        "description": "the same fields as in databricks_job.\n"
                    },
                    "name": {
                        "description": "the job name of databricks.Job if the resource was matched by id.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "id",
                    "jobId",
                    "jobName",
                    "jobSettings",
                    "name"
                ],
                "type": "object"
            }
        },
        "databricks:index/getJobs:getJobs": {
            "description": "## Example Usage\n\nGranting view databricks.Permissions to all databricks.Job within the workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const this = await databricks.getJobs({});\n    const everyoneCanViewAllJobs: databricks.Permissions[] = [];\n    for (const range of Object.entries(_this.ids).map(([k, v]) =\u003e ({key: k, value: v}))) {\n        everyoneCanViewAllJobs.push(new databricks.Permissions(`everyone_can_view_all_jobs-${range.key}`, {\n            jobId: range.value,\n            accessControls: [{\n                groupName: \"users\",\n                permissionLevel: \"CAN_VIEW\",\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_jobs()\neveryone_can_view_all_jobs = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(this.ids)]:\n    everyone_can_view_all_jobs.append(databricks.Permissions(f\"everyone_can_view_all_jobs-{range['key']}\",\n        job_id=range[\"value\"],\n        access_controls=[databricks.PermissionsAccessControlArgs(\n            group_name=\"users\",\n            permission_level=\"CAN_VIEW\",\n        )]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var @this = await Databricks.GetJobs.InvokeAsync();\n\n    var everyoneCanViewAllJobs = new List\u003cDatabricks.Permissions\u003e();\n    foreach (var range in )\n    {\n        everyoneCanViewAllJobs.Add(new Databricks.Permissions($\"everyone_can_view_all_jobs-{range.Key}\", new()\n        {\n            JobId = range.Value,\n            AccessControls = new[]\n            {\n                new Databricks.Inputs.PermissionsAccessControlArgs\n                {\n                    GroupName = \"users\",\n                    PermissionLevel = \"CAN_VIEW\",\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetJobs(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar everyoneCanViewAllJobs []*databricks.Permissions\n\t\tfor key0, val0 := range this.Ids {\n\t\t\t__res, err := databricks.NewPermissions(ctx, fmt.Sprintf(\"everyone_can_view_all_jobs-%v\", key0), \u0026databricks.PermissionsArgs{\n\t\t\t\tJobId: pulumi.Any(val0),\n\t\t\t\tAccessControls: databricks.PermissionsAccessControlArray{\n\t\t\t\t\t\u0026databricks.PermissionsAccessControlArgs{\n\t\t\t\t\t\tGroupName:       pulumi.String(\"users\"),\n\t\t\t\t\t\tPermissionLevel: pulumi.String(\"CAN_VIEW\"),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\teveryoneCanViewAllJobs = append(everyoneCanViewAllJobs, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobsArgs;\nimport com.pulumi.databricks.Permissions;\nimport com.pulumi.databricks.PermissionsArgs;\nimport com.pulumi.databricks.inputs.PermissionsAccessControlArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJobs();\n\n        final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -\u003e {\n            final var resources = new ArrayList\u003cPermissions\u003e();\n            for (var range : KeyedValue.of(getJobsResult.ids()) {\n                var resource = new Permissions(\"everyoneCanViewAllJobs-\" + range.key(), PermissionsArgs.builder()                \n                    .jobId(range.value())\n                    .accessControls(PermissionsAccessControlArgs.builder()\n                        .groupName(\"users\")\n                        .permissionLevel(\"CAN_VIEW\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  everyoneCanViewAllJobs:\n    type: databricks:Permissions\n    name: everyone_can_view_all_jobs\n    properties:\n      jobId: ${range.value}\n      accessControls:\n        - groupName: users\n          permissionLevel: CAN_VIEW\n    options: {}\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getJobs\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nGetting ID of specific databricks.Job by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getJobs({});\nexport const x = _this.then(_this =\u003e `ID of `x` job is ${_this.ids?.x}`);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_jobs()\npulumi.export(\"x\", f\"ID of `x` job is {this.ids['x']}\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetJobs.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"x\"] = @this.Apply(@this =\u003e $\"ID of `x` job is {@this.Apply(getJobsResult =\u003e getJobsResult.Ids?.X)}\"),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetJobs(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"x\", fmt.Sprintf(\"ID of `x` job is %v\", this.Ids.X))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetJobsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getJobs();\n\n        ctx.export(\"x\", String.format(\"ID of `x` job is %s\", this_.ids().x()));\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getJobs\n      Arguments: {}\noutputs:\n  x: ID of `x` job is ${this.ids.x}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getJobs.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getJobs.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "map of databricks.Job names to ids\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMetastore:getMetastore": {
            "description": "## Example Usage\n\nMetastoreInfo response for a given metastore id\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisMetastore = new databricks.Metastore(\"this\", {\n    name: \"primary\",\n    storageRoot: `s3://${metastore.id}/metastore`,\n    owner: unityAdminGroup,\n    forceDestroy: true,\n});\nconst this = databricks.getMetastoreOutput({\n    metastoreId: thisMetastore.id,\n});\nexport const someMetastore = _this.apply(_this =\u003e _this.metastoreInfo);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_metastore = databricks.Metastore(\"this\",\n    name=\"primary\",\n    storage_root=f\"s3://{metastore['id']}/metastore\",\n    owner=unity_admin_group,\n    force_destroy=True)\nthis = databricks.get_metastore_output(metastore_id=this_metastore.id)\npulumi.export(\"someMetastore\", this.metastore_info)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisMetastore = new Databricks.Metastore(\"this\", new()\n    {\n        Name = \"primary\",\n        StorageRoot = $\"s3://{metastore.Id}/metastore\",\n        Owner = unityAdminGroup,\n        ForceDestroy = true,\n    });\n\n    var @this = Databricks.GetMetastore.Invoke(new()\n    {\n        MetastoreId = thisMetastore.Id,\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"someMetastore\"] = @this.Apply(@this =\u003e @this.Apply(getMetastoreResult =\u003e getMetastoreResult.MetastoreInfo)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"fmt\"\n\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthisMetastore, err := databricks.NewMetastore(ctx, \"this\", \u0026databricks.MetastoreArgs{\n\t\t\tName:         pulumi.String(\"primary\"),\n\t\t\tStorageRoot:  pulumi.String(fmt.Sprintf(\"s3://%v/metastore\", metastore.Id)),\n\t\t\tOwner:        pulumi.Any(unityAdminGroup),\n\t\t\tForceDestroy: pulumi.Bool(true),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis := databricks.LookupMetastoreOutput(ctx, databricks.GetMetastoreOutputArgs{\n\t\t\tMetastoreId: thisMetastore.ID(),\n\t\t}, nil)\n\t\tctx.Export(\"someMetastore\", this.ApplyT(func(this databricks.GetMetastoreResult) (databricks.GetMetastoreMetastoreInfo, error) {\n\t\t\treturn this.MetastoreInfo, nil\n\t\t}).(databricks.GetMetastoreMetastoreInfoOutput))\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.Metastore;\nimport com.pulumi.databricks.MetastoreArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMetastoreArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisMetastore = new Metastore(\"thisMetastore\", MetastoreArgs.builder()        \n            .name(\"primary\")\n            .storageRoot(String.format(\"s3://%s/metastore\", metastore.id()))\n            .owner(unityAdminGroup)\n            .forceDestroy(true)\n            .build());\n\n        final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()\n            .metastoreId(thisMetastore.id())\n            .build());\n\n        ctx.export(\"someMetastore\", this_.applyValue(this_ -\u003e this_.metastoreInfo()));\n    }\n}\n```\n```yaml\nresources:\n  thisMetastore:\n    type: databricks:Metastore\n    name: this\n    properties:\n      name: primary\n      storageRoot: s3://${metastore.id}/metastore\n      owner: ${unityAdminGroup}\n      forceDestroy: true\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getMetastore\n      Arguments:\n        metastoreId: ${thisMetastore.id}\noutputs:\n  someMetastore: ${this.metastoreInfo}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.getMetastores to get mapping of name to id of all metastores.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMetastore.\n",
                "properties": {
                    "metastoreId": {
                        "type": "string",
                        "description": "Id of the metastore\n"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo",
                        "description": "MetastoreInfo object for a databricks_metastore. This contains the following attributes:\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the metastore\n"
                    },
                    "region": {
                        "type": "string",
                        "description": "Region of the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMetastore.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "metastoreId": {
                        "type": "string"
                    },
                    "metastoreInfo": {
                        "$ref": "#/types/databricks:index/getMetastoreMetastoreInfo:getMetastoreMetastoreInfo",
                        "description": "MetastoreInfo object for a databricks_metastore. This contains the following attributes:\n"
                    },
                    "name": {
                        "description": "Name of metastore.\n",
                        "type": "string"
                    },
                    "region": {
                        "type": "string"
                    }
                },
                "required": [
                    "metastoreId",
                    "metastoreInfo",
                    "name",
                    "region",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMetastores:getMetastores": {
            "description": "## Example Usage\n\nMapping of name to id of all metastores:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMetastores({});\nexport const allMetastores = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_metastores()\npulumi.export(\"allMetastores\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMetastores.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMetastores\"] = all.Apply(getMetastoresResult =\u003e getMetastoresResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetMetastores(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMetastores\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMetastoresArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMetastores();\n\n        ctx.export(\"allMetastores\", all.applyValue(getMetastoresResult -\u003e getMetastoresResult.ids()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getMetastores\n      Arguments: {}\noutputs:\n  allMetastores: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Metastore to get information about a single metastore.\n* databricks.Metastore to manage Metastores within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMetastores.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Mapping of name to id of databricks_metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMetastores.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "Mapping of name to id of databricks_metastore\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMlflowModel:getMlflowModel": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves the settings of databricks.MlflowModel by name.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst thisMlflowModel = new databricks.MlflowModel(\"this\", {\n    name: \"My MLflow Model\",\n    description: \"My MLflow model description\",\n    tags: [\n        {\n            key: \"key1\",\n            value: \"value1\",\n        },\n        {\n            key: \"key2\",\n            value: \"value2\",\n        },\n    ],\n});\nconst this = databricks.getMlflowModel({\n    name: \"My MLflow Model\",\n});\nexport const model = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis_mlflow_model = databricks.MlflowModel(\"this\",\n    name=\"My MLflow Model\",\n    description=\"My MLflow model description\",\n    tags=[\n        databricks.MlflowModelTagArgs(\n            key=\"key1\",\n            value=\"value1\",\n        ),\n        databricks.MlflowModelTagArgs(\n            key=\"key2\",\n            value=\"value2\",\n        ),\n    ])\nthis = databricks.get_mlflow_model(name=\"My MLflow Model\")\npulumi.export(\"model\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var thisMlflowModel = new Databricks.MlflowModel(\"this\", new()\n    {\n        Name = \"My MLflow Model\",\n        Description = \"My MLflow model description\",\n        Tags = new[]\n        {\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key1\",\n                Value = \"value1\",\n            },\n            new Databricks.Inputs.MlflowModelTagArgs\n            {\n                Key = \"key2\",\n                Value = \"value2\",\n            },\n        },\n    });\n\n    var @this = Databricks.GetMlflowModel.Invoke(new()\n    {\n        Name = \"My MLflow Model\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"model\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.NewMlflowModel(ctx, \"this\", \u0026databricks.MlflowModelArgs{\n\t\t\tName:        pulumi.String(\"My MLflow Model\"),\n\t\t\tDescription: pulumi.String(\"My MLflow model description\"),\n\t\t\tTags: databricks.MlflowModelTagArray{\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key1\"),\n\t\t\t\t\tValue: pulumi.String(\"value1\"),\n\t\t\t\t},\n\t\t\t\t\u0026databricks.MlflowModelTagArgs{\n\t\t\t\t\tKey:   pulumi.String(\"key2\"),\n\t\t\t\t\tValue: pulumi.String(\"value2\"),\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tthis, err := databricks.LookupMlflowModel(ctx, \u0026databricks.LookupMlflowModelArgs{\n\t\t\tName: \"My MLflow Model\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"model\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.MlflowModel;\nimport com.pulumi.databricks.MlflowModelArgs;\nimport com.pulumi.databricks.inputs.MlflowModelTagArgs;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMlflowModelArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        var thisMlflowModel = new MlflowModel(\"thisMlflowModel\", MlflowModelArgs.builder()        \n            .name(\"My MLflow Model\")\n            .description(\"My MLflow model description\")\n            .tags(            \n                MlflowModelTagArgs.builder()\n                    .key(\"key1\")\n                    .value(\"value1\")\n                    .build(),\n                MlflowModelTagArgs.builder()\n                    .key(\"key2\")\n                    .value(\"value2\")\n                    .build())\n            .build());\n\n        final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()\n            .name(\"My MLflow Model\")\n            .build());\n\n        ctx.export(\"model\", this_);\n    }\n}\n```\n```yaml\nresources:\n  thisMlflowModel:\n    type: databricks:MlflowModel\n    name: this\n    properties:\n      name: My MLflow Model\n      description: My MLflow model description\n      tags:\n        - key: key1\n          value: value1\n        - key: key2\n          value: value2\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getMlflowModel\n      Arguments:\n        name: My MLflow Model\noutputs:\n  model: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getMlflowModel({\n    name: \"My MLflow Model with multiple versions\",\n});\nconst thisModelServing = new databricks.ModelServing(\"this\", {\n    name: \"model-serving-endpoint\",\n    config: {\n        servedModels: [{\n            name: \"model_serving_prod\",\n            modelName: _this.then(_this =\u003e _this.name),\n            modelVersion: _this.then(_this =\u003e _this.latestVersions?.[0]?.version),\n            workloadSize: \"Small\",\n            scaleToZeroEnabled: true,\n        }],\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_mlflow_model(name=\"My MLflow Model with multiple versions\")\nthis_model_serving = databricks.ModelServing(\"this\",\n    name=\"model-serving-endpoint\",\n    config=databricks.ModelServingConfigArgs(\n        served_models=[databricks.ModelServingConfigServedModelArgs(\n            name=\"model_serving_prod\",\n            model_name=this.name,\n            model_version=this.latest_versions[0].version,\n            workload_size=\"Small\",\n            scale_to_zero_enabled=True,\n        )],\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetMlflowModel.Invoke(new()\n    {\n        Name = \"My MLflow Model with multiple versions\",\n    });\n\n    var thisModelServing = new Databricks.ModelServing(\"this\", new()\n    {\n        Name = \"model-serving-endpoint\",\n        Config = new Databricks.Inputs.ModelServingConfigArgs\n        {\n            ServedModels = new[]\n            {\n                new Databricks.Inputs.ModelServingConfigServedModelArgs\n                {\n                    Name = \"model_serving_prod\",\n                    ModelName = @this.Apply(@this =\u003e @this.Apply(getMlflowModelResult =\u003e getMlflowModelResult.Name)),\n                    ModelVersion = @this.Apply(@this =\u003e @this.Apply(getMlflowModelResult =\u003e getMlflowModelResult.LatestVersions[0]?.Version)),\n                    WorkloadSize = \"Small\",\n                    ScaleToZeroEnabled = true,\n                },\n            },\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupMlflowModel(ctx, \u0026databricks.LookupMlflowModelArgs{\n\t\t\tName: \"My MLflow Model with multiple versions\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewModelServing(ctx, \"this\", \u0026databricks.ModelServingArgs{\n\t\t\tName: pulumi.String(\"model-serving-endpoint\"),\n\t\t\tConfig: \u0026databricks.ModelServingConfigArgs{\n\t\t\t\tServedModels: databricks.ModelServingConfigServedModelArray{\n\t\t\t\t\t\u0026databricks.ModelServingConfigServedModelArgs{\n\t\t\t\t\t\tName:               pulumi.String(\"model_serving_prod\"),\n\t\t\t\t\t\tModelName:          pulumi.String(this.Name),\n\t\t\t\t\t\tModelVersion:       pulumi.String(this.LatestVersions[0].Version),\n\t\t\t\t\t\tWorkloadSize:       pulumi.String(\"Small\"),\n\t\t\t\t\t\tScaleToZeroEnabled: pulumi.Bool(true),\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMlflowModelArgs;\nimport com.pulumi.databricks.ModelServing;\nimport com.pulumi.databricks.ModelServingArgs;\nimport com.pulumi.databricks.inputs.ModelServingConfigArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()\n            .name(\"My MLflow Model with multiple versions\")\n            .build());\n\n        var thisModelServing = new ModelServing(\"thisModelServing\", ModelServingArgs.builder()        \n            .name(\"model-serving-endpoint\")\n            .config(ModelServingConfigArgs.builder()\n                .servedModels(ModelServingConfigServedModelArgs.builder()\n                    .name(\"model_serving_prod\")\n                    .modelName(this_.name())\n                    .modelVersion(this_.latestVersions()[0].version())\n                    .workloadSize(\"Small\")\n                    .scaleToZeroEnabled(true)\n                    .build())\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  thisModelServing:\n    type: databricks:ModelServing\n    name: this\n    properties:\n      name: model-serving-endpoint\n      config:\n        servedModels:\n          - name: model_serving_prod\n            modelName: ${this.name}\n            modelVersion: ${this.latestVersions[0].version}\n            workloadSize: Small\n            scaleToZeroEnabled: true\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getMlflowModel\n      Arguments:\n        name: My MLflow Model with multiple versions\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMlflowModel.\n",
                "properties": {
                    "description": {
                        "type": "string",
                        "description": "User-specified description for the object.\n"
                    },
                    "latestVersions": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion"
                        },
                        "description": "Array of model versions, each the latest version for its stage.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the registered model.\n",
                        "willReplaceOnChanges": true
                    },
                    "permissionLevel": {
                        "type": "string",
                        "description": "Permission level of the requesting user on the object. For what is allowed at each level, see MLflow Model permissions.\n"
                    },
                    "tags": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelTag:getMlflowModelTag"
                        },
                        "description": "Array of tags associated with the model.\n"
                    },
                    "userId": {
                        "type": "string",
                        "description": "The username of the user that created the object.\n"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getMlflowModel.\n",
                "properties": {
                    "description": {
                        "description": "User-specified description for the object.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "Unique identifier for the object.\n",
                        "type": "string"
                    },
                    "latestVersions": {
                        "description": "Array of model versions, each the latest version for its stage.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelLatestVersion:getMlflowModelLatestVersion"
                        },
                        "type": "array"
                    },
                    "name": {
                        "description": "Name of the model.\n",
                        "type": "string"
                    },
                    "permissionLevel": {
                        "description": "Permission level of the requesting user on the object. For what is allowed at each level, see MLflow Model permissions.\n",
                        "type": "string"
                    },
                    "tags": {
                        "description": "Array of tags associated with the model.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getMlflowModelTag:getMlflowModelTag"
                        },
                        "type": "array"
                    },
                    "userId": {
                        "description": "The username of the user that created the object.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "description",
                    "id",
                    "latestVersions",
                    "name",
                    "permissionLevel",
                    "tags",
                    "userId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsCredentials:getMwsCredentials": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nLists all databricks.MwsCredentials in Databricks Account.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work.\n\n## Example Usage\n\nListing all credentials in Databricks Account\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMwsCredentials({});\nexport const allMwsCredentials = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_mws_credentials()\npulumi.export(\"allMwsCredentials\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMwsCredentials.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMwsCredentials\"] = all.Apply(getMwsCredentialsResult =\u003e getMwsCredentialsResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.LookupMwsCredentials(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMwsCredentials\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMwsCredentials();\n\n        ctx.export(\"allMwsCredentials\", all.applyValue(getMwsCredentialsResult -\u003e getMwsCredentialsResult.ids()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getMwsCredentials\n      Arguments: {}\noutputs:\n  allMwsCredentials: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* Provisioning Databricks on AWS guide.\n* databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.\n* databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).\n* databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) \u0026 subnets for new workspaces within AWS.\n* databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.\n* databricks.MwsWorkspaces to set up [workspaces in E2 architecture on AWS](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsCredentials.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsCredentials.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the credentials in the account\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getMwsWorkspaces:getMwsWorkspaces": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nLists all databricks.MwsWorkspaces in Databricks Account.\n\n\u003e **Note** `account_id` provider configuration property is required for this resource to work.\n\n## Example Usage\n\nListing all workspaces in\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getMwsWorkspaces({});\nexport const allMwsWorkspaces = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_mws_workspaces()\npulumi.export(\"allMwsWorkspaces\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetMwsWorkspaces.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allMwsWorkspaces\"] = all.Apply(getMwsWorkspacesResult =\u003e getMwsWorkspacesResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.LookupMwsWorkspaces(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allMwsWorkspaces\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getMwsWorkspaces();\n\n        ctx.export(\"allMwsWorkspaces\", all.applyValue(getMwsWorkspacesResult -\u003e getMwsWorkspacesResult.ids()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getMwsWorkspaces\n      Arguments: {}\noutputs:\n  allMwsWorkspaces: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.MwsWorkspaces to manage Databricks E2 Workspaces.\n* databricks.MetastoreAssignment\n",
            "inputs": {
                "description": "A collection of arguments for invoking getMwsWorkspaces.\n",
                "properties": {
                    "ids": {
                        "type": "object",
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getMwsWorkspaces.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "additionalProperties": {
                            "$ref": "pulumi.json#/Any"
                        },
                        "description": "name-to-id map for all of the workspaces in the account\n",
                        "type": "object"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNodeType:getNodeType": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nGets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.\n\n\u003e **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn't match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string",
                        "description": "Node category, which can be one of (depending on the cloud environment, could be checked with `databricks clusters list-node-types -o json|jq '.node_types[]|.category'|sort |uniq`):\n* `General Purpose` (all clouds)\n* `General Purpose (HDD)` (Azure)\n* `Compute Optimized` (all clouds)\n* `Memory Optimized` (all clouds)\n* `Memory Optimized (Remote HDD)` (Azure)\n* `Storage Optimized` (AWS, Azure)\n* `GPU Accelerated` (AWS, Azure)\n",
                        "willReplaceOnChanges": true
                    },
                    "fleet": {
                        "type": "boolean",
                        "description": "if we should limit the search only to [AWS fleet instance types](https://docs.databricks.com/compute/aws-fleet-instances.html). Default to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "gbPerCore": {
                        "type": "integer",
                        "description": "Number of gigabytes per core available on instance. Conflicts with `min_memory_gb`. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to nodes with AWS Graviton CPUs. Default to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "id": {
                        "type": "string",
                        "description": "node type, that can be used for databricks_job, databricks_cluster, or databricks_instance_pool.\n"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean",
                        "description": ". Pick only nodes that have IO Cache. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "localDisk": {
                        "type": "boolean",
                        "description": "Pick only nodes with local storage. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "localDiskMinSize": {
                        "type": "integer",
                        "description": "Pick only nodes that have size local storage greater or equal to given value. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minCores": {
                        "type": "integer",
                        "description": "Minimum number of CPU cores available on instance. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minGpus": {
                        "type": "integer",
                        "description": "Minimum number of GPU's attached to instance. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "minMemoryGb": {
                        "type": "integer",
                        "description": "Minimum amount of memory per node in gigabytes. Defaults to _0_.\n",
                        "willReplaceOnChanges": true
                    },
                    "photonDriverCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon driver. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "photonWorkerCapable": {
                        "type": "boolean",
                        "description": "Pick only nodes that can run Photon workers. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    },
                    "supportPortForwarding": {
                        "type": "boolean",
                        "description": "Pick only nodes that support port forwarding. Defaults to _false_.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getNodeType.\n",
                "properties": {
                    "category": {
                        "type": "string"
                    },
                    "fleet": {
                        "type": "boolean"
                    },
                    "gbPerCore": {
                        "type": "integer"
                    },
                    "graviton": {
                        "type": "boolean"
                    },
                    "id": {
                        "description": "node type, that can be used for databricks_job, databricks_cluster, or databricks_instance_pool.\n",
                        "type": "string"
                    },
                    "isIoCacheEnabled": {
                        "type": "boolean"
                    },
                    "localDisk": {
                        "type": "boolean"
                    },
                    "localDiskMinSize": {
                        "type": "integer"
                    },
                    "minCores": {
                        "type": "integer"
                    },
                    "minGpus": {
                        "type": "integer"
                    },
                    "minMemoryGb": {
                        "type": "integer"
                    },
                    "photonDriverCapable": {
                        "type": "boolean"
                    },
                    "photonWorkerCapable": {
                        "type": "boolean"
                    },
                    "supportPortForwarding": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNotebook:getNotebook": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to export a notebook from Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst features = databricks.getNotebook({\n    path: \"/Production/Features\",\n    format: \"SOURCE\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nfeatures = databricks.get_notebook(path=\"/Production/Features\",\n    format=\"SOURCE\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var features = Databricks.GetNotebook.Invoke(new()\n    {\n        Path = \"/Production/Features\",\n        Format = \"SOURCE\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.LookupNotebook(ctx, \u0026databricks.LookupNotebookArgs{\n\t\t\tPath:   \"/Production/Features\",\n\t\t\tFormat: \"SOURCE\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()\n            .path(\"/Production/Features\")\n            .format(\"SOURCE\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  features:\n    fn::invoke:\n      Function: databricks:getNotebook\n      Arguments:\n        path: /Production/Features\n        format: SOURCE\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebook.\n",
                "properties": {
                    "format": {
                        "type": "string",
                        "description": "Notebook format to export. Either `SOURCE`, `HTML`, `JUPYTER`, or `DBC`.\n",
                        "willReplaceOnChanges": true
                    },
                    "language": {
                        "type": "string",
                        "description": "notebook language\n"
                    },
                    "objectId": {
                        "type": "integer",
                        "description": "notebook object ID\n"
                    },
                    "objectType": {
                        "type": "string",
                        "description": "notebook object type\n"
                    },
                    "path": {
                        "type": "string",
                        "description": "Notebook path on the workspace\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "format",
                    "path"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebook.\n",
                "properties": {
                    "content": {
                        "description": "notebook content in selected format\n",
                        "type": "string"
                    },
                    "format": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "language": {
                        "description": "notebook language\n",
                        "type": "string"
                    },
                    "objectId": {
                        "description": "notebook object ID\n",
                        "type": "integer"
                    },
                    "objectType": {
                        "description": "notebook object type\n",
                        "type": "string"
                    },
                    "path": {
                        "type": "string"
                    }
                },
                "required": [
                    "content",
                    "format",
                    "language",
                    "objectId",
                    "objectType",
                    "path",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getNotebookPaths:getNotebookPaths": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows to list notebooks in the Databricks Workspace.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst prod = databricks.getNotebookPaths({\n    path: \"/Production\",\n    recursive: true,\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nprod = databricks.get_notebook_paths(path=\"/Production\",\n    recursive=True)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var prod = Databricks.GetNotebookPaths.Invoke(new()\n    {\n        Path = \"/Production\",\n        Recursive = true,\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetNotebookPaths(ctx, \u0026databricks.GetNotebookPathsArgs{\n\t\t\tPath:      \"/Production\",\n\t\t\tRecursive: true,\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNotebookPathsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()\n            .path(\"/Production\")\n            .recursive(true)\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  prod:\n    fn::invoke:\n      Function: databricks:getNotebookPaths\n      Arguments:\n        path: /Production\n        recursive: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "inputs": {
                "description": "A collection of arguments for invoking getNotebookPaths.\n",
                "properties": {
                    "path": {
                        "type": "string",
                        "description": "Path to workspace directory\n",
                        "willReplaceOnChanges": true
                    },
                    "recursive": {
                        "type": "boolean",
                        "description": "Either or recursively walk given path\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "path",
                    "recursive"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getNotebookPaths.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "notebookPathLists": {
                        "description": "list of objects with `path` and `language` attributes\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getNotebookPathsNotebookPathList:getNotebookPathsNotebookPathList"
                        },
                        "type": "array"
                    },
                    "path": {
                        "type": "string"
                    },
                    "recursive": {
                        "type": "boolean"
                    }
                },
                "required": [
                    "notebookPathLists",
                    "path",
                    "recursive",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getPipelines:getPipelines": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.\n\nRetrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.\n\n## Example Usage\n\nGet all Delta Live Tables pipelines:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getPipelines({});\nexport const allPipelines = all.then(all =\u003e all.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_pipelines()\npulumi.export(\"allPipelines\", all.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetPipelines.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allPipelines\"] = all.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetPipelines(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allPipelines\", all.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getPipelines();\n\n        ctx.export(\"allPipelines\", all.applyValue(getPipelinesResult -\u003e getPipelinesResult.ids()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getPipelines\n      Arguments: {}\noutputs:\n  allPipelines: ${all.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFilter Delta Live Tables pipelines by name (exact match):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getPipelines({\n    pipelineName: \"my_pipeline\",\n});\nexport const myPipeline = _this.then(_this =\u003e _this.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_pipelines(pipeline_name=\"my_pipeline\")\npulumi.export(\"myPipeline\", this.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetPipelines.Invoke(new()\n    {\n        PipelineName = \"my_pipeline\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"myPipeline\"] = @this.Apply(@this =\u003e @this.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetPipelines(ctx, \u0026databricks.GetPipelinesArgs{\n\t\t\tPipelineName: pulumi.StringRef(\"my_pipeline\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"myPipeline\", this.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()\n            .pipelineName(\"my_pipeline\")\n            .build());\n\n        ctx.export(\"myPipeline\", this_.ids());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getPipelines\n      Arguments:\n        pipelineName: my_pipeline\noutputs:\n  myPipeline: ${this.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nFilter Delta Live Tables pipelines by name (wildcard search):\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getPipelines({\n    pipelineName: \"%pipeline%\",\n});\nexport const wildcardPipelines = _this.then(_this =\u003e _this.ids);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_pipelines(pipeline_name=\"%pipeline%\")\npulumi.export(\"wildcardPipelines\", this.ids)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetPipelines.Invoke(new()\n    {\n        PipelineName = \"%pipeline%\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"wildcardPipelines\"] = @this.Apply(@this =\u003e @this.Apply(getPipelinesResult =\u003e getPipelinesResult.Ids)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetPipelines(ctx, \u0026databricks.GetPipelinesArgs{\n\t\t\tPipelineName: pulumi.StringRef(\"%pipeline%\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"wildcardPipelines\", this.Ids)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetPipelinesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()\n            .pipelineName(\"%pipeline%\")\n            .build());\n\n        ctx.export(\"wildcardPipelines\", this_.ids());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getPipelines\n      Arguments:\n        pipelineName: '%pipeline%'\noutputs:\n  wildcardPipelines: ${this.ids}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n* databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getPipelines.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of ids for [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipelines matching the provided search criteria.\n"
                    },
                    "pipelineName": {
                        "type": "string",
                        "description": "Filter Delta Live Tables pipelines by name for a given search term. `%` is the supported wildcard operator.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getPipelines.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "List of ids for [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipelines matching the provided search criteria.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "pipelineName": {
                        "type": "string"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSchemas:getSchemas": {
            "description": "## Example Usage\n\nListing all schemas in a _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst sandbox = databricks.getSchemas({\n    catalogName: \"sandbox\",\n});\nexport const allSandboxSchemas = sandbox;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nsandbox = databricks.get_schemas(catalog_name=\"sandbox\")\npulumi.export(\"allSandboxSchemas\", sandbox)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var sandbox = Databricks.GetSchemas.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allSandboxSchemas\"] = sandbox,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tsandbox, err := databricks.GetSchemas(ctx, \u0026databricks.GetSchemasArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allSandboxSchemas\", sandbox)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSchemasArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()\n            .catalogName(\"sandbox\")\n            .build());\n\n        ctx.export(\"allSandboxSchemas\", sandbox.applyValue(getSchemasResult -\u003e getSchemasResult));\n    }\n}\n```\n```yaml\nvariables:\n  sandbox:\n    fn::invoke:\n      Function: databricks:getSchemas\n      Arguments:\n        catalogName: sandbox\noutputs:\n  allSandboxSchemas: ${sandbox}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n"
                    }
                },
                "type": "object",
                "required": [
                    "catalogName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getSchemas.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Schema full names: *`catalog`.`schema`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getServicePrincipal:getServicePrincipal": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_service_principal.\n\n## Example Usage\n\nAdding service principal `11111111-2222-3333-4444-555666777888` to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst spn = databricks.getServicePrincipal({\n    applicationId: \"11111111-2222-3333-4444-555666777888\",\n});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: spn.then(spn =\u003e spn.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nspn = databricks.get_service_principal(application_id=\"11111111-2222-3333-4444-555666777888\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=spn.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var spn = Databricks.GetServicePrincipal.Invoke(new()\n    {\n        ApplicationId = \"11111111-2222-3333-4444-555666777888\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = spn.Apply(getServicePrincipalResult =\u003e getServicePrincipalResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tspn, err := databricks.LookupServicePrincipal(ctx, \u0026databricks.LookupServicePrincipalArgs{\n\t\t\tApplicationId: pulumi.StringRef(\"11111111-2222-3333-4444-555666777888\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: pulumi.String(spn.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetServicePrincipalArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()\n            .applicationId(\"11111111-2222-3333-4444-555666777888\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(spn.applyValue(getServicePrincipalResult -\u003e getServicePrincipalResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${spn.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n  spn:\n    fn::invoke:\n      Function: databricks:getServicePrincipal\n      Arguments:\n        applicationId: 11111111-2222-3333-4444-555666777888\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n- End to end workspace management guide.\n- databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks_service principal to manage service principals\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipal.\n",
                "properties": {
                    "aclPrincipalId": {
                        "type": "string",
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n"
                    },
                    "active": {
                        "type": "boolean",
                        "description": "Whether service principal is active or not.\n"
                    },
                    "applicationId": {
                        "type": "string",
                        "description": "ID of the service principal. The service principal must exist before this resource can be retrieved.\n"
                    },
                    "displayName": {
                        "type": "string",
                        "description": "Exact display name of the service principal. The service principal must exist before this resource can be retrieved.  In case if there are several service principals with the same name, an error is thrown.\n"
                    },
                    "externalId": {
                        "type": "string",
                        "description": "ID of the service principal in an external identity provider.\n"
                    },
                    "home": {
                        "type": "string",
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The id of the service principal.\n"
                    },
                    "repos": {
                        "type": "string",
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipal.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `servicePrincipals/00000000-0000-0000-0000-000000000000`.\n",
                        "type": "string"
                    },
                    "active": {
                        "description": "Whether service principal is active or not.\n",
                        "type": "boolean"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "description": "Display name of the service principal, e.g. `Foo SPN`.\n",
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the service principal in an external identity provider.\n",
                        "type": "string"
                    },
                    "home": {
                        "description": "Home folder of the service principal, e.g. `/Users/11111111-2222-3333-4444-555666777888`.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The id of the service principal.\n",
                        "type": "string"
                    },
                    "repos": {
                        "description": "Repos location of the service principal, e.g. `/Repos/11111111-2222-3333-4444-555666777888`.\n",
                        "type": "string"
                    },
                    "spId": {
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "active",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "id",
                    "repos",
                    "spId"
                ],
                "type": "object"
            }
        },
        "databricks:index/getServicePrincipals:getServicePrincipals": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n"
                    },
                    "displayNameContains": {
                        "type": "string",
                        "description": "Only return databricks.ServicePrincipal display name that match the given name string\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getServicePrincipals.\n",
                "properties": {
                    "applicationIds": {
                        "description": "List of `application_ids` of service principals Individual service principal can be retrieved using databricks.ServicePrincipal data source\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "displayNameContains": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "applicationIds",
                    "displayNameContains",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getShare:getShare": {
            "description": "## Example Usage\n\nGetting details of an existing share in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getShare({\n    name: \"this\",\n});\nexport const createdBy = _this.then(_this =\u003e _this.createdBy);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_share(name=\"this\")\npulumi.export(\"createdBy\", this.created_by)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShare.Invoke(new()\n    {\n        Name = \"this\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"createdBy\"] = @this.Apply(@this =\u003e @this.Apply(getShareResult =\u003e getShareResult.CreatedBy)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.LookupShare(ctx, \u0026databricks.LookupShareArgs{\n\t\t\tName: pulumi.StringRef(\"this\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"createdBy\", this.CreatedBy)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetShareArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShare(GetShareArgs.builder()\n            .name(\"this\")\n            .build());\n\n        ctx.export(\"createdBy\", this_.createdBy());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getShare\n      Arguments:\n        name: this\noutputs:\n  createdBy: ${this.createdBy}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShare.\n",
                "properties": {
                    "createdAt": {
                        "type": "integer",
                        "description": "Time when the share was created.\n"
                    },
                    "createdBy": {
                        "type": "string",
                        "description": "The principal that created the share.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "The name of the share\n"
                    },
                    "objects": {
                        "type": "array",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "description": "arrays containing details of each object in the share.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShare.\n",
                "properties": {
                    "createdAt": {
                        "description": "Time when the share was created.\n",
                        "type": "integer"
                    },
                    "createdBy": {
                        "description": "The principal that created the share.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "description": "Full name of the object being shared.\n",
                        "type": "string"
                    },
                    "objects": {
                        "description": "arrays containing details of each object in the share.\n",
                        "items": {
                            "$ref": "#/types/databricks:index/getShareObject:getShareObject"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "createdAt",
                    "createdBy",
                    "name",
                    "objects",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getShares:getShares": {
            "description": "## Example Usage\n\nGetting all existing shares in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getShares({});\nexport const shareName = _this.then(_this =\u003e _this.shares);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_shares()\npulumi.export(\"shareName\", this.shares)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetShares.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"shareName\"] = @this.Apply(@this =\u003e @this.Apply(getSharesResult =\u003e getSharesResult.Shares)),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetShares(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"shareName\", this.Shares)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSharesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getShares();\n\n        ctx.export(\"shareName\", this_.shares());\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getShares\n      Arguments: {}\noutputs:\n  shareName: ${this.shares}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Share to create Delta Sharing shares.\n* databricks.Recipient to create Delta Sharing recipients.\n* databricks.Grants to manage Delta Sharing permissions.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getShares.\n",
                "properties": {
                    "shares": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.Share names.\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getShares.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "shares": {
                        "description": "list of databricks.Share names.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "shares",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSparkVersion:getSparkVersion": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nGets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.\n\n\u003e **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst withGpu = databricks.getNodeType({\n    localDisk: true,\n    minCores: 16,\n    gbPerCore: 1,\n    minGpus: 1,\n});\nconst gpuMl = databricks.getSparkVersion({\n    gpu: true,\n    ml: true,\n});\nconst research = new databricks.Cluster(\"research\", {\n    clusterName: \"Research Cluster\",\n    sparkVersion: gpuMl.then(gpuMl =\u003e gpuMl.id),\n    nodeTypeId: withGpu.then(withGpu =\u003e withGpu.id),\n    autoterminationMinutes: 20,\n    autoscale: {\n        minWorkers: 1,\n        maxWorkers: 50,\n    },\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nwith_gpu = databricks.get_node_type(local_disk=True,\n    min_cores=16,\n    gb_per_core=1,\n    min_gpus=1)\ngpu_ml = databricks.get_spark_version(gpu=True,\n    ml=True)\nresearch = databricks.Cluster(\"research\",\n    cluster_name=\"Research Cluster\",\n    spark_version=gpu_ml.id,\n    node_type_id=with_gpu.id,\n    autotermination_minutes=20,\n    autoscale=databricks.ClusterAutoscaleArgs(\n        min_workers=1,\n        max_workers=50,\n    ))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var withGpu = Databricks.GetNodeType.Invoke(new()\n    {\n        LocalDisk = true,\n        MinCores = 16,\n        GbPerCore = 1,\n        MinGpus = 1,\n    });\n\n    var gpuMl = Databricks.GetSparkVersion.Invoke(new()\n    {\n        Gpu = true,\n        Ml = true,\n    });\n\n    var research = new Databricks.Cluster(\"research\", new()\n    {\n        ClusterName = \"Research Cluster\",\n        SparkVersion = gpuMl.Apply(getSparkVersionResult =\u003e getSparkVersionResult.Id),\n        NodeTypeId = withGpu.Apply(getNodeTypeResult =\u003e getNodeTypeResult.Id),\n        AutoterminationMinutes = 20,\n        Autoscale = new Databricks.Inputs.ClusterAutoscaleArgs\n        {\n            MinWorkers = 1,\n            MaxWorkers = 50,\n        },\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\twithGpu, err := databricks.GetNodeType(ctx, \u0026databricks.GetNodeTypeArgs{\n\t\t\tLocalDisk: pulumi.BoolRef(true),\n\t\t\tMinCores:  pulumi.IntRef(16),\n\t\t\tGbPerCore: pulumi.IntRef(1),\n\t\t\tMinGpus:   pulumi.IntRef(1),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tgpuMl, err := databricks.GetSparkVersion(ctx, \u0026databricks.GetSparkVersionArgs{\n\t\t\tGpu: pulumi.BoolRef(true),\n\t\t\tMl:  pulumi.BoolRef(true),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewCluster(ctx, \"research\", \u0026databricks.ClusterArgs{\n\t\t\tClusterName:            pulumi.String(\"Research Cluster\"),\n\t\t\tSparkVersion:           pulumi.String(gpuMl.Id),\n\t\t\tNodeTypeId:             pulumi.String(withGpu.Id),\n\t\t\tAutoterminationMinutes: pulumi.Int(20),\n\t\t\tAutoscale: \u0026databricks.ClusterAutoscaleArgs{\n\t\t\t\tMinWorkers: pulumi.Int(1),\n\t\t\t\tMaxWorkers: pulumi.Int(50),\n\t\t\t},\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetNodeTypeArgs;\nimport com.pulumi.databricks.inputs.GetSparkVersionArgs;\nimport com.pulumi.databricks.Cluster;\nimport com.pulumi.databricks.ClusterArgs;\nimport com.pulumi.databricks.inputs.ClusterAutoscaleArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()\n            .localDisk(true)\n            .minCores(16)\n            .gbPerCore(1)\n            .minGpus(1)\n            .build());\n\n        final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()\n            .gpu(true)\n            .ml(true)\n            .build());\n\n        var research = new Cluster(\"research\", ClusterArgs.builder()        \n            .clusterName(\"Research Cluster\")\n            .sparkVersion(gpuMl.applyValue(getSparkVersionResult -\u003e getSparkVersionResult.id()))\n            .nodeTypeId(withGpu.applyValue(getNodeTypeResult -\u003e getNodeTypeResult.id()))\n            .autoterminationMinutes(20)\n            .autoscale(ClusterAutoscaleArgs.builder()\n                .minWorkers(1)\n                .maxWorkers(50)\n                .build())\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  research:\n    type: databricks:Cluster\n    properties:\n      clusterName: Research Cluster\n      sparkVersion: ${gpuMl.id}\n      nodeTypeId: ${withGpu.id}\n      autoterminationMinutes: 20\n      autoscale:\n        minWorkers: 1\n        maxWorkers: 50\nvariables:\n  withGpu:\n    fn::invoke:\n      Function: databricks:getNodeType\n      Arguments:\n        localDisk: true\n        minCores: 16\n        gbPerCore: 1\n        minGpus: 1\n  gpuMl:\n    fn::invoke:\n      Function: databricks:getSparkVersion\n      Arguments:\n        gpu: true\n        ml: true\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* End to end workspace management guide.\n* databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).\n* databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.\n* databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.\n* databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that are in Beta stage. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "genomics": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "gpu": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes that support GPUs. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "graviton": {
                        "type": "boolean",
                        "description": "if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`. _Deprecated with DBR 14.0 release. DBR version compiled for Graviton will be automatically installed when nodes with Graviton CPUs are specified in the cluster configuration._\n",
                        "deprecationMessage": "Not required anymore - it's automatically enabled on the Graviton-based node types",
                        "willReplaceOnChanges": true
                    },
                    "latest": {
                        "type": "boolean",
                        "description": "if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.\n",
                        "willReplaceOnChanges": true
                    },
                    "longTermSupport": {
                        "type": "boolean",
                        "description": "if we should limit the search only to LTS (long term support) \u0026 ESR (extended support) versions. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "ml": {
                        "type": "boolean",
                        "description": "if we should limit the search only to ML runtimes. Default to `false`.\n",
                        "willReplaceOnChanges": true
                    },
                    "photon": {
                        "type": "boolean",
                        "description": "if we should limit the search only to Photon runtimes. Default to `false`. *Deprecated with DBR 14.0 release. Specify `runtime_engine=\\\"PHOTON\\\"` in the cluster configuration instead!*\n",
                        "deprecationMessage": "Specify runtime_engine=\"PHOTON\" in the cluster configuration",
                        "willReplaceOnChanges": true
                    },
                    "scala": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.\n",
                        "willReplaceOnChanges": true
                    },
                    "sparkVersion": {
                        "type": "string",
                        "description": "if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSparkVersion.\n",
                "properties": {
                    "beta": {
                        "type": "boolean"
                    },
                    "genomics": {
                        "type": "boolean"
                    },
                    "gpu": {
                        "type": "boolean"
                    },
                    "graviton": {
                        "deprecationMessage": "Not required anymore - it's automatically enabled on the Graviton-based node types",
                        "type": "boolean"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "latest": {
                        "type": "boolean"
                    },
                    "longTermSupport": {
                        "type": "boolean"
                    },
                    "ml": {
                        "type": "boolean"
                    },
                    "photon": {
                        "deprecationMessage": "Specify runtime_engine=\"PHOTON\" in the cluster configuration",
                        "type": "boolean"
                    },
                    "scala": {
                        "type": "string"
                    },
                    "sparkVersion": {
                        "type": "string"
                    }
                },
                "required": [
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSqlWarehouse:getSqlWarehouse": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.\n\n## Example Usage\n\n* Retrieve attributes of each SQL warehouses in a workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\nconst allGetSqlWarehouse = .reduce((__obj, [, ]) =\u003e ({ ...__obj, [__key]: databricks.getSqlWarehouse({\n    id: __value,\n}) }));\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\nall_get_sql_warehouse = {__key: databricks.get_sql_warehouse(id=__value) for __key, __value in warehouses[\"ids\"]}\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n    var allGetSqlWarehouse = .ToDictionary(item =\u003e {\n        var __key = item.Key;\n        return __key;\n    }, item =\u003e {\n        var __value = item.Value;\n        return Databricks.GetSqlWarehouse.Invoke(new()\n        {\n            Id = __value,\n        });\n    });\n\n});\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n* Search for a specific SQL Warehouse by name:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouse({\n    name: \"Starter Warehouse\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouse(name=\"Starter Warehouse\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouse.Invoke(new()\n    {\n        Name = \"Starter Warehouse\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouse(ctx, \u0026databricks.GetSqlWarehouseArgs{\n\t\t\tName: pulumi.StringRef(\"Starter Warehouse\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehouseArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()\n            .name(\"Starter Warehouse\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getSqlWarehouse\n      Arguments:\n        name: Starter Warehouse\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "type": "integer",
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "type": "string",
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n"
                    },
                    "creatorName": {
                        "type": "string",
                        "description": "The username of the user who created the endpoint.\n"
                    },
                    "dataSourceId": {
                        "type": "string",
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n"
                    },
                    "enablePhoton": {
                        "type": "boolean",
                        "description": "Whether [Photon](https://databricks.com/product/delta-engine) is enabled.\n"
                    },
                    "enableServerlessCompute": {
                        "type": "boolean",
                        "description": "Whether this SQL warehouse is a serverless SQL warehouse.\n"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth",
                        "description": "Health status of the endpoint.\n"
                    },
                    "id": {
                        "type": "string",
                        "description": "The ID of the SQL warehouse.\n"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "type": "string",
                        "description": "JDBC connection string.\n"
                    },
                    "maxNumClusters": {
                        "type": "integer",
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "minNumClusters": {
                        "type": "integer",
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n"
                    },
                    "name": {
                        "type": "string",
                        "description": "Name of the SQL warehouse to search (case-sensitive).\n"
                    },
                    "numActiveSessions": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "numClusters": {
                        "type": "integer",
                        "description": "The current number of clusters used by the endpoint.\n"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "type": "string",
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n"
                    },
                    "state": {
                        "type": "string",
                        "description": "The current state of the endpoint.\n"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "tags used for SQL warehouse resources.\n"
                    },
                    "warehouseType": {
                        "type": "string",
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/index.html#warehouse-types) or [Azure](https://learn.microsoft.com/azure/databricks/sql/#warehouse-types).\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouse.\n",
                "properties": {
                    "autoStopMins": {
                        "description": "Time in minutes until an idle SQL warehouse terminates all clusters and stops.\n",
                        "type": "integer"
                    },
                    "channel": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseChannel:getSqlWarehouseChannel",
                        "description": "block, consisting of following fields:\n"
                    },
                    "clusterSize": {
                        "description": "The size of the clusters allocated to the warehouse: \"2X-Small\", \"X-Small\", \"Small\", \"Medium\", \"Large\", \"X-Large\", \"2X-Large\", \"3X-Large\", \"4X-Large\".\n",
                        "type": "string"
                    },
                    "creatorName": {
                        "description": "The username of the user who created the endpoint.\n",
                        "type": "string"
                    },
                    "dataSourceId": {
                        "description": "ID of the data source for this warehouse. This is used to bind an Databricks SQL query to an warehouse.\n",
                        "type": "string"
                    },
                    "enablePhoton": {
                        "description": "Whether [Photon](https://databricks.com/product/delta-engine) is enabled.\n",
                        "type": "boolean"
                    },
                    "enableServerlessCompute": {
                        "description": "Whether this SQL warehouse is a serverless SQL warehouse.\n",
                        "type": "boolean"
                    },
                    "health": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseHealth:getSqlWarehouseHealth",
                        "description": "Health status of the endpoint.\n"
                    },
                    "id": {
                        "description": "The ID of the SQL warehouse.\n",
                        "type": "string"
                    },
                    "instanceProfileArn": {
                        "type": "string"
                    },
                    "jdbcUrl": {
                        "description": "JDBC connection string.\n",
                        "type": "string"
                    },
                    "maxNumClusters": {
                        "description": "Maximum number of clusters available when a SQL warehouse is running.\n",
                        "type": "integer"
                    },
                    "minNumClusters": {
                        "description": "Minimum number of clusters available when a SQL warehouse is running.\n",
                        "type": "integer"
                    },
                    "name": {
                        "description": "Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.\n",
                        "type": "string"
                    },
                    "numActiveSessions": {
                        "description": "The current number of clusters used by the endpoint.\n",
                        "type": "integer"
                    },
                    "numClusters": {
                        "description": "The current number of clusters used by the endpoint.\n",
                        "type": "integer"
                    },
                    "odbcParams": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseOdbcParams:getSqlWarehouseOdbcParams",
                        "description": "ODBC connection params: `odbc_params.hostname`, `odbc_params.path`, `odbc_params.protocol`, and `odbc_params.port`.\n"
                    },
                    "spotInstancePolicy": {
                        "description": "The spot policy to use for allocating instances to clusters: `COST_OPTIMIZED` or `RELIABILITY_OPTIMIZED`.\n",
                        "type": "string"
                    },
                    "state": {
                        "description": "The current state of the endpoint.\n",
                        "type": "string"
                    },
                    "tags": {
                        "$ref": "#/types/databricks:index/getSqlWarehouseTags:getSqlWarehouseTags",
                        "description": "tags used for SQL warehouse resources.\n"
                    },
                    "warehouseType": {
                        "description": "SQL warehouse type. See for [AWS](https://docs.databricks.com/sql/index.html#warehouse-types) or [Azure](https://learn.microsoft.com/azure/databricks/sql/#warehouse-types).\n",
                        "type": "string"
                    }
                },
                "required": [
                    "autoStopMins",
                    "channel",
                    "clusterSize",
                    "creatorName",
                    "dataSourceId",
                    "enablePhoton",
                    "enableServerlessCompute",
                    "health",
                    "id",
                    "instanceProfileArn",
                    "jdbcUrl",
                    "maxNumClusters",
                    "minNumClusters",
                    "name",
                    "numActiveSessions",
                    "numClusters",
                    "odbcParams",
                    "spotInstancePolicy",
                    "state",
                    "tags",
                    "warehouseType"
                ],
                "type": "object"
            }
        },
        "databricks:index/getSqlWarehouses:getSqlWarehouses": {
            "description": "## Example Usage\n\nRetrieve all SQL warehouses on this workspace on AWS or GCP:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getSqlWarehouses({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_sql_warehouses()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetSqlWarehouses.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getSqlWarehouses();\n\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\nRetrieve all clusters with \"Shared\" in their cluster name on this Azure Databricks workspace:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst allShared = databricks.getSqlWarehouses({\n    warehouseNameContains: \"shared\",\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall_shared = databricks.get_sql_warehouses(warehouse_name_contains=\"shared\")\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var allShared = Databricks.GetSqlWarehouses.Invoke(new()\n    {\n        WarehouseNameContains = \"shared\",\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetSqlWarehouses(ctx, \u0026databricks.GetSqlWarehousesArgs{\n\t\t\tWarehouseNameContains: pulumi.StringRef(\"shared\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetSqlWarehousesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()\n            .warehouseNameContains(\"shared\")\n            .build());\n\n    }\n}\n```\n```yaml\nvariables:\n  allShared:\n    fn::invoke:\n      Function: databricks:getSqlWarehouses\n      Arguments:\n        warehouseNameContains: shared\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are often used in the same context:\n\n* End to end workspace management guide.\n* databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.\n* databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).\n* databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace.\n* databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).\n",
            "inputs": {
                "description": "A collection of arguments for invoking getSqlWarehouses.\n",
                "properties": {
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "list of databricks.SqlEndpoint ids\n"
                    },
                    "warehouseNameContains": {
                        "type": "string",
                        "description": "Only return databricks.SqlEndpoint ids that match the given name string.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getSqlWarehouses.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "list of databricks.SqlEndpoint ids\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "warehouseNameContains": {
                        "type": "string"
                    }
                },
                "required": [
                    "ids",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getStorageCredential:getStorageCredential": {
            "description": "## Example Usage\n\nGetting details of an existing storage credential in the metastore\n\n",
            "inputs": {
                "description": "A collection of arguments for invoking getStorageCredential.\n",
                "properties": {
                    "name": {
                        "type": "string",
                        "description": "The name of the storage credential\n",
                        "willReplaceOnChanges": true
                    },
                    "storageCredentialInfo": {
                        "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo"
                    }
                },
                "type": "object",
                "required": [
                    "name"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getStorageCredential.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "name": {
                        "type": "string"
                    },
                    "storageCredentialInfo": {
                        "$ref": "#/types/databricks:index/getStorageCredentialStorageCredentialInfo:getStorageCredentialStorageCredentialInfo"
                    }
                },
                "required": [
                    "name",
                    "storageCredentialInfo",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getStorageCredentials:getStorageCredentials": {
            "description": "## Example Usage\n\nList all storage credentials in the metastore\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst all = databricks.getStorageCredentials({});\nexport const allStorageCredentials = all.then(all =\u003e all.names);\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nall = databricks.get_storage_credentials()\npulumi.export(\"allStorageCredentials\", all.names)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var all = Databricks.GetStorageCredentials.Invoke();\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allStorageCredentials\"] = all.Apply(getStorageCredentialsResult =\u003e getStorageCredentialsResult.Names),\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tall, err := databricks.GetStorageCredentials(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allStorageCredentials\", all.Names)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetStorageCredentialsArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var all = DatabricksFunctions.getStorageCredentials();\n\n        ctx.export(\"allStorageCredentials\", all.applyValue(getStorageCredentialsResult -\u003e getStorageCredentialsResult.names()));\n    }\n}\n```\n```yaml\nvariables:\n  all:\n    fn::invoke:\n      Function: databricks:getStorageCredentials\n      Arguments: {}\noutputs:\n  allStorageCredentials: ${all.names}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.StorageCredential to get information about a single credential\n* databricks.StorageCredential to manage Storage Credentials within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getStorageCredentials.\n",
                "properties": {
                    "names": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "List of names of databricks.StorageCredential in the metastore\n"
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getStorageCredentials.\n",
                "properties": {
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "names": {
                        "description": "List of names of databricks.StorageCredential in the metastore\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "names",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getTables:getTables": {
            "description": "## Example Usage\n\nGranting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nexport = async () =\u003e {\n    const things = await databricks.getTables({\n        catalogName: \"sandbox\",\n        schemaName: \"things\",\n    });\n    const thingsGrants: databricks.Grants[] = [];\n    for (const range of things.ids.map((v, k) =\u003e ({key: k, value: v}))) {\n        thingsGrants.push(new databricks.Grants(`things-${range.key}`, {\n            table: range.value,\n            grants: [{\n                principal: \"sensitive\",\n                privileges: [\n                    \"SELECT\",\n                    \"MODIFY\",\n                ],\n            }],\n        }));\n    }\n}\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthings = databricks.get_tables(catalog_name=\"sandbox\",\n    schema_name=\"things\")\nthings_grants = []\nfor range in [{\"key\": k, \"value\": v} for [k, v] in enumerate(things.ids)]:\n    things_grants.append(databricks.Grants(f\"things-{range['key']}\",\n        table=range[\"value\"],\n        grants=[databricks.GrantsGrantArgs(\n            principal=\"sensitive\",\n            privileges=[\n                \"SELECT\",\n                \"MODIFY\",\n            ],\n        )]))\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing System.Threading.Tasks;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(async() =\u003e \n{\n    var things = await Databricks.GetTables.InvokeAsync(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    var thingsGrants = new List\u003cDatabricks.Grants\u003e();\n    foreach (var range in )\n    {\n        thingsGrants.Add(new Databricks.Grants($\"things-{range.Key}\", new()\n        {\n            Table = range.Value,\n            GrantDetails = new[]\n            {\n                new Databricks.Inputs.GrantsGrantArgs\n                {\n                    Principal = \"sensitive\",\n                    Privileges = new[]\n                    {\n                        \"SELECT\",\n                        \"MODIFY\",\n                    },\n                },\n            },\n        }));\n    }\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthings, err := databricks.GetTables(ctx, \u0026databricks.GetTablesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tvar thingsGrants []*databricks.Grants\n\t\tfor key0, val0 := range things.Ids {\n\t\t\t__res, err := databricks.NewGrants(ctx, fmt.Sprintf(\"things-%v\", key0), \u0026databricks.GrantsArgs{\n\t\t\t\tTable: pulumi.String(val0),\n\t\t\t\tGrants: databricks.GrantsGrantArray{\n\t\t\t\t\t\u0026databricks.GrantsGrantArgs{\n\t\t\t\t\t\tPrincipal: pulumi.String(\"sensitive\"),\n\t\t\t\t\t\tPrivileges: pulumi.StringArray{\n\t\t\t\t\t\t\tpulumi.String(\"SELECT\"),\n\t\t\t\t\t\t\tpulumi.String(\"MODIFY\"),\n\t\t\t\t\t\t},\n\t\t\t\t\t},\n\t\t\t\t},\n\t\t\t})\n\t\t\tif err != nil {\n\t\t\t\treturn err\n\t\t\t}\n\t\t\tthingsGrants = append(thingsGrants, __res)\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetTablesArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrants = things.applyValue(getTablesResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getTablesResult.ids()) {\n                var resource = new Grants(\"thingsGrants-\" + range.key(), GrantsArgs.builder()                \n                    .table(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      table: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      Function: databricks:getTables\n      Arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getTables.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks.Table full names: *`catalog`.`schema`.`table`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getUser:getUser": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nRetrieves information about databricks_user.\n\n## Example Usage\n\nAdding user to administrative group\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst admins = databricks.getGroup({\n    displayName: \"admins\",\n});\nconst me = databricks.getUser({\n    userName: \"me@example.com\",\n});\nconst myMemberA = new databricks.GroupMember(\"my_member_a\", {\n    groupId: admins.then(admins =\u003e admins.id),\n    memberId: me.then(me =\u003e me.id),\n});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nadmins = databricks.get_group(display_name=\"admins\")\nme = databricks.get_user(user_name=\"me@example.com\")\nmy_member_a = databricks.GroupMember(\"my_member_a\",\n    group_id=admins.id,\n    member_id=me.id)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var admins = Databricks.GetGroup.Invoke(new()\n    {\n        DisplayName = \"admins\",\n    });\n\n    var me = Databricks.GetUser.Invoke(new()\n    {\n        UserName = \"me@example.com\",\n    });\n\n    var myMemberA = new Databricks.GroupMember(\"my_member_a\", new()\n    {\n        GroupId = admins.Apply(getGroupResult =\u003e getGroupResult.Id),\n        MemberId = me.Apply(getUserResult =\u003e getUserResult.Id),\n    });\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tadmins, err := databricks.LookupGroup(ctx, \u0026databricks.LookupGroupArgs{\n\t\t\tDisplayName: \"admins\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tme, err := databricks.LookupUser(ctx, \u0026databricks.LookupUserArgs{\n\t\t\tUserName: pulumi.StringRef(\"me@example.com\"),\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\t_, err = databricks.NewGroupMember(ctx, \"my_member_a\", \u0026databricks.GroupMemberArgs{\n\t\t\tGroupId:  pulumi.String(admins.Id),\n\t\t\tMemberId: pulumi.String(me.Id),\n\t\t})\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetGroupArgs;\nimport com.pulumi.databricks.inputs.GetUserArgs;\nimport com.pulumi.databricks.GroupMember;\nimport com.pulumi.databricks.GroupMemberArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()\n            .displayName(\"admins\")\n            .build());\n\n        final var me = DatabricksFunctions.getUser(GetUserArgs.builder()\n            .userName(\"me@example.com\")\n            .build());\n\n        var myMemberA = new GroupMember(\"myMemberA\", GroupMemberArgs.builder()        \n            .groupId(admins.applyValue(getGroupResult -\u003e getGroupResult.id()))\n            .memberId(me.applyValue(getUserResult -\u003e getUserResult.id()))\n            .build());\n\n    }\n}\n```\n```yaml\nresources:\n  myMemberA:\n    type: databricks:GroupMember\n    name: my_member_a\n    properties:\n      groupId: ${admins.id}\n      memberId: ${me.id}\nvariables:\n  admins:\n    fn::invoke:\n      Function: databricks:getGroup\n      Arguments:\n        displayName: admins\n  me:\n    fn::invoke:\n      Function: databricks:getUser\n      Arguments:\n        userName: me@example.com\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n- End to end workspace management guide.\n- databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.\n- databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).\n- databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.\n- databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.\n- databricks.GroupMember to attach users and groups as group members.\n- databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.\n- databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.\n- databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getUser.\n",
                "properties": {
                    "userId": {
                        "type": "string",
                        "description": "ID of the user.\n",
                        "willReplaceOnChanges": true
                    },
                    "userName": {
                        "type": "string",
                        "description": "User name of the user. The user must exist before this resource can be planned.\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object"
            },
            "outputs": {
                "description": "A collection of values returned by getUser.\n",
                "properties": {
                    "aclPrincipalId": {
                        "description": "identifier for use in databricks_access_control_rule_set, e.g. `users/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "alphanumeric": {
                        "description": "Alphanumeric representation of user local name. e.g. `mr_foo`.\n",
                        "type": "string"
                    },
                    "applicationId": {
                        "type": "string"
                    },
                    "displayName": {
                        "description": "Display name of the user, e.g. `Mr Foo`.\n",
                        "type": "string"
                    },
                    "externalId": {
                        "description": "ID of the user in an external identity provider.\n",
                        "type": "string"
                    },
                    "home": {
                        "description": "Home folder of the user, e.g. `/Users/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "repos": {
                        "description": "Personal Repos location of the user, e.g. `/Repos/mr.foo@example.com`.\n",
                        "type": "string"
                    },
                    "userId": {
                        "type": "string"
                    },
                    "userName": {
                        "description": "Name of the user, e.g. `mr.foo@example.com`.\n",
                        "type": "string"
                    }
                },
                "required": [
                    "aclPrincipalId",
                    "alphanumeric",
                    "applicationId",
                    "displayName",
                    "externalId",
                    "home",
                    "repos",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getViews:getViews": {
            "description": "## Example Usage\n\nGranting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetViewsArgs;\nimport com.pulumi.databricks.Grants;\nimport com.pulumi.databricks.GrantsArgs;\nimport com.pulumi.databricks.inputs.GrantsGrantArgs;\nimport com.pulumi.codegen.internal.KeyedValue;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var things = DatabricksFunctions.getViews(GetViewsArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        final var thingsGrants = things.applyValue(getViewsResult -\u003e {\n            final var resources = new ArrayList\u003cGrants\u003e();\n            for (var range : KeyedValue.of(getViewsResult.ids()) {\n                var resource = new Grants(\"thingsGrants-\" + range.key(), GrantsArgs.builder()                \n                    .view(range.value())\n                    .grants(GrantsGrantArgs.builder()\n                        .principal(\"sensitive\")\n                        .privileges(                        \n                            \"SELECT\",\n                            \"MODIFY\")\n                        .build())\n                    .build());\n\n                resources.add(resource);\n            }\n\n            return resources;\n        });\n\n    }\n}\n```\n```yaml\nresources:\n  thingsGrants:\n    type: databricks:Grants\n    name: things\n    properties:\n      view: ${range.value}\n      grants:\n        - principal: sensitive\n          privileges:\n            - SELECT\n            - MODIFY\n    options: {}\nvariables:\n  things:\n    fn::invoke:\n      Function: databricks:getViews\n      Arguments:\n        catalogName: sandbox\n        schemaName: things\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getViews.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "set of databricks_view full names: *`catalog`.`schema`.`view`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getVolumes:getVolumes": {
            "description": "## Example Usage\n\nListing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst this = databricks.getVolumes({\n    catalogName: \"sandbox\",\n    schemaName: \"things\",\n});\nexport const allVolumes = _this;\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nthis = databricks.get_volumes(catalog_name=\"sandbox\",\n    schema_name=\"things\")\npulumi.export(\"allVolumes\", this)\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var @this = Databricks.GetVolumes.Invoke(new()\n    {\n        CatalogName = \"sandbox\",\n        SchemaName = \"things\",\n    });\n\n    return new Dictionary\u003cstring, object?\u003e\n    {\n        [\"allVolumes\"] = @this,\n    };\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\tthis, err := databricks.GetVolumes(ctx, \u0026databricks.GetVolumesArgs{\n\t\t\tCatalogName: \"sandbox\",\n\t\t\tSchemaName:  \"things\",\n\t\t}, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\tctx.Export(\"allVolumes\", this)\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport com.pulumi.databricks.inputs.GetVolumesArgs;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()\n            .catalogName(\"sandbox\")\n            .schemaName(\"things\")\n            .build());\n\n        ctx.export(\"allVolumes\", this_);\n    }\n}\n```\n```yaml\nvariables:\n  this:\n    fn::invoke:\n      Function: databricks:getVolumes\n      Arguments:\n        catalogName: sandbox\n        schemaName: things\noutputs:\n  allVolumes: ${this}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n\n## Related Resources\n\nThe following resources are used in the same context:\n\n* databricks.Volume to manage volumes within Unity Catalog.\n* databricks.Schema to manage schemas within Unity Catalog.\n* databricks.Catalog to manage catalogs within Unity Catalog.\n",
            "inputs": {
                "description": "A collection of arguments for invoking getVolumes.\n",
                "properties": {
                    "catalogName": {
                        "type": "string",
                        "description": "Name of databricks_catalog\n",
                        "willReplaceOnChanges": true
                    },
                    "ids": {
                        "type": "array",
                        "items": {
                            "type": "string"
                        },
                        "description": "a list of databricks.Volume full names: *`catalog`.`schema`.`volume`*\n"
                    },
                    "schemaName": {
                        "type": "string",
                        "description": "Name of databricks_schema\n",
                        "willReplaceOnChanges": true
                    }
                },
                "type": "object",
                "required": [
                    "catalogName",
                    "schemaName"
                ]
            },
            "outputs": {
                "description": "A collection of values returned by getVolumes.\n",
                "properties": {
                    "catalogName": {
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "ids": {
                        "description": "a list of databricks.Volume full names: *`catalog`.`schema`.`volume`*\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    },
                    "schemaName": {
                        "type": "string"
                    }
                },
                "required": [
                    "catalogName",
                    "ids",
                    "schemaName",
                    "id"
                ],
                "type": "object"
            }
        },
        "databricks:index/getZones:getZones": {
            "description": "\u003e **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.\n\nThis data source allows you to fetch all available AWS availability zones on your workspace on AWS.\n\n## Example Usage\n\n\u003c!--Start PulumiCodeChooser --\u003e\n```typescript\nimport * as pulumi from \"@pulumi/pulumi\";\nimport * as databricks from \"@pulumi/databricks\";\n\nconst zones = databricks.getZones({});\n```\n```python\nimport pulumi\nimport pulumi_databricks as databricks\n\nzones = databricks.get_zones()\n```\n```csharp\nusing System.Collections.Generic;\nusing System.Linq;\nusing Pulumi;\nusing Databricks = Pulumi.Databricks;\n\nreturn await Deployment.RunAsync(() =\u003e \n{\n    var zones = Databricks.GetZones.Invoke();\n\n});\n```\n```go\npackage main\n\nimport (\n\t\"github.com/pulumi/pulumi-databricks/sdk/go/databricks\"\n\t\"github.com/pulumi/pulumi/sdk/v3/go/pulumi\"\n)\n\nfunc main() {\n\tpulumi.Run(func(ctx *pulumi.Context) error {\n\t\t_, err := databricks.GetZones(ctx, nil, nil)\n\t\tif err != nil {\n\t\t\treturn err\n\t\t}\n\t\treturn nil\n\t})\n}\n```\n```java\npackage generated_program;\n\nimport com.pulumi.Context;\nimport com.pulumi.Pulumi;\nimport com.pulumi.core.Output;\nimport com.pulumi.databricks.DatabricksFunctions;\nimport java.util.List;\nimport java.util.ArrayList;\nimport java.util.Map;\nimport java.io.File;\nimport java.nio.file.Files;\nimport java.nio.file.Paths;\n\npublic class App {\n    public static void main(String[] args) {\n        Pulumi.run(App::stack);\n    }\n\n    public static void stack(Context ctx) {\n        final var zones = DatabricksFunctions.getZones();\n\n    }\n}\n```\n```yaml\nvariables:\n  zones:\n    fn::invoke:\n      Function: databricks:getZones\n      Arguments: {}\n```\n\u003c!--End PulumiCodeChooser --\u003e\n",
            "outputs": {
                "description": "A collection of values returned by getZones.\n",
                "properties": {
                    "defaultZone": {
                        "description": "This is the default zone that gets assigned to your workspace. This is the zone used by default for clusters and instance pools.\n",
                        "type": "string"
                    },
                    "id": {
                        "description": "The provider-assigned unique ID for this managed resource.\n",
                        "type": "string"
                    },
                    "zones": {
                        "description": "This is a list of all the zones available for your subnets in your Databricks workspace.\n",
                        "items": {
                            "type": "string"
                        },
                        "type": "array"
                    }
                },
                "required": [
                    "defaultZone",
                    "zones",
                    "id"
                ],
                "type": "object"
            }
        }
    }
}