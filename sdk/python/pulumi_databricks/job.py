# coding=utf-8
# *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import copy
import warnings
import pulumi
import pulumi.runtime
from typing import Any, Callable, Mapping, Optional, Sequence, Union, overload
from . import _utilities
from . import outputs
from ._inputs import *

__all__ = ['JobArgs', 'Job']

@pulumi.input_type
class JobArgs:
    def __init__(__self__, *,
                 always_running: Optional[pulumi.Input[bool]] = None,
                 computes: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]] = None,
                 continuous: Optional[pulumi.Input['JobContinuousArgs']] = None,
                 control_run_state: Optional[pulumi.Input[bool]] = None,
                 dbt_task: Optional[pulumi.Input['JobDbtTaskArgs']] = None,
                 deployment: Optional[pulumi.Input['JobDeploymentArgs']] = None,
                 email_notifications: Optional[pulumi.Input['JobEmailNotificationsArgs']] = None,
                 existing_cluster_id: Optional[pulumi.Input[str]] = None,
                 format: Optional[pulumi.Input[str]] = None,
                 git_source: Optional[pulumi.Input['JobGitSourceArgs']] = None,
                 health: Optional[pulumi.Input['JobHealthArgs']] = None,
                 job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]] = None,
                 max_concurrent_runs: Optional[pulumi.Input[int]] = None,
                 max_retries: Optional[pulumi.Input[int]] = None,
                 min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 new_cluster: Optional[pulumi.Input['JobNewClusterArgs']] = None,
                 notebook_task: Optional[pulumi.Input['JobNotebookTaskArgs']] = None,
                 notification_settings: Optional[pulumi.Input['JobNotificationSettingsArgs']] = None,
                 parameters: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]] = None,
                 pipeline_task: Optional[pulumi.Input['JobPipelineTaskArgs']] = None,
                 python_wheel_task: Optional[pulumi.Input['JobPythonWheelTaskArgs']] = None,
                 queue: Optional[pulumi.Input['JobQueueArgs']] = None,
                 retry_on_timeout: Optional[pulumi.Input[bool]] = None,
                 run_as: Optional[pulumi.Input['JobRunAsArgs']] = None,
                 run_job_task: Optional[pulumi.Input['JobRunJobTaskArgs']] = None,
                 schedule: Optional[pulumi.Input['JobScheduleArgs']] = None,
                 spark_jar_task: Optional[pulumi.Input['JobSparkJarTaskArgs']] = None,
                 spark_python_task: Optional[pulumi.Input['JobSparkPythonTaskArgs']] = None,
                 spark_submit_task: Optional[pulumi.Input['JobSparkSubmitTaskArgs']] = None,
                 tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 tasks: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]] = None,
                 timeout_seconds: Optional[pulumi.Input[int]] = None,
                 trigger: Optional[pulumi.Input['JobTriggerArgs']] = None,
                 webhook_notifications: Optional[pulumi.Input['JobWebhookNotificationsArgs']] = None):
        """
        The set of arguments for constructing a Job resource.
        :param pulumi.Input[bool] always_running: (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        :param pulumi.Input[bool] control_run_state: (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.
               
               When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
               ```python
               import pulumi
               ```
        :param pulumi.Input['JobEmailNotificationsArgs'] email_notifications: (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        :param pulumi.Input['JobHealthArgs'] health: An optional block that specifies the health conditions for the job (described below).
        :param pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]] job_clusters: A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        :param pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]] libraries: (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        :param pulumi.Input[int] max_concurrent_runs: (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        :param pulumi.Input[int] max_retries: (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        :param pulumi.Input[int] min_retry_interval_millis: (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        :param pulumi.Input[str] name: An optional name for the job. The default value is Untitled.
        :param pulumi.Input['JobNewClusterArgs'] new_cluster: Same set of parameters as for Cluster resource.
        :param pulumi.Input['JobNotificationSettingsArgs'] notification_settings: An optional block controlling the notification settings on the job level (described below).
        :param pulumi.Input[bool] retry_on_timeout: (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        :param pulumi.Input['JobScheduleArgs'] schedule: (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        :param pulumi.Input[int] timeout_seconds: (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        :param pulumi.Input['JobWebhookNotificationsArgs'] webhook_notifications: (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        JobArgs._configure(
            lambda key, value: pulumi.set(__self__, key, value),
            always_running=always_running,
            computes=computes,
            continuous=continuous,
            control_run_state=control_run_state,
            dbt_task=dbt_task,
            deployment=deployment,
            email_notifications=email_notifications,
            existing_cluster_id=existing_cluster_id,
            format=format,
            git_source=git_source,
            health=health,
            job_clusters=job_clusters,
            libraries=libraries,
            max_concurrent_runs=max_concurrent_runs,
            max_retries=max_retries,
            min_retry_interval_millis=min_retry_interval_millis,
            name=name,
            new_cluster=new_cluster,
            notebook_task=notebook_task,
            notification_settings=notification_settings,
            parameters=parameters,
            pipeline_task=pipeline_task,
            python_wheel_task=python_wheel_task,
            queue=queue,
            retry_on_timeout=retry_on_timeout,
            run_as=run_as,
            run_job_task=run_job_task,
            schedule=schedule,
            spark_jar_task=spark_jar_task,
            spark_python_task=spark_python_task,
            spark_submit_task=spark_submit_task,
            tags=tags,
            tasks=tasks,
            timeout_seconds=timeout_seconds,
            trigger=trigger,
            webhook_notifications=webhook_notifications,
        )
    @staticmethod
    def _configure(
             _setter: Callable[[Any, Any], None],
             always_running: Optional[pulumi.Input[bool]] = None,
             computes: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]] = None,
             continuous: Optional[pulumi.Input['JobContinuousArgs']] = None,
             control_run_state: Optional[pulumi.Input[bool]] = None,
             dbt_task: Optional[pulumi.Input['JobDbtTaskArgs']] = None,
             deployment: Optional[pulumi.Input['JobDeploymentArgs']] = None,
             email_notifications: Optional[pulumi.Input['JobEmailNotificationsArgs']] = None,
             existing_cluster_id: Optional[pulumi.Input[str]] = None,
             format: Optional[pulumi.Input[str]] = None,
             git_source: Optional[pulumi.Input['JobGitSourceArgs']] = None,
             health: Optional[pulumi.Input['JobHealthArgs']] = None,
             job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]] = None,
             libraries: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]] = None,
             max_concurrent_runs: Optional[pulumi.Input[int]] = None,
             max_retries: Optional[pulumi.Input[int]] = None,
             min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
             name: Optional[pulumi.Input[str]] = None,
             new_cluster: Optional[pulumi.Input['JobNewClusterArgs']] = None,
             notebook_task: Optional[pulumi.Input['JobNotebookTaskArgs']] = None,
             notification_settings: Optional[pulumi.Input['JobNotificationSettingsArgs']] = None,
             parameters: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]] = None,
             pipeline_task: Optional[pulumi.Input['JobPipelineTaskArgs']] = None,
             python_wheel_task: Optional[pulumi.Input['JobPythonWheelTaskArgs']] = None,
             queue: Optional[pulumi.Input['JobQueueArgs']] = None,
             retry_on_timeout: Optional[pulumi.Input[bool]] = None,
             run_as: Optional[pulumi.Input['JobRunAsArgs']] = None,
             run_job_task: Optional[pulumi.Input['JobRunJobTaskArgs']] = None,
             schedule: Optional[pulumi.Input['JobScheduleArgs']] = None,
             spark_jar_task: Optional[pulumi.Input['JobSparkJarTaskArgs']] = None,
             spark_python_task: Optional[pulumi.Input['JobSparkPythonTaskArgs']] = None,
             spark_submit_task: Optional[pulumi.Input['JobSparkSubmitTaskArgs']] = None,
             tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             tasks: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]] = None,
             timeout_seconds: Optional[pulumi.Input[int]] = None,
             trigger: Optional[pulumi.Input['JobTriggerArgs']] = None,
             webhook_notifications: Optional[pulumi.Input['JobWebhookNotificationsArgs']] = None,
             opts: Optional[pulumi.ResourceOptions]=None,
             **kwargs):
        if always_running is None and 'alwaysRunning' in kwargs:
            always_running = kwargs['alwaysRunning']
        if control_run_state is None and 'controlRunState' in kwargs:
            control_run_state = kwargs['controlRunState']
        if dbt_task is None and 'dbtTask' in kwargs:
            dbt_task = kwargs['dbtTask']
        if email_notifications is None and 'emailNotifications' in kwargs:
            email_notifications = kwargs['emailNotifications']
        if existing_cluster_id is None and 'existingClusterId' in kwargs:
            existing_cluster_id = kwargs['existingClusterId']
        if git_source is None and 'gitSource' in kwargs:
            git_source = kwargs['gitSource']
        if job_clusters is None and 'jobClusters' in kwargs:
            job_clusters = kwargs['jobClusters']
        if max_concurrent_runs is None and 'maxConcurrentRuns' in kwargs:
            max_concurrent_runs = kwargs['maxConcurrentRuns']
        if max_retries is None and 'maxRetries' in kwargs:
            max_retries = kwargs['maxRetries']
        if min_retry_interval_millis is None and 'minRetryIntervalMillis' in kwargs:
            min_retry_interval_millis = kwargs['minRetryIntervalMillis']
        if new_cluster is None and 'newCluster' in kwargs:
            new_cluster = kwargs['newCluster']
        if notebook_task is None and 'notebookTask' in kwargs:
            notebook_task = kwargs['notebookTask']
        if notification_settings is None and 'notificationSettings' in kwargs:
            notification_settings = kwargs['notificationSettings']
        if pipeline_task is None and 'pipelineTask' in kwargs:
            pipeline_task = kwargs['pipelineTask']
        if python_wheel_task is None and 'pythonWheelTask' in kwargs:
            python_wheel_task = kwargs['pythonWheelTask']
        if retry_on_timeout is None and 'retryOnTimeout' in kwargs:
            retry_on_timeout = kwargs['retryOnTimeout']
        if run_as is None and 'runAs' in kwargs:
            run_as = kwargs['runAs']
        if run_job_task is None and 'runJobTask' in kwargs:
            run_job_task = kwargs['runJobTask']
        if spark_jar_task is None and 'sparkJarTask' in kwargs:
            spark_jar_task = kwargs['sparkJarTask']
        if spark_python_task is None and 'sparkPythonTask' in kwargs:
            spark_python_task = kwargs['sparkPythonTask']
        if spark_submit_task is None and 'sparkSubmitTask' in kwargs:
            spark_submit_task = kwargs['sparkSubmitTask']
        if timeout_seconds is None and 'timeoutSeconds' in kwargs:
            timeout_seconds = kwargs['timeoutSeconds']
        if webhook_notifications is None and 'webhookNotifications' in kwargs:
            webhook_notifications = kwargs['webhookNotifications']

        if always_running is not None:
            warnings.warn("""always_running will be replaced by control_run_state in the next major release.""", DeprecationWarning)
            pulumi.log.warn("""always_running is deprecated: always_running will be replaced by control_run_state in the next major release.""")
        if always_running is not None:
            _setter("always_running", always_running)
        if computes is not None:
            _setter("computes", computes)
        if continuous is not None:
            _setter("continuous", continuous)
        if control_run_state is not None:
            _setter("control_run_state", control_run_state)
        if dbt_task is not None:
            _setter("dbt_task", dbt_task)
        if deployment is not None:
            _setter("deployment", deployment)
        if email_notifications is not None:
            _setter("email_notifications", email_notifications)
        if existing_cluster_id is not None:
            _setter("existing_cluster_id", existing_cluster_id)
        if format is not None:
            _setter("format", format)
        if git_source is not None:
            _setter("git_source", git_source)
        if health is not None:
            _setter("health", health)
        if job_clusters is not None:
            _setter("job_clusters", job_clusters)
        if libraries is not None:
            _setter("libraries", libraries)
        if max_concurrent_runs is not None:
            _setter("max_concurrent_runs", max_concurrent_runs)
        if max_retries is not None:
            _setter("max_retries", max_retries)
        if min_retry_interval_millis is not None:
            _setter("min_retry_interval_millis", min_retry_interval_millis)
        if name is not None:
            _setter("name", name)
        if new_cluster is not None:
            _setter("new_cluster", new_cluster)
        if notebook_task is not None:
            _setter("notebook_task", notebook_task)
        if notification_settings is not None:
            _setter("notification_settings", notification_settings)
        if parameters is not None:
            _setter("parameters", parameters)
        if pipeline_task is not None:
            _setter("pipeline_task", pipeline_task)
        if python_wheel_task is not None:
            _setter("python_wheel_task", python_wheel_task)
        if queue is not None:
            _setter("queue", queue)
        if retry_on_timeout is not None:
            _setter("retry_on_timeout", retry_on_timeout)
        if run_as is not None:
            _setter("run_as", run_as)
        if run_job_task is not None:
            _setter("run_job_task", run_job_task)
        if schedule is not None:
            _setter("schedule", schedule)
        if spark_jar_task is not None:
            _setter("spark_jar_task", spark_jar_task)
        if spark_python_task is not None:
            _setter("spark_python_task", spark_python_task)
        if spark_submit_task is not None:
            _setter("spark_submit_task", spark_submit_task)
        if tags is not None:
            _setter("tags", tags)
        if tasks is not None:
            _setter("tasks", tasks)
        if timeout_seconds is not None:
            _setter("timeout_seconds", timeout_seconds)
        if trigger is not None:
            _setter("trigger", trigger)
        if webhook_notifications is not None:
            _setter("webhook_notifications", webhook_notifications)

    @property
    @pulumi.getter(name="alwaysRunning")
    def always_running(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        """
        warnings.warn("""always_running will be replaced by control_run_state in the next major release.""", DeprecationWarning)
        pulumi.log.warn("""always_running is deprecated: always_running will be replaced by control_run_state in the next major release.""")

        return pulumi.get(self, "always_running")

    @always_running.setter
    def always_running(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "always_running", value)

    @property
    @pulumi.getter
    def computes(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]]:
        return pulumi.get(self, "computes")

    @computes.setter
    def computes(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]]):
        pulumi.set(self, "computes", value)

    @property
    @pulumi.getter
    def continuous(self) -> Optional[pulumi.Input['JobContinuousArgs']]:
        return pulumi.get(self, "continuous")

    @continuous.setter
    def continuous(self, value: Optional[pulumi.Input['JobContinuousArgs']]):
        pulumi.set(self, "continuous", value)

    @property
    @pulumi.getter(name="controlRunState")
    def control_run_state(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.

        When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
        ```python
        import pulumi
        ```
        """
        return pulumi.get(self, "control_run_state")

    @control_run_state.setter
    def control_run_state(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "control_run_state", value)

    @property
    @pulumi.getter(name="dbtTask")
    def dbt_task(self) -> Optional[pulumi.Input['JobDbtTaskArgs']]:
        return pulumi.get(self, "dbt_task")

    @dbt_task.setter
    def dbt_task(self, value: Optional[pulumi.Input['JobDbtTaskArgs']]):
        pulumi.set(self, "dbt_task", value)

    @property
    @pulumi.getter
    def deployment(self) -> Optional[pulumi.Input['JobDeploymentArgs']]:
        return pulumi.get(self, "deployment")

    @deployment.setter
    def deployment(self, value: Optional[pulumi.Input['JobDeploymentArgs']]):
        pulumi.set(self, "deployment", value)

    @property
    @pulumi.getter(name="emailNotifications")
    def email_notifications(self) -> Optional[pulumi.Input['JobEmailNotificationsArgs']]:
        """
        (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        """
        return pulumi.get(self, "email_notifications")

    @email_notifications.setter
    def email_notifications(self, value: Optional[pulumi.Input['JobEmailNotificationsArgs']]):
        pulumi.set(self, "email_notifications", value)

    @property
    @pulumi.getter(name="existingClusterId")
    def existing_cluster_id(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "existing_cluster_id")

    @existing_cluster_id.setter
    def existing_cluster_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "existing_cluster_id", value)

    @property
    @pulumi.getter
    def format(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "format")

    @format.setter
    def format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "format", value)

    @property
    @pulumi.getter(name="gitSource")
    def git_source(self) -> Optional[pulumi.Input['JobGitSourceArgs']]:
        return pulumi.get(self, "git_source")

    @git_source.setter
    def git_source(self, value: Optional[pulumi.Input['JobGitSourceArgs']]):
        pulumi.set(self, "git_source", value)

    @property
    @pulumi.getter
    def health(self) -> Optional[pulumi.Input['JobHealthArgs']]:
        """
        An optional block that specifies the health conditions for the job (described below).
        """
        return pulumi.get(self, "health")

    @health.setter
    def health(self, value: Optional[pulumi.Input['JobHealthArgs']]):
        pulumi.set(self, "health", value)

    @property
    @pulumi.getter(name="jobClusters")
    def job_clusters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]]:
        """
        A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        """
        return pulumi.get(self, "job_clusters")

    @job_clusters.setter
    def job_clusters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]]):
        pulumi.set(self, "job_clusters", value)

    @property
    @pulumi.getter
    def libraries(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]]:
        """
        (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        """
        return pulumi.get(self, "libraries")

    @libraries.setter
    def libraries(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]]):
        pulumi.set(self, "libraries", value)

    @property
    @pulumi.getter(name="maxConcurrentRuns")
    def max_concurrent_runs(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        """
        return pulumi.get(self, "max_concurrent_runs")

    @max_concurrent_runs.setter
    def max_concurrent_runs(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_concurrent_runs", value)

    @property
    @pulumi.getter(name="maxRetries")
    def max_retries(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        """
        return pulumi.get(self, "max_retries")

    @max_retries.setter
    def max_retries(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_retries", value)

    @property
    @pulumi.getter(name="minRetryIntervalMillis")
    def min_retry_interval_millis(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        """
        return pulumi.get(self, "min_retry_interval_millis")

    @min_retry_interval_millis.setter
    def min_retry_interval_millis(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_retry_interval_millis", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        An optional name for the job. The default value is Untitled.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter(name="newCluster")
    def new_cluster(self) -> Optional[pulumi.Input['JobNewClusterArgs']]:
        """
        Same set of parameters as for Cluster resource.
        """
        return pulumi.get(self, "new_cluster")

    @new_cluster.setter
    def new_cluster(self, value: Optional[pulumi.Input['JobNewClusterArgs']]):
        pulumi.set(self, "new_cluster", value)

    @property
    @pulumi.getter(name="notebookTask")
    def notebook_task(self) -> Optional[pulumi.Input['JobNotebookTaskArgs']]:
        return pulumi.get(self, "notebook_task")

    @notebook_task.setter
    def notebook_task(self, value: Optional[pulumi.Input['JobNotebookTaskArgs']]):
        pulumi.set(self, "notebook_task", value)

    @property
    @pulumi.getter(name="notificationSettings")
    def notification_settings(self) -> Optional[pulumi.Input['JobNotificationSettingsArgs']]:
        """
        An optional block controlling the notification settings on the job level (described below).
        """
        return pulumi.get(self, "notification_settings")

    @notification_settings.setter
    def notification_settings(self, value: Optional[pulumi.Input['JobNotificationSettingsArgs']]):
        pulumi.set(self, "notification_settings", value)

    @property
    @pulumi.getter
    def parameters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]]:
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]]):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter(name="pipelineTask")
    def pipeline_task(self) -> Optional[pulumi.Input['JobPipelineTaskArgs']]:
        return pulumi.get(self, "pipeline_task")

    @pipeline_task.setter
    def pipeline_task(self, value: Optional[pulumi.Input['JobPipelineTaskArgs']]):
        pulumi.set(self, "pipeline_task", value)

    @property
    @pulumi.getter(name="pythonWheelTask")
    def python_wheel_task(self) -> Optional[pulumi.Input['JobPythonWheelTaskArgs']]:
        return pulumi.get(self, "python_wheel_task")

    @python_wheel_task.setter
    def python_wheel_task(self, value: Optional[pulumi.Input['JobPythonWheelTaskArgs']]):
        pulumi.set(self, "python_wheel_task", value)

    @property
    @pulumi.getter
    def queue(self) -> Optional[pulumi.Input['JobQueueArgs']]:
        return pulumi.get(self, "queue")

    @queue.setter
    def queue(self, value: Optional[pulumi.Input['JobQueueArgs']]):
        pulumi.set(self, "queue", value)

    @property
    @pulumi.getter(name="retryOnTimeout")
    def retry_on_timeout(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        """
        return pulumi.get(self, "retry_on_timeout")

    @retry_on_timeout.setter
    def retry_on_timeout(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "retry_on_timeout", value)

    @property
    @pulumi.getter(name="runAs")
    def run_as(self) -> Optional[pulumi.Input['JobRunAsArgs']]:
        return pulumi.get(self, "run_as")

    @run_as.setter
    def run_as(self, value: Optional[pulumi.Input['JobRunAsArgs']]):
        pulumi.set(self, "run_as", value)

    @property
    @pulumi.getter(name="runJobTask")
    def run_job_task(self) -> Optional[pulumi.Input['JobRunJobTaskArgs']]:
        return pulumi.get(self, "run_job_task")

    @run_job_task.setter
    def run_job_task(self, value: Optional[pulumi.Input['JobRunJobTaskArgs']]):
        pulumi.set(self, "run_job_task", value)

    @property
    @pulumi.getter
    def schedule(self) -> Optional[pulumi.Input['JobScheduleArgs']]:
        """
        (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        """
        return pulumi.get(self, "schedule")

    @schedule.setter
    def schedule(self, value: Optional[pulumi.Input['JobScheduleArgs']]):
        pulumi.set(self, "schedule", value)

    @property
    @pulumi.getter(name="sparkJarTask")
    def spark_jar_task(self) -> Optional[pulumi.Input['JobSparkJarTaskArgs']]:
        return pulumi.get(self, "spark_jar_task")

    @spark_jar_task.setter
    def spark_jar_task(self, value: Optional[pulumi.Input['JobSparkJarTaskArgs']]):
        pulumi.set(self, "spark_jar_task", value)

    @property
    @pulumi.getter(name="sparkPythonTask")
    def spark_python_task(self) -> Optional[pulumi.Input['JobSparkPythonTaskArgs']]:
        return pulumi.get(self, "spark_python_task")

    @spark_python_task.setter
    def spark_python_task(self, value: Optional[pulumi.Input['JobSparkPythonTaskArgs']]):
        pulumi.set(self, "spark_python_task", value)

    @property
    @pulumi.getter(name="sparkSubmitTask")
    def spark_submit_task(self) -> Optional[pulumi.Input['JobSparkSubmitTaskArgs']]:
        return pulumi.get(self, "spark_submit_task")

    @spark_submit_task.setter
    def spark_submit_task(self, value: Optional[pulumi.Input['JobSparkSubmitTaskArgs']]):
        pulumi.set(self, "spark_submit_task", value)

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        return pulumi.get(self, "tags")

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "tags", value)

    @property
    @pulumi.getter
    def tasks(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]]:
        return pulumi.get(self, "tasks")

    @tasks.setter
    def tasks(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]]):
        pulumi.set(self, "tasks", value)

    @property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        """
        return pulumi.get(self, "timeout_seconds")

    @timeout_seconds.setter
    def timeout_seconds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "timeout_seconds", value)

    @property
    @pulumi.getter
    def trigger(self) -> Optional[pulumi.Input['JobTriggerArgs']]:
        return pulumi.get(self, "trigger")

    @trigger.setter
    def trigger(self, value: Optional[pulumi.Input['JobTriggerArgs']]):
        pulumi.set(self, "trigger", value)

    @property
    @pulumi.getter(name="webhookNotifications")
    def webhook_notifications(self) -> Optional[pulumi.Input['JobWebhookNotificationsArgs']]:
        """
        (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        return pulumi.get(self, "webhook_notifications")

    @webhook_notifications.setter
    def webhook_notifications(self, value: Optional[pulumi.Input['JobWebhookNotificationsArgs']]):
        pulumi.set(self, "webhook_notifications", value)


@pulumi.input_type
class _JobState:
    def __init__(__self__, *,
                 always_running: Optional[pulumi.Input[bool]] = None,
                 computes: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]] = None,
                 continuous: Optional[pulumi.Input['JobContinuousArgs']] = None,
                 control_run_state: Optional[pulumi.Input[bool]] = None,
                 dbt_task: Optional[pulumi.Input['JobDbtTaskArgs']] = None,
                 deployment: Optional[pulumi.Input['JobDeploymentArgs']] = None,
                 email_notifications: Optional[pulumi.Input['JobEmailNotificationsArgs']] = None,
                 existing_cluster_id: Optional[pulumi.Input[str]] = None,
                 format: Optional[pulumi.Input[str]] = None,
                 git_source: Optional[pulumi.Input['JobGitSourceArgs']] = None,
                 health: Optional[pulumi.Input['JobHealthArgs']] = None,
                 job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]] = None,
                 max_concurrent_runs: Optional[pulumi.Input[int]] = None,
                 max_retries: Optional[pulumi.Input[int]] = None,
                 min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 new_cluster: Optional[pulumi.Input['JobNewClusterArgs']] = None,
                 notebook_task: Optional[pulumi.Input['JobNotebookTaskArgs']] = None,
                 notification_settings: Optional[pulumi.Input['JobNotificationSettingsArgs']] = None,
                 parameters: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]] = None,
                 pipeline_task: Optional[pulumi.Input['JobPipelineTaskArgs']] = None,
                 python_wheel_task: Optional[pulumi.Input['JobPythonWheelTaskArgs']] = None,
                 queue: Optional[pulumi.Input['JobQueueArgs']] = None,
                 retry_on_timeout: Optional[pulumi.Input[bool]] = None,
                 run_as: Optional[pulumi.Input['JobRunAsArgs']] = None,
                 run_job_task: Optional[pulumi.Input['JobRunJobTaskArgs']] = None,
                 schedule: Optional[pulumi.Input['JobScheduleArgs']] = None,
                 spark_jar_task: Optional[pulumi.Input['JobSparkJarTaskArgs']] = None,
                 spark_python_task: Optional[pulumi.Input['JobSparkPythonTaskArgs']] = None,
                 spark_submit_task: Optional[pulumi.Input['JobSparkSubmitTaskArgs']] = None,
                 tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 tasks: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]] = None,
                 timeout_seconds: Optional[pulumi.Input[int]] = None,
                 trigger: Optional[pulumi.Input['JobTriggerArgs']] = None,
                 url: Optional[pulumi.Input[str]] = None,
                 webhook_notifications: Optional[pulumi.Input['JobWebhookNotificationsArgs']] = None):
        """
        Input properties used for looking up and filtering Job resources.
        :param pulumi.Input[bool] always_running: (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        :param pulumi.Input[bool] control_run_state: (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.
               
               When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
               ```python
               import pulumi
               ```
        :param pulumi.Input['JobEmailNotificationsArgs'] email_notifications: (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        :param pulumi.Input['JobHealthArgs'] health: An optional block that specifies the health conditions for the job (described below).
        :param pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]] job_clusters: A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        :param pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]] libraries: (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        :param pulumi.Input[int] max_concurrent_runs: (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        :param pulumi.Input[int] max_retries: (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        :param pulumi.Input[int] min_retry_interval_millis: (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        :param pulumi.Input[str] name: An optional name for the job. The default value is Untitled.
        :param pulumi.Input['JobNewClusterArgs'] new_cluster: Same set of parameters as for Cluster resource.
        :param pulumi.Input['JobNotificationSettingsArgs'] notification_settings: An optional block controlling the notification settings on the job level (described below).
        :param pulumi.Input[bool] retry_on_timeout: (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        :param pulumi.Input['JobScheduleArgs'] schedule: (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        :param pulumi.Input[int] timeout_seconds: (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        :param pulumi.Input[str] url: URL of the job on the given workspace
        :param pulumi.Input['JobWebhookNotificationsArgs'] webhook_notifications: (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        _JobState._configure(
            lambda key, value: pulumi.set(__self__, key, value),
            always_running=always_running,
            computes=computes,
            continuous=continuous,
            control_run_state=control_run_state,
            dbt_task=dbt_task,
            deployment=deployment,
            email_notifications=email_notifications,
            existing_cluster_id=existing_cluster_id,
            format=format,
            git_source=git_source,
            health=health,
            job_clusters=job_clusters,
            libraries=libraries,
            max_concurrent_runs=max_concurrent_runs,
            max_retries=max_retries,
            min_retry_interval_millis=min_retry_interval_millis,
            name=name,
            new_cluster=new_cluster,
            notebook_task=notebook_task,
            notification_settings=notification_settings,
            parameters=parameters,
            pipeline_task=pipeline_task,
            python_wheel_task=python_wheel_task,
            queue=queue,
            retry_on_timeout=retry_on_timeout,
            run_as=run_as,
            run_job_task=run_job_task,
            schedule=schedule,
            spark_jar_task=spark_jar_task,
            spark_python_task=spark_python_task,
            spark_submit_task=spark_submit_task,
            tags=tags,
            tasks=tasks,
            timeout_seconds=timeout_seconds,
            trigger=trigger,
            url=url,
            webhook_notifications=webhook_notifications,
        )
    @staticmethod
    def _configure(
             _setter: Callable[[Any, Any], None],
             always_running: Optional[pulumi.Input[bool]] = None,
             computes: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]] = None,
             continuous: Optional[pulumi.Input['JobContinuousArgs']] = None,
             control_run_state: Optional[pulumi.Input[bool]] = None,
             dbt_task: Optional[pulumi.Input['JobDbtTaskArgs']] = None,
             deployment: Optional[pulumi.Input['JobDeploymentArgs']] = None,
             email_notifications: Optional[pulumi.Input['JobEmailNotificationsArgs']] = None,
             existing_cluster_id: Optional[pulumi.Input[str]] = None,
             format: Optional[pulumi.Input[str]] = None,
             git_source: Optional[pulumi.Input['JobGitSourceArgs']] = None,
             health: Optional[pulumi.Input['JobHealthArgs']] = None,
             job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]] = None,
             libraries: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]] = None,
             max_concurrent_runs: Optional[pulumi.Input[int]] = None,
             max_retries: Optional[pulumi.Input[int]] = None,
             min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
             name: Optional[pulumi.Input[str]] = None,
             new_cluster: Optional[pulumi.Input['JobNewClusterArgs']] = None,
             notebook_task: Optional[pulumi.Input['JobNotebookTaskArgs']] = None,
             notification_settings: Optional[pulumi.Input['JobNotificationSettingsArgs']] = None,
             parameters: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]] = None,
             pipeline_task: Optional[pulumi.Input['JobPipelineTaskArgs']] = None,
             python_wheel_task: Optional[pulumi.Input['JobPythonWheelTaskArgs']] = None,
             queue: Optional[pulumi.Input['JobQueueArgs']] = None,
             retry_on_timeout: Optional[pulumi.Input[bool]] = None,
             run_as: Optional[pulumi.Input['JobRunAsArgs']] = None,
             run_job_task: Optional[pulumi.Input['JobRunJobTaskArgs']] = None,
             schedule: Optional[pulumi.Input['JobScheduleArgs']] = None,
             spark_jar_task: Optional[pulumi.Input['JobSparkJarTaskArgs']] = None,
             spark_python_task: Optional[pulumi.Input['JobSparkPythonTaskArgs']] = None,
             spark_submit_task: Optional[pulumi.Input['JobSparkSubmitTaskArgs']] = None,
             tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
             tasks: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]] = None,
             timeout_seconds: Optional[pulumi.Input[int]] = None,
             trigger: Optional[pulumi.Input['JobTriggerArgs']] = None,
             url: Optional[pulumi.Input[str]] = None,
             webhook_notifications: Optional[pulumi.Input['JobWebhookNotificationsArgs']] = None,
             opts: Optional[pulumi.ResourceOptions]=None,
             **kwargs):
        if always_running is None and 'alwaysRunning' in kwargs:
            always_running = kwargs['alwaysRunning']
        if control_run_state is None and 'controlRunState' in kwargs:
            control_run_state = kwargs['controlRunState']
        if dbt_task is None and 'dbtTask' in kwargs:
            dbt_task = kwargs['dbtTask']
        if email_notifications is None and 'emailNotifications' in kwargs:
            email_notifications = kwargs['emailNotifications']
        if existing_cluster_id is None and 'existingClusterId' in kwargs:
            existing_cluster_id = kwargs['existingClusterId']
        if git_source is None and 'gitSource' in kwargs:
            git_source = kwargs['gitSource']
        if job_clusters is None and 'jobClusters' in kwargs:
            job_clusters = kwargs['jobClusters']
        if max_concurrent_runs is None and 'maxConcurrentRuns' in kwargs:
            max_concurrent_runs = kwargs['maxConcurrentRuns']
        if max_retries is None and 'maxRetries' in kwargs:
            max_retries = kwargs['maxRetries']
        if min_retry_interval_millis is None and 'minRetryIntervalMillis' in kwargs:
            min_retry_interval_millis = kwargs['minRetryIntervalMillis']
        if new_cluster is None and 'newCluster' in kwargs:
            new_cluster = kwargs['newCluster']
        if notebook_task is None and 'notebookTask' in kwargs:
            notebook_task = kwargs['notebookTask']
        if notification_settings is None and 'notificationSettings' in kwargs:
            notification_settings = kwargs['notificationSettings']
        if pipeline_task is None and 'pipelineTask' in kwargs:
            pipeline_task = kwargs['pipelineTask']
        if python_wheel_task is None and 'pythonWheelTask' in kwargs:
            python_wheel_task = kwargs['pythonWheelTask']
        if retry_on_timeout is None and 'retryOnTimeout' in kwargs:
            retry_on_timeout = kwargs['retryOnTimeout']
        if run_as is None and 'runAs' in kwargs:
            run_as = kwargs['runAs']
        if run_job_task is None and 'runJobTask' in kwargs:
            run_job_task = kwargs['runJobTask']
        if spark_jar_task is None and 'sparkJarTask' in kwargs:
            spark_jar_task = kwargs['sparkJarTask']
        if spark_python_task is None and 'sparkPythonTask' in kwargs:
            spark_python_task = kwargs['sparkPythonTask']
        if spark_submit_task is None and 'sparkSubmitTask' in kwargs:
            spark_submit_task = kwargs['sparkSubmitTask']
        if timeout_seconds is None and 'timeoutSeconds' in kwargs:
            timeout_seconds = kwargs['timeoutSeconds']
        if webhook_notifications is None and 'webhookNotifications' in kwargs:
            webhook_notifications = kwargs['webhookNotifications']

        if always_running is not None:
            warnings.warn("""always_running will be replaced by control_run_state in the next major release.""", DeprecationWarning)
            pulumi.log.warn("""always_running is deprecated: always_running will be replaced by control_run_state in the next major release.""")
        if always_running is not None:
            _setter("always_running", always_running)
        if computes is not None:
            _setter("computes", computes)
        if continuous is not None:
            _setter("continuous", continuous)
        if control_run_state is not None:
            _setter("control_run_state", control_run_state)
        if dbt_task is not None:
            _setter("dbt_task", dbt_task)
        if deployment is not None:
            _setter("deployment", deployment)
        if email_notifications is not None:
            _setter("email_notifications", email_notifications)
        if existing_cluster_id is not None:
            _setter("existing_cluster_id", existing_cluster_id)
        if format is not None:
            _setter("format", format)
        if git_source is not None:
            _setter("git_source", git_source)
        if health is not None:
            _setter("health", health)
        if job_clusters is not None:
            _setter("job_clusters", job_clusters)
        if libraries is not None:
            _setter("libraries", libraries)
        if max_concurrent_runs is not None:
            _setter("max_concurrent_runs", max_concurrent_runs)
        if max_retries is not None:
            _setter("max_retries", max_retries)
        if min_retry_interval_millis is not None:
            _setter("min_retry_interval_millis", min_retry_interval_millis)
        if name is not None:
            _setter("name", name)
        if new_cluster is not None:
            _setter("new_cluster", new_cluster)
        if notebook_task is not None:
            _setter("notebook_task", notebook_task)
        if notification_settings is not None:
            _setter("notification_settings", notification_settings)
        if parameters is not None:
            _setter("parameters", parameters)
        if pipeline_task is not None:
            _setter("pipeline_task", pipeline_task)
        if python_wheel_task is not None:
            _setter("python_wheel_task", python_wheel_task)
        if queue is not None:
            _setter("queue", queue)
        if retry_on_timeout is not None:
            _setter("retry_on_timeout", retry_on_timeout)
        if run_as is not None:
            _setter("run_as", run_as)
        if run_job_task is not None:
            _setter("run_job_task", run_job_task)
        if schedule is not None:
            _setter("schedule", schedule)
        if spark_jar_task is not None:
            _setter("spark_jar_task", spark_jar_task)
        if spark_python_task is not None:
            _setter("spark_python_task", spark_python_task)
        if spark_submit_task is not None:
            _setter("spark_submit_task", spark_submit_task)
        if tags is not None:
            _setter("tags", tags)
        if tasks is not None:
            _setter("tasks", tasks)
        if timeout_seconds is not None:
            _setter("timeout_seconds", timeout_seconds)
        if trigger is not None:
            _setter("trigger", trigger)
        if url is not None:
            _setter("url", url)
        if webhook_notifications is not None:
            _setter("webhook_notifications", webhook_notifications)

    @property
    @pulumi.getter(name="alwaysRunning")
    def always_running(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        """
        warnings.warn("""always_running will be replaced by control_run_state in the next major release.""", DeprecationWarning)
        pulumi.log.warn("""always_running is deprecated: always_running will be replaced by control_run_state in the next major release.""")

        return pulumi.get(self, "always_running")

    @always_running.setter
    def always_running(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "always_running", value)

    @property
    @pulumi.getter
    def computes(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]]:
        return pulumi.get(self, "computes")

    @computes.setter
    def computes(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobComputeArgs']]]]):
        pulumi.set(self, "computes", value)

    @property
    @pulumi.getter
    def continuous(self) -> Optional[pulumi.Input['JobContinuousArgs']]:
        return pulumi.get(self, "continuous")

    @continuous.setter
    def continuous(self, value: Optional[pulumi.Input['JobContinuousArgs']]):
        pulumi.set(self, "continuous", value)

    @property
    @pulumi.getter(name="controlRunState")
    def control_run_state(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.

        When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
        ```python
        import pulumi
        ```
        """
        return pulumi.get(self, "control_run_state")

    @control_run_state.setter
    def control_run_state(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "control_run_state", value)

    @property
    @pulumi.getter(name="dbtTask")
    def dbt_task(self) -> Optional[pulumi.Input['JobDbtTaskArgs']]:
        return pulumi.get(self, "dbt_task")

    @dbt_task.setter
    def dbt_task(self, value: Optional[pulumi.Input['JobDbtTaskArgs']]):
        pulumi.set(self, "dbt_task", value)

    @property
    @pulumi.getter
    def deployment(self) -> Optional[pulumi.Input['JobDeploymentArgs']]:
        return pulumi.get(self, "deployment")

    @deployment.setter
    def deployment(self, value: Optional[pulumi.Input['JobDeploymentArgs']]):
        pulumi.set(self, "deployment", value)

    @property
    @pulumi.getter(name="emailNotifications")
    def email_notifications(self) -> Optional[pulumi.Input['JobEmailNotificationsArgs']]:
        """
        (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        """
        return pulumi.get(self, "email_notifications")

    @email_notifications.setter
    def email_notifications(self, value: Optional[pulumi.Input['JobEmailNotificationsArgs']]):
        pulumi.set(self, "email_notifications", value)

    @property
    @pulumi.getter(name="existingClusterId")
    def existing_cluster_id(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "existing_cluster_id")

    @existing_cluster_id.setter
    def existing_cluster_id(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "existing_cluster_id", value)

    @property
    @pulumi.getter
    def format(self) -> Optional[pulumi.Input[str]]:
        return pulumi.get(self, "format")

    @format.setter
    def format(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "format", value)

    @property
    @pulumi.getter(name="gitSource")
    def git_source(self) -> Optional[pulumi.Input['JobGitSourceArgs']]:
        return pulumi.get(self, "git_source")

    @git_source.setter
    def git_source(self, value: Optional[pulumi.Input['JobGitSourceArgs']]):
        pulumi.set(self, "git_source", value)

    @property
    @pulumi.getter
    def health(self) -> Optional[pulumi.Input['JobHealthArgs']]:
        """
        An optional block that specifies the health conditions for the job (described below).
        """
        return pulumi.get(self, "health")

    @health.setter
    def health(self, value: Optional[pulumi.Input['JobHealthArgs']]):
        pulumi.set(self, "health", value)

    @property
    @pulumi.getter(name="jobClusters")
    def job_clusters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]]:
        """
        A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        """
        return pulumi.get(self, "job_clusters")

    @job_clusters.setter
    def job_clusters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobJobClusterArgs']]]]):
        pulumi.set(self, "job_clusters", value)

    @property
    @pulumi.getter
    def libraries(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]]:
        """
        (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        """
        return pulumi.get(self, "libraries")

    @libraries.setter
    def libraries(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobLibraryArgs']]]]):
        pulumi.set(self, "libraries", value)

    @property
    @pulumi.getter(name="maxConcurrentRuns")
    def max_concurrent_runs(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        """
        return pulumi.get(self, "max_concurrent_runs")

    @max_concurrent_runs.setter
    def max_concurrent_runs(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_concurrent_runs", value)

    @property
    @pulumi.getter(name="maxRetries")
    def max_retries(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        """
        return pulumi.get(self, "max_retries")

    @max_retries.setter
    def max_retries(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "max_retries", value)

    @property
    @pulumi.getter(name="minRetryIntervalMillis")
    def min_retry_interval_millis(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        """
        return pulumi.get(self, "min_retry_interval_millis")

    @min_retry_interval_millis.setter
    def min_retry_interval_millis(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "min_retry_interval_millis", value)

    @property
    @pulumi.getter
    def name(self) -> Optional[pulumi.Input[str]]:
        """
        An optional name for the job. The default value is Untitled.
        """
        return pulumi.get(self, "name")

    @name.setter
    def name(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "name", value)

    @property
    @pulumi.getter(name="newCluster")
    def new_cluster(self) -> Optional[pulumi.Input['JobNewClusterArgs']]:
        """
        Same set of parameters as for Cluster resource.
        """
        return pulumi.get(self, "new_cluster")

    @new_cluster.setter
    def new_cluster(self, value: Optional[pulumi.Input['JobNewClusterArgs']]):
        pulumi.set(self, "new_cluster", value)

    @property
    @pulumi.getter(name="notebookTask")
    def notebook_task(self) -> Optional[pulumi.Input['JobNotebookTaskArgs']]:
        return pulumi.get(self, "notebook_task")

    @notebook_task.setter
    def notebook_task(self, value: Optional[pulumi.Input['JobNotebookTaskArgs']]):
        pulumi.set(self, "notebook_task", value)

    @property
    @pulumi.getter(name="notificationSettings")
    def notification_settings(self) -> Optional[pulumi.Input['JobNotificationSettingsArgs']]:
        """
        An optional block controlling the notification settings on the job level (described below).
        """
        return pulumi.get(self, "notification_settings")

    @notification_settings.setter
    def notification_settings(self, value: Optional[pulumi.Input['JobNotificationSettingsArgs']]):
        pulumi.set(self, "notification_settings", value)

    @property
    @pulumi.getter
    def parameters(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]]:
        return pulumi.get(self, "parameters")

    @parameters.setter
    def parameters(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobParameterArgs']]]]):
        pulumi.set(self, "parameters", value)

    @property
    @pulumi.getter(name="pipelineTask")
    def pipeline_task(self) -> Optional[pulumi.Input['JobPipelineTaskArgs']]:
        return pulumi.get(self, "pipeline_task")

    @pipeline_task.setter
    def pipeline_task(self, value: Optional[pulumi.Input['JobPipelineTaskArgs']]):
        pulumi.set(self, "pipeline_task", value)

    @property
    @pulumi.getter(name="pythonWheelTask")
    def python_wheel_task(self) -> Optional[pulumi.Input['JobPythonWheelTaskArgs']]:
        return pulumi.get(self, "python_wheel_task")

    @python_wheel_task.setter
    def python_wheel_task(self, value: Optional[pulumi.Input['JobPythonWheelTaskArgs']]):
        pulumi.set(self, "python_wheel_task", value)

    @property
    @pulumi.getter
    def queue(self) -> Optional[pulumi.Input['JobQueueArgs']]:
        return pulumi.get(self, "queue")

    @queue.setter
    def queue(self, value: Optional[pulumi.Input['JobQueueArgs']]):
        pulumi.set(self, "queue", value)

    @property
    @pulumi.getter(name="retryOnTimeout")
    def retry_on_timeout(self) -> Optional[pulumi.Input[bool]]:
        """
        (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        """
        return pulumi.get(self, "retry_on_timeout")

    @retry_on_timeout.setter
    def retry_on_timeout(self, value: Optional[pulumi.Input[bool]]):
        pulumi.set(self, "retry_on_timeout", value)

    @property
    @pulumi.getter(name="runAs")
    def run_as(self) -> Optional[pulumi.Input['JobRunAsArgs']]:
        return pulumi.get(self, "run_as")

    @run_as.setter
    def run_as(self, value: Optional[pulumi.Input['JobRunAsArgs']]):
        pulumi.set(self, "run_as", value)

    @property
    @pulumi.getter(name="runJobTask")
    def run_job_task(self) -> Optional[pulumi.Input['JobRunJobTaskArgs']]:
        return pulumi.get(self, "run_job_task")

    @run_job_task.setter
    def run_job_task(self, value: Optional[pulumi.Input['JobRunJobTaskArgs']]):
        pulumi.set(self, "run_job_task", value)

    @property
    @pulumi.getter
    def schedule(self) -> Optional[pulumi.Input['JobScheduleArgs']]:
        """
        (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        """
        return pulumi.get(self, "schedule")

    @schedule.setter
    def schedule(self, value: Optional[pulumi.Input['JobScheduleArgs']]):
        pulumi.set(self, "schedule", value)

    @property
    @pulumi.getter(name="sparkJarTask")
    def spark_jar_task(self) -> Optional[pulumi.Input['JobSparkJarTaskArgs']]:
        return pulumi.get(self, "spark_jar_task")

    @spark_jar_task.setter
    def spark_jar_task(self, value: Optional[pulumi.Input['JobSparkJarTaskArgs']]):
        pulumi.set(self, "spark_jar_task", value)

    @property
    @pulumi.getter(name="sparkPythonTask")
    def spark_python_task(self) -> Optional[pulumi.Input['JobSparkPythonTaskArgs']]:
        return pulumi.get(self, "spark_python_task")

    @spark_python_task.setter
    def spark_python_task(self, value: Optional[pulumi.Input['JobSparkPythonTaskArgs']]):
        pulumi.set(self, "spark_python_task", value)

    @property
    @pulumi.getter(name="sparkSubmitTask")
    def spark_submit_task(self) -> Optional[pulumi.Input['JobSparkSubmitTaskArgs']]:
        return pulumi.get(self, "spark_submit_task")

    @spark_submit_task.setter
    def spark_submit_task(self, value: Optional[pulumi.Input['JobSparkSubmitTaskArgs']]):
        pulumi.set(self, "spark_submit_task", value)

    @property
    @pulumi.getter
    def tags(self) -> Optional[pulumi.Input[Mapping[str, Any]]]:
        return pulumi.get(self, "tags")

    @tags.setter
    def tags(self, value: Optional[pulumi.Input[Mapping[str, Any]]]):
        pulumi.set(self, "tags", value)

    @property
    @pulumi.getter
    def tasks(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]]:
        return pulumi.get(self, "tasks")

    @tasks.setter
    def tasks(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['JobTaskArgs']]]]):
        pulumi.set(self, "tasks", value)

    @property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> Optional[pulumi.Input[int]]:
        """
        (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        """
        return pulumi.get(self, "timeout_seconds")

    @timeout_seconds.setter
    def timeout_seconds(self, value: Optional[pulumi.Input[int]]):
        pulumi.set(self, "timeout_seconds", value)

    @property
    @pulumi.getter
    def trigger(self) -> Optional[pulumi.Input['JobTriggerArgs']]:
        return pulumi.get(self, "trigger")

    @trigger.setter
    def trigger(self, value: Optional[pulumi.Input['JobTriggerArgs']]):
        pulumi.set(self, "trigger", value)

    @property
    @pulumi.getter
    def url(self) -> Optional[pulumi.Input[str]]:
        """
        URL of the job on the given workspace
        """
        return pulumi.get(self, "url")

    @url.setter
    def url(self, value: Optional[pulumi.Input[str]]):
        pulumi.set(self, "url", value)

    @property
    @pulumi.getter(name="webhookNotifications")
    def webhook_notifications(self) -> Optional[pulumi.Input['JobWebhookNotificationsArgs']]:
        """
        (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        return pulumi.get(self, "webhook_notifications")

    @webhook_notifications.setter
    def webhook_notifications(self, value: Optional[pulumi.Input['JobWebhookNotificationsArgs']]):
        pulumi.set(self, "webhook_notifications", value)


class Job(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 always_running: Optional[pulumi.Input[bool]] = None,
                 computes: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobComputeArgs']]]]] = None,
                 continuous: Optional[pulumi.Input[pulumi.InputType['JobContinuousArgs']]] = None,
                 control_run_state: Optional[pulumi.Input[bool]] = None,
                 dbt_task: Optional[pulumi.Input[pulumi.InputType['JobDbtTaskArgs']]] = None,
                 deployment: Optional[pulumi.Input[pulumi.InputType['JobDeploymentArgs']]] = None,
                 email_notifications: Optional[pulumi.Input[pulumi.InputType['JobEmailNotificationsArgs']]] = None,
                 existing_cluster_id: Optional[pulumi.Input[str]] = None,
                 format: Optional[pulumi.Input[str]] = None,
                 git_source: Optional[pulumi.Input[pulumi.InputType['JobGitSourceArgs']]] = None,
                 health: Optional[pulumi.Input[pulumi.InputType['JobHealthArgs']]] = None,
                 job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobJobClusterArgs']]]]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobLibraryArgs']]]]] = None,
                 max_concurrent_runs: Optional[pulumi.Input[int]] = None,
                 max_retries: Optional[pulumi.Input[int]] = None,
                 min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 new_cluster: Optional[pulumi.Input[pulumi.InputType['JobNewClusterArgs']]] = None,
                 notebook_task: Optional[pulumi.Input[pulumi.InputType['JobNotebookTaskArgs']]] = None,
                 notification_settings: Optional[pulumi.Input[pulumi.InputType['JobNotificationSettingsArgs']]] = None,
                 parameters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobParameterArgs']]]]] = None,
                 pipeline_task: Optional[pulumi.Input[pulumi.InputType['JobPipelineTaskArgs']]] = None,
                 python_wheel_task: Optional[pulumi.Input[pulumi.InputType['JobPythonWheelTaskArgs']]] = None,
                 queue: Optional[pulumi.Input[pulumi.InputType['JobQueueArgs']]] = None,
                 retry_on_timeout: Optional[pulumi.Input[bool]] = None,
                 run_as: Optional[pulumi.Input[pulumi.InputType['JobRunAsArgs']]] = None,
                 run_job_task: Optional[pulumi.Input[pulumi.InputType['JobRunJobTaskArgs']]] = None,
                 schedule: Optional[pulumi.Input[pulumi.InputType['JobScheduleArgs']]] = None,
                 spark_jar_task: Optional[pulumi.Input[pulumi.InputType['JobSparkJarTaskArgs']]] = None,
                 spark_python_task: Optional[pulumi.Input[pulumi.InputType['JobSparkPythonTaskArgs']]] = None,
                 spark_submit_task: Optional[pulumi.Input[pulumi.InputType['JobSparkSubmitTaskArgs']]] = None,
                 tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 tasks: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobTaskArgs']]]]] = None,
                 timeout_seconds: Optional[pulumi.Input[int]] = None,
                 trigger: Optional[pulumi.Input[pulumi.InputType['JobTriggerArgs']]] = None,
                 webhook_notifications: Optional[pulumi.Input[pulumi.InputType['JobWebhookNotificationsArgs']]] = None,
                 __props__=None):
        """
        ## Import

        The resource job can be imported using the id of the job bash

        ```sh
         $ pulumi import databricks:index/job:Job this <job-id>
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[bool] always_running: (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        :param pulumi.Input[bool] control_run_state: (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.
               
               When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
               ```python
               import pulumi
               ```
        :param pulumi.Input[pulumi.InputType['JobEmailNotificationsArgs']] email_notifications: (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        :param pulumi.Input[pulumi.InputType['JobHealthArgs']] health: An optional block that specifies the health conditions for the job (described below).
        :param pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobJobClusterArgs']]]] job_clusters: A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        :param pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobLibraryArgs']]]] libraries: (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        :param pulumi.Input[int] max_concurrent_runs: (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        :param pulumi.Input[int] max_retries: (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        :param pulumi.Input[int] min_retry_interval_millis: (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        :param pulumi.Input[str] name: An optional name for the job. The default value is Untitled.
        :param pulumi.Input[pulumi.InputType['JobNewClusterArgs']] new_cluster: Same set of parameters as for Cluster resource.
        :param pulumi.Input[pulumi.InputType['JobNotificationSettingsArgs']] notification_settings: An optional block controlling the notification settings on the job level (described below).
        :param pulumi.Input[bool] retry_on_timeout: (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        :param pulumi.Input[pulumi.InputType['JobScheduleArgs']] schedule: (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        :param pulumi.Input[int] timeout_seconds: (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        :param pulumi.Input[pulumi.InputType['JobWebhookNotificationsArgs']] webhook_notifications: (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: Optional[JobArgs] = None,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        ## Import

        The resource job can be imported using the id of the job bash

        ```sh
         $ pulumi import databricks:index/job:Job this <job-id>
        ```

        :param str resource_name: The name of the resource.
        :param JobArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(JobArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            kwargs = kwargs or {}
            def _setter(key, value):
                kwargs[key] = value
            JobArgs._configure(_setter, **kwargs)
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 always_running: Optional[pulumi.Input[bool]] = None,
                 computes: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobComputeArgs']]]]] = None,
                 continuous: Optional[pulumi.Input[pulumi.InputType['JobContinuousArgs']]] = None,
                 control_run_state: Optional[pulumi.Input[bool]] = None,
                 dbt_task: Optional[pulumi.Input[pulumi.InputType['JobDbtTaskArgs']]] = None,
                 deployment: Optional[pulumi.Input[pulumi.InputType['JobDeploymentArgs']]] = None,
                 email_notifications: Optional[pulumi.Input[pulumi.InputType['JobEmailNotificationsArgs']]] = None,
                 existing_cluster_id: Optional[pulumi.Input[str]] = None,
                 format: Optional[pulumi.Input[str]] = None,
                 git_source: Optional[pulumi.Input[pulumi.InputType['JobGitSourceArgs']]] = None,
                 health: Optional[pulumi.Input[pulumi.InputType['JobHealthArgs']]] = None,
                 job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobJobClusterArgs']]]]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobLibraryArgs']]]]] = None,
                 max_concurrent_runs: Optional[pulumi.Input[int]] = None,
                 max_retries: Optional[pulumi.Input[int]] = None,
                 min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
                 name: Optional[pulumi.Input[str]] = None,
                 new_cluster: Optional[pulumi.Input[pulumi.InputType['JobNewClusterArgs']]] = None,
                 notebook_task: Optional[pulumi.Input[pulumi.InputType['JobNotebookTaskArgs']]] = None,
                 notification_settings: Optional[pulumi.Input[pulumi.InputType['JobNotificationSettingsArgs']]] = None,
                 parameters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobParameterArgs']]]]] = None,
                 pipeline_task: Optional[pulumi.Input[pulumi.InputType['JobPipelineTaskArgs']]] = None,
                 python_wheel_task: Optional[pulumi.Input[pulumi.InputType['JobPythonWheelTaskArgs']]] = None,
                 queue: Optional[pulumi.Input[pulumi.InputType['JobQueueArgs']]] = None,
                 retry_on_timeout: Optional[pulumi.Input[bool]] = None,
                 run_as: Optional[pulumi.Input[pulumi.InputType['JobRunAsArgs']]] = None,
                 run_job_task: Optional[pulumi.Input[pulumi.InputType['JobRunJobTaskArgs']]] = None,
                 schedule: Optional[pulumi.Input[pulumi.InputType['JobScheduleArgs']]] = None,
                 spark_jar_task: Optional[pulumi.Input[pulumi.InputType['JobSparkJarTaskArgs']]] = None,
                 spark_python_task: Optional[pulumi.Input[pulumi.InputType['JobSparkPythonTaskArgs']]] = None,
                 spark_submit_task: Optional[pulumi.Input[pulumi.InputType['JobSparkSubmitTaskArgs']]] = None,
                 tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
                 tasks: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobTaskArgs']]]]] = None,
                 timeout_seconds: Optional[pulumi.Input[int]] = None,
                 trigger: Optional[pulumi.Input[pulumi.InputType['JobTriggerArgs']]] = None,
                 webhook_notifications: Optional[pulumi.Input[pulumi.InputType['JobWebhookNotificationsArgs']]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = JobArgs.__new__(JobArgs)

            __props__.__dict__["always_running"] = always_running
            __props__.__dict__["computes"] = computes
            if continuous is not None and not isinstance(continuous, JobContinuousArgs):
                continuous = continuous or {}
                def _setter(key, value):
                    continuous[key] = value
                JobContinuousArgs._configure(_setter, **continuous)
            __props__.__dict__["continuous"] = continuous
            __props__.__dict__["control_run_state"] = control_run_state
            if dbt_task is not None and not isinstance(dbt_task, JobDbtTaskArgs):
                dbt_task = dbt_task or {}
                def _setter(key, value):
                    dbt_task[key] = value
                JobDbtTaskArgs._configure(_setter, **dbt_task)
            __props__.__dict__["dbt_task"] = dbt_task
            if deployment is not None and not isinstance(deployment, JobDeploymentArgs):
                deployment = deployment or {}
                def _setter(key, value):
                    deployment[key] = value
                JobDeploymentArgs._configure(_setter, **deployment)
            __props__.__dict__["deployment"] = deployment
            if email_notifications is not None and not isinstance(email_notifications, JobEmailNotificationsArgs):
                email_notifications = email_notifications or {}
                def _setter(key, value):
                    email_notifications[key] = value
                JobEmailNotificationsArgs._configure(_setter, **email_notifications)
            __props__.__dict__["email_notifications"] = email_notifications
            __props__.__dict__["existing_cluster_id"] = existing_cluster_id
            __props__.__dict__["format"] = format
            if git_source is not None and not isinstance(git_source, JobGitSourceArgs):
                git_source = git_source or {}
                def _setter(key, value):
                    git_source[key] = value
                JobGitSourceArgs._configure(_setter, **git_source)
            __props__.__dict__["git_source"] = git_source
            if health is not None and not isinstance(health, JobHealthArgs):
                health = health or {}
                def _setter(key, value):
                    health[key] = value
                JobHealthArgs._configure(_setter, **health)
            __props__.__dict__["health"] = health
            __props__.__dict__["job_clusters"] = job_clusters
            __props__.__dict__["libraries"] = libraries
            __props__.__dict__["max_concurrent_runs"] = max_concurrent_runs
            __props__.__dict__["max_retries"] = max_retries
            __props__.__dict__["min_retry_interval_millis"] = min_retry_interval_millis
            __props__.__dict__["name"] = name
            if new_cluster is not None and not isinstance(new_cluster, JobNewClusterArgs):
                new_cluster = new_cluster or {}
                def _setter(key, value):
                    new_cluster[key] = value
                JobNewClusterArgs._configure(_setter, **new_cluster)
            __props__.__dict__["new_cluster"] = new_cluster
            if notebook_task is not None and not isinstance(notebook_task, JobNotebookTaskArgs):
                notebook_task = notebook_task or {}
                def _setter(key, value):
                    notebook_task[key] = value
                JobNotebookTaskArgs._configure(_setter, **notebook_task)
            __props__.__dict__["notebook_task"] = notebook_task
            if notification_settings is not None and not isinstance(notification_settings, JobNotificationSettingsArgs):
                notification_settings = notification_settings or {}
                def _setter(key, value):
                    notification_settings[key] = value
                JobNotificationSettingsArgs._configure(_setter, **notification_settings)
            __props__.__dict__["notification_settings"] = notification_settings
            __props__.__dict__["parameters"] = parameters
            if pipeline_task is not None and not isinstance(pipeline_task, JobPipelineTaskArgs):
                pipeline_task = pipeline_task or {}
                def _setter(key, value):
                    pipeline_task[key] = value
                JobPipelineTaskArgs._configure(_setter, **pipeline_task)
            __props__.__dict__["pipeline_task"] = pipeline_task
            if python_wheel_task is not None and not isinstance(python_wheel_task, JobPythonWheelTaskArgs):
                python_wheel_task = python_wheel_task or {}
                def _setter(key, value):
                    python_wheel_task[key] = value
                JobPythonWheelTaskArgs._configure(_setter, **python_wheel_task)
            __props__.__dict__["python_wheel_task"] = python_wheel_task
            if queue is not None and not isinstance(queue, JobQueueArgs):
                queue = queue or {}
                def _setter(key, value):
                    queue[key] = value
                JobQueueArgs._configure(_setter, **queue)
            __props__.__dict__["queue"] = queue
            __props__.__dict__["retry_on_timeout"] = retry_on_timeout
            if run_as is not None and not isinstance(run_as, JobRunAsArgs):
                run_as = run_as or {}
                def _setter(key, value):
                    run_as[key] = value
                JobRunAsArgs._configure(_setter, **run_as)
            __props__.__dict__["run_as"] = run_as
            if run_job_task is not None and not isinstance(run_job_task, JobRunJobTaskArgs):
                run_job_task = run_job_task or {}
                def _setter(key, value):
                    run_job_task[key] = value
                JobRunJobTaskArgs._configure(_setter, **run_job_task)
            __props__.__dict__["run_job_task"] = run_job_task
            if schedule is not None and not isinstance(schedule, JobScheduleArgs):
                schedule = schedule or {}
                def _setter(key, value):
                    schedule[key] = value
                JobScheduleArgs._configure(_setter, **schedule)
            __props__.__dict__["schedule"] = schedule
            if spark_jar_task is not None and not isinstance(spark_jar_task, JobSparkJarTaskArgs):
                spark_jar_task = spark_jar_task or {}
                def _setter(key, value):
                    spark_jar_task[key] = value
                JobSparkJarTaskArgs._configure(_setter, **spark_jar_task)
            __props__.__dict__["spark_jar_task"] = spark_jar_task
            if spark_python_task is not None and not isinstance(spark_python_task, JobSparkPythonTaskArgs):
                spark_python_task = spark_python_task or {}
                def _setter(key, value):
                    spark_python_task[key] = value
                JobSparkPythonTaskArgs._configure(_setter, **spark_python_task)
            __props__.__dict__["spark_python_task"] = spark_python_task
            if spark_submit_task is not None and not isinstance(spark_submit_task, JobSparkSubmitTaskArgs):
                spark_submit_task = spark_submit_task or {}
                def _setter(key, value):
                    spark_submit_task[key] = value
                JobSparkSubmitTaskArgs._configure(_setter, **spark_submit_task)
            __props__.__dict__["spark_submit_task"] = spark_submit_task
            __props__.__dict__["tags"] = tags
            __props__.__dict__["tasks"] = tasks
            __props__.__dict__["timeout_seconds"] = timeout_seconds
            if trigger is not None and not isinstance(trigger, JobTriggerArgs):
                trigger = trigger or {}
                def _setter(key, value):
                    trigger[key] = value
                JobTriggerArgs._configure(_setter, **trigger)
            __props__.__dict__["trigger"] = trigger
            if webhook_notifications is not None and not isinstance(webhook_notifications, JobWebhookNotificationsArgs):
                webhook_notifications = webhook_notifications or {}
                def _setter(key, value):
                    webhook_notifications[key] = value
                JobWebhookNotificationsArgs._configure(_setter, **webhook_notifications)
            __props__.__dict__["webhook_notifications"] = webhook_notifications
            __props__.__dict__["url"] = None
        super(Job, __self__).__init__(
            'databricks:index/job:Job',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            always_running: Optional[pulumi.Input[bool]] = None,
            computes: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobComputeArgs']]]]] = None,
            continuous: Optional[pulumi.Input[pulumi.InputType['JobContinuousArgs']]] = None,
            control_run_state: Optional[pulumi.Input[bool]] = None,
            dbt_task: Optional[pulumi.Input[pulumi.InputType['JobDbtTaskArgs']]] = None,
            deployment: Optional[pulumi.Input[pulumi.InputType['JobDeploymentArgs']]] = None,
            email_notifications: Optional[pulumi.Input[pulumi.InputType['JobEmailNotificationsArgs']]] = None,
            existing_cluster_id: Optional[pulumi.Input[str]] = None,
            format: Optional[pulumi.Input[str]] = None,
            git_source: Optional[pulumi.Input[pulumi.InputType['JobGitSourceArgs']]] = None,
            health: Optional[pulumi.Input[pulumi.InputType['JobHealthArgs']]] = None,
            job_clusters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobJobClusterArgs']]]]] = None,
            libraries: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobLibraryArgs']]]]] = None,
            max_concurrent_runs: Optional[pulumi.Input[int]] = None,
            max_retries: Optional[pulumi.Input[int]] = None,
            min_retry_interval_millis: Optional[pulumi.Input[int]] = None,
            name: Optional[pulumi.Input[str]] = None,
            new_cluster: Optional[pulumi.Input[pulumi.InputType['JobNewClusterArgs']]] = None,
            notebook_task: Optional[pulumi.Input[pulumi.InputType['JobNotebookTaskArgs']]] = None,
            notification_settings: Optional[pulumi.Input[pulumi.InputType['JobNotificationSettingsArgs']]] = None,
            parameters: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobParameterArgs']]]]] = None,
            pipeline_task: Optional[pulumi.Input[pulumi.InputType['JobPipelineTaskArgs']]] = None,
            python_wheel_task: Optional[pulumi.Input[pulumi.InputType['JobPythonWheelTaskArgs']]] = None,
            queue: Optional[pulumi.Input[pulumi.InputType['JobQueueArgs']]] = None,
            retry_on_timeout: Optional[pulumi.Input[bool]] = None,
            run_as: Optional[pulumi.Input[pulumi.InputType['JobRunAsArgs']]] = None,
            run_job_task: Optional[pulumi.Input[pulumi.InputType['JobRunJobTaskArgs']]] = None,
            schedule: Optional[pulumi.Input[pulumi.InputType['JobScheduleArgs']]] = None,
            spark_jar_task: Optional[pulumi.Input[pulumi.InputType['JobSparkJarTaskArgs']]] = None,
            spark_python_task: Optional[pulumi.Input[pulumi.InputType['JobSparkPythonTaskArgs']]] = None,
            spark_submit_task: Optional[pulumi.Input[pulumi.InputType['JobSparkSubmitTaskArgs']]] = None,
            tags: Optional[pulumi.Input[Mapping[str, Any]]] = None,
            tasks: Optional[pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobTaskArgs']]]]] = None,
            timeout_seconds: Optional[pulumi.Input[int]] = None,
            trigger: Optional[pulumi.Input[pulumi.InputType['JobTriggerArgs']]] = None,
            url: Optional[pulumi.Input[str]] = None,
            webhook_notifications: Optional[pulumi.Input[pulumi.InputType['JobWebhookNotificationsArgs']]] = None) -> 'Job':
        """
        Get an existing Job resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[bool] always_running: (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        :param pulumi.Input[bool] control_run_state: (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.
               
               When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
               ```python
               import pulumi
               ```
        :param pulumi.Input[pulumi.InputType['JobEmailNotificationsArgs']] email_notifications: (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        :param pulumi.Input[pulumi.InputType['JobHealthArgs']] health: An optional block that specifies the health conditions for the job (described below).
        :param pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobJobClusterArgs']]]] job_clusters: A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        :param pulumi.Input[Sequence[pulumi.Input[pulumi.InputType['JobLibraryArgs']]]] libraries: (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        :param pulumi.Input[int] max_concurrent_runs: (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        :param pulumi.Input[int] max_retries: (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        :param pulumi.Input[int] min_retry_interval_millis: (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        :param pulumi.Input[str] name: An optional name for the job. The default value is Untitled.
        :param pulumi.Input[pulumi.InputType['JobNewClusterArgs']] new_cluster: Same set of parameters as for Cluster resource.
        :param pulumi.Input[pulumi.InputType['JobNotificationSettingsArgs']] notification_settings: An optional block controlling the notification settings on the job level (described below).
        :param pulumi.Input[bool] retry_on_timeout: (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        :param pulumi.Input[pulumi.InputType['JobScheduleArgs']] schedule: (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        :param pulumi.Input[int] timeout_seconds: (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        :param pulumi.Input[str] url: URL of the job on the given workspace
        :param pulumi.Input[pulumi.InputType['JobWebhookNotificationsArgs']] webhook_notifications: (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _JobState.__new__(_JobState)

        __props__.__dict__["always_running"] = always_running
        __props__.__dict__["computes"] = computes
        __props__.__dict__["continuous"] = continuous
        __props__.__dict__["control_run_state"] = control_run_state
        __props__.__dict__["dbt_task"] = dbt_task
        __props__.__dict__["deployment"] = deployment
        __props__.__dict__["email_notifications"] = email_notifications
        __props__.__dict__["existing_cluster_id"] = existing_cluster_id
        __props__.__dict__["format"] = format
        __props__.__dict__["git_source"] = git_source
        __props__.__dict__["health"] = health
        __props__.__dict__["job_clusters"] = job_clusters
        __props__.__dict__["libraries"] = libraries
        __props__.__dict__["max_concurrent_runs"] = max_concurrent_runs
        __props__.__dict__["max_retries"] = max_retries
        __props__.__dict__["min_retry_interval_millis"] = min_retry_interval_millis
        __props__.__dict__["name"] = name
        __props__.__dict__["new_cluster"] = new_cluster
        __props__.__dict__["notebook_task"] = notebook_task
        __props__.__dict__["notification_settings"] = notification_settings
        __props__.__dict__["parameters"] = parameters
        __props__.__dict__["pipeline_task"] = pipeline_task
        __props__.__dict__["python_wheel_task"] = python_wheel_task
        __props__.__dict__["queue"] = queue
        __props__.__dict__["retry_on_timeout"] = retry_on_timeout
        __props__.__dict__["run_as"] = run_as
        __props__.__dict__["run_job_task"] = run_job_task
        __props__.__dict__["schedule"] = schedule
        __props__.__dict__["spark_jar_task"] = spark_jar_task
        __props__.__dict__["spark_python_task"] = spark_python_task
        __props__.__dict__["spark_submit_task"] = spark_submit_task
        __props__.__dict__["tags"] = tags
        __props__.__dict__["tasks"] = tasks
        __props__.__dict__["timeout_seconds"] = timeout_seconds
        __props__.__dict__["trigger"] = trigger
        __props__.__dict__["url"] = url
        __props__.__dict__["webhook_notifications"] = webhook_notifications
        return Job(resource_name, opts=opts, __props__=__props__)

    @property
    @pulumi.getter(name="alwaysRunning")
    def always_running(self) -> pulumi.Output[Optional[bool]]:
        """
        (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `spark_jar_task` or `spark_submit_task` or `spark_python_task` or `notebook_task` blocks.
        """
        warnings.warn("""always_running will be replaced by control_run_state in the next major release.""", DeprecationWarning)
        pulumi.log.warn("""always_running is deprecated: always_running will be replaced by control_run_state in the next major release.""")

        return pulumi.get(self, "always_running")

    @property
    @pulumi.getter
    def computes(self) -> pulumi.Output[Optional[Sequence['outputs.JobCompute']]]:
        return pulumi.get(self, "computes")

    @property
    @pulumi.getter
    def continuous(self) -> pulumi.Output[Optional['outputs.JobContinuous']]:
        return pulumi.get(self, "continuous")

    @property
    @pulumi.getter(name="controlRunState")
    def control_run_state(self) -> pulumi.Output[Optional[bool]]:
        """
        (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pause_status` by stopping the current active run. This flag cannot be set for non-continuous jobs.

        When migrating from `always_running` to `control_run_state`, set `continuous` as follows:
        ```python
        import pulumi
        ```
        """
        return pulumi.get(self, "control_run_state")

    @property
    @pulumi.getter(name="dbtTask")
    def dbt_task(self) -> pulumi.Output[Optional['outputs.JobDbtTask']]:
        return pulumi.get(self, "dbt_task")

    @property
    @pulumi.getter
    def deployment(self) -> pulumi.Output[Optional['outputs.JobDeployment']]:
        return pulumi.get(self, "deployment")

    @property
    @pulumi.getter(name="emailNotifications")
    def email_notifications(self) -> pulumi.Output[Optional['outputs.JobEmailNotifications']]:
        """
        (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
        """
        return pulumi.get(self, "email_notifications")

    @property
    @pulumi.getter(name="existingClusterId")
    def existing_cluster_id(self) -> pulumi.Output[Optional[str]]:
        return pulumi.get(self, "existing_cluster_id")

    @property
    @pulumi.getter
    def format(self) -> pulumi.Output[str]:
        return pulumi.get(self, "format")

    @property
    @pulumi.getter(name="gitSource")
    def git_source(self) -> pulumi.Output[Optional['outputs.JobGitSource']]:
        return pulumi.get(self, "git_source")

    @property
    @pulumi.getter
    def health(self) -> pulumi.Output[Optional['outputs.JobHealth']]:
        """
        An optional block that specifies the health conditions for the job (described below).
        """
        return pulumi.get(self, "health")

    @property
    @pulumi.getter(name="jobClusters")
    def job_clusters(self) -> pulumi.Output[Optional[Sequence['outputs.JobJobCluster']]]:
        """
        A list of job Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
        """
        return pulumi.get(self, "job_clusters")

    @property
    @pulumi.getter
    def libraries(self) -> pulumi.Output[Optional[Sequence['outputs.JobLibrary']]]:
        """
        (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for Cluster resource.
        """
        return pulumi.get(self, "libraries")

    @property
    @pulumi.getter(name="maxConcurrentRuns")
    def max_concurrent_runs(self) -> pulumi.Output[Optional[int]]:
        """
        (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
        """
        return pulumi.get(self, "max_concurrent_runs")

    @property
    @pulumi.getter(name="maxRetries")
    def max_retries(self) -> pulumi.Output[Optional[int]]:
        """
        (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry.
        """
        return pulumi.get(self, "max_retries")

    @property
    @pulumi.getter(name="minRetryIntervalMillis")
    def min_retry_interval_millis(self) -> pulumi.Output[Optional[int]]:
        """
        (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
        """
        return pulumi.get(self, "min_retry_interval_millis")

    @property
    @pulumi.getter
    def name(self) -> pulumi.Output[str]:
        """
        An optional name for the job. The default value is Untitled.
        """
        return pulumi.get(self, "name")

    @property
    @pulumi.getter(name="newCluster")
    def new_cluster(self) -> pulumi.Output[Optional['outputs.JobNewCluster']]:
        """
        Same set of parameters as for Cluster resource.
        """
        return pulumi.get(self, "new_cluster")

    @property
    @pulumi.getter(name="notebookTask")
    def notebook_task(self) -> pulumi.Output[Optional['outputs.JobNotebookTask']]:
        return pulumi.get(self, "notebook_task")

    @property
    @pulumi.getter(name="notificationSettings")
    def notification_settings(self) -> pulumi.Output[Optional['outputs.JobNotificationSettings']]:
        """
        An optional block controlling the notification settings on the job level (described below).
        """
        return pulumi.get(self, "notification_settings")

    @property
    @pulumi.getter
    def parameters(self) -> pulumi.Output[Optional[Sequence['outputs.JobParameter']]]:
        return pulumi.get(self, "parameters")

    @property
    @pulumi.getter(name="pipelineTask")
    def pipeline_task(self) -> pulumi.Output[Optional['outputs.JobPipelineTask']]:
        return pulumi.get(self, "pipeline_task")

    @property
    @pulumi.getter(name="pythonWheelTask")
    def python_wheel_task(self) -> pulumi.Output[Optional['outputs.JobPythonWheelTask']]:
        return pulumi.get(self, "python_wheel_task")

    @property
    @pulumi.getter
    def queue(self) -> pulumi.Output[Optional['outputs.JobQueue']]:
        return pulumi.get(self, "queue")

    @property
    @pulumi.getter(name="retryOnTimeout")
    def retry_on_timeout(self) -> pulumi.Output[Optional[bool]]:
        """
        (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
        """
        return pulumi.get(self, "retry_on_timeout")

    @property
    @pulumi.getter(name="runAs")
    def run_as(self) -> pulumi.Output[Optional['outputs.JobRunAs']]:
        return pulumi.get(self, "run_as")

    @property
    @pulumi.getter(name="runJobTask")
    def run_job_task(self) -> pulumi.Output[Optional['outputs.JobRunJobTask']]:
        return pulumi.get(self, "run_job_task")

    @property
    @pulumi.getter
    def schedule(self) -> pulumi.Output[Optional['outputs.JobSchedule']]:
        """
        (List) An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. This field is a block and is documented below.
        """
        return pulumi.get(self, "schedule")

    @property
    @pulumi.getter(name="sparkJarTask")
    def spark_jar_task(self) -> pulumi.Output[Optional['outputs.JobSparkJarTask']]:
        return pulumi.get(self, "spark_jar_task")

    @property
    @pulumi.getter(name="sparkPythonTask")
    def spark_python_task(self) -> pulumi.Output[Optional['outputs.JobSparkPythonTask']]:
        return pulumi.get(self, "spark_python_task")

    @property
    @pulumi.getter(name="sparkSubmitTask")
    def spark_submit_task(self) -> pulumi.Output[Optional['outputs.JobSparkSubmitTask']]:
        return pulumi.get(self, "spark_submit_task")

    @property
    @pulumi.getter
    def tags(self) -> pulumi.Output[Optional[Mapping[str, Any]]]:
        return pulumi.get(self, "tags")

    @property
    @pulumi.getter
    def tasks(self) -> pulumi.Output[Optional[Sequence['outputs.JobTask']]]:
        return pulumi.get(self, "tasks")

    @property
    @pulumi.getter(name="timeoutSeconds")
    def timeout_seconds(self) -> pulumi.Output[Optional[int]]:
        """
        (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
        """
        return pulumi.get(self, "timeout_seconds")

    @property
    @pulumi.getter
    def trigger(self) -> pulumi.Output[Optional['outputs.JobTrigger']]:
        return pulumi.get(self, "trigger")

    @property
    @pulumi.getter
    def url(self) -> pulumi.Output[str]:
        """
        URL of the job on the given workspace
        """
        return pulumi.get(self, "url")

    @property
    @pulumi.getter(name="webhookNotifications")
    def webhook_notifications(self) -> pulumi.Output[Optional['outputs.JobWebhookNotifications']]:
        """
        (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes and fails. The default behavior is to not send any notifications. This field is a block and is documented below.
        """
        return pulumi.get(self, "webhook_notifications")

