# coding=utf-8
# *** WARNING: this file was generated by pulumi-language-python. ***
# *** Do not edit by hand unless you're certain you know what you are doing! ***

import builtins
import copy
import warnings
import sys
import pulumi
import pulumi.runtime
from typing import Any, Mapping, Optional, Sequence, Union, overload
if sys.version_info >= (3, 11):
    from typing import NotRequired, TypedDict, TypeAlias
else:
    from typing_extensions import NotRequired, TypedDict, TypeAlias
from . import _utilities
from . import outputs
from ._inputs import *

__all__ = ['ClusterArgs', 'Cluster']

@pulumi.input_type
class ClusterArgs:
    def __init__(__self__, *,
                 spark_version: pulumi.Input[builtins.str],
                 apply_policy_default_values: Optional[pulumi.Input[builtins.bool]] = None,
                 autoscale: Optional[pulumi.Input['ClusterAutoscaleArgs']] = None,
                 autotermination_minutes: Optional[pulumi.Input[builtins.int]] = None,
                 aws_attributes: Optional[pulumi.Input['ClusterAwsAttributesArgs']] = None,
                 azure_attributes: Optional[pulumi.Input['ClusterAzureAttributesArgs']] = None,
                 cluster_log_conf: Optional[pulumi.Input['ClusterClusterLogConfArgs']] = None,
                 cluster_mount_infos: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]] = None,
                 cluster_name: Optional[pulumi.Input[builtins.str]] = None,
                 custom_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 data_security_mode: Optional[pulumi.Input[builtins.str]] = None,
                 docker_image: Optional[pulumi.Input['ClusterDockerImageArgs']] = None,
                 driver_instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 driver_node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 enable_elastic_disk: Optional[pulumi.Input[builtins.bool]] = None,
                 enable_local_disk_encryption: Optional[pulumi.Input[builtins.bool]] = None,
                 gcp_attributes: Optional[pulumi.Input['ClusterGcpAttributesArgs']] = None,
                 idempotency_token: Optional[pulumi.Input[builtins.str]] = None,
                 init_scripts: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]] = None,
                 instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 is_pinned: Optional[pulumi.Input[builtins.bool]] = None,
                 is_single_node: Optional[pulumi.Input[builtins.bool]] = None,
                 kind: Optional[pulumi.Input[builtins.str]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]] = None,
                 no_wait: Optional[pulumi.Input[builtins.bool]] = None,
                 node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 num_workers: Optional[pulumi.Input[builtins.int]] = None,
                 policy_id: Optional[pulumi.Input[builtins.str]] = None,
                 runtime_engine: Optional[pulumi.Input[builtins.str]] = None,
                 single_user_name: Optional[pulumi.Input[builtins.str]] = None,
                 spark_conf: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_env_vars: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 ssh_public_keys: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]] = None,
                 use_ml_runtime: Optional[pulumi.Input[builtins.bool]] = None,
                 workload_type: Optional[pulumi.Input['ClusterWorkloadTypeArgs']] = None):
        """
        The set of arguments for constructing a Cluster resource.
        :param pulumi.Input[builtins.str] spark_version: [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        :param pulumi.Input[builtins.bool] apply_policy_default_values: Whether to use policy default values for missing cluster attributes.
        :param pulumi.Input[builtins.int] autotermination_minutes: Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        :param pulumi.Input[builtins.str] cluster_name: Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] custom_tags: should have tag `ResourceClass` set to value `Serverless`
               
               For example:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
                   cluster_name="Shared High-Concurrency",
                   spark_version=latest_lts["id"],
                   node_type_id=smallest["id"],
                   autotermination_minutes=20,
                   spark_conf={
                       "spark.databricks.repl.allowedLanguages": "python,sql",
                       "spark.databricks.cluster.profile": "serverless",
                   },
                   custom_tags={
                       "ResourceClass": "Serverless",
                   })
               ```
        :param pulumi.Input[builtins.str] data_security_mode: Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
               * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
               * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
               * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        :param pulumi.Input[builtins.str] driver_instance_pool_id: similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        :param pulumi.Input[builtins.str] driver_node_type_id: The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        :param pulumi.Input[builtins.bool] enable_elastic_disk: If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        :param pulumi.Input[builtins.bool] enable_local_disk_encryption: Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        :param pulumi.Input[builtins.str] idempotency_token: An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        :param pulumi.Input[builtins.str] instance_pool_id: To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        :param pulumi.Input[builtins.bool] is_pinned: boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        :param pulumi.Input[builtins.bool] is_single_node: When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        :param pulumi.Input[builtins.str] kind: The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        :param pulumi.Input[builtins.bool] no_wait: If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
               
               The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               smallest = databricks.get_node_type(local_disk=True)
               latest_lts = databricks.get_spark_version(long_term_support=True)
               shared_autoscaling = databricks.Cluster("shared_autoscaling",
                   cluster_name="Shared Autoscaling",
                   spark_version=latest_lts.id,
                   node_type_id=smallest.id,
                   autotermination_minutes=20,
                   autoscale={
                       "min_workers": 1,
                       "max_workers": 50,
                   },
                   spark_conf={
                       "spark.databricks.io.cache.enabled": "true",
                       "spark.databricks.io.cache.maxDiskUsage": "50g",
                       "spark.databricks.io.cache.maxMetaDataCache": "1g",
                   })
               ```
        :param pulumi.Input[builtins.str] node_type_id: Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        :param pulumi.Input[builtins.int] num_workers: Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        :param pulumi.Input[builtins.str] policy_id: Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        :param pulumi.Input[builtins.str] runtime_engine: The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        :param pulumi.Input[builtins.str] single_user_name: The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_conf: should have following items:
               * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
               * `spark.databricks.cluster.profile` set to `serverless`
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_env_vars: Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        :param pulumi.Input[Sequence[pulumi.Input[builtins.str]]] ssh_public_keys: SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        :param pulumi.Input[builtins.bool] use_ml_runtime: Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        pulumi.set(__self__, "spark_version", spark_version)
        if apply_policy_default_values is not None:
            pulumi.set(__self__, "apply_policy_default_values", apply_policy_default_values)
        if autoscale is not None:
            pulumi.set(__self__, "autoscale", autoscale)
        if autotermination_minutes is not None:
            pulumi.set(__self__, "autotermination_minutes", autotermination_minutes)
        if aws_attributes is not None:
            pulumi.set(__self__, "aws_attributes", aws_attributes)
        if azure_attributes is not None:
            pulumi.set(__self__, "azure_attributes", azure_attributes)
        if cluster_log_conf is not None:
            pulumi.set(__self__, "cluster_log_conf", cluster_log_conf)
        if cluster_mount_infos is not None:
            pulumi.set(__self__, "cluster_mount_infos", cluster_mount_infos)
        if cluster_name is not None:
            pulumi.set(__self__, "cluster_name", cluster_name)
        if custom_tags is not None:
            pulumi.set(__self__, "custom_tags", custom_tags)
        if data_security_mode is not None:
            pulumi.set(__self__, "data_security_mode", data_security_mode)
        if docker_image is not None:
            pulumi.set(__self__, "docker_image", docker_image)
        if driver_instance_pool_id is not None:
            pulumi.set(__self__, "driver_instance_pool_id", driver_instance_pool_id)
        if driver_node_type_id is not None:
            pulumi.set(__self__, "driver_node_type_id", driver_node_type_id)
        if enable_elastic_disk is not None:
            pulumi.set(__self__, "enable_elastic_disk", enable_elastic_disk)
        if enable_local_disk_encryption is not None:
            pulumi.set(__self__, "enable_local_disk_encryption", enable_local_disk_encryption)
        if gcp_attributes is not None:
            pulumi.set(__self__, "gcp_attributes", gcp_attributes)
        if idempotency_token is not None:
            pulumi.set(__self__, "idempotency_token", idempotency_token)
        if init_scripts is not None:
            pulumi.set(__self__, "init_scripts", init_scripts)
        if instance_pool_id is not None:
            pulumi.set(__self__, "instance_pool_id", instance_pool_id)
        if is_pinned is not None:
            pulumi.set(__self__, "is_pinned", is_pinned)
        if is_single_node is not None:
            pulumi.set(__self__, "is_single_node", is_single_node)
        if kind is not None:
            pulumi.set(__self__, "kind", kind)
        if libraries is not None:
            pulumi.set(__self__, "libraries", libraries)
        if no_wait is not None:
            pulumi.set(__self__, "no_wait", no_wait)
        if node_type_id is not None:
            pulumi.set(__self__, "node_type_id", node_type_id)
        if num_workers is not None:
            pulumi.set(__self__, "num_workers", num_workers)
        if policy_id is not None:
            pulumi.set(__self__, "policy_id", policy_id)
        if runtime_engine is not None:
            pulumi.set(__self__, "runtime_engine", runtime_engine)
        if single_user_name is not None:
            pulumi.set(__self__, "single_user_name", single_user_name)
        if spark_conf is not None:
            pulumi.set(__self__, "spark_conf", spark_conf)
        if spark_env_vars is not None:
            pulumi.set(__self__, "spark_env_vars", spark_env_vars)
        if ssh_public_keys is not None:
            pulumi.set(__self__, "ssh_public_keys", ssh_public_keys)
        if use_ml_runtime is not None:
            pulumi.set(__self__, "use_ml_runtime", use_ml_runtime)
        if workload_type is not None:
            pulumi.set(__self__, "workload_type", workload_type)

    @property
    @pulumi.getter(name="sparkVersion")
    def spark_version(self) -> pulumi.Input[builtins.str]:
        """
        [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        """
        return pulumi.get(self, "spark_version")

    @spark_version.setter
    def spark_version(self, value: pulumi.Input[builtins.str]):
        pulumi.set(self, "spark_version", value)

    @property
    @pulumi.getter(name="applyPolicyDefaultValues")
    def apply_policy_default_values(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Whether to use policy default values for missing cluster attributes.
        """
        return pulumi.get(self, "apply_policy_default_values")

    @apply_policy_default_values.setter
    def apply_policy_default_values(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "apply_policy_default_values", value)

    @property
    @pulumi.getter
    def autoscale(self) -> Optional[pulumi.Input['ClusterAutoscaleArgs']]:
        return pulumi.get(self, "autoscale")

    @autoscale.setter
    def autoscale(self, value: Optional[pulumi.Input['ClusterAutoscaleArgs']]):
        pulumi.set(self, "autoscale", value)

    @property
    @pulumi.getter(name="autoterminationMinutes")
    def autotermination_minutes(self) -> Optional[pulumi.Input[builtins.int]]:
        """
        Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        """
        return pulumi.get(self, "autotermination_minutes")

    @autotermination_minutes.setter
    def autotermination_minutes(self, value: Optional[pulumi.Input[builtins.int]]):
        pulumi.set(self, "autotermination_minutes", value)

    @property
    @pulumi.getter(name="awsAttributes")
    def aws_attributes(self) -> Optional[pulumi.Input['ClusterAwsAttributesArgs']]:
        return pulumi.get(self, "aws_attributes")

    @aws_attributes.setter
    def aws_attributes(self, value: Optional[pulumi.Input['ClusterAwsAttributesArgs']]):
        pulumi.set(self, "aws_attributes", value)

    @property
    @pulumi.getter(name="azureAttributes")
    def azure_attributes(self) -> Optional[pulumi.Input['ClusterAzureAttributesArgs']]:
        return pulumi.get(self, "azure_attributes")

    @azure_attributes.setter
    def azure_attributes(self, value: Optional[pulumi.Input['ClusterAzureAttributesArgs']]):
        pulumi.set(self, "azure_attributes", value)

    @property
    @pulumi.getter(name="clusterLogConf")
    def cluster_log_conf(self) -> Optional[pulumi.Input['ClusterClusterLogConfArgs']]:
        return pulumi.get(self, "cluster_log_conf")

    @cluster_log_conf.setter
    def cluster_log_conf(self, value: Optional[pulumi.Input['ClusterClusterLogConfArgs']]):
        pulumi.set(self, "cluster_log_conf", value)

    @property
    @pulumi.getter(name="clusterMountInfos")
    def cluster_mount_infos(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]]:
        return pulumi.get(self, "cluster_mount_infos")

    @cluster_mount_infos.setter
    def cluster_mount_infos(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]]):
        pulumi.set(self, "cluster_mount_infos", value)

    @property
    @pulumi.getter(name="clusterName")
    def cluster_name(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        """
        return pulumi.get(self, "cluster_name")

    @cluster_name.setter
    def cluster_name(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "cluster_name", value)

    @property
    @pulumi.getter(name="customTags")
    def custom_tags(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        should have tag `ResourceClass` set to value `Serverless`

        For example:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
            cluster_name="Shared High-Concurrency",
            spark_version=latest_lts["id"],
            node_type_id=smallest["id"],
            autotermination_minutes=20,
            spark_conf={
                "spark.databricks.repl.allowedLanguages": "python,sql",
                "spark.databricks.cluster.profile": "serverless",
            },
            custom_tags={
                "ResourceClass": "Serverless",
            })
        ```
        """
        return pulumi.get(self, "custom_tags")

    @custom_tags.setter
    def custom_tags(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "custom_tags", value)

    @property
    @pulumi.getter(name="dataSecurityMode")
    def data_security_mode(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
        * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
        * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
        * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        """
        return pulumi.get(self, "data_security_mode")

    @data_security_mode.setter
    def data_security_mode(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "data_security_mode", value)

    @property
    @pulumi.getter(name="dockerImage")
    def docker_image(self) -> Optional[pulumi.Input['ClusterDockerImageArgs']]:
        return pulumi.get(self, "docker_image")

    @docker_image.setter
    def docker_image(self, value: Optional[pulumi.Input['ClusterDockerImageArgs']]):
        pulumi.set(self, "docker_image", value)

    @property
    @pulumi.getter(name="driverInstancePoolId")
    def driver_instance_pool_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        """
        return pulumi.get(self, "driver_instance_pool_id")

    @driver_instance_pool_id.setter
    def driver_instance_pool_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "driver_instance_pool_id", value)

    @property
    @pulumi.getter(name="driverNodeTypeId")
    def driver_node_type_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        """
        return pulumi.get(self, "driver_node_type_id")

    @driver_node_type_id.setter
    def driver_node_type_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "driver_node_type_id", value)

    @property
    @pulumi.getter(name="enableElasticDisk")
    def enable_elastic_disk(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        """
        return pulumi.get(self, "enable_elastic_disk")

    @enable_elastic_disk.setter
    def enable_elastic_disk(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "enable_elastic_disk", value)

    @property
    @pulumi.getter(name="enableLocalDiskEncryption")
    def enable_local_disk_encryption(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        """
        return pulumi.get(self, "enable_local_disk_encryption")

    @enable_local_disk_encryption.setter
    def enable_local_disk_encryption(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "enable_local_disk_encryption", value)

    @property
    @pulumi.getter(name="gcpAttributes")
    def gcp_attributes(self) -> Optional[pulumi.Input['ClusterGcpAttributesArgs']]:
        return pulumi.get(self, "gcp_attributes")

    @gcp_attributes.setter
    def gcp_attributes(self, value: Optional[pulumi.Input['ClusterGcpAttributesArgs']]):
        pulumi.set(self, "gcp_attributes", value)

    @property
    @pulumi.getter(name="idempotencyToken")
    def idempotency_token(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        """
        return pulumi.get(self, "idempotency_token")

    @idempotency_token.setter
    def idempotency_token(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "idempotency_token", value)

    @property
    @pulumi.getter(name="initScripts")
    def init_scripts(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]]:
        return pulumi.get(self, "init_scripts")

    @init_scripts.setter
    def init_scripts(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]]):
        pulumi.set(self, "init_scripts", value)

    @property
    @pulumi.getter(name="instancePoolId")
    def instance_pool_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        """
        return pulumi.get(self, "instance_pool_id")

    @instance_pool_id.setter
    def instance_pool_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "instance_pool_id", value)

    @property
    @pulumi.getter(name="isPinned")
    def is_pinned(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        """
        return pulumi.get(self, "is_pinned")

    @is_pinned.setter
    def is_pinned(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "is_pinned", value)

    @property
    @pulumi.getter(name="isSingleNode")
    def is_single_node(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        """
        return pulumi.get(self, "is_single_node")

    @is_single_node.setter
    def is_single_node(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "is_single_node", value)

    @property
    @pulumi.getter
    def kind(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        """
        return pulumi.get(self, "kind")

    @kind.setter
    def kind(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "kind", value)

    @property
    @pulumi.getter
    def libraries(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]]:
        return pulumi.get(self, "libraries")

    @libraries.setter
    def libraries(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]]):
        pulumi.set(self, "libraries", value)

    @property
    @pulumi.getter(name="noWait")
    def no_wait(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).

        The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        smallest = databricks.get_node_type(local_disk=True)
        latest_lts = databricks.get_spark_version(long_term_support=True)
        shared_autoscaling = databricks.Cluster("shared_autoscaling",
            cluster_name="Shared Autoscaling",
            spark_version=latest_lts.id,
            node_type_id=smallest.id,
            autotermination_minutes=20,
            autoscale={
                "min_workers": 1,
                "max_workers": 50,
            },
            spark_conf={
                "spark.databricks.io.cache.enabled": "true",
                "spark.databricks.io.cache.maxDiskUsage": "50g",
                "spark.databricks.io.cache.maxMetaDataCache": "1g",
            })
        ```
        """
        return pulumi.get(self, "no_wait")

    @no_wait.setter
    def no_wait(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "no_wait", value)

    @property
    @pulumi.getter(name="nodeTypeId")
    def node_type_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        """
        return pulumi.get(self, "node_type_id")

    @node_type_id.setter
    def node_type_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "node_type_id", value)

    @property
    @pulumi.getter(name="numWorkers")
    def num_workers(self) -> Optional[pulumi.Input[builtins.int]]:
        """
        Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        """
        return pulumi.get(self, "num_workers")

    @num_workers.setter
    def num_workers(self, value: Optional[pulumi.Input[builtins.int]]):
        pulumi.set(self, "num_workers", value)

    @property
    @pulumi.getter(name="policyId")
    def policy_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        """
        return pulumi.get(self, "policy_id")

    @policy_id.setter
    def policy_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "policy_id", value)

    @property
    @pulumi.getter(name="runtimeEngine")
    def runtime_engine(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        """
        return pulumi.get(self, "runtime_engine")

    @runtime_engine.setter
    def runtime_engine(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "runtime_engine", value)

    @property
    @pulumi.getter(name="singleUserName")
    def single_user_name(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        """
        return pulumi.get(self, "single_user_name")

    @single_user_name.setter
    def single_user_name(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "single_user_name", value)

    @property
    @pulumi.getter(name="sparkConf")
    def spark_conf(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        should have following items:
        * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
        * `spark.databricks.cluster.profile` set to `serverless`
        """
        return pulumi.get(self, "spark_conf")

    @spark_conf.setter
    def spark_conf(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "spark_conf", value)

    @property
    @pulumi.getter(name="sparkEnvVars")
    def spark_env_vars(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        """
        return pulumi.get(self, "spark_env_vars")

    @spark_env_vars.setter
    def spark_env_vars(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "spark_env_vars", value)

    @property
    @pulumi.getter(name="sshPublicKeys")
    def ssh_public_keys(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]]:
        """
        SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        """
        return pulumi.get(self, "ssh_public_keys")

    @ssh_public_keys.setter
    def ssh_public_keys(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "ssh_public_keys", value)

    @property
    @pulumi.getter(name="useMlRuntime")
    def use_ml_runtime(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        return pulumi.get(self, "use_ml_runtime")

    @use_ml_runtime.setter
    def use_ml_runtime(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "use_ml_runtime", value)

    @property
    @pulumi.getter(name="workloadType")
    def workload_type(self) -> Optional[pulumi.Input['ClusterWorkloadTypeArgs']]:
        return pulumi.get(self, "workload_type")

    @workload_type.setter
    def workload_type(self, value: Optional[pulumi.Input['ClusterWorkloadTypeArgs']]):
        pulumi.set(self, "workload_type", value)


@pulumi.input_type
class _ClusterState:
    def __init__(__self__, *,
                 apply_policy_default_values: Optional[pulumi.Input[builtins.bool]] = None,
                 autoscale: Optional[pulumi.Input['ClusterAutoscaleArgs']] = None,
                 autotermination_minutes: Optional[pulumi.Input[builtins.int]] = None,
                 aws_attributes: Optional[pulumi.Input['ClusterAwsAttributesArgs']] = None,
                 azure_attributes: Optional[pulumi.Input['ClusterAzureAttributesArgs']] = None,
                 cluster_id: Optional[pulumi.Input[builtins.str]] = None,
                 cluster_log_conf: Optional[pulumi.Input['ClusterClusterLogConfArgs']] = None,
                 cluster_mount_infos: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]] = None,
                 cluster_name: Optional[pulumi.Input[builtins.str]] = None,
                 custom_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 data_security_mode: Optional[pulumi.Input[builtins.str]] = None,
                 default_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 docker_image: Optional[pulumi.Input['ClusterDockerImageArgs']] = None,
                 driver_instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 driver_node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 enable_elastic_disk: Optional[pulumi.Input[builtins.bool]] = None,
                 enable_local_disk_encryption: Optional[pulumi.Input[builtins.bool]] = None,
                 gcp_attributes: Optional[pulumi.Input['ClusterGcpAttributesArgs']] = None,
                 idempotency_token: Optional[pulumi.Input[builtins.str]] = None,
                 init_scripts: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]] = None,
                 instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 is_pinned: Optional[pulumi.Input[builtins.bool]] = None,
                 is_single_node: Optional[pulumi.Input[builtins.bool]] = None,
                 kind: Optional[pulumi.Input[builtins.str]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]] = None,
                 no_wait: Optional[pulumi.Input[builtins.bool]] = None,
                 node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 num_workers: Optional[pulumi.Input[builtins.int]] = None,
                 policy_id: Optional[pulumi.Input[builtins.str]] = None,
                 runtime_engine: Optional[pulumi.Input[builtins.str]] = None,
                 single_user_name: Optional[pulumi.Input[builtins.str]] = None,
                 spark_conf: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_env_vars: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_version: Optional[pulumi.Input[builtins.str]] = None,
                 ssh_public_keys: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]] = None,
                 state: Optional[pulumi.Input[builtins.str]] = None,
                 url: Optional[pulumi.Input[builtins.str]] = None,
                 use_ml_runtime: Optional[pulumi.Input[builtins.bool]] = None,
                 workload_type: Optional[pulumi.Input['ClusterWorkloadTypeArgs']] = None):
        """
        Input properties used for looking up and filtering Cluster resources.
        :param pulumi.Input[builtins.bool] apply_policy_default_values: Whether to use policy default values for missing cluster attributes.
        :param pulumi.Input[builtins.int] autotermination_minutes: Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        :param pulumi.Input[builtins.str] cluster_name: Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] custom_tags: should have tag `ResourceClass` set to value `Serverless`
               
               For example:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
                   cluster_name="Shared High-Concurrency",
                   spark_version=latest_lts["id"],
                   node_type_id=smallest["id"],
                   autotermination_minutes=20,
                   spark_conf={
                       "spark.databricks.repl.allowedLanguages": "python,sql",
                       "spark.databricks.cluster.profile": "serverless",
                   },
                   custom_tags={
                       "ResourceClass": "Serverless",
                   })
               ```
        :param pulumi.Input[builtins.str] data_security_mode: Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
               * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
               * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
               * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] default_tags: (map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
        :param pulumi.Input[builtins.str] driver_instance_pool_id: similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        :param pulumi.Input[builtins.str] driver_node_type_id: The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        :param pulumi.Input[builtins.bool] enable_elastic_disk: If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        :param pulumi.Input[builtins.bool] enable_local_disk_encryption: Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        :param pulumi.Input[builtins.str] idempotency_token: An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        :param pulumi.Input[builtins.str] instance_pool_id: To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        :param pulumi.Input[builtins.bool] is_pinned: boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        :param pulumi.Input[builtins.bool] is_single_node: When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        :param pulumi.Input[builtins.str] kind: The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        :param pulumi.Input[builtins.bool] no_wait: If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
               
               The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               smallest = databricks.get_node_type(local_disk=True)
               latest_lts = databricks.get_spark_version(long_term_support=True)
               shared_autoscaling = databricks.Cluster("shared_autoscaling",
                   cluster_name="Shared Autoscaling",
                   spark_version=latest_lts.id,
                   node_type_id=smallest.id,
                   autotermination_minutes=20,
                   autoscale={
                       "min_workers": 1,
                       "max_workers": 50,
                   },
                   spark_conf={
                       "spark.databricks.io.cache.enabled": "true",
                       "spark.databricks.io.cache.maxDiskUsage": "50g",
                       "spark.databricks.io.cache.maxMetaDataCache": "1g",
                   })
               ```
        :param pulumi.Input[builtins.str] node_type_id: Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        :param pulumi.Input[builtins.int] num_workers: Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        :param pulumi.Input[builtins.str] policy_id: Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        :param pulumi.Input[builtins.str] runtime_engine: The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        :param pulumi.Input[builtins.str] single_user_name: The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_conf: should have following items:
               * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
               * `spark.databricks.cluster.profile` set to `serverless`
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_env_vars: Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        :param pulumi.Input[builtins.str] spark_version: [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        :param pulumi.Input[Sequence[pulumi.Input[builtins.str]]] ssh_public_keys: SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        :param pulumi.Input[builtins.str] state: (string) State of the cluster.
        :param pulumi.Input[builtins.bool] use_ml_runtime: Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        if apply_policy_default_values is not None:
            pulumi.set(__self__, "apply_policy_default_values", apply_policy_default_values)
        if autoscale is not None:
            pulumi.set(__self__, "autoscale", autoscale)
        if autotermination_minutes is not None:
            pulumi.set(__self__, "autotermination_minutes", autotermination_minutes)
        if aws_attributes is not None:
            pulumi.set(__self__, "aws_attributes", aws_attributes)
        if azure_attributes is not None:
            pulumi.set(__self__, "azure_attributes", azure_attributes)
        if cluster_id is not None:
            pulumi.set(__self__, "cluster_id", cluster_id)
        if cluster_log_conf is not None:
            pulumi.set(__self__, "cluster_log_conf", cluster_log_conf)
        if cluster_mount_infos is not None:
            pulumi.set(__self__, "cluster_mount_infos", cluster_mount_infos)
        if cluster_name is not None:
            pulumi.set(__self__, "cluster_name", cluster_name)
        if custom_tags is not None:
            pulumi.set(__self__, "custom_tags", custom_tags)
        if data_security_mode is not None:
            pulumi.set(__self__, "data_security_mode", data_security_mode)
        if default_tags is not None:
            pulumi.set(__self__, "default_tags", default_tags)
        if docker_image is not None:
            pulumi.set(__self__, "docker_image", docker_image)
        if driver_instance_pool_id is not None:
            pulumi.set(__self__, "driver_instance_pool_id", driver_instance_pool_id)
        if driver_node_type_id is not None:
            pulumi.set(__self__, "driver_node_type_id", driver_node_type_id)
        if enable_elastic_disk is not None:
            pulumi.set(__self__, "enable_elastic_disk", enable_elastic_disk)
        if enable_local_disk_encryption is not None:
            pulumi.set(__self__, "enable_local_disk_encryption", enable_local_disk_encryption)
        if gcp_attributes is not None:
            pulumi.set(__self__, "gcp_attributes", gcp_attributes)
        if idempotency_token is not None:
            pulumi.set(__self__, "idempotency_token", idempotency_token)
        if init_scripts is not None:
            pulumi.set(__self__, "init_scripts", init_scripts)
        if instance_pool_id is not None:
            pulumi.set(__self__, "instance_pool_id", instance_pool_id)
        if is_pinned is not None:
            pulumi.set(__self__, "is_pinned", is_pinned)
        if is_single_node is not None:
            pulumi.set(__self__, "is_single_node", is_single_node)
        if kind is not None:
            pulumi.set(__self__, "kind", kind)
        if libraries is not None:
            pulumi.set(__self__, "libraries", libraries)
        if no_wait is not None:
            pulumi.set(__self__, "no_wait", no_wait)
        if node_type_id is not None:
            pulumi.set(__self__, "node_type_id", node_type_id)
        if num_workers is not None:
            pulumi.set(__self__, "num_workers", num_workers)
        if policy_id is not None:
            pulumi.set(__self__, "policy_id", policy_id)
        if runtime_engine is not None:
            pulumi.set(__self__, "runtime_engine", runtime_engine)
        if single_user_name is not None:
            pulumi.set(__self__, "single_user_name", single_user_name)
        if spark_conf is not None:
            pulumi.set(__self__, "spark_conf", spark_conf)
        if spark_env_vars is not None:
            pulumi.set(__self__, "spark_env_vars", spark_env_vars)
        if spark_version is not None:
            pulumi.set(__self__, "spark_version", spark_version)
        if ssh_public_keys is not None:
            pulumi.set(__self__, "ssh_public_keys", ssh_public_keys)
        if state is not None:
            pulumi.set(__self__, "state", state)
        if url is not None:
            pulumi.set(__self__, "url", url)
        if use_ml_runtime is not None:
            pulumi.set(__self__, "use_ml_runtime", use_ml_runtime)
        if workload_type is not None:
            pulumi.set(__self__, "workload_type", workload_type)

    @property
    @pulumi.getter(name="applyPolicyDefaultValues")
    def apply_policy_default_values(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Whether to use policy default values for missing cluster attributes.
        """
        return pulumi.get(self, "apply_policy_default_values")

    @apply_policy_default_values.setter
    def apply_policy_default_values(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "apply_policy_default_values", value)

    @property
    @pulumi.getter
    def autoscale(self) -> Optional[pulumi.Input['ClusterAutoscaleArgs']]:
        return pulumi.get(self, "autoscale")

    @autoscale.setter
    def autoscale(self, value: Optional[pulumi.Input['ClusterAutoscaleArgs']]):
        pulumi.set(self, "autoscale", value)

    @property
    @pulumi.getter(name="autoterminationMinutes")
    def autotermination_minutes(self) -> Optional[pulumi.Input[builtins.int]]:
        """
        Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        """
        return pulumi.get(self, "autotermination_minutes")

    @autotermination_minutes.setter
    def autotermination_minutes(self, value: Optional[pulumi.Input[builtins.int]]):
        pulumi.set(self, "autotermination_minutes", value)

    @property
    @pulumi.getter(name="awsAttributes")
    def aws_attributes(self) -> Optional[pulumi.Input['ClusterAwsAttributesArgs']]:
        return pulumi.get(self, "aws_attributes")

    @aws_attributes.setter
    def aws_attributes(self, value: Optional[pulumi.Input['ClusterAwsAttributesArgs']]):
        pulumi.set(self, "aws_attributes", value)

    @property
    @pulumi.getter(name="azureAttributes")
    def azure_attributes(self) -> Optional[pulumi.Input['ClusterAzureAttributesArgs']]:
        return pulumi.get(self, "azure_attributes")

    @azure_attributes.setter
    def azure_attributes(self, value: Optional[pulumi.Input['ClusterAzureAttributesArgs']]):
        pulumi.set(self, "azure_attributes", value)

    @property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> Optional[pulumi.Input[builtins.str]]:
        return pulumi.get(self, "cluster_id")

    @cluster_id.setter
    def cluster_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "cluster_id", value)

    @property
    @pulumi.getter(name="clusterLogConf")
    def cluster_log_conf(self) -> Optional[pulumi.Input['ClusterClusterLogConfArgs']]:
        return pulumi.get(self, "cluster_log_conf")

    @cluster_log_conf.setter
    def cluster_log_conf(self, value: Optional[pulumi.Input['ClusterClusterLogConfArgs']]):
        pulumi.set(self, "cluster_log_conf", value)

    @property
    @pulumi.getter(name="clusterMountInfos")
    def cluster_mount_infos(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]]:
        return pulumi.get(self, "cluster_mount_infos")

    @cluster_mount_infos.setter
    def cluster_mount_infos(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterClusterMountInfoArgs']]]]):
        pulumi.set(self, "cluster_mount_infos", value)

    @property
    @pulumi.getter(name="clusterName")
    def cluster_name(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        """
        return pulumi.get(self, "cluster_name")

    @cluster_name.setter
    def cluster_name(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "cluster_name", value)

    @property
    @pulumi.getter(name="customTags")
    def custom_tags(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        should have tag `ResourceClass` set to value `Serverless`

        For example:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
            cluster_name="Shared High-Concurrency",
            spark_version=latest_lts["id"],
            node_type_id=smallest["id"],
            autotermination_minutes=20,
            spark_conf={
                "spark.databricks.repl.allowedLanguages": "python,sql",
                "spark.databricks.cluster.profile": "serverless",
            },
            custom_tags={
                "ResourceClass": "Serverless",
            })
        ```
        """
        return pulumi.get(self, "custom_tags")

    @custom_tags.setter
    def custom_tags(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "custom_tags", value)

    @property
    @pulumi.getter(name="dataSecurityMode")
    def data_security_mode(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
        * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
        * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
        * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        """
        return pulumi.get(self, "data_security_mode")

    @data_security_mode.setter
    def data_security_mode(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "data_security_mode", value)

    @property
    @pulumi.getter(name="defaultTags")
    def default_tags(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        (map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
        """
        return pulumi.get(self, "default_tags")

    @default_tags.setter
    def default_tags(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "default_tags", value)

    @property
    @pulumi.getter(name="dockerImage")
    def docker_image(self) -> Optional[pulumi.Input['ClusterDockerImageArgs']]:
        return pulumi.get(self, "docker_image")

    @docker_image.setter
    def docker_image(self, value: Optional[pulumi.Input['ClusterDockerImageArgs']]):
        pulumi.set(self, "docker_image", value)

    @property
    @pulumi.getter(name="driverInstancePoolId")
    def driver_instance_pool_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        """
        return pulumi.get(self, "driver_instance_pool_id")

    @driver_instance_pool_id.setter
    def driver_instance_pool_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "driver_instance_pool_id", value)

    @property
    @pulumi.getter(name="driverNodeTypeId")
    def driver_node_type_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        """
        return pulumi.get(self, "driver_node_type_id")

    @driver_node_type_id.setter
    def driver_node_type_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "driver_node_type_id", value)

    @property
    @pulumi.getter(name="enableElasticDisk")
    def enable_elastic_disk(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        """
        return pulumi.get(self, "enable_elastic_disk")

    @enable_elastic_disk.setter
    def enable_elastic_disk(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "enable_elastic_disk", value)

    @property
    @pulumi.getter(name="enableLocalDiskEncryption")
    def enable_local_disk_encryption(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        """
        return pulumi.get(self, "enable_local_disk_encryption")

    @enable_local_disk_encryption.setter
    def enable_local_disk_encryption(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "enable_local_disk_encryption", value)

    @property
    @pulumi.getter(name="gcpAttributes")
    def gcp_attributes(self) -> Optional[pulumi.Input['ClusterGcpAttributesArgs']]:
        return pulumi.get(self, "gcp_attributes")

    @gcp_attributes.setter
    def gcp_attributes(self, value: Optional[pulumi.Input['ClusterGcpAttributesArgs']]):
        pulumi.set(self, "gcp_attributes", value)

    @property
    @pulumi.getter(name="idempotencyToken")
    def idempotency_token(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        """
        return pulumi.get(self, "idempotency_token")

    @idempotency_token.setter
    def idempotency_token(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "idempotency_token", value)

    @property
    @pulumi.getter(name="initScripts")
    def init_scripts(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]]:
        return pulumi.get(self, "init_scripts")

    @init_scripts.setter
    def init_scripts(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterInitScriptArgs']]]]):
        pulumi.set(self, "init_scripts", value)

    @property
    @pulumi.getter(name="instancePoolId")
    def instance_pool_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        """
        return pulumi.get(self, "instance_pool_id")

    @instance_pool_id.setter
    def instance_pool_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "instance_pool_id", value)

    @property
    @pulumi.getter(name="isPinned")
    def is_pinned(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        """
        return pulumi.get(self, "is_pinned")

    @is_pinned.setter
    def is_pinned(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "is_pinned", value)

    @property
    @pulumi.getter(name="isSingleNode")
    def is_single_node(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        """
        return pulumi.get(self, "is_single_node")

    @is_single_node.setter
    def is_single_node(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "is_single_node", value)

    @property
    @pulumi.getter
    def kind(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        """
        return pulumi.get(self, "kind")

    @kind.setter
    def kind(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "kind", value)

    @property
    @pulumi.getter
    def libraries(self) -> Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]]:
        return pulumi.get(self, "libraries")

    @libraries.setter
    def libraries(self, value: Optional[pulumi.Input[Sequence[pulumi.Input['ClusterLibraryArgs']]]]):
        pulumi.set(self, "libraries", value)

    @property
    @pulumi.getter(name="noWait")
    def no_wait(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).

        The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        smallest = databricks.get_node_type(local_disk=True)
        latest_lts = databricks.get_spark_version(long_term_support=True)
        shared_autoscaling = databricks.Cluster("shared_autoscaling",
            cluster_name="Shared Autoscaling",
            spark_version=latest_lts.id,
            node_type_id=smallest.id,
            autotermination_minutes=20,
            autoscale={
                "min_workers": 1,
                "max_workers": 50,
            },
            spark_conf={
                "spark.databricks.io.cache.enabled": "true",
                "spark.databricks.io.cache.maxDiskUsage": "50g",
                "spark.databricks.io.cache.maxMetaDataCache": "1g",
            })
        ```
        """
        return pulumi.get(self, "no_wait")

    @no_wait.setter
    def no_wait(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "no_wait", value)

    @property
    @pulumi.getter(name="nodeTypeId")
    def node_type_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        """
        return pulumi.get(self, "node_type_id")

    @node_type_id.setter
    def node_type_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "node_type_id", value)

    @property
    @pulumi.getter(name="numWorkers")
    def num_workers(self) -> Optional[pulumi.Input[builtins.int]]:
        """
        Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        """
        return pulumi.get(self, "num_workers")

    @num_workers.setter
    def num_workers(self, value: Optional[pulumi.Input[builtins.int]]):
        pulumi.set(self, "num_workers", value)

    @property
    @pulumi.getter(name="policyId")
    def policy_id(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        """
        return pulumi.get(self, "policy_id")

    @policy_id.setter
    def policy_id(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "policy_id", value)

    @property
    @pulumi.getter(name="runtimeEngine")
    def runtime_engine(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        """
        return pulumi.get(self, "runtime_engine")

    @runtime_engine.setter
    def runtime_engine(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "runtime_engine", value)

    @property
    @pulumi.getter(name="singleUserName")
    def single_user_name(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        """
        return pulumi.get(self, "single_user_name")

    @single_user_name.setter
    def single_user_name(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "single_user_name", value)

    @property
    @pulumi.getter(name="sparkConf")
    def spark_conf(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        should have following items:
        * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
        * `spark.databricks.cluster.profile` set to `serverless`
        """
        return pulumi.get(self, "spark_conf")

    @spark_conf.setter
    def spark_conf(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "spark_conf", value)

    @property
    @pulumi.getter(name="sparkEnvVars")
    def spark_env_vars(self) -> Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]:
        """
        Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        """
        return pulumi.get(self, "spark_env_vars")

    @spark_env_vars.setter
    def spark_env_vars(self, value: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "spark_env_vars", value)

    @property
    @pulumi.getter(name="sparkVersion")
    def spark_version(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        """
        return pulumi.get(self, "spark_version")

    @spark_version.setter
    def spark_version(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "spark_version", value)

    @property
    @pulumi.getter(name="sshPublicKeys")
    def ssh_public_keys(self) -> Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]]:
        """
        SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        """
        return pulumi.get(self, "ssh_public_keys")

    @ssh_public_keys.setter
    def ssh_public_keys(self, value: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]]):
        pulumi.set(self, "ssh_public_keys", value)

    @property
    @pulumi.getter
    def state(self) -> Optional[pulumi.Input[builtins.str]]:
        """
        (string) State of the cluster.
        """
        return pulumi.get(self, "state")

    @state.setter
    def state(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "state", value)

    @property
    @pulumi.getter
    def url(self) -> Optional[pulumi.Input[builtins.str]]:
        return pulumi.get(self, "url")

    @url.setter
    def url(self, value: Optional[pulumi.Input[builtins.str]]):
        pulumi.set(self, "url", value)

    @property
    @pulumi.getter(name="useMlRuntime")
    def use_ml_runtime(self) -> Optional[pulumi.Input[builtins.bool]]:
        """
        Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        return pulumi.get(self, "use_ml_runtime")

    @use_ml_runtime.setter
    def use_ml_runtime(self, value: Optional[pulumi.Input[builtins.bool]]):
        pulumi.set(self, "use_ml_runtime", value)

    @property
    @pulumi.getter(name="workloadType")
    def workload_type(self) -> Optional[pulumi.Input['ClusterWorkloadTypeArgs']]:
        return pulumi.get(self, "workload_type")

    @workload_type.setter
    def workload_type(self, value: Optional[pulumi.Input['ClusterWorkloadTypeArgs']]):
        pulumi.set(self, "workload_type", value)


@pulumi.type_token("databricks:index/cluster:Cluster")
class Cluster(pulumi.CustomResource):
    @overload
    def __init__(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 apply_policy_default_values: Optional[pulumi.Input[builtins.bool]] = None,
                 autoscale: Optional[pulumi.Input[Union['ClusterAutoscaleArgs', 'ClusterAutoscaleArgsDict']]] = None,
                 autotermination_minutes: Optional[pulumi.Input[builtins.int]] = None,
                 aws_attributes: Optional[pulumi.Input[Union['ClusterAwsAttributesArgs', 'ClusterAwsAttributesArgsDict']]] = None,
                 azure_attributes: Optional[pulumi.Input[Union['ClusterAzureAttributesArgs', 'ClusterAzureAttributesArgsDict']]] = None,
                 cluster_log_conf: Optional[pulumi.Input[Union['ClusterClusterLogConfArgs', 'ClusterClusterLogConfArgsDict']]] = None,
                 cluster_mount_infos: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterClusterMountInfoArgs', 'ClusterClusterMountInfoArgsDict']]]]] = None,
                 cluster_name: Optional[pulumi.Input[builtins.str]] = None,
                 custom_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 data_security_mode: Optional[pulumi.Input[builtins.str]] = None,
                 docker_image: Optional[pulumi.Input[Union['ClusterDockerImageArgs', 'ClusterDockerImageArgsDict']]] = None,
                 driver_instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 driver_node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 enable_elastic_disk: Optional[pulumi.Input[builtins.bool]] = None,
                 enable_local_disk_encryption: Optional[pulumi.Input[builtins.bool]] = None,
                 gcp_attributes: Optional[pulumi.Input[Union['ClusterGcpAttributesArgs', 'ClusterGcpAttributesArgsDict']]] = None,
                 idempotency_token: Optional[pulumi.Input[builtins.str]] = None,
                 init_scripts: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterInitScriptArgs', 'ClusterInitScriptArgsDict']]]]] = None,
                 instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 is_pinned: Optional[pulumi.Input[builtins.bool]] = None,
                 is_single_node: Optional[pulumi.Input[builtins.bool]] = None,
                 kind: Optional[pulumi.Input[builtins.str]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterLibraryArgs', 'ClusterLibraryArgsDict']]]]] = None,
                 no_wait: Optional[pulumi.Input[builtins.bool]] = None,
                 node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 num_workers: Optional[pulumi.Input[builtins.int]] = None,
                 policy_id: Optional[pulumi.Input[builtins.str]] = None,
                 runtime_engine: Optional[pulumi.Input[builtins.str]] = None,
                 single_user_name: Optional[pulumi.Input[builtins.str]] = None,
                 spark_conf: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_env_vars: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_version: Optional[pulumi.Input[builtins.str]] = None,
                 ssh_public_keys: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]] = None,
                 use_ml_runtime: Optional[pulumi.Input[builtins.bool]] = None,
                 workload_type: Optional[pulumi.Input[Union['ClusterWorkloadTypeArgs', 'ClusterWorkloadTypeArgsDict']]] = None,
                 __props__=None):
        """
        ## Import

        The resource cluster can be imported using cluster id.

        bash

        ```sh
        $ pulumi import databricks:index/cluster:Cluster this <cluster-id>
        ```

        :param str resource_name: The name of the resource.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[builtins.bool] apply_policy_default_values: Whether to use policy default values for missing cluster attributes.
        :param pulumi.Input[builtins.int] autotermination_minutes: Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        :param pulumi.Input[builtins.str] cluster_name: Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] custom_tags: should have tag `ResourceClass` set to value `Serverless`
               
               For example:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
                   cluster_name="Shared High-Concurrency",
                   spark_version=latest_lts["id"],
                   node_type_id=smallest["id"],
                   autotermination_minutes=20,
                   spark_conf={
                       "spark.databricks.repl.allowedLanguages": "python,sql",
                       "spark.databricks.cluster.profile": "serverless",
                   },
                   custom_tags={
                       "ResourceClass": "Serverless",
                   })
               ```
        :param pulumi.Input[builtins.str] data_security_mode: Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
               * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
               * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
               * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        :param pulumi.Input[builtins.str] driver_instance_pool_id: similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        :param pulumi.Input[builtins.str] driver_node_type_id: The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        :param pulumi.Input[builtins.bool] enable_elastic_disk: If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        :param pulumi.Input[builtins.bool] enable_local_disk_encryption: Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        :param pulumi.Input[builtins.str] idempotency_token: An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        :param pulumi.Input[builtins.str] instance_pool_id: To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        :param pulumi.Input[builtins.bool] is_pinned: boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        :param pulumi.Input[builtins.bool] is_single_node: When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        :param pulumi.Input[builtins.str] kind: The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        :param pulumi.Input[builtins.bool] no_wait: If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
               
               The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               smallest = databricks.get_node_type(local_disk=True)
               latest_lts = databricks.get_spark_version(long_term_support=True)
               shared_autoscaling = databricks.Cluster("shared_autoscaling",
                   cluster_name="Shared Autoscaling",
                   spark_version=latest_lts.id,
                   node_type_id=smallest.id,
                   autotermination_minutes=20,
                   autoscale={
                       "min_workers": 1,
                       "max_workers": 50,
                   },
                   spark_conf={
                       "spark.databricks.io.cache.enabled": "true",
                       "spark.databricks.io.cache.maxDiskUsage": "50g",
                       "spark.databricks.io.cache.maxMetaDataCache": "1g",
                   })
               ```
        :param pulumi.Input[builtins.str] node_type_id: Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        :param pulumi.Input[builtins.int] num_workers: Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        :param pulumi.Input[builtins.str] policy_id: Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        :param pulumi.Input[builtins.str] runtime_engine: The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        :param pulumi.Input[builtins.str] single_user_name: The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_conf: should have following items:
               * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
               * `spark.databricks.cluster.profile` set to `serverless`
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_env_vars: Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        :param pulumi.Input[builtins.str] spark_version: [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        :param pulumi.Input[Sequence[pulumi.Input[builtins.str]]] ssh_public_keys: SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        :param pulumi.Input[builtins.bool] use_ml_runtime: Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        ...
    @overload
    def __init__(__self__,
                 resource_name: str,
                 args: ClusterArgs,
                 opts: Optional[pulumi.ResourceOptions] = None):
        """
        ## Import

        The resource cluster can be imported using cluster id.

        bash

        ```sh
        $ pulumi import databricks:index/cluster:Cluster this <cluster-id>
        ```

        :param str resource_name: The name of the resource.
        :param ClusterArgs args: The arguments to use to populate this resource's properties.
        :param pulumi.ResourceOptions opts: Options for the resource.
        """
        ...
    def __init__(__self__, resource_name: str, *args, **kwargs):
        resource_args, opts = _utilities.get_resource_args_opts(ClusterArgs, pulumi.ResourceOptions, *args, **kwargs)
        if resource_args is not None:
            __self__._internal_init(resource_name, opts, **resource_args.__dict__)
        else:
            __self__._internal_init(resource_name, *args, **kwargs)

    def _internal_init(__self__,
                 resource_name: str,
                 opts: Optional[pulumi.ResourceOptions] = None,
                 apply_policy_default_values: Optional[pulumi.Input[builtins.bool]] = None,
                 autoscale: Optional[pulumi.Input[Union['ClusterAutoscaleArgs', 'ClusterAutoscaleArgsDict']]] = None,
                 autotermination_minutes: Optional[pulumi.Input[builtins.int]] = None,
                 aws_attributes: Optional[pulumi.Input[Union['ClusterAwsAttributesArgs', 'ClusterAwsAttributesArgsDict']]] = None,
                 azure_attributes: Optional[pulumi.Input[Union['ClusterAzureAttributesArgs', 'ClusterAzureAttributesArgsDict']]] = None,
                 cluster_log_conf: Optional[pulumi.Input[Union['ClusterClusterLogConfArgs', 'ClusterClusterLogConfArgsDict']]] = None,
                 cluster_mount_infos: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterClusterMountInfoArgs', 'ClusterClusterMountInfoArgsDict']]]]] = None,
                 cluster_name: Optional[pulumi.Input[builtins.str]] = None,
                 custom_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 data_security_mode: Optional[pulumi.Input[builtins.str]] = None,
                 docker_image: Optional[pulumi.Input[Union['ClusterDockerImageArgs', 'ClusterDockerImageArgsDict']]] = None,
                 driver_instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 driver_node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 enable_elastic_disk: Optional[pulumi.Input[builtins.bool]] = None,
                 enable_local_disk_encryption: Optional[pulumi.Input[builtins.bool]] = None,
                 gcp_attributes: Optional[pulumi.Input[Union['ClusterGcpAttributesArgs', 'ClusterGcpAttributesArgsDict']]] = None,
                 idempotency_token: Optional[pulumi.Input[builtins.str]] = None,
                 init_scripts: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterInitScriptArgs', 'ClusterInitScriptArgsDict']]]]] = None,
                 instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
                 is_pinned: Optional[pulumi.Input[builtins.bool]] = None,
                 is_single_node: Optional[pulumi.Input[builtins.bool]] = None,
                 kind: Optional[pulumi.Input[builtins.str]] = None,
                 libraries: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterLibraryArgs', 'ClusterLibraryArgsDict']]]]] = None,
                 no_wait: Optional[pulumi.Input[builtins.bool]] = None,
                 node_type_id: Optional[pulumi.Input[builtins.str]] = None,
                 num_workers: Optional[pulumi.Input[builtins.int]] = None,
                 policy_id: Optional[pulumi.Input[builtins.str]] = None,
                 runtime_engine: Optional[pulumi.Input[builtins.str]] = None,
                 single_user_name: Optional[pulumi.Input[builtins.str]] = None,
                 spark_conf: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_env_vars: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
                 spark_version: Optional[pulumi.Input[builtins.str]] = None,
                 ssh_public_keys: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]] = None,
                 use_ml_runtime: Optional[pulumi.Input[builtins.bool]] = None,
                 workload_type: Optional[pulumi.Input[Union['ClusterWorkloadTypeArgs', 'ClusterWorkloadTypeArgsDict']]] = None,
                 __props__=None):
        opts = pulumi.ResourceOptions.merge(_utilities.get_resource_opts_defaults(), opts)
        if not isinstance(opts, pulumi.ResourceOptions):
            raise TypeError('Expected resource options to be a ResourceOptions instance')
        if opts.id is None:
            if __props__ is not None:
                raise TypeError('__props__ is only valid when passed in combination with a valid opts.id to get an existing resource')
            __props__ = ClusterArgs.__new__(ClusterArgs)

            __props__.__dict__["apply_policy_default_values"] = apply_policy_default_values
            __props__.__dict__["autoscale"] = autoscale
            __props__.__dict__["autotermination_minutes"] = autotermination_minutes
            __props__.__dict__["aws_attributes"] = aws_attributes
            __props__.__dict__["azure_attributes"] = azure_attributes
            __props__.__dict__["cluster_log_conf"] = cluster_log_conf
            __props__.__dict__["cluster_mount_infos"] = cluster_mount_infos
            __props__.__dict__["cluster_name"] = cluster_name
            __props__.__dict__["custom_tags"] = custom_tags
            __props__.__dict__["data_security_mode"] = data_security_mode
            __props__.__dict__["docker_image"] = docker_image
            __props__.__dict__["driver_instance_pool_id"] = driver_instance_pool_id
            __props__.__dict__["driver_node_type_id"] = driver_node_type_id
            __props__.__dict__["enable_elastic_disk"] = enable_elastic_disk
            __props__.__dict__["enable_local_disk_encryption"] = enable_local_disk_encryption
            __props__.__dict__["gcp_attributes"] = gcp_attributes
            __props__.__dict__["idempotency_token"] = idempotency_token
            __props__.__dict__["init_scripts"] = init_scripts
            __props__.__dict__["instance_pool_id"] = instance_pool_id
            __props__.__dict__["is_pinned"] = is_pinned
            __props__.__dict__["is_single_node"] = is_single_node
            __props__.__dict__["kind"] = kind
            __props__.__dict__["libraries"] = libraries
            __props__.__dict__["no_wait"] = no_wait
            __props__.__dict__["node_type_id"] = node_type_id
            __props__.__dict__["num_workers"] = num_workers
            __props__.__dict__["policy_id"] = policy_id
            __props__.__dict__["runtime_engine"] = runtime_engine
            __props__.__dict__["single_user_name"] = single_user_name
            __props__.__dict__["spark_conf"] = spark_conf
            __props__.__dict__["spark_env_vars"] = spark_env_vars
            if spark_version is None and not opts.urn:
                raise TypeError("Missing required property 'spark_version'")
            __props__.__dict__["spark_version"] = spark_version
            __props__.__dict__["ssh_public_keys"] = ssh_public_keys
            __props__.__dict__["use_ml_runtime"] = use_ml_runtime
            __props__.__dict__["workload_type"] = workload_type
            __props__.__dict__["cluster_id"] = None
            __props__.__dict__["default_tags"] = None
            __props__.__dict__["state"] = None
            __props__.__dict__["url"] = None
        super(Cluster, __self__).__init__(
            'databricks:index/cluster:Cluster',
            resource_name,
            __props__,
            opts)

    @staticmethod
    def get(resource_name: str,
            id: pulumi.Input[str],
            opts: Optional[pulumi.ResourceOptions] = None,
            apply_policy_default_values: Optional[pulumi.Input[builtins.bool]] = None,
            autoscale: Optional[pulumi.Input[Union['ClusterAutoscaleArgs', 'ClusterAutoscaleArgsDict']]] = None,
            autotermination_minutes: Optional[pulumi.Input[builtins.int]] = None,
            aws_attributes: Optional[pulumi.Input[Union['ClusterAwsAttributesArgs', 'ClusterAwsAttributesArgsDict']]] = None,
            azure_attributes: Optional[pulumi.Input[Union['ClusterAzureAttributesArgs', 'ClusterAzureAttributesArgsDict']]] = None,
            cluster_id: Optional[pulumi.Input[builtins.str]] = None,
            cluster_log_conf: Optional[pulumi.Input[Union['ClusterClusterLogConfArgs', 'ClusterClusterLogConfArgsDict']]] = None,
            cluster_mount_infos: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterClusterMountInfoArgs', 'ClusterClusterMountInfoArgsDict']]]]] = None,
            cluster_name: Optional[pulumi.Input[builtins.str]] = None,
            custom_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
            data_security_mode: Optional[pulumi.Input[builtins.str]] = None,
            default_tags: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
            docker_image: Optional[pulumi.Input[Union['ClusterDockerImageArgs', 'ClusterDockerImageArgsDict']]] = None,
            driver_instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
            driver_node_type_id: Optional[pulumi.Input[builtins.str]] = None,
            enable_elastic_disk: Optional[pulumi.Input[builtins.bool]] = None,
            enable_local_disk_encryption: Optional[pulumi.Input[builtins.bool]] = None,
            gcp_attributes: Optional[pulumi.Input[Union['ClusterGcpAttributesArgs', 'ClusterGcpAttributesArgsDict']]] = None,
            idempotency_token: Optional[pulumi.Input[builtins.str]] = None,
            init_scripts: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterInitScriptArgs', 'ClusterInitScriptArgsDict']]]]] = None,
            instance_pool_id: Optional[pulumi.Input[builtins.str]] = None,
            is_pinned: Optional[pulumi.Input[builtins.bool]] = None,
            is_single_node: Optional[pulumi.Input[builtins.bool]] = None,
            kind: Optional[pulumi.Input[builtins.str]] = None,
            libraries: Optional[pulumi.Input[Sequence[pulumi.Input[Union['ClusterLibraryArgs', 'ClusterLibraryArgsDict']]]]] = None,
            no_wait: Optional[pulumi.Input[builtins.bool]] = None,
            node_type_id: Optional[pulumi.Input[builtins.str]] = None,
            num_workers: Optional[pulumi.Input[builtins.int]] = None,
            policy_id: Optional[pulumi.Input[builtins.str]] = None,
            runtime_engine: Optional[pulumi.Input[builtins.str]] = None,
            single_user_name: Optional[pulumi.Input[builtins.str]] = None,
            spark_conf: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
            spark_env_vars: Optional[pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]]] = None,
            spark_version: Optional[pulumi.Input[builtins.str]] = None,
            ssh_public_keys: Optional[pulumi.Input[Sequence[pulumi.Input[builtins.str]]]] = None,
            state: Optional[pulumi.Input[builtins.str]] = None,
            url: Optional[pulumi.Input[builtins.str]] = None,
            use_ml_runtime: Optional[pulumi.Input[builtins.bool]] = None,
            workload_type: Optional[pulumi.Input[Union['ClusterWorkloadTypeArgs', 'ClusterWorkloadTypeArgsDict']]] = None) -> 'Cluster':
        """
        Get an existing Cluster resource's state with the given name, id, and optional extra
        properties used to qualify the lookup.

        :param str resource_name: The unique name of the resulting resource.
        :param pulumi.Input[str] id: The unique provider ID of the resource to lookup.
        :param pulumi.ResourceOptions opts: Options for the resource.
        :param pulumi.Input[builtins.bool] apply_policy_default_values: Whether to use policy default values for missing cluster attributes.
        :param pulumi.Input[builtins.int] autotermination_minutes: Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        :param pulumi.Input[builtins.str] cluster_name: Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] custom_tags: should have tag `ResourceClass` set to value `Serverless`
               
               For example:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
                   cluster_name="Shared High-Concurrency",
                   spark_version=latest_lts["id"],
                   node_type_id=smallest["id"],
                   autotermination_minutes=20,
                   spark_conf={
                       "spark.databricks.repl.allowedLanguages": "python,sql",
                       "spark.databricks.cluster.profile": "serverless",
                   },
                   custom_tags={
                       "ResourceClass": "Serverless",
                   })
               ```
        :param pulumi.Input[builtins.str] data_security_mode: Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
               * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
               * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
               * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] default_tags: (map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
        :param pulumi.Input[builtins.str] driver_instance_pool_id: similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        :param pulumi.Input[builtins.str] driver_node_type_id: The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        :param pulumi.Input[builtins.bool] enable_elastic_disk: If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        :param pulumi.Input[builtins.bool] enable_local_disk_encryption: Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        :param pulumi.Input[builtins.str] idempotency_token: An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        :param pulumi.Input[builtins.str] instance_pool_id: To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        :param pulumi.Input[builtins.bool] is_pinned: boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        :param pulumi.Input[builtins.bool] is_single_node: When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        :param pulumi.Input[builtins.str] kind: The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        :param pulumi.Input[builtins.bool] no_wait: If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
               
               The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
               
               ```python
               import pulumi
               import pulumi_databricks as databricks
               
               smallest = databricks.get_node_type(local_disk=True)
               latest_lts = databricks.get_spark_version(long_term_support=True)
               shared_autoscaling = databricks.Cluster("shared_autoscaling",
                   cluster_name="Shared Autoscaling",
                   spark_version=latest_lts.id,
                   node_type_id=smallest.id,
                   autotermination_minutes=20,
                   autoscale={
                       "min_workers": 1,
                       "max_workers": 50,
                   },
                   spark_conf={
                       "spark.databricks.io.cache.enabled": "true",
                       "spark.databricks.io.cache.maxDiskUsage": "50g",
                       "spark.databricks.io.cache.maxMetaDataCache": "1g",
                   })
               ```
        :param pulumi.Input[builtins.str] node_type_id: Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        :param pulumi.Input[builtins.int] num_workers: Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        :param pulumi.Input[builtins.str] policy_id: Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        :param pulumi.Input[builtins.str] runtime_engine: The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        :param pulumi.Input[builtins.str] single_user_name: The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_conf: should have following items:
               * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
               * `spark.databricks.cluster.profile` set to `serverless`
        :param pulumi.Input[Mapping[str, pulumi.Input[builtins.str]]] spark_env_vars: Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        :param pulumi.Input[builtins.str] spark_version: [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        :param pulumi.Input[Sequence[pulumi.Input[builtins.str]]] ssh_public_keys: SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        :param pulumi.Input[builtins.str] state: (string) State of the cluster.
        :param pulumi.Input[builtins.bool] use_ml_runtime: Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        opts = pulumi.ResourceOptions.merge(opts, pulumi.ResourceOptions(id=id))

        __props__ = _ClusterState.__new__(_ClusterState)

        __props__.__dict__["apply_policy_default_values"] = apply_policy_default_values
        __props__.__dict__["autoscale"] = autoscale
        __props__.__dict__["autotermination_minutes"] = autotermination_minutes
        __props__.__dict__["aws_attributes"] = aws_attributes
        __props__.__dict__["azure_attributes"] = azure_attributes
        __props__.__dict__["cluster_id"] = cluster_id
        __props__.__dict__["cluster_log_conf"] = cluster_log_conf
        __props__.__dict__["cluster_mount_infos"] = cluster_mount_infos
        __props__.__dict__["cluster_name"] = cluster_name
        __props__.__dict__["custom_tags"] = custom_tags
        __props__.__dict__["data_security_mode"] = data_security_mode
        __props__.__dict__["default_tags"] = default_tags
        __props__.__dict__["docker_image"] = docker_image
        __props__.__dict__["driver_instance_pool_id"] = driver_instance_pool_id
        __props__.__dict__["driver_node_type_id"] = driver_node_type_id
        __props__.__dict__["enable_elastic_disk"] = enable_elastic_disk
        __props__.__dict__["enable_local_disk_encryption"] = enable_local_disk_encryption
        __props__.__dict__["gcp_attributes"] = gcp_attributes
        __props__.__dict__["idempotency_token"] = idempotency_token
        __props__.__dict__["init_scripts"] = init_scripts
        __props__.__dict__["instance_pool_id"] = instance_pool_id
        __props__.__dict__["is_pinned"] = is_pinned
        __props__.__dict__["is_single_node"] = is_single_node
        __props__.__dict__["kind"] = kind
        __props__.__dict__["libraries"] = libraries
        __props__.__dict__["no_wait"] = no_wait
        __props__.__dict__["node_type_id"] = node_type_id
        __props__.__dict__["num_workers"] = num_workers
        __props__.__dict__["policy_id"] = policy_id
        __props__.__dict__["runtime_engine"] = runtime_engine
        __props__.__dict__["single_user_name"] = single_user_name
        __props__.__dict__["spark_conf"] = spark_conf
        __props__.__dict__["spark_env_vars"] = spark_env_vars
        __props__.__dict__["spark_version"] = spark_version
        __props__.__dict__["ssh_public_keys"] = ssh_public_keys
        __props__.__dict__["state"] = state
        __props__.__dict__["url"] = url
        __props__.__dict__["use_ml_runtime"] = use_ml_runtime
        __props__.__dict__["workload_type"] = workload_type
        return Cluster(resource_name, opts=opts, __props__=__props__)

    @property
    @pulumi.getter(name="applyPolicyDefaultValues")
    def apply_policy_default_values(self) -> pulumi.Output[Optional[builtins.bool]]:
        """
        Whether to use policy default values for missing cluster attributes.
        """
        return pulumi.get(self, "apply_policy_default_values")

    @property
    @pulumi.getter
    def autoscale(self) -> pulumi.Output[Optional['outputs.ClusterAutoscale']]:
        return pulumi.get(self, "autoscale")

    @property
    @pulumi.getter(name="autoterminationMinutes")
    def autotermination_minutes(self) -> pulumi.Output[Optional[builtins.int]]:
        """
        Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
        """
        return pulumi.get(self, "autotermination_minutes")

    @property
    @pulumi.getter(name="awsAttributes")
    def aws_attributes(self) -> pulumi.Output[Optional['outputs.ClusterAwsAttributes']]:
        return pulumi.get(self, "aws_attributes")

    @property
    @pulumi.getter(name="azureAttributes")
    def azure_attributes(self) -> pulumi.Output[Optional['outputs.ClusterAzureAttributes']]:
        return pulumi.get(self, "azure_attributes")

    @property
    @pulumi.getter(name="clusterId")
    def cluster_id(self) -> pulumi.Output[builtins.str]:
        return pulumi.get(self, "cluster_id")

    @property
    @pulumi.getter(name="clusterLogConf")
    def cluster_log_conf(self) -> pulumi.Output[Optional['outputs.ClusterClusterLogConf']]:
        return pulumi.get(self, "cluster_log_conf")

    @property
    @pulumi.getter(name="clusterMountInfos")
    def cluster_mount_infos(self) -> pulumi.Output[Optional[Sequence['outputs.ClusterClusterMountInfo']]]:
        return pulumi.get(self, "cluster_mount_infos")

    @property
    @pulumi.getter(name="clusterName")
    def cluster_name(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        Cluster name, which doesn't have to be unique. If not specified at creation, the cluster name will be an empty string.
        """
        return pulumi.get(self, "cluster_name")

    @property
    @pulumi.getter(name="customTags")
    def custom_tags(self) -> pulumi.Output[Optional[Mapping[str, builtins.str]]]:
        """
        should have tag `ResourceClass` set to value `Serverless`

        For example:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        cluster_with_table_access_control = databricks.Cluster("cluster_with_table_access_control",
            cluster_name="Shared High-Concurrency",
            spark_version=latest_lts["id"],
            node_type_id=smallest["id"],
            autotermination_minutes=20,
            spark_conf={
                "spark.databricks.repl.allowedLanguages": "python,sql",
                "spark.databricks.cluster.profile": "serverless",
            },
            custom_tags={
                "ResourceClass": "Serverless",
            })
        ```
        """
        return pulumi.get(self, "custom_tags")

    @property
    @pulumi.getter(name="dataSecurityMode")
    def data_security_mode(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
        * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
        * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
        * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
        """
        return pulumi.get(self, "data_security_mode")

    @property
    @pulumi.getter(name="defaultTags")
    def default_tags(self) -> pulumi.Output[Mapping[str, builtins.str]]:
        """
        (map) Tags that are added by Databricks by default, regardless of any `custom_tags` that may have been added. These include: Vendor: Databricks, Creator: <username_of_creator>, ClusterName: <name_of_cluster>, ClusterId: <id_of_cluster>, Name: <Databricks internal use>, and any workspace and pool tags.
        """
        return pulumi.get(self, "default_tags")

    @property
    @pulumi.getter(name="dockerImage")
    def docker_image(self) -> pulumi.Output[Optional['outputs.ClusterDockerImage']]:
        return pulumi.get(self, "docker_image")

    @property
    @pulumi.getter(name="driverInstancePoolId")
    def driver_instance_pool_id(self) -> pulumi.Output[builtins.str]:
        """
        similar to `instance_pool_id`, but for driver node. If omitted, and `instance_pool_id` is specified, then the driver will be allocated from that pool.
        """
        return pulumi.get(self, "driver_instance_pool_id")

    @property
    @pulumi.getter(name="driverNodeTypeId")
    def driver_node_type_id(self) -> pulumi.Output[builtins.str]:
        """
        The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `node_type_id` defined above.
        """
        return pulumi.get(self, "driver_node_type_id")

    @property
    @pulumi.getter(name="enableElasticDisk")
    def enable_elastic_disk(self) -> pulumi.Output[builtins.bool]:
        """
        If you don't want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster's Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance's local storage). To scale down EBS usage, make sure you have `autotermination_minutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
        """
        return pulumi.get(self, "enable_elastic_disk")

    @property
    @pulumi.getter(name="enableLocalDiskEncryption")
    def enable_local_disk_encryption(self) -> pulumi.Output[builtins.bool]:
        """
        Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster's local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
        """
        return pulumi.get(self, "enable_local_disk_encryption")

    @property
    @pulumi.getter(name="gcpAttributes")
    def gcp_attributes(self) -> pulumi.Output[Optional['outputs.ClusterGcpAttributes']]:
        return pulumi.get(self, "gcp_attributes")

    @property
    @pulumi.getter(name="idempotencyToken")
    def idempotency_token(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster's ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
        """
        return pulumi.get(self, "idempotency_token")

    @property
    @pulumi.getter(name="initScripts")
    def init_scripts(self) -> pulumi.Output[Optional[Sequence['outputs.ClusterInitScript']]]:
        return pulumi.get(self, "init_scripts")

    @property
    @pulumi.getter(name="instancePoolId")
    def instance_pool_id(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster's request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
        """
        return pulumi.get(self, "instance_pool_id")

    @property
    @pulumi.getter(name="isPinned")
    def is_pinned(self) -> pulumi.Output[Optional[builtins.bool]]:
        """
        boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters' maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
        """
        return pulumi.get(self, "is_pinned")

    @property
    @pulumi.getter(name="isSingleNode")
    def is_single_node(self) -> pulumi.Output[Optional[builtins.bool]]:
        """
        When set to true, Databricks will automatically set single node related `custom_tags`, `spark_conf`, and `num_workers`.
        """
        return pulumi.get(self, "is_single_node")

    @property
    @pulumi.getter
    def kind(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
        """
        return pulumi.get(self, "kind")

    @property
    @pulumi.getter
    def libraries(self) -> pulumi.Output[Optional[Sequence['outputs.ClusterLibrary']]]:
        return pulumi.get(self, "libraries")

    @property
    @pulumi.getter(name="noWait")
    def no_wait(self) -> pulumi.Output[Optional[builtins.bool]]:
        """
        If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).

        The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:

        ```python
        import pulumi
        import pulumi_databricks as databricks

        smallest = databricks.get_node_type(local_disk=True)
        latest_lts = databricks.get_spark_version(long_term_support=True)
        shared_autoscaling = databricks.Cluster("shared_autoscaling",
            cluster_name="Shared Autoscaling",
            spark_version=latest_lts.id,
            node_type_id=smallest.id,
            autotermination_minutes=20,
            autoscale={
                "min_workers": 1,
                "max_workers": 50,
            },
            spark_conf={
                "spark.databricks.io.cache.enabled": "true",
                "spark.databricks.io.cache.maxDiskUsage": "50g",
                "spark.databricks.io.cache.maxMetaDataCache": "1g",
            })
        ```
        """
        return pulumi.get(self, "no_wait")

    @property
    @pulumi.getter(name="nodeTypeId")
    def node_type_id(self) -> pulumi.Output[builtins.str]:
        """
        Any supported get_node_type id. If `instance_pool_id` is specified, this field is not needed.
        """
        return pulumi.get(self, "node_type_id")

    @property
    @pulumi.getter(name="numWorkers")
    def num_workers(self) -> pulumi.Output[Optional[builtins.int]]:
        """
        Number of worker nodes that this cluster should have. A cluster has one Spark driver and `num_workers` executors for a total of `num_workers` + 1 Spark nodes.
        """
        return pulumi.get(self, "num_workers")

    @property
    @pulumi.getter(name="policyId")
    def policy_id(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policy_id` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `spark_conf`.  If relevant fields aren't filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
        """
        return pulumi.get(self, "policy_id")

    @property
    @pulumi.getter(name="runtimeEngine")
    def runtime_engine(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the spark_version value. Allowed values include: `PHOTON`, `STANDARD`.
        """
        return pulumi.get(self, "runtime_engine")

    @property
    @pulumi.getter(name="singleUserName")
    def single_user_name(self) -> pulumi.Output[Optional[builtins.str]]:
        """
        The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `data_security_mode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
        """
        return pulumi.get(self, "single_user_name")

    @property
    @pulumi.getter(name="sparkConf")
    def spark_conf(self) -> pulumi.Output[Optional[Mapping[str, builtins.str]]]:
        """
        should have following items:
        * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
        * `spark.databricks.cluster.profile` set to `serverless`
        """
        return pulumi.get(self, "spark_conf")

    @property
    @pulumi.getter(name="sparkEnvVars")
    def spark_env_vars(self) -> pulumi.Output[Optional[Mapping[str, builtins.str]]]:
        """
        Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
        """
        return pulumi.get(self, "spark_env_vars")

    @property
    @pulumi.getter(name="sparkVersion")
    def spark_version(self) -> pulumi.Output[builtins.str]:
        """
        [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported get_spark_version id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
        """
        return pulumi.get(self, "spark_version")

    @property
    @pulumi.getter(name="sshPublicKeys")
    def ssh_public_keys(self) -> pulumi.Output[Optional[Sequence[builtins.str]]]:
        """
        SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
        """
        return pulumi.get(self, "ssh_public_keys")

    @property
    @pulumi.getter
    def state(self) -> pulumi.Output[builtins.str]:
        """
        (string) State of the cluster.
        """
        return pulumi.get(self, "state")

    @property
    @pulumi.getter
    def url(self) -> pulumi.Output[builtins.str]:
        return pulumi.get(self, "url")

    @property
    @pulumi.getter(name="useMlRuntime")
    def use_ml_runtime(self) -> pulumi.Output[Optional[builtins.bool]]:
        """
        Whenever ML runtime should be selected or not.  Actual runtime is determined by `spark_version` (DBR release), this field `use_ml_runtime`, and whether `node_type_id` is GPU node or not.
        """
        return pulumi.get(self, "use_ml_runtime")

    @property
    @pulumi.getter(name="workloadType")
    def workload_type(self) -> pulumi.Output[Optional['outputs.ClusterWorkloadType']]:
        return pulumi.get(self, "workload_type")

