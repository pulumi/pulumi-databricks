// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
import * as utilities from "./utilities";

/**
 * > Please switch to databricks_volume. DBFS mounts are deprecated.
 *
 * > This resource can only be used with a workspace-level provider!
 *
 * > When `clusterId` is not specified, it will create the smallest possible cluster in the default availability zone with name equal to or starting with `terraform-mount` for the shortest possible amount of time. To avoid mount failure due to potentially quota or capacity issues with the default cluster, we recommend specifying a cluster to use for mounting.
 *
 * > CRUD operations on a databricks mount require a running cluster. Due to limitations of terraform and the databricks mounts APIs, if the cluster the mount was most recently created / updated using no longer exists AND the mount is destroyed as a part of a pulumi up, we mark it as deleted without cleaning it up from the workspace.
 *
 * This resource will [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`. Right now it supports mounting AWS S3, Azure (Blob Storage, ADLS Gen1 & Gen2), Google Cloud Storage.  It is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and may take some time to validate the mount.
 *
 * This resource provides two ways of mounting a storage account:
 *
 * 1. Use a storage-specific configuration block - this could be used for the most cases, as it will fill most of the necessary details. Currently we support following configuration blocks:
 *
 * * `s3` - to [mount AWS S3](https://docs.databricks.com/data/data-sources/aws/amazon-s3.html)
 * * `gs` - to [mount Google Cloud Storage](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html)
 * * `abfs` - to [mount ADLS Gen2](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/) using Azure Blob Filesystem (ABFS) driver
 * * `adl` - to [mount ADLS Gen1](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake) using Azure Data Lake (ADL) driver
 * * `wasb`  - to [mount Azure Blob Storage](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage) using Windows Azure Storage Blob (WASB) driver
 *
 * 1. Use generic arguments - you have a responsibility for providing all necessary parameters that are required to mount specific storage. This is most flexible option
 *
 * ## Common arguments
 *
 * * `clusterId` - (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it's going to be started, so be aware to set auto-termination rules on it.
 * * `name` - (Optional, String) Name, under which mount will be accessible in `dbfs:/mnt/<MOUNT_NAME>`. If not specified, provider will try to infer it from depending on the resource type:
 *   * `bucketName` for AWS S3 and Google Cloud Storage
 *   * `containerName` for ADLS Gen2 and Azure Blob Storage
 *   * `storageResourceName` for ADLS Gen1
 * * `uri` - (Optional, String) the URI for accessing specific storage (`s3a://....`, `abfss://....`, `gs://....`, etc.)
 * * `extraConfigs` - (Optional, String map) configuration parameters that are necessary for mounting of specific storage
 * * `resourceId` - (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account & container names on Azure.
 * * `encryptionType` - (Optional, String) encryption type. Currently used only for [AWS S3 mounts](https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#encrypt-data-in-s3-buckets)
 *
 * ### Example mounting ADLS Gen2 using uri and extraConfigs
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const tenantId = "00000000-1111-2222-3333-444444444444";
 * const clientId = "55555555-6666-7777-8888-999999999999";
 * const secretScope = "some-kv";
 * const secretKey = "some-sp-secret";
 * const container = "test";
 * const storageAcc = "lrs";
 * const _this = new databricks.Mount("this", {
 *     name: "tf-abfss",
 *     uri: `abfss://${container}@${storageAcc}.dfs.core.windows.net`,
 *     extraConfigs: {
 *         "fs.azure.account.auth.type": "OAuth",
 *         "fs.azure.account.oauth.provider.type": "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider",
 *         "fs.azure.account.oauth2.client.id": clientId,
 *         "fs.azure.account.oauth2.client.secret": `{{secrets/${secretScope}/${secretKey}}}`,
 *         "fs.azure.account.oauth2.client.endpoint": `https://login.microsoftonline.com/${tenantId}/oauth2/token`,
 *         "fs.azure.createRemoteFileSystemDuringInitialization": "false",
 *     },
 * });
 * ```
 *
 * ### Example mounting ADLS Gen2 with AAD passthrough
 *
 * > AAD passthrough is considered a legacy data access pattern. Use Unity Catalog for fine-grained data access control.
 *
 * > Mounts using AAD passthrough cannot be created using a service principal.
 *
 * To mount ALDS Gen2 with Azure Active Directory Credentials passthrough we need to execute the mount commands using the cluster configured with AAD Credentials passthrough & provide necessary configuration parameters (see [documentation](https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough#--mount-azure-data-lake-storage-to-dbfs-using-credential-passthrough) for more details).
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as azurerm from "@pulumi/azurerm";
 * import * as databricks from "@pulumi/databricks";
 *
 * const config = new pulumi.Config();
 * // Resource group for Databricks Workspace
 * const resourceGroup = config.require("resourceGroup");
 * // Name of the Databricks Workspace
 * const workspaceName = config.require("workspaceName");
 * const _this = azurerm.index.DatabricksWorkspace({
 *     name: workspaceName,
 *     resourceGroupName: resourceGroup,
 * });
 * const smallest = databricks.getNodeType({
 *     localDisk: true,
 * });
 * const latest = databricks.getSparkVersion({});
 * const sharedPassthrough = new databricks.Cluster("shared_passthrough", {
 *     clusterName: "Shared Passthrough for mount",
 *     sparkVersion: latest.then(latest => latest.id),
 *     nodeTypeId: smallest.then(smallest => smallest.id),
 *     autoterminationMinutes: 10,
 *     numWorkers: 1,
 *     sparkConf: {
 *         "spark.databricks.cluster.profile": "serverless",
 *         "spark.databricks.repl.allowedLanguages": "python,sql",
 *         "spark.databricks.passthrough.enabled": "true",
 *         "spark.databricks.pyspark.enableProcessIsolation": "true",
 *     },
 *     customTags: {
 *         ResourceClass: "Serverless",
 *     },
 * });
 * // Name of the ADLS Gen2 storage container
 * const storageAcc = config.require("storageAcc");
 * // Name of container inside storage account
 * const container = config.require("container");
 * const passthrough = new databricks.Mount("passthrough", {
 *     name: "passthrough-test",
 *     clusterId: sharedPassthrough.id,
 *     uri: `abfss://${container}@${storageAcc}.dfs.core.windows.net`,
 *     extraConfigs: {
 *         "fs.azure.account.auth.type": "CustomAccessToken",
 *         "fs.azure.account.custom.token.provider.class": "{{sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}",
 *     },
 * });
 * ```
 *
 * ## s3 block
 *
 * This block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `s3` block:
 *
 * * `instanceProfile` - (Optional) (String) ARN of registered instance profile for data access.  If it's not specified, then the `clusterId` should be provided, and the cluster should have an instance profile attached to it. If both `clusterId` & `instanceProfile` are specified, then `clusterId` takes precedence.
 * * `bucketName` - (Required) (String) S3 bucket name to be mounted.
 *
 * ### Example of mounting S3
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * // now you can do `%fs ls /mnt/experiments` in notebooks
 * const _this = new databricks.Mount("this", {
 *     name: "experiments",
 *     s3: {
 *         instanceProfile: ds.id,
 *         bucketName: thisAwsS3Bucket.bucket,
 *     },
 * });
 * ```
 *
 * ## abfs block
 *
 * This block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `abfs` block:
 *
 * * `clientId` - (Required) (String) This is the clientId (Application Object ID) for the enterprise application for the service principal.
 * * `tenantId` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenantId` from it).
 * * `clientSecretKey` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.
 * * `clientSecretScope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.
 * * `containerName` - (Required) (String) ADLS gen2 container name. (Could be omitted if `resourceId` is provided)
 * * `storageAccountName` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resourceId` is provided)
 * * `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a "/".
 * * `initializeFileSystem` - (Required) (Bool) either or not initialize FS for the first use
 *
 * ### Creating mount for ADLS Gen2 using abfs block
 *
 * In this example, we're using Azure authentication, so we can omit some parameters (`tenantId`, `storageAccountName`, and `containerName`) that will be detected automatically.
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as azurerm from "@pulumi/azurerm";
 * import * as databricks from "@pulumi/databricks";
 *
 * const terraform = new databricks.SecretScope("terraform", {
 *     name: "application",
 *     initialManagePrincipal: "users",
 * });
 * const servicePrincipalKey = new databricks.Secret("service_principal_key", {
 *     key: "service_principal_key",
 *     stringValue: ARM_CLIENT_SECRET,
 *     scope: terraform.name,
 * });
 * const _this = new azurerm.index.StorageAccount("this", {
 *     name: `${prefix}datalake`,
 *     resourceGroupName: resourceGroupName,
 *     location: resourceGroupLocation,
 *     accountTier: "Standard",
 *     accountReplicationType: "GRS",
 *     accountKind: "StorageV2",
 *     isHnsEnabled: true,
 * });
 * const thisRoleAssignment = new azurerm.index.RoleAssignment("this", {
 *     scope: _this.id,
 *     roleDefinitionName: "Storage Blob Data Contributor",
 *     principalId: current.objectId,
 * });
 * const thisStorageContainer = new azurerm.index.StorageContainer("this", {
 *     name: "marketing",
 *     storageAccountName: _this.name,
 *     containerAccessType: "private",
 * });
 * const marketing = new databricks.Mount("marketing", {
 *     name: "marketing",
 *     resourceId: thisStorageContainer.resourceManagerId,
 *     abfs: {
 *         clientId: current.clientId,
 *         clientSecretScope: terraform.name,
 *         clientSecretKey: servicePrincipalKey.key,
 *         initializeFileSystem: true,
 *     },
 * });
 * ```
 *
 * ## gs block
 *
 * This block allows specifying parameters for mounting of the Google Cloud Storage. The following arguments are required inside the `gs` block:
 *
 * * `serviceAccount` - (Optional) (String) email of registered [Google Service Account](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html#step-1-set-up-google-cloud-service-account-using-google-cloud-console) for data access.  If it's not specified, then the `clusterId` should be provided, and the cluster should have a Google service account attached to it.
 * * `bucketName` - (Required) (String) GCS bucket name to be mounted.
 *
 * ### Example mounting Google Cloud Storage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const thisGs = new databricks.Mount("this_gs", {
 *     name: "gs-mount",
 *     gs: {
 *         serviceAccount: "acc@company.iam.gserviceaccount.com",
 *         bucketName: "mybucket",
 *     },
 * });
 * ```
 *
 * ## adl block
 *
 * This block allows specifying parameters for mounting of the ADLS Gen1. The following arguments are required inside the `adl` block:
 *
 * * `clientId` - (Required) (String) This is the clientId for the enterprise application for the service principal.
 * * `tenantId` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenantId` from it)
 * * `clientSecretKey` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.
 * * `clientSecretScope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.
 *
 * * `storageResourceName` - (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if `resourceId` is provided)
 * * `sparkConfPrefix` - (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are `fs.adl`, `dfs.adls`. Use `fs.adl` for runtime 6.0 and above for the clusters. Otherwise use `dfs.adls`. The default value is: `fs.adl`.
 * * `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a "/".
 *
 * ### Example mounting ADLS Gen1
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const mount = new databricks.Mount("mount", {
 *     name: "{var.RANDOM}",
 *     adl: {
 *         storageResourceName: "{env.TEST_STORAGE_ACCOUNT_NAME}",
 *         tenantId: current.tenantId,
 *         clientId: current.clientId,
 *         clientSecretScope: terraform.name,
 *         clientSecretKey: servicePrincipalKey.key,
 *         sparkConfPrefix: "fs.adl",
 *     },
 * });
 * ```
 *
 * ## wasb block
 *
 * This block allows specifying parameters for mounting of the Azure Blob Storage. The following arguments are required inside the `wasb` block:
 *
 * * `authType` - (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (`SAS`) or account access keys (`ACCESS_KEY`).
 * * `tokenSecretScope` - (Required) (String) This is the secret scope in which your auth type token is stored.
 * * `tokenSecretKey` - (Required) (String) This is the secret key in which your auth type token is stored.
 * * `containerName` - (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if `resourceId` is provided)
 * * `storageAccountName` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resourceId` is provided)
 * * `directory` - (Computed) (String) This is optional if you don't want to add an additional directory that you wish to mount. This must start with a "/".
 *
 * ### Example mounting Azure Blob Storage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as azurerm from "@pulumi/azurerm";
 * import * as databricks from "@pulumi/databricks";
 *
 * const blobaccount = new azurerm.index.StorageAccount("blobaccount", {
 *     name: `${prefix}blob`,
 *     resourceGroupName: resourceGroupName,
 *     location: resourceGroupLocation,
 *     accountTier: "Standard",
 *     accountReplicationType: "LRS",
 *     accountKind: "StorageV2",
 * });
 * const marketing = new azurerm.index.StorageContainer("marketing", {
 *     name: "marketing",
 *     storageAccountName: blobaccount.name,
 *     containerAccessType: "private",
 * });
 * const terraform = new databricks.SecretScope("terraform", {
 *     name: "application",
 *     initialManagePrincipal: "users",
 * });
 * const storageKey = new databricks.Secret("storage_key", {
 *     key: "blob_storage_key",
 *     stringValue: blobaccount.primaryAccessKey,
 *     scope: terraform.name,
 * });
 * const marketingMount = new databricks.Mount("marketing", {
 *     name: "marketing",
 *     wasb: {
 *         containerName: marketing.name,
 *         storageAccountName: blobaccount.name,
 *         authType: "ACCESS_KEY",
 *         tokenSecretScope: terraform.name,
 *         tokenSecretKey: storageKey.key,
 *     },
 * });
 * ```
 *
 * ## Migration from other mount resources
 *
 * Migration from the specific mount resource is straightforward:
 *
 * * rename `mountName` to `name`
 * * wrap storage-specific settings (`containerName`, ...) into corresponding block (`adl`, `abfs`, `s3`, `wasbs`)
 * * for S3 mounts, rename `s3BucketName` to `bucketName`
 *
 * ## Related Resources
 *
 * The following resources are often used in the same context:
 *
 * * End to end workspace management guide.
 * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
 * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
 *
 * ## Import
 *
 * !> Importing this resource is not currently supported.
 */
export class Mount extends pulumi.CustomResource {
    /**
     * Get an existing Mount resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: MountState, opts?: pulumi.CustomResourceOptions): Mount {
        return new Mount(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'databricks:index/mount:Mount';

    /**
     * Returns true if the given object is an instance of Mount.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is Mount {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Mount.__pulumiType;
    }

    declare public readonly abfs: pulumi.Output<outputs.MountAbfs | undefined>;
    declare public readonly adl: pulumi.Output<outputs.MountAdl | undefined>;
    declare public readonly clusterId: pulumi.Output<string>;
    declare public readonly encryptionType: pulumi.Output<string | undefined>;
    declare public readonly extraConfigs: pulumi.Output<{[key: string]: string} | undefined>;
    declare public readonly gs: pulumi.Output<outputs.MountGs | undefined>;
    declare public readonly name: pulumi.Output<string>;
    declare public readonly resourceId: pulumi.Output<string | undefined>;
    declare public readonly s3: pulumi.Output<outputs.MountS3 | undefined>;
    /**
     * (String) HDFS-compatible url
     */
    declare public /*out*/ readonly source: pulumi.Output<string>;
    declare public readonly uri: pulumi.Output<string | undefined>;
    declare public readonly wasb: pulumi.Output<outputs.MountWasb | undefined>;

    /**
     * Create a Mount resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args?: MountArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: MountArgs | MountState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as MountState | undefined;
            resourceInputs["abfs"] = state?.abfs;
            resourceInputs["adl"] = state?.adl;
            resourceInputs["clusterId"] = state?.clusterId;
            resourceInputs["encryptionType"] = state?.encryptionType;
            resourceInputs["extraConfigs"] = state?.extraConfigs;
            resourceInputs["gs"] = state?.gs;
            resourceInputs["name"] = state?.name;
            resourceInputs["resourceId"] = state?.resourceId;
            resourceInputs["s3"] = state?.s3;
            resourceInputs["source"] = state?.source;
            resourceInputs["uri"] = state?.uri;
            resourceInputs["wasb"] = state?.wasb;
        } else {
            const args = argsOrState as MountArgs | undefined;
            resourceInputs["abfs"] = args?.abfs;
            resourceInputs["adl"] = args?.adl;
            resourceInputs["clusterId"] = args?.clusterId;
            resourceInputs["encryptionType"] = args?.encryptionType;
            resourceInputs["extraConfigs"] = args?.extraConfigs;
            resourceInputs["gs"] = args?.gs;
            resourceInputs["name"] = args?.name;
            resourceInputs["resourceId"] = args?.resourceId;
            resourceInputs["s3"] = args?.s3;
            resourceInputs["uri"] = args?.uri;
            resourceInputs["wasb"] = args?.wasb;
            resourceInputs["source"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Mount.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering Mount resources.
 */
export interface MountState {
    abfs?: pulumi.Input<inputs.MountAbfs>;
    adl?: pulumi.Input<inputs.MountAdl>;
    clusterId?: pulumi.Input<string>;
    encryptionType?: pulumi.Input<string>;
    extraConfigs?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    gs?: pulumi.Input<inputs.MountGs>;
    name?: pulumi.Input<string>;
    resourceId?: pulumi.Input<string>;
    s3?: pulumi.Input<inputs.MountS3>;
    /**
     * (String) HDFS-compatible url
     */
    source?: pulumi.Input<string>;
    uri?: pulumi.Input<string>;
    wasb?: pulumi.Input<inputs.MountWasb>;
}

/**
 * The set of arguments for constructing a Mount resource.
 */
export interface MountArgs {
    abfs?: pulumi.Input<inputs.MountAbfs>;
    adl?: pulumi.Input<inputs.MountAdl>;
    clusterId?: pulumi.Input<string>;
    encryptionType?: pulumi.Input<string>;
    extraConfigs?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    gs?: pulumi.Input<inputs.MountGs>;
    name?: pulumi.Input<string>;
    resourceId?: pulumi.Input<string>;
    s3?: pulumi.Input<inputs.MountS3>;
    uri?: pulumi.Input<string>;
    wasb?: pulumi.Input<inputs.MountWasb>;
}
