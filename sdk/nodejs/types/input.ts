// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AccessControlRuleSetGrantRule {
    /**
     * a list of principals who are granted a role. The following format is supported:
     * * `users/{username}` (also exposed as `aclPrincipalId` attribute of `databricks.User` resource).
     * * `groups/{groupname}` (also exposed as `aclPrincipalId` attribute of `databricks.Group` resource).
     * * `servicePrincipals/{applicationId}` (also exposed as `aclPrincipalId` attribute of `databricks.ServicePrincipal` resource).
     */
    principals?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * Role to be granted. The supported roles are listed below. For more information about these roles, refer to [service principal roles](https://docs.databricks.com/security/auth-authz/access-control/service-principal-acl.html#service-principal-roles), [group roles](https://docs.databricks.com/en/administration-guide/users-groups/groups.html#manage-roles-on-an-account-group-using-the-workspace-admin-settings-page) or [marketplace roles](https://docs.databricks.com/en/marketplace/get-started-provider.html#assign-the-marketplace-admin-role).
     * * `roles/servicePrincipal.manager` - Manager of a service principal.
     * * `roles/servicePrincipal.user` - User of a service principal.
     * * `roles/group.manager` - Manager of a group.
     * * `roles/marketplace.admin` - Admin of marketplace.
     */
    role: pulumi.Input<string>;
}

export interface ClusterAutoscale {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface ClusterAwsAttributes {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface ClusterAzureAttributes {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface ClusterClusterLogConf {
    dbfs?: pulumi.Input<inputs.ClusterClusterLogConfDbfs>;
    s3?: pulumi.Input<inputs.ClusterClusterLogConfS3>;
}

export interface ClusterClusterLogConfDbfs {
    destination: pulumi.Input<string>;
}

export interface ClusterClusterLogConfS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface ClusterClusterMountInfo {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.ClusterClusterMountInfoNetworkFilesystemInfo>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface ClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface ClusterDockerImage {
    basicAuth?: pulumi.Input<inputs.ClusterDockerImageBasicAuth>;
    url: pulumi.Input<string>;
}

export interface ClusterDockerImageBasicAuth {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface ClusterGcpAttributes {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    /**
     * @deprecated Please use 'availability' instead.
     */
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface ClusterInitScript {
    abfss?: pulumi.Input<inputs.ClusterInitScriptAbfss>;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: pulumi.Input<inputs.ClusterInitScriptDbfs>;
    file?: pulumi.Input<inputs.ClusterInitScriptFile>;
    gcs?: pulumi.Input<inputs.ClusterInitScriptGcs>;
    s3?: pulumi.Input<inputs.ClusterInitScriptS3>;
    volumes?: pulumi.Input<inputs.ClusterInitScriptVolumes>;
    workspace?: pulumi.Input<inputs.ClusterInitScriptWorkspace>;
}

export interface ClusterInitScriptAbfss {
    destination?: pulumi.Input<string>;
}

export interface ClusterInitScriptDbfs {
    destination: pulumi.Input<string>;
}

export interface ClusterInitScriptFile {
    destination?: pulumi.Input<string>;
}

export interface ClusterInitScriptGcs {
    destination?: pulumi.Input<string>;
}

export interface ClusterInitScriptS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface ClusterInitScriptVolumes {
    destination?: pulumi.Input<string>;
}

export interface ClusterInitScriptWorkspace {
    destination?: pulumi.Input<string>;
}

export interface ClusterLibrary {
    cran?: pulumi.Input<inputs.ClusterLibraryCran>;
    egg?: pulumi.Input<string>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.ClusterLibraryMaven>;
    pypi?: pulumi.Input<inputs.ClusterLibraryPypi>;
    whl?: pulumi.Input<string>;
}

export interface ClusterLibraryCran {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface ClusterLibraryMaven {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface ClusterLibraryPypi {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface ClusterWorkloadType {
    clients: pulumi.Input<inputs.ClusterWorkloadTypeClients>;
}

export interface ClusterWorkloadTypeClients {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface ExternalLocationEncryptionDetails {
    sseEncryptionDetails?: pulumi.Input<inputs.ExternalLocationEncryptionDetailsSseEncryptionDetails>;
}

export interface ExternalLocationEncryptionDetailsSseEncryptionDetails {
    algorithm?: pulumi.Input<string>;
    awsKmsKeyArn?: pulumi.Input<string>;
}

export interface GetClusterClusterInfo {
    autoscale?: inputs.GetClusterClusterInfoAutoscale;
    /**
     * Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.
     */
    autoterminationMinutes?: number;
    awsAttributes?: inputs.GetClusterClusterInfoAwsAttributes;
    azureAttributes?: inputs.GetClusterClusterInfoAzureAttributes;
    clusterCores?: number;
    /**
     * The id of the cluster
     */
    clusterId?: string;
    clusterLogConf?: inputs.GetClusterClusterInfoClusterLogConf;
    clusterLogStatus?: inputs.GetClusterClusterInfoClusterLogStatus;
    clusterMemoryMb?: number;
    /**
     * The exact name of the cluster to search
     */
    clusterName?: string;
    clusterSource?: string;
    creatorUserName?: string;
    /**
     * Additional tags for cluster resources.
     */
    customTags?: {[key: string]: any};
    /**
     * Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.
     */
    dataSecurityMode?: string;
    defaultTags: {[key: string]: any};
    dockerImage?: inputs.GetClusterClusterInfoDockerImage;
    driver?: inputs.GetClusterClusterInfoDriver;
    /**
     * similar to `instancePoolId`, but for driver node.
     */
    driverInstancePoolId?: string;
    /**
     * The node type of the Spark driver.
     */
    driverNodeTypeId?: string;
    /**
     * Use autoscaling local storage.
     */
    enableElasticDisk?: boolean;
    /**
     * Enable local disk encryption.
     */
    enableLocalDiskEncryption?: boolean;
    executors?: inputs.GetClusterClusterInfoExecutor[];
    gcpAttributes?: inputs.GetClusterClusterInfoGcpAttributes;
    initScripts?: inputs.GetClusterClusterInfoInitScript[];
    /**
     * The pool of idle instances the cluster is attached to.
     */
    instancePoolId?: string;
    jdbcPort?: number;
    lastActivityTime?: number;
    lastStateLossTime?: number;
    /**
     * Any supported databricks.getNodeType id.
     */
    nodeTypeId?: string;
    numWorkers?: number;
    /**
     * Identifier of Cluster Policy to validate cluster and preset certain defaults.
     */
    policyId?: string;
    /**
     * The type of runtime of the cluster
     */
    runtimeEngine?: string;
    /**
     * The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     */
    singleUserName?: string;
    /**
     * Map with key-value pairs to fine-tune Spark clusters.
     */
    sparkConf?: {[key: string]: any};
    sparkContextId?: number;
    /**
     * Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
     */
    sparkEnvVars?: {[key: string]: any};
    /**
     * [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.
     */
    sparkVersion: string;
    /**
     * SSH public key contents that will be added to each Spark node in this cluster.
     */
    sshPublicKeys?: string[];
    startTime?: number;
    state: string;
    stateMessage?: string;
    terminateTime?: number;
    terminationReason?: inputs.GetClusterClusterInfoTerminationReason;
}

export interface GetClusterClusterInfoArgs {
    autoscale?: pulumi.Input<inputs.GetClusterClusterInfoAutoscaleArgs>;
    /**
     * Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.
     */
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.GetClusterClusterInfoAwsAttributesArgs>;
    azureAttributes?: pulumi.Input<inputs.GetClusterClusterInfoAzureAttributesArgs>;
    clusterCores?: pulumi.Input<number>;
    /**
     * The id of the cluster
     */
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.GetClusterClusterInfoClusterLogConfArgs>;
    clusterLogStatus?: pulumi.Input<inputs.GetClusterClusterInfoClusterLogStatusArgs>;
    clusterMemoryMb?: pulumi.Input<number>;
    /**
     * The exact name of the cluster to search
     */
    clusterName?: pulumi.Input<string>;
    clusterSource?: pulumi.Input<string>;
    creatorUserName?: pulumi.Input<string>;
    /**
     * Additional tags for cluster resources.
     */
    customTags?: pulumi.Input<{[key: string]: any}>;
    /**
     * Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.
     */
    dataSecurityMode?: pulumi.Input<string>;
    defaultTags: pulumi.Input<{[key: string]: any}>;
    dockerImage?: pulumi.Input<inputs.GetClusterClusterInfoDockerImageArgs>;
    driver?: pulumi.Input<inputs.GetClusterClusterInfoDriverArgs>;
    /**
     * similar to `instancePoolId`, but for driver node.
     */
    driverInstancePoolId?: pulumi.Input<string>;
    /**
     * The node type of the Spark driver.
     */
    driverNodeTypeId?: pulumi.Input<string>;
    /**
     * Use autoscaling local storage.
     */
    enableElasticDisk?: pulumi.Input<boolean>;
    /**
     * Enable local disk encryption.
     */
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    executors?: pulumi.Input<pulumi.Input<inputs.GetClusterClusterInfoExecutorArgs>[]>;
    gcpAttributes?: pulumi.Input<inputs.GetClusterClusterInfoGcpAttributesArgs>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.GetClusterClusterInfoInitScriptArgs>[]>;
    /**
     * The pool of idle instances the cluster is attached to.
     */
    instancePoolId?: pulumi.Input<string>;
    jdbcPort?: pulumi.Input<number>;
    lastActivityTime?: pulumi.Input<number>;
    lastStateLossTime?: pulumi.Input<number>;
    /**
     * Any supported databricks.getNodeType id.
     */
    nodeTypeId?: pulumi.Input<string>;
    numWorkers?: pulumi.Input<number>;
    /**
     * Identifier of Cluster Policy to validate cluster and preset certain defaults.
     */
    policyId?: pulumi.Input<string>;
    /**
     * The type of runtime of the cluster
     */
    runtimeEngine?: pulumi.Input<string>;
    /**
     * The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     */
    singleUserName?: pulumi.Input<string>;
    /**
     * Map with key-value pairs to fine-tune Spark clusters.
     */
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkContextId?: pulumi.Input<number>;
    /**
     * Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
     */
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    /**
     * [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.
     */
    sparkVersion: pulumi.Input<string>;
    /**
     * SSH public key contents that will be added to each Spark node in this cluster.
     */
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    startTime?: pulumi.Input<number>;
    state: pulumi.Input<string>;
    stateMessage?: pulumi.Input<string>;
    terminateTime?: pulumi.Input<number>;
    terminationReason?: pulumi.Input<inputs.GetClusterClusterInfoTerminationReasonArgs>;
}

export interface GetClusterClusterInfoAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetClusterClusterInfoAutoscaleArgs {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface GetClusterClusterInfoAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetClusterClusterInfoAwsAttributesArgs {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetClusterClusterInfoAzureAttributesArgs {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface GetClusterClusterInfoClusterLogConf {
    dbfs?: inputs.GetClusterClusterInfoClusterLogConfDbfs;
    s3?: inputs.GetClusterClusterInfoClusterLogConfS3;
}

export interface GetClusterClusterInfoClusterLogConfArgs {
    dbfs?: pulumi.Input<inputs.GetClusterClusterInfoClusterLogConfDbfsArgs>;
    s3?: pulumi.Input<inputs.GetClusterClusterInfoClusterLogConfS3Args>;
}

export interface GetClusterClusterInfoClusterLogConfDbfs {
    destination: string;
}

export interface GetClusterClusterInfoClusterLogConfDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetClusterClusterInfoClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoClusterLogConfS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoClusterLogStatus {
    lastAttempted?: number;
    lastException?: string;
}

export interface GetClusterClusterInfoClusterLogStatusArgs {
    lastAttempted?: pulumi.Input<number>;
    lastException?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoDockerImage {
    basicAuth?: inputs.GetClusterClusterInfoDockerImageBasicAuth;
    url: string;
}

export interface GetClusterClusterInfoDockerImageArgs {
    basicAuth?: pulumi.Input<inputs.GetClusterClusterInfoDockerImageBasicAuthArgs>;
    url: pulumi.Input<string>;
}

export interface GetClusterClusterInfoDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetClusterClusterInfoDockerImageBasicAuthArgs {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface GetClusterClusterInfoDriver {
    hostPrivateIp?: string;
    instanceId?: string;
    nodeAwsAttributes?: inputs.GetClusterClusterInfoDriverNodeAwsAttributes;
    nodeId?: string;
    privateIp?: string;
    publicDns?: string;
    startTimestamp?: number;
}

export interface GetClusterClusterInfoDriverArgs {
    hostPrivateIp?: pulumi.Input<string>;
    instanceId?: pulumi.Input<string>;
    nodeAwsAttributes?: pulumi.Input<inputs.GetClusterClusterInfoDriverNodeAwsAttributesArgs>;
    nodeId?: pulumi.Input<string>;
    privateIp?: pulumi.Input<string>;
    publicDns?: pulumi.Input<string>;
    startTimestamp?: pulumi.Input<number>;
}

export interface GetClusterClusterInfoDriverNodeAwsAttributes {
    isSpot?: boolean;
}

export interface GetClusterClusterInfoDriverNodeAwsAttributesArgs {
    isSpot?: pulumi.Input<boolean>;
}

export interface GetClusterClusterInfoExecutor {
    hostPrivateIp?: string;
    instanceId?: string;
    nodeAwsAttributes?: inputs.GetClusterClusterInfoExecutorNodeAwsAttributes;
    nodeId?: string;
    privateIp?: string;
    publicDns?: string;
    startTimestamp?: number;
}

export interface GetClusterClusterInfoExecutorArgs {
    hostPrivateIp?: pulumi.Input<string>;
    instanceId?: pulumi.Input<string>;
    nodeAwsAttributes?: pulumi.Input<inputs.GetClusterClusterInfoExecutorNodeAwsAttributesArgs>;
    nodeId?: pulumi.Input<string>;
    privateIp?: pulumi.Input<string>;
    publicDns?: pulumi.Input<string>;
    startTimestamp?: pulumi.Input<number>;
}

export interface GetClusterClusterInfoExecutorNodeAwsAttributes {
    isSpot?: boolean;
}

export interface GetClusterClusterInfoExecutorNodeAwsAttributesArgs {
    isSpot?: pulumi.Input<boolean>;
}

export interface GetClusterClusterInfoGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetClusterClusterInfoGcpAttributesArgs {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScript {
    abfss?: inputs.GetClusterClusterInfoInitScriptAbfss;
    dbfs?: inputs.GetClusterClusterInfoInitScriptDbfs;
    file?: inputs.GetClusterClusterInfoInitScriptFile;
    gcs?: inputs.GetClusterClusterInfoInitScriptGcs;
    s3?: inputs.GetClusterClusterInfoInitScriptS3;
    volumes?: inputs.GetClusterClusterInfoInitScriptVolumes;
    workspace?: inputs.GetClusterClusterInfoInitScriptWorkspace;
}

export interface GetClusterClusterInfoInitScriptArgs {
    abfss?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptAbfssArgs>;
    dbfs?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptDbfsArgs>;
    file?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptFileArgs>;
    gcs?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptGcsArgs>;
    s3?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptS3Args>;
    volumes?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptVolumesArgs>;
    workspace?: pulumi.Input<inputs.GetClusterClusterInfoInitScriptWorkspaceArgs>;
}

export interface GetClusterClusterInfoInitScriptAbfss {
    destination?: string;
}

export interface GetClusterClusterInfoInitScriptAbfssArgs {
    destination?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptDbfs {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptFile {
    destination?: string;
}

export interface GetClusterClusterInfoInitScriptFileArgs {
    destination?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptGcs {
    destination?: string;
}

export interface GetClusterClusterInfoInitScriptGcsArgs {
    destination?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoInitScriptS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptVolumes {
    destination?: string;
}

export interface GetClusterClusterInfoInitScriptVolumesArgs {
    destination?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoInitScriptWorkspace {
    destination?: string;
}

export interface GetClusterClusterInfoInitScriptWorkspaceArgs {
    destination?: pulumi.Input<string>;
}

export interface GetClusterClusterInfoTerminationReason {
    code?: string;
    parameters?: {[key: string]: any};
    type?: string;
}

export interface GetClusterClusterInfoTerminationReasonArgs {
    code?: pulumi.Input<string>;
    parameters?: pulumi.Input<{[key: string]: any}>;
    type?: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfo {
    awsAttributes?: inputs.GetInstancePoolPoolInfoAwsAttributes;
    azureAttributes?: inputs.GetInstancePoolPoolInfoAzureAttributes;
    customTags?: {[key: string]: any};
    defaultTags?: {[key: string]: any};
    diskSpec?: inputs.GetInstancePoolPoolInfoDiskSpec;
    enableElasticDisk?: boolean;
    gcpAttributes?: inputs.GetInstancePoolPoolInfoGcpAttributes;
    idleInstanceAutoterminationMinutes: number;
    instancePoolFleetAttributes?: inputs.GetInstancePoolPoolInfoInstancePoolFleetAttribute[];
    instancePoolId?: string;
    instancePoolName: string;
    maxCapacity?: number;
    minIdleInstances?: number;
    nodeTypeId?: string;
    preloadedDockerImages?: inputs.GetInstancePoolPoolInfoPreloadedDockerImage[];
    preloadedSparkVersions?: string[];
    state?: string;
    stats?: inputs.GetInstancePoolPoolInfoStats;
}

export interface GetInstancePoolPoolInfoArgs {
    awsAttributes?: pulumi.Input<inputs.GetInstancePoolPoolInfoAwsAttributesArgs>;
    azureAttributes?: pulumi.Input<inputs.GetInstancePoolPoolInfoAzureAttributesArgs>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    defaultTags?: pulumi.Input<{[key: string]: any}>;
    diskSpec?: pulumi.Input<inputs.GetInstancePoolPoolInfoDiskSpecArgs>;
    enableElasticDisk?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.GetInstancePoolPoolInfoGcpAttributesArgs>;
    idleInstanceAutoterminationMinutes: pulumi.Input<number>;
    instancePoolFleetAttributes?: pulumi.Input<pulumi.Input<inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeArgs>[]>;
    instancePoolId?: pulumi.Input<string>;
    instancePoolName: pulumi.Input<string>;
    maxCapacity?: pulumi.Input<number>;
    minIdleInstances?: pulumi.Input<number>;
    nodeTypeId?: pulumi.Input<string>;
    preloadedDockerImages?: pulumi.Input<pulumi.Input<inputs.GetInstancePoolPoolInfoPreloadedDockerImageArgs>[]>;
    preloadedSparkVersions?: pulumi.Input<pulumi.Input<string>[]>;
    state?: pulumi.Input<string>;
    stats?: pulumi.Input<inputs.GetInstancePoolPoolInfoStatsArgs>;
}

export interface GetInstancePoolPoolInfoAwsAttributes {
    availability?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetInstancePoolPoolInfoAwsAttributesArgs {
    availability?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfoAzureAttributes {
    availability?: string;
    spotBidMaxPrice?: number;
}

export interface GetInstancePoolPoolInfoAzureAttributesArgs {
    availability?: pulumi.Input<string>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface GetInstancePoolPoolInfoDiskSpec {
    diskCount?: number;
    diskSize?: number;
    diskType?: inputs.GetInstancePoolPoolInfoDiskSpecDiskType;
}

export interface GetInstancePoolPoolInfoDiskSpecArgs {
    diskCount?: pulumi.Input<number>;
    diskSize?: pulumi.Input<number>;
    diskType?: pulumi.Input<inputs.GetInstancePoolPoolInfoDiskSpecDiskTypeArgs>;
}

export interface GetInstancePoolPoolInfoDiskSpecDiskType {
    azureDiskVolumeType?: string;
    ebsVolumeType?: string;
}

export interface GetInstancePoolPoolInfoDiskSpecDiskTypeArgs {
    azureDiskVolumeType?: pulumi.Input<string>;
    ebsVolumeType?: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfoGcpAttributes {
    gcpAvailability?: string;
    localSsdCount?: number;
}

export interface GetInstancePoolPoolInfoGcpAttributesArgs {
    gcpAvailability?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttribute {
    fleetOnDemandOption?: inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption;
    fleetSpotOption?: inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption;
    launchTemplateOverrides: inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride[];
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeArgs {
    fleetOnDemandOption?: pulumi.Input<inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOptionArgs>;
    fleetSpotOption?: pulumi.Input<inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOptionArgs>;
    launchTemplateOverrides: pulumi.Input<pulumi.Input<inputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverrideArgs>[]>;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOptionArgs {
    allocationStrategy: pulumi.Input<string>;
    instancePoolsToUseCount?: pulumi.Input<number>;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOptionArgs {
    allocationStrategy: pulumi.Input<string>;
    instancePoolsToUseCount?: pulumi.Input<number>;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride {
    availabilityZone: string;
    instanceType: string;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverrideArgs {
    availabilityZone: pulumi.Input<string>;
    instanceType: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImage {
    basicAuth?: inputs.GetInstancePoolPoolInfoPreloadedDockerImageBasicAuth;
    url: string;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImageArgs {
    basicAuth?: pulumi.Input<inputs.GetInstancePoolPoolInfoPreloadedDockerImageBasicAuthArgs>;
    url: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImageBasicAuthArgs {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface GetInstancePoolPoolInfoStats {
    idleCount?: number;
    pendingIdleCount?: number;
    pendingUsedCount?: number;
    usedCount?: number;
}

export interface GetInstancePoolPoolInfoStatsArgs {
    idleCount?: pulumi.Input<number>;
    pendingIdleCount?: pulumi.Input<number>;
    pendingUsedCount?: pulumi.Input<number>;
    usedCount?: pulumi.Input<number>;
}

export interface GetJobJobSettings {
    createdTime?: number;
    creatorUserName?: string;
    jobId?: number;
    runAsUserName?: string;
    settings?: inputs.GetJobJobSettingsSettings;
}

export interface GetJobJobSettingsArgs {
    createdTime?: pulumi.Input<number>;
    creatorUserName?: pulumi.Input<string>;
    jobId?: pulumi.Input<number>;
    runAsUserName?: pulumi.Input<string>;
    settings?: pulumi.Input<inputs.GetJobJobSettingsSettingsArgs>;
}

export interface GetJobJobSettingsSettings {
    computes?: inputs.GetJobJobSettingsSettingsCompute[];
    continuous?: inputs.GetJobJobSettingsSettingsContinuous;
    dbtTask?: inputs.GetJobJobSettingsSettingsDbtTask;
    emailNotifications?: inputs.GetJobJobSettingsSettingsEmailNotifications;
    existingClusterId?: string;
    format?: string;
    gitSource?: inputs.GetJobJobSettingsSettingsGitSource;
    health?: inputs.GetJobJobSettingsSettingsHealth;
    jobClusters?: inputs.GetJobJobSettingsSettingsJobCluster[];
    libraries?: inputs.GetJobJobSettingsSettingsLibrary[];
    maxConcurrentRuns?: number;
    maxRetries?: number;
    minRetryIntervalMillis?: number;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: string;
    newCluster?: inputs.GetJobJobSettingsSettingsNewCluster;
    notebookTask?: inputs.GetJobJobSettingsSettingsNotebookTask;
    notificationSettings?: inputs.GetJobJobSettingsSettingsNotificationSettings;
    parameters?: inputs.GetJobJobSettingsSettingsParameter[];
    pipelineTask?: inputs.GetJobJobSettingsSettingsPipelineTask;
    pythonWheelTask?: inputs.GetJobJobSettingsSettingsPythonWheelTask;
    queue?: inputs.GetJobJobSettingsSettingsQueue;
    retryOnTimeout?: boolean;
    runAs?: inputs.GetJobJobSettingsSettingsRunAs;
    runJobTask?: inputs.GetJobJobSettingsSettingsRunJobTask;
    schedule?: inputs.GetJobJobSettingsSettingsSchedule;
    sparkJarTask?: inputs.GetJobJobSettingsSettingsSparkJarTask;
    sparkPythonTask?: inputs.GetJobJobSettingsSettingsSparkPythonTask;
    sparkSubmitTask?: inputs.GetJobJobSettingsSettingsSparkSubmitTask;
    tags?: {[key: string]: any};
    tasks?: inputs.GetJobJobSettingsSettingsTask[];
    timeoutSeconds?: number;
    trigger?: inputs.GetJobJobSettingsSettingsTrigger;
    webhookNotifications?: inputs.GetJobJobSettingsSettingsWebhookNotifications;
}

export interface GetJobJobSettingsSettingsArgs {
    computes?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsComputeArgs>[]>;
    continuous?: pulumi.Input<inputs.GetJobJobSettingsSettingsContinuousArgs>;
    dbtTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsDbtTaskArgs>;
    emailNotifications?: pulumi.Input<inputs.GetJobJobSettingsSettingsEmailNotificationsArgs>;
    existingClusterId?: pulumi.Input<string>;
    format?: pulumi.Input<string>;
    gitSource?: pulumi.Input<inputs.GetJobJobSettingsSettingsGitSourceArgs>;
    health?: pulumi.Input<inputs.GetJobJobSettingsSettingsHealthArgs>;
    jobClusters?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterArgs>[]>;
    libraries?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsLibraryArgs>[]>;
    maxConcurrentRuns?: pulumi.Input<number>;
    maxRetries?: pulumi.Input<number>;
    minRetryIntervalMillis?: pulumi.Input<number>;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: pulumi.Input<string>;
    newCluster?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterArgs>;
    notebookTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsNotebookTaskArgs>;
    notificationSettings?: pulumi.Input<inputs.GetJobJobSettingsSettingsNotificationSettingsArgs>;
    parameters?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsParameterArgs>[]>;
    pipelineTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsPipelineTaskArgs>;
    pythonWheelTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsPythonWheelTaskArgs>;
    queue?: pulumi.Input<inputs.GetJobJobSettingsSettingsQueueArgs>;
    retryOnTimeout?: pulumi.Input<boolean>;
    runAs?: pulumi.Input<inputs.GetJobJobSettingsSettingsRunAsArgs>;
    runJobTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsRunJobTaskArgs>;
    schedule?: pulumi.Input<inputs.GetJobJobSettingsSettingsScheduleArgs>;
    sparkJarTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsSparkJarTaskArgs>;
    sparkPythonTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsSparkPythonTaskArgs>;
    sparkSubmitTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsSparkSubmitTaskArgs>;
    tags?: pulumi.Input<{[key: string]: any}>;
    tasks?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskArgs>[]>;
    timeoutSeconds?: pulumi.Input<number>;
    trigger?: pulumi.Input<inputs.GetJobJobSettingsSettingsTriggerArgs>;
    webhookNotifications?: pulumi.Input<inputs.GetJobJobSettingsSettingsWebhookNotificationsArgs>;
}

export interface GetJobJobSettingsSettingsCompute {
    computeKey?: string;
    spec?: inputs.GetJobJobSettingsSettingsComputeSpec;
}

export interface GetJobJobSettingsSettingsComputeArgs {
    computeKey?: pulumi.Input<string>;
    spec?: pulumi.Input<inputs.GetJobJobSettingsSettingsComputeSpecArgs>;
}

export interface GetJobJobSettingsSettingsComputeSpec {
    kind?: string;
}

export interface GetJobJobSettingsSettingsComputeSpecArgs {
    kind?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsContinuous {
    pauseStatus?: string;
}

export interface GetJobJobSettingsSettingsContinuousArgs {
    pauseStatus?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsDbtTask {
    catalog?: string;
    commands: string[];
    profilesDirectory?: string;
    projectDirectory?: string;
    schema?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsDbtTaskArgs {
    catalog?: pulumi.Input<string>;
    commands: pulumi.Input<pulumi.Input<string>[]>;
    profilesDirectory?: pulumi.Input<string>;
    projectDirectory?: pulumi.Input<string>;
    schema?: pulumi.Input<string>;
    warehouseId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsEmailNotifications {
    alertOnLastAttempt?: boolean;
    noAlertForSkippedRuns?: boolean;
    onDurationWarningThresholdExceededs?: string[];
    onFailures?: string[];
    onStarts?: string[];
    onSuccesses?: string[];
}

export interface GetJobJobSettingsSettingsEmailNotificationsArgs {
    alertOnLastAttempt?: pulumi.Input<boolean>;
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<string>[]>;
    onFailures?: pulumi.Input<pulumi.Input<string>[]>;
    onStarts?: pulumi.Input<pulumi.Input<string>[]>;
    onSuccesses?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsGitSource {
    branch?: string;
    commit?: string;
    jobSource?: inputs.GetJobJobSettingsSettingsGitSourceJobSource;
    provider?: string;
    tag?: string;
    url: string;
}

export interface GetJobJobSettingsSettingsGitSourceArgs {
    branch?: pulumi.Input<string>;
    commit?: pulumi.Input<string>;
    jobSource?: pulumi.Input<inputs.GetJobJobSettingsSettingsGitSourceJobSourceArgs>;
    provider?: pulumi.Input<string>;
    tag?: pulumi.Input<string>;
    url: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsGitSourceJobSource {
    dirtyState?: string;
    importFromGitBranch: string;
    jobConfigPath: string;
}

export interface GetJobJobSettingsSettingsGitSourceJobSourceArgs {
    dirtyState?: pulumi.Input<string>;
    importFromGitBranch: pulumi.Input<string>;
    jobConfigPath: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsHealth {
    rules: inputs.GetJobJobSettingsSettingsHealthRule[];
}

export interface GetJobJobSettingsSettingsHealthArgs {
    rules: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsHealthRuleArgs>[]>;
}

export interface GetJobJobSettingsSettingsHealthRule {
    metric?: string;
    op?: string;
    value?: number;
}

export interface GetJobJobSettingsSettingsHealthRuleArgs {
    metric?: pulumi.Input<string>;
    op?: pulumi.Input<string>;
    value?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsJobCluster {
    jobClusterKey?: string;
    newCluster?: inputs.GetJobJobSettingsSettingsJobClusterNewCluster;
}

export interface GetJobJobSettingsSettingsJobClusterArgs {
    jobClusterKey?: pulumi.Input<string>;
    newCluster?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterArgs>;
}

export interface GetJobJobSettingsSettingsJobClusterNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributes;
    azureAttributes?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConf;
    clusterMountInfos?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: any};
    dataSecurityMode?: string;
    dockerImage?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImage;
    driverInstancePoolId?: string;
    driverNodeTypeId?: string;
    enableElasticDisk?: boolean;
    enableLocalDiskEncryption?: boolean;
    gcpAttributes?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId?: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: any};
    sparkEnvVars?: {[key: string]: any};
    sparkVersion: string;
    sshPublicKeys?: string[];
    workloadType?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterArgs {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterAutoscaleArgs>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributesArgs>;
    azureAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributesArgs>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfArgs>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoArgs>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImageArgs>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributesArgs>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptArgs>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeArgs>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAutoscaleArgs {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributesArgs {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributesArgs {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConf {
    dbfs?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs;
    s3?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfArgs {
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3Args>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoArgs {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfoArgs>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfoArgs {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImage {
    basicAuth?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImageArgs {
    basicAuth?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuthArgs>;
    url: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuthArgs {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributesArgs {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScript {
    abfss?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss;
    dbfs?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs;
    file?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFile;
    gcs?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs;
    s3?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3;
    volumes?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes;
    workspace?: inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptArgs {
    abfss?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfssArgs>;
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfsArgs>;
    file?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFileArgs>;
    gcs?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3Args>;
    volumes?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumesArgs>;
    workspace?: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspaceArgs>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss {
    destination?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfssArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFile {
    destination?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFileArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs {
    destination?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcsArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes {
    destination?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumesArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace {
    destination?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspaceArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadType {
    clients: inputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeArgs {
    clients: pulumi.Input<inputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClientsArgs>;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClientsArgs {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface GetJobJobSettingsSettingsLibrary {
    cran?: inputs.GetJobJobSettingsSettingsLibraryCran;
    egg?: string;
    jar?: string;
    maven?: inputs.GetJobJobSettingsSettingsLibraryMaven;
    pypi?: inputs.GetJobJobSettingsSettingsLibraryPypi;
    whl?: string;
}

export interface GetJobJobSettingsSettingsLibraryArgs {
    cran?: pulumi.Input<inputs.GetJobJobSettingsSettingsLibraryCranArgs>;
    egg?: pulumi.Input<string>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.GetJobJobSettingsSettingsLibraryMavenArgs>;
    pypi?: pulumi.Input<inputs.GetJobJobSettingsSettingsLibraryPypiArgs>;
    whl?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsLibraryCran {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsLibraryCranArgs {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetJobJobSettingsSettingsLibraryMavenArgs {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsLibraryPypiArgs {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: inputs.GetJobJobSettingsSettingsNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: inputs.GetJobJobSettingsSettingsNewClusterAwsAttributes;
    azureAttributes?: inputs.GetJobJobSettingsSettingsNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: inputs.GetJobJobSettingsSettingsNewClusterClusterLogConf;
    clusterMountInfos?: inputs.GetJobJobSettingsSettingsNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: any};
    dataSecurityMode?: string;
    dockerImage?: inputs.GetJobJobSettingsSettingsNewClusterDockerImage;
    driverInstancePoolId?: string;
    driverNodeTypeId?: string;
    enableElasticDisk?: boolean;
    enableLocalDiskEncryption?: boolean;
    gcpAttributes?: inputs.GetJobJobSettingsSettingsNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: inputs.GetJobJobSettingsSettingsNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId?: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: any};
    sparkEnvVars?: {[key: string]: any};
    sparkVersion: string;
    sshPublicKeys?: string[];
    workloadType?: inputs.GetJobJobSettingsSettingsNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsNewClusterArgs {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterAutoscaleArgs>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterAwsAttributesArgs>;
    azureAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterAzureAttributesArgs>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterClusterLogConfArgs>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterClusterMountInfoArgs>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterDockerImageArgs>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterGcpAttributesArgs>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptArgs>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterWorkloadTypeArgs>;
}

export interface GetJobJobSettingsSettingsNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsNewClusterAutoscaleArgs {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsNewClusterAwsAttributesArgs {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsNewClusterAzureAttributesArgs {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConf {
    dbfs?: inputs.GetJobJobSettingsSettingsNewClusterClusterLogConfDbfs;
    s3?: inputs.GetJobJobSettingsSettingsNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfArgs {
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterClusterLogConfDbfsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterClusterLogConfS3Args>;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: inputs.GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfoArgs {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfoArgs>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfoArgs {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImage {
    basicAuth?: inputs.GetJobJobSettingsSettingsNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImageArgs {
    basicAuth?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterDockerImageBasicAuthArgs>;
    url: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImageBasicAuthArgs {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsNewClusterGcpAttributesArgs {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScript {
    abfss?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptAbfss;
    dbfs?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptDbfs;
    file?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptFile;
    gcs?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptGcs;
    s3?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptS3;
    volumes?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptVolumes;
    workspace?: inputs.GetJobJobSettingsSettingsNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptArgs {
    abfss?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptAbfssArgs>;
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptDbfsArgs>;
    file?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptFileArgs>;
    gcs?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptGcsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptS3Args>;
    volumes?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptVolumesArgs>;
    workspace?: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterInitScriptWorkspaceArgs>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptAbfss {
    destination?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptAbfssArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptFile {
    destination?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptFileArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptGcs {
    destination?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptGcsArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptVolumes {
    destination?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptVolumesArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptWorkspace {
    destination?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptWorkspaceArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadType {
    clients: inputs.GetJobJobSettingsSettingsNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadTypeArgs {
    clients: pulumi.Input<inputs.GetJobJobSettingsSettingsNewClusterWorkloadTypeClientsArgs>;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadTypeClientsArgs {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface GetJobJobSettingsSettingsNotebookTask {
    baseParameters?: {[key: string]: any};
    notebookPath: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsNotebookTaskArgs {
    baseParameters?: pulumi.Input<{[key: string]: any}>;
    notebookPath: pulumi.Input<string>;
    source?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsNotificationSettings {
    noAlertForCanceledRuns?: boolean;
    noAlertForSkippedRuns?: boolean;
}

export interface GetJobJobSettingsSettingsNotificationSettingsArgs {
    noAlertForCanceledRuns?: pulumi.Input<boolean>;
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
}

export interface GetJobJobSettingsSettingsParameter {
    default?: string;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: string;
}

export interface GetJobJobSettingsSettingsParameterArgs {
    default?: pulumi.Input<string>;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsPipelineTask {
    fullRefresh?: boolean;
    pipelineId: string;
}

export interface GetJobJobSettingsSettingsPipelineTaskArgs {
    fullRefresh?: pulumi.Input<boolean>;
    pipelineId: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsPythonWheelTask {
    entryPoint?: string;
    namedParameters?: {[key: string]: any};
    packageName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsPythonWheelTaskArgs {
    entryPoint?: pulumi.Input<string>;
    namedParameters?: pulumi.Input<{[key: string]: any}>;
    packageName?: pulumi.Input<string>;
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsQueue {
}

export interface GetJobJobSettingsSettingsQueueArgs {
}

export interface GetJobJobSettingsSettingsRunAs {
    servicePrincipalName?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsRunAsArgs {
    servicePrincipalName?: pulumi.Input<string>;
    userName?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsRunJobTask {
    jobId: number;
    jobParameters?: {[key: string]: any};
}

export interface GetJobJobSettingsSettingsRunJobTaskArgs {
    jobId: pulumi.Input<number>;
    jobParameters?: pulumi.Input<{[key: string]: any}>;
}

export interface GetJobJobSettingsSettingsSchedule {
    pauseStatus?: string;
    quartzCronExpression: string;
    timezoneId: string;
}

export interface GetJobJobSettingsSettingsScheduleArgs {
    pauseStatus?: pulumi.Input<string>;
    quartzCronExpression: pulumi.Input<string>;
    timezoneId: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsSparkJarTask {
    jarUri?: string;
    mainClassName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsSparkJarTaskArgs {
    jarUri?: pulumi.Input<string>;
    mainClassName?: pulumi.Input<string>;
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsSparkPythonTask {
    parameters?: string[];
    pythonFile: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsSparkPythonTaskArgs {
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
    pythonFile: pulumi.Input<string>;
    source?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsSparkSubmitTask {
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsSparkSubmitTaskArgs {
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsTask {
    computeKey?: string;
    conditionTask?: inputs.GetJobJobSettingsSettingsTaskConditionTask;
    dbtTask?: inputs.GetJobJobSettingsSettingsTaskDbtTask;
    dependsOns?: inputs.GetJobJobSettingsSettingsTaskDependsOn[];
    description?: string;
    emailNotifications?: inputs.GetJobJobSettingsSettingsTaskEmailNotifications;
    existingClusterId?: string;
    health?: inputs.GetJobJobSettingsSettingsTaskHealth;
    jobClusterKey?: string;
    libraries?: inputs.GetJobJobSettingsSettingsTaskLibrary[];
    maxRetries?: number;
    minRetryIntervalMillis?: number;
    newCluster?: inputs.GetJobJobSettingsSettingsTaskNewCluster;
    notebookTask?: inputs.GetJobJobSettingsSettingsTaskNotebookTask;
    notificationSettings?: inputs.GetJobJobSettingsSettingsTaskNotificationSettings;
    pipelineTask?: inputs.GetJobJobSettingsSettingsTaskPipelineTask;
    pythonWheelTask?: inputs.GetJobJobSettingsSettingsTaskPythonWheelTask;
    retryOnTimeout?: boolean;
    runIf?: string;
    runJobTask?: inputs.GetJobJobSettingsSettingsTaskRunJobTask;
    sparkJarTask?: inputs.GetJobJobSettingsSettingsTaskSparkJarTask;
    sparkPythonTask?: inputs.GetJobJobSettingsSettingsTaskSparkPythonTask;
    sparkSubmitTask?: inputs.GetJobJobSettingsSettingsTaskSparkSubmitTask;
    sqlTask?: inputs.GetJobJobSettingsSettingsTaskSqlTask;
    taskKey?: string;
    timeoutSeconds?: number;
}

export interface GetJobJobSettingsSettingsTaskArgs {
    computeKey?: pulumi.Input<string>;
    conditionTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskConditionTaskArgs>;
    dbtTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskDbtTaskArgs>;
    dependsOns?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskDependsOnArgs>[]>;
    description?: pulumi.Input<string>;
    emailNotifications?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskEmailNotificationsArgs>;
    existingClusterId?: pulumi.Input<string>;
    health?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskHealthArgs>;
    jobClusterKey?: pulumi.Input<string>;
    libraries?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskLibraryArgs>[]>;
    maxRetries?: pulumi.Input<number>;
    minRetryIntervalMillis?: pulumi.Input<number>;
    newCluster?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterArgs>;
    notebookTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNotebookTaskArgs>;
    notificationSettings?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNotificationSettingsArgs>;
    pipelineTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskPipelineTaskArgs>;
    pythonWheelTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskPythonWheelTaskArgs>;
    retryOnTimeout?: pulumi.Input<boolean>;
    runIf?: pulumi.Input<string>;
    runJobTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskRunJobTaskArgs>;
    sparkJarTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSparkJarTaskArgs>;
    sparkPythonTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSparkPythonTaskArgs>;
    sparkSubmitTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSparkSubmitTaskArgs>;
    sqlTask?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskArgs>;
    taskKey?: pulumi.Input<string>;
    timeoutSeconds?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsTaskConditionTask {
    left?: string;
    op?: string;
    right?: string;
}

export interface GetJobJobSettingsSettingsTaskConditionTaskArgs {
    left?: pulumi.Input<string>;
    op?: pulumi.Input<string>;
    right?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskDbtTask {
    catalog?: string;
    commands: string[];
    profilesDirectory?: string;
    projectDirectory?: string;
    schema?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskDbtTaskArgs {
    catalog?: pulumi.Input<string>;
    commands: pulumi.Input<pulumi.Input<string>[]>;
    profilesDirectory?: pulumi.Input<string>;
    projectDirectory?: pulumi.Input<string>;
    schema?: pulumi.Input<string>;
    warehouseId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskDependsOn {
    outcome?: string;
    taskKey: string;
}

export interface GetJobJobSettingsSettingsTaskDependsOnArgs {
    outcome?: pulumi.Input<string>;
    taskKey: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskEmailNotifications {
    alertOnLastAttempt?: boolean;
    noAlertForSkippedRuns?: boolean;
    onDurationWarningThresholdExceededs?: string[];
    onFailures?: string[];
    onStarts?: string[];
    onSuccesses?: string[];
}

export interface GetJobJobSettingsSettingsTaskEmailNotificationsArgs {
    alertOnLastAttempt?: pulumi.Input<boolean>;
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<string>[]>;
    onFailures?: pulumi.Input<pulumi.Input<string>[]>;
    onStarts?: pulumi.Input<pulumi.Input<string>[]>;
    onSuccesses?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsTaskHealth {
    rules: inputs.GetJobJobSettingsSettingsTaskHealthRule[];
}

export interface GetJobJobSettingsSettingsTaskHealthArgs {
    rules: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskHealthRuleArgs>[]>;
}

export interface GetJobJobSettingsSettingsTaskHealthRule {
    metric?: string;
    op?: string;
    value?: number;
}

export interface GetJobJobSettingsSettingsTaskHealthRuleArgs {
    metric?: pulumi.Input<string>;
    op?: pulumi.Input<string>;
    value?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsTaskLibrary {
    cran?: inputs.GetJobJobSettingsSettingsTaskLibraryCran;
    egg?: string;
    jar?: string;
    maven?: inputs.GetJobJobSettingsSettingsTaskLibraryMaven;
    pypi?: inputs.GetJobJobSettingsSettingsTaskLibraryPypi;
    whl?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryArgs {
    cran?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskLibraryCranArgs>;
    egg?: pulumi.Input<string>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskLibraryMavenArgs>;
    pypi?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskLibraryPypiArgs>;
    whl?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskLibraryCran {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryCranArgs {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryMavenArgs {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryPypiArgs {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: inputs.GetJobJobSettingsSettingsTaskNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: inputs.GetJobJobSettingsSettingsTaskNewClusterAwsAttributes;
    azureAttributes?: inputs.GetJobJobSettingsSettingsTaskNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConf;
    clusterMountInfos?: inputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: any};
    dataSecurityMode?: string;
    dockerImage?: inputs.GetJobJobSettingsSettingsTaskNewClusterDockerImage;
    driverInstancePoolId?: string;
    driverNodeTypeId?: string;
    enableElasticDisk?: boolean;
    enableLocalDiskEncryption?: boolean;
    gcpAttributes?: inputs.GetJobJobSettingsSettingsTaskNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId?: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: any};
    sparkEnvVars?: {[key: string]: any};
    sparkVersion: string;
    sshPublicKeys?: string[];
    workloadType?: inputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsTaskNewClusterArgs {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterAutoscaleArgs>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterAwsAttributesArgs>;
    azureAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterAzureAttributesArgs>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfArgs>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoArgs>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterDockerImageArgs>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterGcpAttributesArgs>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptArgs>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeArgs>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAutoscaleArgs {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAwsAttributesArgs {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAzureAttributesArgs {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConf {
    dbfs?: inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs;
    s3?: inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfArgs {
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3Args>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: inputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoArgs {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfoArgs>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfoArgs {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImage {
    basicAuth?: inputs.GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImageArgs {
    basicAuth?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuthArgs>;
    url: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuthArgs {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterGcpAttributesArgs {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScript {
    abfss?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfss;
    dbfs?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfs;
    file?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptFile;
    gcs?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptGcs;
    s3?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptS3;
    volumes?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumes;
    workspace?: inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptArgs {
    abfss?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfssArgs>;
    dbfs?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfsArgs>;
    file?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptFileArgs>;
    gcs?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptGcsArgs>;
    s3?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptS3Args>;
    volumes?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumesArgs>;
    workspace?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspaceArgs>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfss {
    destination?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfssArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfsArgs {
    destination: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptFile {
    destination?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptFileArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptGcs {
    destination?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptGcsArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptS3Args {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumes {
    destination?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumesArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace {
    destination?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspaceArgs {
    destination?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadType {
    clients: inputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeArgs {
    clients: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClientsArgs>;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClientsArgs {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface GetJobJobSettingsSettingsTaskNotebookTask {
    baseParameters?: {[key: string]: any};
    notebookPath: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskNotebookTaskArgs {
    baseParameters?: pulumi.Input<{[key: string]: any}>;
    notebookPath: pulumi.Input<string>;
    source?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskNotificationSettings {
    alertOnLastAttempt?: boolean;
    noAlertForCanceledRuns?: boolean;
    noAlertForSkippedRuns?: boolean;
}

export interface GetJobJobSettingsSettingsTaskNotificationSettingsArgs {
    alertOnLastAttempt?: pulumi.Input<boolean>;
    noAlertForCanceledRuns?: pulumi.Input<boolean>;
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
}

export interface GetJobJobSettingsSettingsTaskPipelineTask {
    fullRefresh?: boolean;
    pipelineId: string;
}

export interface GetJobJobSettingsSettingsTaskPipelineTaskArgs {
    fullRefresh?: pulumi.Input<boolean>;
    pipelineId: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskPythonWheelTask {
    entryPoint?: string;
    namedParameters?: {[key: string]: any};
    packageName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskPythonWheelTaskArgs {
    entryPoint?: pulumi.Input<string>;
    namedParameters?: pulumi.Input<{[key: string]: any}>;
    packageName?: pulumi.Input<string>;
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsTaskRunJobTask {
    jobId: number;
    jobParameters?: {[key: string]: any};
}

export interface GetJobJobSettingsSettingsTaskRunJobTaskArgs {
    jobId: pulumi.Input<number>;
    jobParameters?: pulumi.Input<{[key: string]: any}>;
}

export interface GetJobJobSettingsSettingsTaskSparkJarTask {
    jarUri?: string;
    mainClassName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskSparkJarTaskArgs {
    jarUri?: pulumi.Input<string>;
    mainClassName?: pulumi.Input<string>;
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsTaskSparkPythonTask {
    parameters?: string[];
    pythonFile: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskSparkPythonTaskArgs {
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
    pythonFile: pulumi.Input<string>;
    source?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskSparkSubmitTask {
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskSparkSubmitTaskArgs {
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface GetJobJobSettingsSettingsTaskSqlTask {
    alert?: inputs.GetJobJobSettingsSettingsTaskSqlTaskAlert;
    dashboard?: inputs.GetJobJobSettingsSettingsTaskSqlTaskDashboard;
    file?: inputs.GetJobJobSettingsSettingsTaskSqlTaskFile;
    parameters?: {[key: string]: any};
    query?: inputs.GetJobJobSettingsSettingsTaskSqlTaskQuery;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskArgs {
    alert?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskAlertArgs>;
    dashboard?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskDashboardArgs>;
    file?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskFileArgs>;
    parameters?: pulumi.Input<{[key: string]: any}>;
    query?: pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskQueryArgs>;
    warehouseId?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlert {
    alertId: string;
    pauseSubscriptions?: boolean;
    subscriptions: inputs.GetJobJobSettingsSettingsTaskSqlTaskAlertSubscription[];
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlertArgs {
    alertId: pulumi.Input<string>;
    pauseSubscriptions?: pulumi.Input<boolean>;
    subscriptions: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskAlertSubscriptionArgs>[]>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlertSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlertSubscriptionArgs {
    destinationId?: pulumi.Input<string>;
    userName?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboard {
    customSubject?: string;
    dashboardId: string;
    pauseSubscriptions?: boolean;
    subscriptions?: inputs.GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscription[];
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboardArgs {
    customSubject?: pulumi.Input<string>;
    dashboardId: pulumi.Input<string>;
    pauseSubscriptions?: pulumi.Input<boolean>;
    subscriptions?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscriptionArgs>[]>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscriptionArgs {
    destinationId?: pulumi.Input<string>;
    userName?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskFile {
    path: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskFileArgs {
    path: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskQuery {
    queryId: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskQueryArgs {
    queryId: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTrigger {
    fileArrival: inputs.GetJobJobSettingsSettingsTriggerFileArrival;
    pauseStatus?: string;
}

export interface GetJobJobSettingsSettingsTriggerArgs {
    fileArrival: pulumi.Input<inputs.GetJobJobSettingsSettingsTriggerFileArrivalArgs>;
    pauseStatus?: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsTriggerFileArrival {
    minTimeBetweenTriggersSeconds?: number;
    url: string;
    waitAfterLastChangeSeconds?: number;
}

export interface GetJobJobSettingsSettingsTriggerFileArrivalArgs {
    minTimeBetweenTriggersSeconds?: pulumi.Input<number>;
    url: pulumi.Input<string>;
    waitAfterLastChangeSeconds?: pulumi.Input<number>;
}

export interface GetJobJobSettingsSettingsWebhookNotifications {
    onDurationWarningThresholdExceededs?: inputs.GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded[];
    onFailures?: inputs.GetJobJobSettingsSettingsWebhookNotificationsOnFailure[];
    onStarts?: inputs.GetJobJobSettingsSettingsWebhookNotificationsOnStart[];
    onSuccesses?: inputs.GetJobJobSettingsSettingsWebhookNotificationsOnSuccess[];
}

export interface GetJobJobSettingsSettingsWebhookNotificationsArgs {
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceededArgs>[]>;
    onFailures?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsWebhookNotificationsOnFailureArgs>[]>;
    onStarts?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsWebhookNotificationsOnStartArgs>[]>;
    onSuccesses?: pulumi.Input<pulumi.Input<inputs.GetJobJobSettingsSettingsWebhookNotificationsOnSuccessArgs>[]>;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceededArgs {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnFailure {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnFailureArgs {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnStart {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnStartArgs {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: pulumi.Input<string>;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnSuccess {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnSuccessArgs {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: pulumi.Input<string>;
}

export interface GetMetastoreMetastoreInfo {
    cloud?: string;
    createdAt?: number;
    createdBy?: string;
    defaultDataAccessConfigId?: string;
    /**
     * The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.
     */
    deltaSharingOrganizationName?: string;
    /**
     * Used to set expiration duration in seconds on recipient data access tokens.
     */
    deltaSharingRecipientTokenLifetimeInSeconds?: number;
    /**
     * Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.
     */
    deltaSharingScope?: string;
    globalMetastoreId?: string;
    /**
     * Id of the metastore to be fetched
     */
    metastoreId?: string;
    /**
     * Name of metastore.
     */
    name?: string;
    /**
     * Username/groupname/sp applicationId of the metastore owner.
     */
    owner?: string;
    privilegeModelVersion?: string;
    region?: string;
    /**
     * Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource.
     */
    storageRoot?: string;
    storageRootCredentialId?: string;
    storageRootCredentialName?: string;
    updatedAt?: number;
    updatedBy?: string;
}

export interface GetMetastoreMetastoreInfoArgs {
    cloud?: pulumi.Input<string>;
    createdAt?: pulumi.Input<number>;
    createdBy?: pulumi.Input<string>;
    defaultDataAccessConfigId?: pulumi.Input<string>;
    /**
     * The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.
     */
    deltaSharingOrganizationName?: pulumi.Input<string>;
    /**
     * Used to set expiration duration in seconds on recipient data access tokens.
     */
    deltaSharingRecipientTokenLifetimeInSeconds?: pulumi.Input<number>;
    /**
     * Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL.
     */
    deltaSharingScope?: pulumi.Input<string>;
    globalMetastoreId?: pulumi.Input<string>;
    /**
     * Id of the metastore to be fetched
     */
    metastoreId?: pulumi.Input<string>;
    /**
     * Name of metastore.
     */
    name?: pulumi.Input<string>;
    /**
     * Username/groupname/sp applicationId of the metastore owner.
     */
    owner?: pulumi.Input<string>;
    privilegeModelVersion?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
    /**
     * Path on cloud storage account, where managed `databricks.Table` are stored. Change forces creation of a new resource.
     */
    storageRoot?: pulumi.Input<string>;
    storageRootCredentialId?: pulumi.Input<string>;
    storageRootCredentialName?: pulumi.Input<string>;
    updatedAt?: pulumi.Input<number>;
    updatedBy?: pulumi.Input<string>;
}

export interface GetShareObject {
    addedAt?: number;
    addedBy?: string;
    cdfEnabled?: boolean;
    /**
     * Description about the object.
     */
    comment?: string;
    /**
     * Type of the object.
     */
    dataObjectType: string;
    historyDataSharingStatus?: string;
    /**
     * The name of the share
     */
    name: string;
    partitions?: inputs.GetShareObjectPartition[];
    sharedAs?: string;
    startVersion?: number;
    status?: string;
}

export interface GetShareObjectArgs {
    addedAt?: pulumi.Input<number>;
    addedBy?: pulumi.Input<string>;
    cdfEnabled?: pulumi.Input<boolean>;
    /**
     * Description about the object.
     */
    comment?: pulumi.Input<string>;
    /**
     * Type of the object.
     */
    dataObjectType: pulumi.Input<string>;
    historyDataSharingStatus?: pulumi.Input<string>;
    /**
     * The name of the share
     */
    name: pulumi.Input<string>;
    partitions?: pulumi.Input<pulumi.Input<inputs.GetShareObjectPartitionArgs>[]>;
    sharedAs?: pulumi.Input<string>;
    startVersion?: pulumi.Input<number>;
    status?: pulumi.Input<string>;
}

export interface GetShareObjectPartition {
    values: inputs.GetShareObjectPartitionValue[];
}

export interface GetShareObjectPartitionArgs {
    values: pulumi.Input<pulumi.Input<inputs.GetShareObjectPartitionValueArgs>[]>;
}

export interface GetShareObjectPartitionValue {
    /**
     * The name of the share
     */
    name: string;
    op: string;
    recipientPropertyKey?: string;
    value?: string;
}

export interface GetShareObjectPartitionValueArgs {
    /**
     * The name of the share
     */
    name: pulumi.Input<string>;
    op: pulumi.Input<string>;
    recipientPropertyKey?: pulumi.Input<string>;
    value?: pulumi.Input<string>;
}

export interface GetSqlWarehouseChannel {
    /**
     * Name of the SQL warehouse to search (case-sensitive).
     */
    name?: string;
}

export interface GetSqlWarehouseChannelArgs {
    /**
     * Name of the SQL warehouse to search (case-sensitive).
     */
    name?: pulumi.Input<string>;
}

export interface GetSqlWarehouseOdbcParams {
    host?: string;
    hostname?: string;
    path: string;
    port: number;
    protocol: string;
}

export interface GetSqlWarehouseOdbcParamsArgs {
    host?: pulumi.Input<string>;
    hostname?: pulumi.Input<string>;
    path: pulumi.Input<string>;
    port: pulumi.Input<number>;
    protocol: pulumi.Input<string>;
}

export interface GetSqlWarehouseTags {
    customTags: inputs.GetSqlWarehouseTagsCustomTag[];
}

export interface GetSqlWarehouseTagsArgs {
    customTags: pulumi.Input<pulumi.Input<inputs.GetSqlWarehouseTagsCustomTagArgs>[]>;
}

export interface GetSqlWarehouseTagsCustomTag {
    key: string;
    value: string;
}

export interface GetSqlWarehouseTagsCustomTagArgs {
    key: pulumi.Input<string>;
    value: pulumi.Input<string>;
}

export interface GrantsGrant {
    principal: pulumi.Input<string>;
    privileges: pulumi.Input<pulumi.Input<string>[]>;
}

export interface InstancePoolAwsAttributes {
    /**
     * (String) Availability type used for all instances in the pool. Only `ON_DEMAND` and `SPOT` are supported.
     */
    availability?: pulumi.Input<string>;
    /**
     * (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*
     */
    spotBidPricePercent?: pulumi.Input<number>;
    /**
     * (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `"us-west-2a"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `"us-west-2a"` is not a valid zone ID if the Databricks deployment resides in the `"us-east-1"` region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).
     */
    zoneId?: pulumi.Input<string>;
}

export interface InstancePoolAzureAttributes {
    /**
     * Availability type used for all nodes. Valid values are `SPOT_AZURE` and `ON_DEMAND_AZURE`.
     */
    availability?: pulumi.Input<string>;
    /**
     * The max price for Azure spot instances.  Use `-1` to specify the lowest price.
     */
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface InstancePoolDiskSpec {
    /**
     * (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.
     */
    diskCount?: pulumi.Input<number>;
    /**
     * (Integer) The size of each disk (in GiB) to attach.
     */
    diskSize?: pulumi.Input<number>;
    diskType?: pulumi.Input<inputs.InstancePoolDiskSpecDiskType>;
}

export interface InstancePoolDiskSpecDiskType {
    azureDiskVolumeType?: pulumi.Input<string>;
    ebsVolumeType?: pulumi.Input<string>;
}

export interface InstancePoolGcpAttributes {
    /**
     * Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.
     */
    gcpAvailability?: pulumi.Input<string>;
    /**
     * Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
     */
    localSsdCount?: pulumi.Input<number>;
}

export interface InstancePoolInstancePoolFleetAttributes {
    fleetOnDemandOption?: pulumi.Input<inputs.InstancePoolInstancePoolFleetAttributesFleetOnDemandOption>;
    fleetSpotOption?: pulumi.Input<inputs.InstancePoolInstancePoolFleetAttributesFleetSpotOption>;
    launchTemplateOverrides: pulumi.Input<pulumi.Input<inputs.InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride>[]>;
}

export interface InstancePoolInstancePoolFleetAttributesFleetOnDemandOption {
    allocationStrategy: pulumi.Input<string>;
    instancePoolsToUseCount?: pulumi.Input<number>;
}

export interface InstancePoolInstancePoolFleetAttributesFleetSpotOption {
    allocationStrategy: pulumi.Input<string>;
    instancePoolsToUseCount?: pulumi.Input<number>;
}

export interface InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride {
    availabilityZone: pulumi.Input<string>;
    instanceType: pulumi.Input<string>;
}

export interface InstancePoolPreloadedDockerImage {
    basicAuth?: pulumi.Input<inputs.InstancePoolPreloadedDockerImageBasicAuth>;
    url: pulumi.Input<string>;
}

export interface InstancePoolPreloadedDockerImageBasicAuth {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface JobCompute {
    computeKey?: pulumi.Input<string>;
    spec?: pulumi.Input<inputs.JobComputeSpec>;
}

export interface JobComputeSpec {
    kind?: pulumi.Input<string>;
}

export interface JobContinuous {
    /**
     * Indicate whether this continuous job is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: pulumi.Input<string>;
}

export interface JobDbtTask {
    /**
     * The name of the catalog to use inside Unity Catalog.
     */
    catalog?: pulumi.Input<string>;
    /**
     * (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".
     */
    commands: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The relative path to the directory in the repository specified by `gitSource` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.
     */
    profilesDirectory?: pulumi.Input<string>;
    /**
     * The relative path to the directory in the repository specified in `gitSource` where dbt should look in for the `dbt_project.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--project-dir` to a dbt command.
     */
    projectDirectory?: pulumi.Input<string>;
    /**
     * The name of the schema dbt should run in. Defaults to `default`.
     */
    schema?: pulumi.Input<string>;
    /**
     * The ID of the SQL warehouse that dbt should execute against.
     *
     * You also need to include a `gitSource` block to configure the repository that contains the dbt project.
     */
    warehouseId?: pulumi.Input<string>;
}

export interface JobEmailNotifications {
    /**
     * (Bool) do not send notifications to recipients specified in `onStart` for the retried runs and do not send notifications to recipients specified in `onFailure` until the last retry of the run.
     */
    alertOnLastAttempt?: pulumi.Input<boolean>;
    /**
     * (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notificationSettings` configuration block).
     */
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
    /**
     * (List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run fails.
     */
    onFailures?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run starts.
     */
    onStarts?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run completes successfully.
     */
    onSuccesses?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobGitSource {
    /**
     * name of the Git branch to use. Conflicts with `tag` and `commit`.
     */
    branch?: pulumi.Input<string>;
    /**
     * hash of Git commit to use. Conflicts with `branch` and `tag`.
     */
    commit?: pulumi.Input<string>;
    jobSource?: pulumi.Input<inputs.JobGitSourceJobSource>;
    /**
     * case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.
     */
    provider?: pulumi.Input<string>;
    /**
     * name of the Git branch to use. Conflicts with `branch` and `commit`.
     */
    tag?: pulumi.Input<string>;
    /**
     * URL of the Git repository to use.
     */
    url: pulumi.Input<string>;
}

export interface JobGitSourceJobSource {
    dirtyState?: pulumi.Input<string>;
    importFromGitBranch: pulumi.Input<string>;
    jobConfigPath: pulumi.Input<string>;
}

export interface JobHealth {
    /**
     * list of rules that are represented as objects with the following attributes:
     */
    rules: pulumi.Input<pulumi.Input<inputs.JobHealthRule>[]>;
}

export interface JobHealthRule {
    /**
     * string specifying the metric to check.  The only supported metric is `RUN_DURATION_SECONDS` (check [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).
     */
    metric?: pulumi.Input<string>;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op?: pulumi.Input<string>;
    /**
     * integer value used to compare to the given metric.
     */
    value?: pulumi.Input<number>;
}

export interface JobJobCluster {
    /**
     * Identifier that can be referenced in `task` block, so that cluster is shared between tasks
     */
    jobClusterKey?: pulumi.Input<string>;
    /**
     * Same set of parameters as for databricks.Cluster resource.
     */
    newCluster?: pulumi.Input<inputs.JobJobClusterNewCluster>;
}

export interface JobJobClusterNewCluster {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.JobJobClusterNewClusterAutoscale>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.JobJobClusterNewClusterAwsAttributes>;
    azureAttributes?: pulumi.Input<inputs.JobJobClusterNewClusterAzureAttributes>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.JobJobClusterNewClusterClusterLogConf>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.JobJobClusterNewClusterClusterMountInfo>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.JobJobClusterNewClusterDockerImage>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.JobJobClusterNewClusterGcpAttributes>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.JobJobClusterNewClusterInitScript>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers?: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.JobJobClusterNewClusterWorkloadType>;
}

export interface JobJobClusterNewClusterAutoscale {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface JobJobClusterNewClusterAwsAttributes {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterAzureAttributes {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface JobJobClusterNewClusterClusterLogConf {
    dbfs?: pulumi.Input<inputs.JobJobClusterNewClusterClusterLogConfDbfs>;
    s3?: pulumi.Input<inputs.JobJobClusterNewClusterClusterLogConfS3>;
}

export interface JobJobClusterNewClusterClusterLogConfDbfs {
    destination: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterClusterLogConfS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterClusterMountInfo {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterDockerImage {
    basicAuth?: pulumi.Input<inputs.JobJobClusterNewClusterDockerImageBasicAuth>;
    /**
     * string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (`/`).
     */
    url: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterDockerImageBasicAuth {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterGcpAttributes {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScript {
    abfss?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptAbfss>;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptDbfs>;
    /**
     * block consisting of single string field: `path` - a relative path to the file (inside the Git repository) with SQL commands to execute.  *Requires `gitSource` configuration block*.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sqlAggregationJob", {tasks: [
     *     {
     *         taskKey: "run_agg_query",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             query: {
     *                 queryId: databricks_sql_query.agg_query.id,
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_dashboard",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             dashboard: {
     *                 dashboardId: databricks_sql_dashboard.dash.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_alert",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             alert: {
     *                 alertId: databricks_sql_alert.alert.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     * ]});
     * ```
     */
    file?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptFile>;
    gcs?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptGcs>;
    s3?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptS3>;
    volumes?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptVolumes>;
    workspace?: pulumi.Input<inputs.JobJobClusterNewClusterInitScriptWorkspace>;
}

export interface JobJobClusterNewClusterInitScriptAbfss {
    destination?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptDbfs {
    destination: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptFile {
    destination?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptGcs {
    destination?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptVolumes {
    destination?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterInitScriptWorkspace {
    destination?: pulumi.Input<string>;
}

export interface JobJobClusterNewClusterWorkloadType {
    clients: pulumi.Input<inputs.JobJobClusterNewClusterWorkloadTypeClients>;
}

export interface JobJobClusterNewClusterWorkloadTypeClients {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface JobLibrary {
    cran?: pulumi.Input<inputs.JobLibraryCran>;
    egg?: pulumi.Input<string>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.JobLibraryMaven>;
    pypi?: pulumi.Input<inputs.JobLibraryPypi>;
    whl?: pulumi.Input<string>;
}

export interface JobLibraryCran {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface JobLibraryMaven {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface JobLibraryPypi {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface JobNewCluster {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.JobNewClusterAutoscale>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.JobNewClusterAwsAttributes>;
    azureAttributes?: pulumi.Input<inputs.JobNewClusterAzureAttributes>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.JobNewClusterClusterLogConf>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.JobNewClusterClusterMountInfo>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.JobNewClusterDockerImage>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.JobNewClusterGcpAttributes>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.JobNewClusterInitScript>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers?: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.JobNewClusterWorkloadType>;
}

export interface JobNewClusterAutoscale {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface JobNewClusterAwsAttributes {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface JobNewClusterAzureAttributes {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface JobNewClusterClusterLogConf {
    dbfs?: pulumi.Input<inputs.JobNewClusterClusterLogConfDbfs>;
    s3?: pulumi.Input<inputs.JobNewClusterClusterLogConfS3>;
}

export interface JobNewClusterClusterLogConfDbfs {
    destination: pulumi.Input<string>;
}

export interface JobNewClusterClusterLogConfS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobNewClusterClusterMountInfo {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.JobNewClusterClusterMountInfoNetworkFilesystemInfo>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface JobNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface JobNewClusterDockerImage {
    basicAuth?: pulumi.Input<inputs.JobNewClusterDockerImageBasicAuth>;
    /**
     * string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (`/`).
     */
    url: pulumi.Input<string>;
}

export interface JobNewClusterDockerImageBasicAuth {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface JobNewClusterGcpAttributes {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface JobNewClusterInitScript {
    abfss?: pulumi.Input<inputs.JobNewClusterInitScriptAbfss>;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: pulumi.Input<inputs.JobNewClusterInitScriptDbfs>;
    /**
     * block consisting of single string field: `path` - a relative path to the file (inside the Git repository) with SQL commands to execute.  *Requires `gitSource` configuration block*.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sqlAggregationJob", {tasks: [
     *     {
     *         taskKey: "run_agg_query",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             query: {
     *                 queryId: databricks_sql_query.agg_query.id,
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_dashboard",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             dashboard: {
     *                 dashboardId: databricks_sql_dashboard.dash.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_alert",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             alert: {
     *                 alertId: databricks_sql_alert.alert.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     * ]});
     * ```
     */
    file?: pulumi.Input<inputs.JobNewClusterInitScriptFile>;
    gcs?: pulumi.Input<inputs.JobNewClusterInitScriptGcs>;
    s3?: pulumi.Input<inputs.JobNewClusterInitScriptS3>;
    volumes?: pulumi.Input<inputs.JobNewClusterInitScriptVolumes>;
    workspace?: pulumi.Input<inputs.JobNewClusterInitScriptWorkspace>;
}

export interface JobNewClusterInitScriptAbfss {
    destination?: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptDbfs {
    destination: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptFile {
    destination?: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptGcs {
    destination?: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptVolumes {
    destination?: pulumi.Input<string>;
}

export interface JobNewClusterInitScriptWorkspace {
    destination?: pulumi.Input<string>;
}

export interface JobNewClusterWorkloadType {
    clients: pulumi.Input<inputs.JobNewClusterWorkloadTypeClients>;
}

export interface JobNewClusterWorkloadTypeClients {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface JobNotebookTask {
    /**
     * (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in baseParameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s baseParameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.
     */
    baseParameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.
     */
    notebookPath: pulumi.Input<string>;
    /**
     * Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `gitSource`. If the value is empty, the task will use `GIT` if `gitSource` is defined and `WORKSPACE` otherwise.
     */
    source?: pulumi.Input<string>;
}

export interface JobNotificationSettings {
    /**
     * (Bool) don't send alert for cancelled runs.
     */
    noAlertForCanceledRuns?: pulumi.Input<boolean>;
    /**
     * (Bool) don't send alert for skipped runs.
     */
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
}

export interface JobParameter {
    /**
     * Default value of the parameter.
     */
    default?: pulumi.Input<string>;
    /**
     * An optional name for the job. The default value is Untitled.
     */
    name?: pulumi.Input<string>;
}

export interface JobPipelineTask {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > **Note** The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: pulumi.Input<boolean>;
    /**
     * The pipeline's unique ID.
     */
    pipelineId: pulumi.Input<string>;
}

export interface JobPythonWheelTask {
    /**
     * Python function as entry point for the task
     */
    entryPoint?: pulumi.Input<string>;
    /**
     * Named parameters for the task
     */
    namedParameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * Name of Python package
     */
    packageName?: pulumi.Input<string>;
    /**
     * Parameters for the task
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobQueue {
}

export interface JobRunAs {
    /**
     * The application ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const _this = new databricks.Job("this", {runAs: {
     *     servicePrincipalName: "8d23ae77-912e-4a19-81e4-b9c3f5cc9349",
     * }});
     * ```
     */
    servicePrincipalName?: pulumi.Input<string>;
    /**
     * The email of an active workspace user. Non-admin users can only set this field to their own email.
     */
    userName?: pulumi.Input<string>;
}

export interface JobRunJobTask {
    /**
     * (String) ID of the job
     */
    jobId: pulumi.Input<number>;
    /**
     * (Map) Job parameters for the task
     */
    jobParameters?: pulumi.Input<{[key: string]: any}>;
}

export interface JobSchedule {
    /**
     * Indicate whether this schedule is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted and a schedule is provided, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: pulumi.Input<string>;
    /**
     * A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.
     */
    quartzCronExpression: pulumi.Input<string>;
    /**
     * A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.
     */
    timezoneId: pulumi.Input<string>;
}

export interface JobSparkJarTask {
    jarUri?: pulumi.Input<string>;
    /**
     * The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
     */
    mainClassName?: pulumi.Input<string>;
    /**
     * (List) Parameters passed to the main method.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobSparkPythonTask {
    /**
     * (List) Command line parameters passed to the Python file.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.
     */
    pythonFile: pulumi.Input<string>;
    /**
     * Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `gitSource`.
     */
    source?: pulumi.Input<string>;
}

export interface JobSparkSubmitTask {
    /**
     * (List) Command-line parameters passed to spark submit.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobTask {
    computeKey?: pulumi.Input<string>;
    conditionTask?: pulumi.Input<inputs.JobTaskConditionTask>;
    dbtTask?: pulumi.Input<inputs.JobTaskDbtTask>;
    /**
     * block specifying dependency(-ies) for a given task.
     */
    dependsOns?: pulumi.Input<pulumi.Input<inputs.JobTaskDependsOn>[]>;
    description?: pulumi.Input<string>;
    /**
     * (List) An optional set of email addresses notified when runs of this job begins, completes and fails. The default behavior is to not send any emails. This field is a block and is documented below.
     */
    emailNotifications?: pulumi.Input<inputs.JobTaskEmailNotifications>;
    existingClusterId?: pulumi.Input<string>;
    /**
     * block described below that specifies health conditions for a given task.
     */
    health?: pulumi.Input<inputs.JobTaskHealth>;
    /**
     * Identifier that can be referenced in `task` block, so that cluster is shared between tasks
     */
    jobClusterKey?: pulumi.Input<string>;
    /**
     * (Set) An optional list of libraries to be installed on the cluster that will execute the job. Please consult libraries section for databricks.Cluster resource.
     */
    libraries?: pulumi.Input<pulumi.Input<inputs.JobTaskLibrary>[]>;
    /**
     * (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.
     */
    maxRetries?: pulumi.Input<number>;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     */
    minRetryIntervalMillis?: pulumi.Input<number>;
    /**
     * Same set of parameters as for databricks.Cluster resource.
     */
    newCluster?: pulumi.Input<inputs.JobTaskNewCluster>;
    notebookTask?: pulumi.Input<inputs.JobTaskNotebookTask>;
    /**
     * An optional block controlling the notification settings on the job level (described below).
     */
    notificationSettings?: pulumi.Input<inputs.JobTaskNotificationSettings>;
    pipelineTask?: pulumi.Input<inputs.JobTaskPipelineTask>;
    pythonWheelTask?: pulumi.Input<inputs.JobTaskPythonWheelTask>;
    /**
     * (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
     */
    retryOnTimeout?: pulumi.Input<boolean>;
    /**
     * An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. When omitted, defaults to `ALL_SUCCESS`.
     */
    runIf?: pulumi.Input<string>;
    runJobTask?: pulumi.Input<inputs.JobTaskRunJobTask>;
    sparkJarTask?: pulumi.Input<inputs.JobTaskSparkJarTask>;
    sparkPythonTask?: pulumi.Input<inputs.JobTaskSparkPythonTask>;
    sparkSubmitTask?: pulumi.Input<inputs.JobTaskSparkSubmitTask>;
    sqlTask?: pulumi.Input<inputs.JobTaskSqlTask>;
    /**
     * string specifying an unique key for a given task.
     * * `*_task` - (Required) one of the specific task blocks described below:
     */
    taskKey?: pulumi.Input<string>;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    timeoutSeconds?: pulumi.Input<number>;
}

export interface JobTaskConditionTask {
    left?: pulumi.Input<string>;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op?: pulumi.Input<string>;
    right?: pulumi.Input<string>;
}

export interface JobTaskDbtTask {
    /**
     * The name of the catalog to use inside Unity Catalog.
     */
    catalog?: pulumi.Input<string>;
    /**
     * (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".
     */
    commands: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The relative path to the directory in the repository specified by `gitSource` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.
     */
    profilesDirectory?: pulumi.Input<string>;
    /**
     * The relative path to the directory in the repository specified in `gitSource` where dbt should look in for the `dbt_project.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--project-dir` to a dbt command.
     */
    projectDirectory?: pulumi.Input<string>;
    /**
     * The name of the schema dbt should run in. Defaults to `default`.
     */
    schema?: pulumi.Input<string>;
    /**
     * The ID of the SQL warehouse that dbt should execute against.
     *
     * You also need to include a `gitSource` block to configure the repository that contains the dbt project.
     */
    warehouseId?: pulumi.Input<string>;
}

export interface JobTaskDependsOn {
    outcome?: pulumi.Input<string>;
    /**
     * The name of the task this task depends on.
     */
    taskKey: pulumi.Input<string>;
}

export interface JobTaskEmailNotifications {
    /**
     * (Bool) do not send notifications to recipients specified in `onStart` for the retried runs and do not send notifications to recipients specified in `onFailure` until the last retry of the run.
     */
    alertOnLastAttempt?: pulumi.Input<boolean>;
    /**
     * (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notificationSettings` configuration block).
     */
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
    /**
     * (List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run fails.
     */
    onFailures?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run starts.
     */
    onStarts?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * (List) list of emails to notify when the run completes successfully.
     */
    onSuccesses?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobTaskHealth {
    /**
     * list of rules that are represented as objects with the following attributes:
     */
    rules: pulumi.Input<pulumi.Input<inputs.JobTaskHealthRule>[]>;
}

export interface JobTaskHealthRule {
    /**
     * string specifying the metric to check.  The only supported metric is `RUN_DURATION_SECONDS` (check [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).
     */
    metric?: pulumi.Input<string>;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op?: pulumi.Input<string>;
    /**
     * integer value used to compare to the given metric.
     */
    value?: pulumi.Input<number>;
}

export interface JobTaskLibrary {
    cran?: pulumi.Input<inputs.JobTaskLibraryCran>;
    egg?: pulumi.Input<string>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.JobTaskLibraryMaven>;
    pypi?: pulumi.Input<inputs.JobTaskLibraryPypi>;
    whl?: pulumi.Input<string>;
}

export interface JobTaskLibraryCran {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface JobTaskLibraryMaven {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface JobTaskLibraryPypi {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface JobTaskNewCluster {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.JobTaskNewClusterAutoscale>;
    autoterminationMinutes?: pulumi.Input<number>;
    awsAttributes?: pulumi.Input<inputs.JobTaskNewClusterAwsAttributes>;
    azureAttributes?: pulumi.Input<inputs.JobTaskNewClusterAzureAttributes>;
    clusterId?: pulumi.Input<string>;
    clusterLogConf?: pulumi.Input<inputs.JobTaskNewClusterClusterLogConf>;
    clusterMountInfos?: pulumi.Input<pulumi.Input<inputs.JobTaskNewClusterClusterMountInfo>[]>;
    clusterName?: pulumi.Input<string>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    dataSecurityMode?: pulumi.Input<string>;
    dockerImage?: pulumi.Input<inputs.JobTaskNewClusterDockerImage>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableElasticDisk?: pulumi.Input<boolean>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.JobTaskNewClusterGcpAttributes>;
    idempotencyToken?: pulumi.Input<string>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.JobTaskNewClusterInitScript>[]>;
    instancePoolId?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers?: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    runtimeEngine?: pulumi.Input<string>;
    singleUserName?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sparkVersion: pulumi.Input<string>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
    workloadType?: pulumi.Input<inputs.JobTaskNewClusterWorkloadType>;
}

export interface JobTaskNewClusterAutoscale {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
}

export interface JobTaskNewClusterAwsAttributes {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface JobTaskNewClusterAzureAttributes {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface JobTaskNewClusterClusterLogConf {
    dbfs?: pulumi.Input<inputs.JobTaskNewClusterClusterLogConfDbfs>;
    s3?: pulumi.Input<inputs.JobTaskNewClusterClusterLogConfS3>;
}

export interface JobTaskNewClusterClusterLogConfDbfs {
    destination: pulumi.Input<string>;
}

export interface JobTaskNewClusterClusterLogConfS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobTaskNewClusterClusterMountInfo {
    localMountDirPath: pulumi.Input<string>;
    networkFilesystemInfo: pulumi.Input<inputs.JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo>;
    remoteMountDirPath?: pulumi.Input<string>;
}

export interface JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: pulumi.Input<string>;
    serverAddress: pulumi.Input<string>;
}

export interface JobTaskNewClusterDockerImage {
    basicAuth?: pulumi.Input<inputs.JobTaskNewClusterDockerImageBasicAuth>;
    /**
     * string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (`/`).
     */
    url: pulumi.Input<string>;
}

export interface JobTaskNewClusterDockerImageBasicAuth {
    password: pulumi.Input<string>;
    username: pulumi.Input<string>;
}

export interface JobTaskNewClusterGcpAttributes {
    availability?: pulumi.Input<string>;
    bootDiskSize?: pulumi.Input<number>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    usePreemptibleExecutors?: pulumi.Input<boolean>;
    zoneId?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScript {
    abfss?: pulumi.Input<inputs.JobTaskNewClusterInitScriptAbfss>;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: pulumi.Input<inputs.JobTaskNewClusterInitScriptDbfs>;
    /**
     * block consisting of single string field: `path` - a relative path to the file (inside the Git repository) with SQL commands to execute.  *Requires `gitSource` configuration block*.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sqlAggregationJob", {tasks: [
     *     {
     *         taskKey: "run_agg_query",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             query: {
     *                 queryId: databricks_sql_query.agg_query.id,
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_dashboard",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             dashboard: {
     *                 dashboardId: databricks_sql_dashboard.dash.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_alert",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             alert: {
     *                 alertId: databricks_sql_alert.alert.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     * ]});
     * ```
     */
    file?: pulumi.Input<inputs.JobTaskNewClusterInitScriptFile>;
    gcs?: pulumi.Input<inputs.JobTaskNewClusterInitScriptGcs>;
    s3?: pulumi.Input<inputs.JobTaskNewClusterInitScriptS3>;
    volumes?: pulumi.Input<inputs.JobTaskNewClusterInitScriptVolumes>;
    workspace?: pulumi.Input<inputs.JobTaskNewClusterInitScriptWorkspace>;
}

export interface JobTaskNewClusterInitScriptAbfss {
    destination?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptDbfs {
    destination: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptFile {
    destination?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptGcs {
    destination?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptVolumes {
    destination?: pulumi.Input<string>;
}

export interface JobTaskNewClusterInitScriptWorkspace {
    destination?: pulumi.Input<string>;
}

export interface JobTaskNewClusterWorkloadType {
    clients: pulumi.Input<inputs.JobTaskNewClusterWorkloadTypeClients>;
}

export interface JobTaskNewClusterWorkloadTypeClients {
    jobs?: pulumi.Input<boolean>;
    notebooks?: pulumi.Input<boolean>;
}

export interface JobTaskNotebookTask {
    /**
     * (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in baseParameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s baseParameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.
     */
    baseParameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.
     */
    notebookPath: pulumi.Input<string>;
    /**
     * Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `gitSource`. If the value is empty, the task will use `GIT` if `gitSource` is defined and `WORKSPACE` otherwise.
     */
    source?: pulumi.Input<string>;
}

export interface JobTaskNotificationSettings {
    /**
     * (Bool) do not send notifications to recipients specified in `onStart` for the retried runs and do not send notifications to recipients specified in `onFailure` until the last retry of the run.
     */
    alertOnLastAttempt?: pulumi.Input<boolean>;
    /**
     * (Bool) don't send alert for cancelled runs.
     */
    noAlertForCanceledRuns?: pulumi.Input<boolean>;
    /**
     * (Bool) don't send alert for skipped runs.
     */
    noAlertForSkippedRuns?: pulumi.Input<boolean>;
}

export interface JobTaskPipelineTask {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > **Note** The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: pulumi.Input<boolean>;
    /**
     * The pipeline's unique ID.
     */
    pipelineId: pulumi.Input<string>;
}

export interface JobTaskPythonWheelTask {
    /**
     * Python function as entry point for the task
     */
    entryPoint?: pulumi.Input<string>;
    /**
     * Named parameters for the task
     */
    namedParameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * Name of Python package
     */
    packageName?: pulumi.Input<string>;
    /**
     * Parameters for the task
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobTaskRunJobTask {
    /**
     * (String) ID of the job
     */
    jobId: pulumi.Input<number>;
    /**
     * (Map) Job parameters for the task
     */
    jobParameters?: pulumi.Input<{[key: string]: any}>;
}

export interface JobTaskSparkJarTask {
    jarUri?: pulumi.Input<string>;
    /**
     * The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
     */
    mainClassName?: pulumi.Input<string>;
    /**
     * (List) Parameters passed to the main method.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobTaskSparkPythonTask {
    /**
     * (List) Command line parameters passed to the Python file.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/Repos`. For files stored in a remote repository, the path must be relative. This field is required.
     */
    pythonFile: pulumi.Input<string>;
    /**
     * Location type of the Python file, can only be `GIT`. When set to `GIT`, the Python file will be retrieved from a Git repository defined in `gitSource`.
     */
    source?: pulumi.Input<string>;
}

export interface JobTaskSparkSubmitTask {
    /**
     * (List) Command-line parameters passed to spark submit.
     */
    parameters?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface JobTaskSqlTask {
    /**
     * block consisting of following fields:
     */
    alert?: pulumi.Input<inputs.JobTaskSqlTaskAlert>;
    /**
     * block consisting of following fields:
     */
    dashboard?: pulumi.Input<inputs.JobTaskSqlTaskDashboard>;
    /**
     * block consisting of single string field: `path` - a relative path to the file (inside the Git repository) with SQL commands to execute.  *Requires `gitSource` configuration block*.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sqlAggregationJob", {tasks: [
     *     {
     *         taskKey: "run_agg_query",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             query: {
     *                 queryId: databricks_sql_query.agg_query.id,
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_dashboard",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             dashboard: {
     *                 dashboardId: databricks_sql_dashboard.dash.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     *     {
     *         taskKey: "run_alert",
     *         sqlTask: {
     *             warehouseId: databricks_sql_endpoint.sql_job_warehouse.id,
     *             alert: {
     *                 alertId: databricks_sql_alert.alert.id,
     *                 subscriptions: [{
     *                     userName: "user@domain.com",
     *                 }],
     *             },
     *         },
     *     },
     * ]});
     * ```
     */
    file?: pulumi.Input<inputs.JobTaskSqlTaskFile>;
    /**
     * (Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.
     */
    parameters?: pulumi.Input<{[key: string]: any}>;
    /**
     * block consisting of single string field: `queryId` - identifier of the Databricks SQL Query (databricks_sql_query).
     */
    query?: pulumi.Input<inputs.JobTaskSqlTaskQuery>;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless & Pro warehouses are supported right now.
     */
    warehouseId?: pulumi.Input<string>;
}

export interface JobTaskSqlTaskAlert {
    /**
     * (String) identifier of the Databricks SQL Alert.
     */
    alertId: pulumi.Input<string>;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: pulumi.Input<boolean>;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions: pulumi.Input<pulumi.Input<inputs.JobTaskSqlTaskAlertSubscription>[]>;
}

export interface JobTaskSqlTaskAlertSubscription {
    destinationId?: pulumi.Input<string>;
    /**
     * The email of an active workspace user. Non-admin users can only set this field to their own email.
     */
    userName?: pulumi.Input<string>;
}

export interface JobTaskSqlTaskDashboard {
    /**
     * string specifying a custom subject of email sent.
     */
    customSubject?: pulumi.Input<string>;
    /**
     * (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.
     */
    dashboardId: pulumi.Input<string>;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: pulumi.Input<boolean>;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions?: pulumi.Input<pulumi.Input<inputs.JobTaskSqlTaskDashboardSubscription>[]>;
}

export interface JobTaskSqlTaskDashboardSubscription {
    destinationId?: pulumi.Input<string>;
    /**
     * The email of an active workspace user. Non-admin users can only set this field to their own email.
     */
    userName?: pulumi.Input<string>;
}

export interface JobTaskSqlTaskFile {
    path: pulumi.Input<string>;
}

export interface JobTaskSqlTaskQuery {
    queryId: pulumi.Input<string>;
}

export interface JobTrigger {
    /**
     * configuration block to define a trigger for [File Arrival events](https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/file-arrival-triggers) consisting of following attributes:
     */
    fileArrival: pulumi.Input<inputs.JobTriggerFileArrival>;
    /**
     * Indicate whether this trigger is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: pulumi.Input<string>;
}

export interface JobTriggerFileArrival {
    /**
     * If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.
     */
    minTimeBetweenTriggersSeconds?: pulumi.Input<number>;
    /**
     * string with URL under the Unity Catalog external location that will be monitored for new files. Please note that have a trailing slash character (`/`).
     */
    url: pulumi.Input<string>;
    /**
     * If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.
     */
    waitAfterLastChangeSeconds?: pulumi.Input<number>;
}

export interface JobWebhookNotifications {
    /**
     * (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     *
     * Note that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://<workspace host>/sql/destinations/<notification id>?o=<workspace id>`
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * ```
     */
    onDurationWarningThresholdExceededs?: pulumi.Input<pulumi.Input<inputs.JobWebhookNotificationsOnDurationWarningThresholdExceeded>[]>;
    /**
     * (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.
     */
    onFailures?: pulumi.Input<pulumi.Input<inputs.JobWebhookNotificationsOnFailure>[]>;
    /**
     * (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.
     */
    onStarts?: pulumi.Input<pulumi.Input<inputs.JobWebhookNotificationsOnStart>[]>;
    /**
     * (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.
     */
    onSuccesses?: pulumi.Input<pulumi.Input<inputs.JobWebhookNotificationsOnSuccess>[]>;
}

export interface JobWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * ID of the system notification that is notified when an event defined in `webhookNotifications` is triggered.
     *
     * > **Note** The following configuration blocks can be standalone or nested inside a `task` block
     */
    id: pulumi.Input<string>;
}

export interface JobWebhookNotificationsOnFailure {
    /**
     * ID of the system notification that is notified when an event defined in `webhookNotifications` is triggered.
     *
     * > **Note** The following configuration blocks can be standalone or nested inside a `task` block
     */
    id: pulumi.Input<string>;
}

export interface JobWebhookNotificationsOnStart {
    /**
     * ID of the system notification that is notified when an event defined in `webhookNotifications` is triggered.
     *
     * > **Note** The following configuration blocks can be standalone or nested inside a `task` block
     */
    id: pulumi.Input<string>;
}

export interface JobWebhookNotificationsOnSuccess {
    /**
     * ID of the system notification that is notified when an event defined in `webhookNotifications` is triggered.
     *
     * > **Note** The following configuration blocks can be standalone or nested inside a `task` block
     */
    id: pulumi.Input<string>;
}

export interface LibraryCran {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface LibraryMaven {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface LibraryPypi {
    package: pulumi.Input<string>;
    repo?: pulumi.Input<string>;
}

export interface MetastoreDataAccessAwsIamRole {
    /**
     * The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`
     *
     * `azureManagedIdentity` optional configuration block for using managed identity as credential details for Azure (Recommended):
     */
    roleArn: pulumi.Input<string>;
}

export interface MetastoreDataAccessAzureManagedIdentity {
    /**
     * The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.
     */
    accessConnectorId: pulumi.Input<string>;
    credentialId?: pulumi.Input<string>;
    /**
     * The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.
     *
     * `databricksGcpServiceAccount` optional configuration block for creating a Databricks-managed GCP Service Account:
     */
    managedIdentityId?: pulumi.Input<string>;
}

export interface MetastoreDataAccessAzureServicePrincipal {
    /**
     * The application ID of the application registration within the referenced AAD tenant
     */
    applicationId: pulumi.Input<string>;
    /**
     * The client secret generated for the above app ID in AAD. **This field is redacted on output**
     */
    clientSecret: pulumi.Input<string>;
    /**
     * The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application
     */
    directoryId: pulumi.Input<string>;
}

export interface MetastoreDataAccessDatabricksGcpServiceAccount {
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     *
     * `azureServicePrincipal` optional configuration block for credential details for Azure (Legacy):
     */
    email?: pulumi.Input<string>;
}

export interface MetastoreDataAccessGcpServiceAccountKey {
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     *
     * `azureServicePrincipal` optional configuration block for credential details for Azure (Legacy):
     */
    email: pulumi.Input<string>;
    privateKey: pulumi.Input<string>;
    privateKeyId: pulumi.Input<string>;
}

export interface MlflowModelTag {
    key?: pulumi.Input<string>;
    value?: pulumi.Input<string>;
}

export interface MlflowWebhookHttpUrlSpec {
    /**
     * Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `<auth type> <credentials>`, e.g. `Bearer <access_token>`. If set to an empty string, no authorization header will be included in the request.
     */
    authorization?: pulumi.Input<string>;
    /**
     * Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.
     */
    enableSslVerification?: pulumi.Input<boolean>;
    /**
     * Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as `X-Databricks-Signature: encodedPayload`.
     */
    secret?: pulumi.Input<string>;
    /**
     * External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.
     */
    url: pulumi.Input<string>;
}

export interface MlflowWebhookJobSpec {
    /**
     * The personal access token used to authorize webhook's job runs.
     */
    accessToken: pulumi.Input<string>;
    /**
     * ID of the Databricks job that the webhook runs.
     */
    jobId: pulumi.Input<string>;
    /**
     * URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.
     */
    workspaceUrl?: pulumi.Input<string>;
}

export interface ModelServingConfig {
    /**
     * Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.
     */
    servedModels: pulumi.Input<pulumi.Input<inputs.ModelServingConfigServedModel>[]>;
    /**
     * A single block represents the traffic split configuration amongst the served models.
     */
    trafficConfig?: pulumi.Input<inputs.ModelServingConfigTrafficConfig>;
}

export interface ModelServingConfigServedModel {
    /**
     * a map of environment variable name/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: `{{secrets/secret_scope/secret_key}}`.
     */
    environmentVars?: pulumi.Input<{[key: string]: any}>;
    /**
     * ARN of the instance profile that the served model will use to access AWS resources.
     */
    instanceProfileArn?: pulumi.Input<string>;
    /**
     * The name of the model in Databricks Model Registry to be served.
     */
    modelName: pulumi.Input<string>;
    /**
     * The version of the model in Databricks Model Registry to be served.
     */
    modelVersion: pulumi.Input<string>;
    /**
     * The name of a served model. It must be unique across an endpoint. If not specified, this field will default to `modelname-modelversion`. A served model name can consist of alphanumeric characters, dashes, and underscores.
     */
    name?: pulumi.Input<string>;
    /**
     * Whether the compute resources for the served model should scale down to zero. If scale-to-zero is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is `true`.
     */
    scaleToZeroEnabled?: pulumi.Input<boolean>;
    /**
     * The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are "Small" (4 - 4 provisioned concurrency), "Medium" (8 - 16 provisioned concurrency), and "Large" (16 - 64 provisioned concurrency).
     */
    workloadSize: pulumi.Input<string>;
}

export interface ModelServingConfigTrafficConfig {
    /**
     * Each block represents a route that defines traffic to each served model. Each `servedModels` block needs to have a corresponding `routes` block
     */
    routes?: pulumi.Input<pulumi.Input<inputs.ModelServingConfigTrafficConfigRoute>[]>;
}

export interface ModelServingConfigTrafficConfigRoute {
    /**
     * The name of the served model this route configures traffic for. This needs to match the name of a `servedModels` block
     */
    servedModelName: pulumi.Input<string>;
    /**
     * The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.
     */
    trafficPercentage: pulumi.Input<number>;
}

export interface MountAbfs {
    clientId: pulumi.Input<string>;
    clientSecretKey: pulumi.Input<string>;
    clientSecretScope: pulumi.Input<string>;
    containerName?: pulumi.Input<string>;
    directory?: pulumi.Input<string>;
    initializeFileSystem: pulumi.Input<boolean>;
    storageAccountName?: pulumi.Input<string>;
    tenantId?: pulumi.Input<string>;
}

export interface MountAdl {
    clientId: pulumi.Input<string>;
    clientSecretKey: pulumi.Input<string>;
    clientSecretScope: pulumi.Input<string>;
    directory?: pulumi.Input<string>;
    sparkConfPrefix?: pulumi.Input<string>;
    storageResourceName?: pulumi.Input<string>;
    tenantId?: pulumi.Input<string>;
}

export interface MountGs {
    bucketName: pulumi.Input<string>;
    serviceAccount?: pulumi.Input<string>;
}

export interface MountS3 {
    bucketName: pulumi.Input<string>;
    instanceProfile?: pulumi.Input<string>;
}

export interface MountWasb {
    authType: pulumi.Input<string>;
    containerName?: pulumi.Input<string>;
    directory?: pulumi.Input<string>;
    storageAccountName?: pulumi.Input<string>;
    tokenSecretKey: pulumi.Input<string>;
    tokenSecretScope: pulumi.Input<string>;
}

export interface MwsCustomerManagedKeysAwsKeyInfo {
    /**
     * The AWS KMS key alias.
     */
    keyAlias: pulumi.Input<string>;
    /**
     * The AWS KMS key's Amazon Resource Name (ARN).
     */
    keyArn: pulumi.Input<string>;
    /**
     * (Computed) The AWS region in which KMS key is deployed to. This is not required.
     */
    keyRegion?: pulumi.Input<string>;
}

export interface MwsCustomerManagedKeysGcpKeyInfo {
    /**
     * The GCP KMS key's resource name.
     */
    kmsKeyId: pulumi.Input<string>;
}

export interface MwsNetworksErrorMessage {
    errorMessage?: pulumi.Input<string>;
    errorType?: pulumi.Input<string>;
}

export interface MwsNetworksGcpNetworkInfo {
    /**
     * The Google Cloud project ID of the VPC network.
     */
    networkProjectId: pulumi.Input<string>;
    /**
     * The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.
     */
    podIpRangeName: pulumi.Input<string>;
    /**
     * The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.
     */
    serviceIpRangeName: pulumi.Input<string>;
    /**
     * The ID of the subnet associated with this network.
     */
    subnetId: pulumi.Input<string>;
    /**
     * The Google Cloud region of the workspace data plane. For example, `us-east4`.
     */
    subnetRegion: pulumi.Input<string>;
    /**
     * The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.
     */
    vpcId: pulumi.Input<string>;
}

export interface MwsNetworksVpcEndpoints {
    dataplaneRelays: pulumi.Input<pulumi.Input<string>[]>;
    restApis: pulumi.Input<pulumi.Input<string>[]>;
}

export interface MwsVpcEndpointGcpVpcEndpointInfo {
    /**
     * Region of the PSC endpoint.
     */
    endpointRegion: pulumi.Input<string>;
    /**
     * The Google Cloud project ID of the VPC network where the PSC connection resides.
     */
    projectId: pulumi.Input<string>;
    /**
     * The unique ID of this PSC connection.
     */
    pscConnectionId?: pulumi.Input<string>;
    /**
     * The name of the PSC endpoint in the Google Cloud project.
     */
    pscEndpointName: pulumi.Input<string>;
    /**
     * The service attachment this PSC connection connects to.
     */
    serviceAttachmentId?: pulumi.Input<string>;
}

export interface MwsWorkspacesCloudResourceContainer {
    /**
     * A block that consists of the following field:
     */
    gcp: pulumi.Input<inputs.MwsWorkspacesCloudResourceContainerGcp>;
}

export interface MwsWorkspacesCloudResourceContainerGcp {
    /**
     * The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.
     */
    projectId: pulumi.Input<string>;
}

export interface MwsWorkspacesExternalCustomerInfo {
    authoritativeUserEmail: pulumi.Input<string>;
    authoritativeUserFullName: pulumi.Input<string>;
    customerName: pulumi.Input<string>;
}

export interface MwsWorkspacesGcpManagedNetworkConfig {
    gkeClusterPodIpRange: pulumi.Input<string>;
    gkeClusterServiceIpRange: pulumi.Input<string>;
    subnetCidr: pulumi.Input<string>;
}

export interface MwsWorkspacesGkeConfig {
    /**
     * Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: `PRIVATE_NODE_PUBLIC_MASTER`, `PUBLIC_NODE_PUBLIC_MASTER`.
     */
    connectivityType: pulumi.Input<string>;
    /**
     * The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as `/28`.
     */
    masterIpRange: pulumi.Input<string>;
}

export interface MwsWorkspacesToken {
    comment?: pulumi.Input<string>;
    /**
     * Token expiry lifetime. By default its 2592000 (30 days).
     */
    lifetimeSeconds?: pulumi.Input<number>;
    tokenId?: pulumi.Input<string>;
    tokenValue?: pulumi.Input<string>;
}

export interface PermissionsAccessControl {
    /**
     * name of the group. We recommend setting permissions on groups.
     */
    groupName?: pulumi.Input<string>;
    /**
     * permission level according to specific resource. See examples above for the reference.
     *
     * Exactly one of the below arguments is required:
     */
    permissionLevel: pulumi.Input<string>;
    /**
     * Application ID of the service_principal.
     */
    servicePrincipalName?: pulumi.Input<string>;
    /**
     * name of the user.
     */
    userName?: pulumi.Input<string>;
}

export interface PipelineCluster {
    applyPolicyDefaultValues?: pulumi.Input<boolean>;
    autoscale?: pulumi.Input<inputs.PipelineClusterAutoscale>;
    awsAttributes?: pulumi.Input<inputs.PipelineClusterAwsAttributes>;
    azureAttributes?: pulumi.Input<inputs.PipelineClusterAzureAttributes>;
    clusterLogConf?: pulumi.Input<inputs.PipelineClusterClusterLogConf>;
    customTags?: pulumi.Input<{[key: string]: any}>;
    driverInstancePoolId?: pulumi.Input<string>;
    driverNodeTypeId?: pulumi.Input<string>;
    enableLocalDiskEncryption?: pulumi.Input<boolean>;
    gcpAttributes?: pulumi.Input<inputs.PipelineClusterGcpAttributes>;
    initScripts?: pulumi.Input<pulumi.Input<inputs.PipelineClusterInitScript>[]>;
    instancePoolId?: pulumi.Input<string>;
    label?: pulumi.Input<string>;
    nodeTypeId?: pulumi.Input<string>;
    numWorkers?: pulumi.Input<number>;
    policyId?: pulumi.Input<string>;
    sparkConf?: pulumi.Input<{[key: string]: any}>;
    sparkEnvVars?: pulumi.Input<{[key: string]: any}>;
    sshPublicKeys?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface PipelineClusterAutoscale {
    maxWorkers?: pulumi.Input<number>;
    minWorkers?: pulumi.Input<number>;
    mode?: pulumi.Input<string>;
}

export interface PipelineClusterAwsAttributes {
    availability?: pulumi.Input<string>;
    ebsVolumeCount?: pulumi.Input<number>;
    ebsVolumeSize?: pulumi.Input<number>;
    ebsVolumeType?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    instanceProfileArn?: pulumi.Input<string>;
    spotBidPricePercent?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface PipelineClusterAzureAttributes {
    availability?: pulumi.Input<string>;
    firstOnDemand?: pulumi.Input<number>;
    spotBidMaxPrice?: pulumi.Input<number>;
}

export interface PipelineClusterClusterLogConf {
    dbfs?: pulumi.Input<inputs.PipelineClusterClusterLogConfDbfs>;
    s3?: pulumi.Input<inputs.PipelineClusterClusterLogConfS3>;
}

export interface PipelineClusterClusterLogConfDbfs {
    destination: pulumi.Input<string>;
}

export interface PipelineClusterClusterLogConfS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface PipelineClusterGcpAttributes {
    availability?: pulumi.Input<string>;
    googleServiceAccount?: pulumi.Input<string>;
    localSsdCount?: pulumi.Input<number>;
    zoneId?: pulumi.Input<string>;
}

export interface PipelineClusterInitScript {
    abfss?: pulumi.Input<inputs.PipelineClusterInitScriptAbfss>;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: pulumi.Input<inputs.PipelineClusterInitScriptDbfs>;
    file?: pulumi.Input<inputs.PipelineClusterInitScriptFile>;
    gcs?: pulumi.Input<inputs.PipelineClusterInitScriptGcs>;
    s3?: pulumi.Input<inputs.PipelineClusterInitScriptS3>;
    volumes?: pulumi.Input<inputs.PipelineClusterInitScriptVolumes>;
    workspace?: pulumi.Input<inputs.PipelineClusterInitScriptWorkspace>;
}

export interface PipelineClusterInitScriptAbfss {
    destination?: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptDbfs {
    destination: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptFile {
    destination?: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptGcs {
    destination?: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptS3 {
    cannedAcl?: pulumi.Input<string>;
    destination: pulumi.Input<string>;
    enableEncryption?: pulumi.Input<boolean>;
    encryptionType?: pulumi.Input<string>;
    endpoint?: pulumi.Input<string>;
    kmsKey?: pulumi.Input<string>;
    region?: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptVolumes {
    destination?: pulumi.Input<string>;
}

export interface PipelineClusterInitScriptWorkspace {
    destination?: pulumi.Input<string>;
}

export interface PipelineFilters {
    excludes?: pulumi.Input<pulumi.Input<string>[]>;
    includes?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface PipelineLibrary {
    file?: pulumi.Input<inputs.PipelineLibraryFile>;
    jar?: pulumi.Input<string>;
    maven?: pulumi.Input<inputs.PipelineLibraryMaven>;
    notebook?: pulumi.Input<inputs.PipelineLibraryNotebook>;
    whl?: pulumi.Input<string>;
}

export interface PipelineLibraryFile {
    path: pulumi.Input<string>;
}

export interface PipelineLibraryMaven {
    coordinates: pulumi.Input<string>;
    exclusions?: pulumi.Input<pulumi.Input<string>[]>;
    repo?: pulumi.Input<string>;
}

export interface PipelineLibraryNotebook {
    path: pulumi.Input<string>;
}

export interface PipelineNotification {
    /**
     * non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
     * * `on-update-success` - a pipeline update completes successfully.
     * * `on-update-failure` - a pipeline update fails with a retryable error.
     * * `on-update-fatal-failure` - a pipeline update fails with a non-retryable (fatal) error.
     * * `on-flow-failure` - a single data flow fails.
     */
    alerts: pulumi.Input<pulumi.Input<string>[]>;
    /**
     * non-empty list of emails to notify.
     */
    emailRecipients: pulumi.Input<pulumi.Input<string>[]>;
}

export interface RecipientIpAccessList {
    /**
     * Allowed IP Addresses in CIDR notation. Limit of 100.
     */
    allowedIpAddresses: pulumi.Input<pulumi.Input<string>[]>;
}

export interface RecipientToken {
    /**
     * Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.
     */
    activationUrl?: pulumi.Input<string>;
    /**
     * Time at which this recipient Token was created, in epoch milliseconds.
     */
    createdAt?: pulumi.Input<number>;
    /**
     * Username of recipient token creator.
     */
    createdBy?: pulumi.Input<string>;
    /**
     * Expiration timestamp of the token in epoch milliseconds.
     */
    expirationTime?: pulumi.Input<number>;
    /**
     * ID of this recipient - same as the `name`.
     */
    id?: pulumi.Input<string>;
    /**
     * Time at which this recipient Token was updated, in epoch milliseconds.
     */
    updatedAt?: pulumi.Input<number>;
    /**
     * Username of recipient Token updater.
     */
    updatedBy?: pulumi.Input<string>;
}

export interface RepoSparseCheckout {
    patterns: pulumi.Input<pulumi.Input<string>[]>;
}

export interface SecretScopeKeyvaultMetadata {
    dnsName: pulumi.Input<string>;
    resourceId: pulumi.Input<string>;
}

export interface ShareObject {
    addedAt?: pulumi.Input<number>;
    addedBy?: pulumi.Input<string>;
    /**
     * Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field `historyDataSharingStatus` can not be set.
     */
    cdfEnabled?: pulumi.Input<boolean>;
    /**
     * Description about the object.
     */
    comment?: pulumi.Input<string>;
    /**
     * Type of the object, currently only `TABLE` is allowed.
     */
    dataObjectType: pulumi.Input<string>;
    /**
     * Whether to enable history sharing, one of: `ENABLED`, `DISABLED`. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. *NOTE*: The startVersion should be less than or equal the current version of the object. When this field is set, field `cdfEnabled` can not be set.
     *
     * To share only part of a table when you add the table to a share, you can provide partition specifications. This is specified by a number of `partition` blocks. Each entry in `partition` block takes a list of `value` blocks. The field is documented below.
     */
    historyDataSharingStatus?: pulumi.Input<string>;
    /**
     * Full name of the object, e.g. `catalog.schema.name` for a table.
     */
    name: pulumi.Input<string>;
    partitions?: pulumi.Input<pulumi.Input<inputs.ShareObjectPartition>[]>;
    /**
     * A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the `sharedAs` name. The `sharedAs` name must be unique within a Share. Change forces creation of a new resource.
     */
    sharedAs?: pulumi.Input<string>;
    /**
     * The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.
     */
    startVersion?: pulumi.Input<number>;
    /**
     * Status of the object, one of: `ACTIVE`, `PERMISSION_DENIED`.
     */
    status?: pulumi.Input<string>;
}

export interface ShareObjectPartition {
    values: pulumi.Input<pulumi.Input<inputs.ShareObjectPartitionValue>[]>;
}

export interface ShareObjectPartitionValue {
    /**
     * The name of the partition column.
     */
    name: pulumi.Input<string>;
    /**
     * The operator to apply for the value, one of: `EQUAL`, `LIKE`
     */
    op: pulumi.Input<string>;
    /**
     * The key of a Delta Sharing recipient's property. For example `databricks-account-id`. When this field is set, field `value` can not be set.
     */
    recipientPropertyKey?: pulumi.Input<string>;
    /**
     * The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipientPropertyKey` can not be set.
     */
    value?: pulumi.Input<string>;
}

export interface SqlAlertOptions {
    /**
     * Name of column in the query result to compare in alert evaluation.
     */
    column: pulumi.Input<string>;
    /**
     * Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.
     */
    customBody?: pulumi.Input<string>;
    /**
     * Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.
     */
    customSubject?: pulumi.Input<string>;
    /**
     * Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.
     */
    muted?: pulumi.Input<boolean>;
    /**
     * Operator used to compare in alert evaluation. (Enum: `>`, `>=`, `<`, `<=`, `==`, `!=`)
     */
    op: pulumi.Input<string>;
    /**
     * Value used to compare in alert evaluation.
     */
    value: pulumi.Input<string>;
}

export interface SqlEndpointChannel {
    /**
     * Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.
     */
    name?: pulumi.Input<string>;
}

export interface SqlEndpointOdbcParams {
    host?: pulumi.Input<string>;
    hostname?: pulumi.Input<string>;
    path: pulumi.Input<string>;
    port: pulumi.Input<number>;
    protocol: pulumi.Input<string>;
}

export interface SqlEndpointTags {
    customTags: pulumi.Input<pulumi.Input<inputs.SqlEndpointTagsCustomTag>[]>;
}

export interface SqlEndpointTagsCustomTag {
    key: pulumi.Input<string>;
    value: pulumi.Input<string>;
}

export interface SqlPermissionsPrivilegeAssignment {
    /**
     * `displayName` for a databricks.Group or databricks_user, `applicationId` for a databricks_service_principal.
     */
    principal: pulumi.Input<string>;
    /**
     * set of available privilege names in upper case.
     *
     * [Available](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html) privilege names are:
     */
    privileges: pulumi.Input<pulumi.Input<string>[]>;
}

export interface SqlQueryParameter {
    date?: pulumi.Input<inputs.SqlQueryParameterDate>;
    dateRange?: pulumi.Input<inputs.SqlQueryParameterDateRange>;
    datetime?: pulumi.Input<inputs.SqlQueryParameterDatetime>;
    datetimeRange?: pulumi.Input<inputs.SqlQueryParameterDatetimeRange>;
    datetimesec?: pulumi.Input<inputs.SqlQueryParameterDatetimesec>;
    datetimesecRange?: pulumi.Input<inputs.SqlQueryParameterDatetimesecRange>;
    enum?: pulumi.Input<inputs.SqlQueryParameterEnum>;
    name: pulumi.Input<string>;
    number?: pulumi.Input<inputs.SqlQueryParameterNumber>;
    query?: pulumi.Input<inputs.SqlQueryParameterQuery>;
    text?: pulumi.Input<inputs.SqlQueryParameterText>;
    title?: pulumi.Input<string>;
}

export interface SqlQueryParameterDate {
    value: pulumi.Input<string>;
}

export interface SqlQueryParameterDateRange {
    range?: pulumi.Input<inputs.SqlQueryParameterDateRangeRange>;
    value?: pulumi.Input<string>;
}

export interface SqlQueryParameterDateRangeRange {
    end: pulumi.Input<string>;
    start: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetime {
    value: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetimeRange {
    range?: pulumi.Input<inputs.SqlQueryParameterDatetimeRangeRange>;
    value?: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetimeRangeRange {
    end: pulumi.Input<string>;
    start: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetimesec {
    value: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetimesecRange {
    range?: pulumi.Input<inputs.SqlQueryParameterDatetimesecRangeRange>;
    value?: pulumi.Input<string>;
}

export interface SqlQueryParameterDatetimesecRangeRange {
    end: pulumi.Input<string>;
    start: pulumi.Input<string>;
}

export interface SqlQueryParameterEnum {
    multiple?: pulumi.Input<inputs.SqlQueryParameterEnumMultiple>;
    options: pulumi.Input<pulumi.Input<string>[]>;
    value?: pulumi.Input<string>;
    values?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface SqlQueryParameterEnumMultiple {
    prefix: pulumi.Input<string>;
    separator: pulumi.Input<string>;
    suffix: pulumi.Input<string>;
}

export interface SqlQueryParameterNumber {
    value: pulumi.Input<number>;
}

export interface SqlQueryParameterQuery {
    multiple?: pulumi.Input<inputs.SqlQueryParameterQueryMultiple>;
    queryId: pulumi.Input<string>;
    value?: pulumi.Input<string>;
    values?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface SqlQueryParameterQueryMultiple {
    prefix: pulumi.Input<string>;
    separator: pulumi.Input<string>;
    suffix: pulumi.Input<string>;
}

export interface SqlQueryParameterText {
    value: pulumi.Input<string>;
}

export interface SqlQuerySchedule {
    continuous?: pulumi.Input<inputs.SqlQueryScheduleContinuous>;
    daily?: pulumi.Input<inputs.SqlQueryScheduleDaily>;
    weekly?: pulumi.Input<inputs.SqlQueryScheduleWeekly>;
}

export interface SqlQueryScheduleContinuous {
    intervalSeconds: pulumi.Input<number>;
    untilDate?: pulumi.Input<string>;
}

export interface SqlQueryScheduleDaily {
    intervalDays: pulumi.Input<number>;
    timeOfDay: pulumi.Input<string>;
    untilDate?: pulumi.Input<string>;
}

export interface SqlQueryScheduleWeekly {
    dayOfWeek: pulumi.Input<string>;
    intervalWeeks: pulumi.Input<number>;
    timeOfDay: pulumi.Input<string>;
    untilDate?: pulumi.Input<string>;
}

export interface SqlTableColumn {
    /**
     * User-supplied free-form text.
     */
    comment?: pulumi.Input<string>;
    /**
     * User-visible name of column
     */
    name: pulumi.Input<string>;
    /**
     * Whether field is nullable (Default: `true`)
     */
    nullable?: pulumi.Input<boolean>;
    /**
     * Column type spec (with metadata) as SQL text. Not supported for `VIEW` table_type.
     */
    type?: pulumi.Input<string>;
}

export interface SqlWidgetParameter {
    mapTo?: pulumi.Input<string>;
    name: pulumi.Input<string>;
    title?: pulumi.Input<string>;
    type: pulumi.Input<string>;
    value?: pulumi.Input<string>;
    values?: pulumi.Input<pulumi.Input<string>[]>;
}

export interface SqlWidgetPosition {
    autoHeight?: pulumi.Input<boolean>;
    posX?: pulumi.Input<number>;
    posY?: pulumi.Input<number>;
    sizeX: pulumi.Input<number>;
    sizeY: pulumi.Input<number>;
}

export interface StorageCredentialAwsIamRole {
    /**
     * The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`
     *
     * `azureManagedIdentity` optional configuration block for using managed identity as credential details for Azure (recommended over service principal):
     */
    roleArn: pulumi.Input<string>;
}

export interface StorageCredentialAzureManagedIdentity {
    /**
     * The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.
     */
    accessConnectorId: pulumi.Input<string>;
    credentialId?: pulumi.Input<string>;
    /**
     * The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.
     *
     * `azureServicePrincipal` optional configuration block to use service principal as credential details for Azure:
     */
    managedIdentityId?: pulumi.Input<string>;
}

export interface StorageCredentialAzureServicePrincipal {
    /**
     * The application ID of the application registration within the referenced AAD tenant
     */
    applicationId: pulumi.Input<string>;
    /**
     * The client secret generated for the above app ID in AAD. **This field is redacted on output**
     *
     * `databricksGcpServiceAccount` optional configuration block for creating a Databricks-managed GCP Service Account:
     */
    clientSecret: pulumi.Input<string>;
    /**
     * The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application
     */
    directoryId: pulumi.Input<string>;
}

export interface StorageCredentialDatabricksGcpServiceAccount {
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     */
    email?: pulumi.Input<string>;
}

export interface StorageCredentialGcpServiceAccountKey {
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     */
    email: pulumi.Input<string>;
    privateKey: pulumi.Input<string>;
    privateKeyId: pulumi.Input<string>;
}

export interface TableColumn {
    comment?: pulumi.Input<string>;
    name: pulumi.Input<string>;
    nullable?: pulumi.Input<boolean>;
    partitionIndex?: pulumi.Input<number>;
    position: pulumi.Input<number>;
    typeIntervalType?: pulumi.Input<string>;
    typeJson?: pulumi.Input<string>;
    typeName: pulumi.Input<string>;
    typePrecision?: pulumi.Input<number>;
    typeScale?: pulumi.Input<number>;
    typeText: pulumi.Input<string>;
}
