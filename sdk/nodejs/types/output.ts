// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "../types/input";
import * as outputs from "../types/output";

export interface AccessControlRuleSetGrantRule {
    /**
     * a list of principals who are granted a role. The following format is supported:
     * * `users/{username}` (also exposed as `aclPrincipalId` attribute of `databricks.User` resource).
     * * `groups/{groupname}` (also exposed as `aclPrincipalId` attribute of `databricks.Group` resource).
     * * `servicePrincipals/{applicationId}` (also exposed as `aclPrincipalId` attribute of `databricks.ServicePrincipal` resource).
     */
    principals?: string[];
    /**
     * Role to be granted. The supported roles are listed below. For more information about these roles, refer to [service principal roles](https://docs.databricks.com/security/auth-authz/access-control/service-principal-acl.html#service-principal-roles), [group roles](https://docs.databricks.com/en/administration-guide/users-groups/groups.html#manage-roles-on-an-account-group-using-the-workspace-admin-settings-page), [marketplace roles](https://docs.databricks.com/en/marketplace/get-started-provider.html#assign-the-marketplace-admin-role) or [budget policy permissions](https://docs.databricks.com/aws/en/admin/usage/budget-policies#manage-budget-policy-permissions), depending on the `name` defined:
     * * `accounts/{account_id}/ruleSets/default`
     * * `roles/marketplace.admin` - Databricks Marketplace administrator.
     * * `roles/billing.admin` - Billing administrator.
     * * `accounts/{account_id}/servicePrincipals/{service_principal_application_id}/ruleSets/default`
     * * `roles/servicePrincipal.manager` - Manager of a service principal.
     * * `roles/servicePrincipal.user` - User of a service principal.
     * * `accounts/{account_id}/groups/{group_id}/ruleSets/default`
     * * `roles/group.manager` - Manager of a group.
     * * `accounts/{account_id}/budgetPolicies/{budget_policy_id}/ruleSets/default`
     * * `roles/budgetPolicy.manager` - Manager of a budget policy.
     * * `roles/budgetPolicy.user` - User of a budget policy.
     */
    role: string;
}

export interface AibiDashboardEmbeddingAccessPolicySettingAibiDashboardEmbeddingAccessPolicy {
    /**
     * Configured embedding policy. Possible values are `ALLOW_ALL_DOMAINS`, `ALLOW_APPROVED_DOMAINS`, `DENY_ALL_DOMAINS`.
     */
    accessPolicyType: string;
}

export interface AibiDashboardEmbeddingApprovedDomainsSettingAibiDashboardEmbeddingApprovedDomains {
    /**
     * the list of approved domains. To allow all subdomains for a given domain, use a wildcard symbol (`*`) before the domain name, i.e., `*.databricks.com` will allow to embed into any site under the `databricks.com`.
     */
    approvedDomains: string[];
}

export interface AlertCondition {
    /**
     * Alert state if the result is empty (`UNKNOWN`, `OK`, `TRIGGERED`)
     */
    emptyResultState?: string;
    /**
     * Operator used for comparison in alert evaluation. (Enum: `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `EQUAL`, `NOT_EQUAL`, `IS_NULL`)
     */
    op: string;
    /**
     * Name of the column from the query result to use for comparison in alert evaluation:
     */
    operand: outputs.AlertConditionOperand;
    /**
     * Threshold value used for comparison in alert evaluation:
     */
    threshold?: outputs.AlertConditionThreshold;
}

export interface AlertConditionOperand {
    /**
     * Block describing the column from the query result to use for comparison in alert evaluation:
     */
    column: outputs.AlertConditionOperandColumn;
}

export interface AlertConditionOperandColumn {
    /**
     * Name of the column.
     */
    name: string;
}

export interface AlertConditionThreshold {
    /**
     * actual value used in comparison (one of the attributes is required):
     */
    value: outputs.AlertConditionThresholdValue;
}

export interface AlertConditionThresholdValue {
    /**
     * boolean value (`true` or `false`) to compare against boolean results.
     */
    boolValue?: boolean;
    /**
     * double value to compare against integer and double results.
     */
    doubleValue?: number;
    /**
     * string value to compare against string results.
     */
    stringValue?: string;
}

export interface AppActiveDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.AppActiveDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.AppActiveDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface AppActiveDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface AppActiveDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface AppAppStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface AppComputeStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface AppPendingDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.AppPendingDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.AppPendingDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface AppPendingDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface AppPendingDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface AppResource {
    /**
     * The description of the resource.
     *
     * Exactly one of the following attributes must be provided:
     */
    description?: string;
    /**
     * attribute
     */
    job?: outputs.AppResourceJob;
    /**
     * The name of the resource.
     */
    name: string;
    /**
     * attribute
     */
    secret?: outputs.AppResourceSecret;
    /**
     * attribute
     */
    servingEndpoint?: outputs.AppResourceServingEndpoint;
    /**
     * attribute
     */
    sqlWarehouse?: outputs.AppResourceSqlWarehouse;
}

export interface AppResourceJob {
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface AppResourceSecret {
    /**
     * Key of the secret to grant permission on.
     */
    key: string;
    /**
     * Permission to grant on the secret scope. For secrets, only one permission is allowed. Permission must be one of: `READ`, `WRITE`, `MANAGE`.
     */
    permission: string;
    /**
     * Scope of the secret to grant permission on.
     */
    scope: string;
}

export interface AppResourceServingEndpoint {
    /**
     * Name of the serving endpoint to grant permission on.
     */
    name: string;
    /**
     * Permission to grant on the serving endpoint. Supported permissions are: `CAN_MANAGE`, `CAN_QUERY`, `CAN_VIEW`.
     */
    permission: string;
}

export interface AppResourceSqlWarehouse {
    /**
     * Id of the SQL warehouse to grant permission on.
     */
    id: string;
    /**
     * Permission to grant on the SQL warehouse. Supported permissions are: `CAN_MANAGE`, `CAN_USE`, `IS_OWNER`.
     */
    permission: string;
}

export interface ArtifactAllowlistArtifactMatcher {
    /**
     * The artifact path or maven coordinate.
     */
    artifact: string;
    /**
     * The pattern matching type of the artifact. Only `PREFIX_MATCH` is supported.
     */
    matchType: string;
}

export interface AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspace {
    canToggle?: boolean;
    enabled: boolean;
    enablementDetails: outputs.AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails;
    maintenanceWindow?: outputs.AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow;
    restartEvenIfNoUpdatesAvailable?: boolean;
}

export interface AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceEnablementDetails {
    forcedForComplianceMode?: boolean;
    unavailableForDisabledEntitlement?: boolean;
    unavailableForNonEnterpriseTier?: boolean;
}

export interface AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindow {
    weekDayBasedSchedule?: outputs.AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule;
}

export interface AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedSchedule {
    dayOfWeek: string;
    frequency: string;
    windowStartTime?: outputs.AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime;
}

export interface AutomaticClusterUpdateWorkspaceSettingAutomaticClusterUpdateWorkspaceMaintenanceWindowWeekDayBasedScheduleWindowStartTime {
    hours: number;
    minutes: number;
}

export interface BudgetAlertConfiguration {
    /**
     * List of action configurations to take when the budget alert is triggered. Consists of the following fields:
     */
    actionConfigurations?: outputs.BudgetAlertConfigurationActionConfiguration[];
    alertConfigurationId: string;
    /**
     * The threshold for the budget alert to determine if it is in a triggered state. The number is evaluated based on `quantityType`.
     */
    quantityThreshold?: string;
    /**
     * The way to calculate cost for this budget alert. This is what quantityThreshold is measured in. (Enum: `LIST_PRICE_DOLLARS_USD`)
     */
    quantityType?: string;
    /**
     * The time window of usage data for the budget. (Enum: `MONTH`)
     */
    timePeriod?: string;
    /**
     * The evaluation method to determine when this budget alert is in a triggered state. (Enum: `CUMULATIVE_SPENDING_EXCEEDED`)
     */
    triggerType?: string;
}

export interface BudgetAlertConfigurationActionConfiguration {
    actionConfigurationId: string;
    /**
     * The type of action to take when the budget alert is triggered. (Enum: `EMAIL_NOTIFICATION`)
     */
    actionType?: string;
    /**
     * The target of the action. For `EMAIL_NOTIFICATION`, this is the email address to send the notification to.
     */
    target?: string;
}

export interface BudgetFilter {
    /**
     * List of tags to filter by. Consists of the following fields:
     */
    tags?: outputs.BudgetFilterTag[];
    /**
     * Filter by workspace ID (if empty, include usage all usage for this account). Consists of the following fields:
     */
    workspaceId?: outputs.BudgetFilterWorkspaceId;
}

export interface BudgetFilterTag {
    /**
     * The key of the tag.
     */
    key?: string;
    /**
     * Consists of the following fields:
     */
    value?: outputs.BudgetFilterTagValue;
}

export interface BudgetFilterTagValue {
    /**
     * The operator to use for the filter. (Enum: `IN`)
     */
    operator?: string;
    /**
     * The values to filter by.
     */
    values?: string[];
}

export interface BudgetFilterWorkspaceId {
    /**
     * The operator to use for the filter. (Enum: `IN`)
     */
    operator?: string;
    /**
     * The values to filter by.
     */
    values?: number[];
}

export interface BudgetPolicyCustomTag {
    /**
     * The key of the tag. - Must be unique among all custom tags of the same policy. Cannot be “budget-policy-name”, “budget-policy-id” or "budget-policy-resolution-result" as these tags are preserved.
     */
    key: string;
    /**
     * The value of the tag.
     */
    value?: string;
}

export interface ClusterAutoscale {
    /**
     * The maximum number of workers to which the cluster can scale up when overloaded. maxWorkers must be strictly greater than min_workers.
     *
     * When using a [Single Node cluster](https://docs.databricks.com/clusters/single-node.html), `numWorkers` needs to be `0`. It can be set to `0` explicitly, or simply not specified, as it defaults to `0`.  When `numWorkers` is `0`, provider checks for presence of the required Spark configurations:
     *
     * * `spark.master` must have prefix `local`, like `local[*]`
     * * `spark.databricks.cluster.profile` must have value `singleNode`
     *
     * and also `customTag` entry:
     *
     * * `"ResourceClass" = "SingleNode"`
     *
     * The following example demonstrates how to create an single node cluster:
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const smallest = databricks.getNodeType({
     *     localDisk: true,
     * });
     * const latestLts = databricks.getSparkVersion({
     *     longTermSupport: true,
     * });
     * const singleNode = new databricks.Cluster("single_node", {
     *     clusterName: "Single Node",
     *     sparkVersion: latestLts.then(latestLts => latestLts.id),
     *     nodeTypeId: smallest.then(smallest => smallest.id),
     *     autoterminationMinutes: 20,
     *     sparkConf: {
     *         "spark.databricks.cluster.profile": "singleNode",
     *         "spark.master": "local[*]",
     *     },
     *     customTags: {
     *         ResourceClass: "SingleNode",
     *     },
     * });
     * ```
     */
    maxWorkers?: number;
    /**
     * The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
     */
    minWorkers?: number;
}

export interface ClusterAwsAttributes {
    /**
     * Availability type used for all subsequent nodes past the `firstOnDemand` ones. Valid values are `SPOT`, `SPOT_WITH_FALLBACK` and `ON_DEMAND`. Note: If `firstOnDemand` is zero, this availability type will be used for the entire cluster. Backend default value is `SPOT_WITH_FALLBACK` and could change in the future
     */
    availability?: string;
    /**
     * The number of volumes launched for each instance. You can choose up to 10 volumes. This feature is only enabled for supported node types. Legacy node types cannot specify custom EBS volumes. For node types with no instance store, at least one EBS volume needs to be specified; otherwise, cluster creation will fail. These EBS volumes will be mounted at /ebs0, /ebs1, and etc. Instance store volumes will be mounted at /local_disk0, /local_disk1, and etc. If EBS volumes are attached, Databricks will configure Spark to use only the EBS volumes for scratch storage because heterogeneously sized scratch devices can lead to inefficient disk utilization. If no EBS volumes are attached, Databricks will configure Spark to use instance store volumes. If EBS volumes are specified, then the Spark configuration spark.local.dir will be overridden.
     */
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    /**
     * The size of each EBS volume (in GiB) launched for each instance. For general purpose SSD, this value must be within the range 100 - 4096. For throughput optimized HDD, this value must be within the range 500 - 4096. Custom EBS volumes cannot be specified for the legacy node types (memory-optimized and compute-optimized).
     */
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    /**
     * The type of EBS volumes that will be launched with this cluster. Valid values are `GENERAL_PURPOSE_SSD` or `THROUGHPUT_OPTIMIZED_HDD`. Use this option only if you're not picking *Delta Optimized `i3.*`* node types.
     */
    ebsVolumeType?: string;
    /**
     * The first `firstOnDemand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `firstOnDemand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster. If unspecified, the default value is 0.
     */
    firstOnDemand?: number;
    /**
     * Nodes for this cluster will only be placed on AWS instances with this instance profile. Please see databricks.InstanceProfile resource documentation for extended examples on adding a valid instance profile using Pulumi.
     */
    instanceProfileArn?: string;
    /**
     * The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the cluster needs a new `i3.xlarge` spot instance, then the max price is half of the price of on-demand `i3.xlarge` instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand `i3.xlarge` instances. If not specified, the default value is `100`. When spot instances are requested for this cluster, only spot instances whose max price percentage matches this field will be considered. For safety, we enforce this field to be no more than `10000`.
     */
    spotBidPricePercent?: number;
    /**
     * Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-west-2a`. The provided availability zone must be in the same region as the Databricks deployment. For example, `us-west-2a` is not a valid zone ID if the Databricks deployment resides in the `us-east-1` region. Enable automatic availability zone selection ("Auto-AZ"), by setting the value `auto`. Databricks selects the AZ based on available IPs in the workspace subnets and retries in other availability zones if AWS returns insufficient capacity errors.
     */
    zoneId?: string;
}

export interface ClusterAzureAttributes {
    /**
     * Availability type used for all subsequent nodes past the `firstOnDemand` ones. Valid values are `SPOT_AZURE`, `SPOT_WITH_FALLBACK_AZURE`, and `ON_DEMAND_AZURE`. Note: If `firstOnDemand` is zero, this availability type will be used for the entire cluster.
     */
    availability?: string;
    /**
     * The first `firstOnDemand` nodes of the cluster will be placed on on-demand instances. If this value is greater than 0, the cluster driver node will be placed on an on-demand instance. If this value is greater than or equal to the current cluster size, all nodes will be placed on on-demand instances. If this value is less than the current cluster size, `firstOnDemand` nodes will be placed on on-demand instances, and the remainder will be placed on availability instances. This value does not affect cluster size and cannot be mutated over the lifetime of a cluster.
     */
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.ClusterAzureAttributesLogAnalyticsInfo;
    /**
     * The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to `-1`, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.
     */
    spotBidMaxPrice?: number;
}

export interface ClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface ClusterClusterLogConf {
    dbfs?: outputs.ClusterClusterLogConfDbfs;
    s3?: outputs.ClusterClusterLogConfS3;
    volumes?: outputs.ClusterClusterLogConfVolumes;
}

export interface ClusterClusterLogConfDbfs {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterClusterLogConfS3 {
    /**
     * Set canned access control list, e.g. `bucket-owner-full-control`. If `cannedCal` is set, the cluster instance profile must have `s3:PutObjectAcl` permission on the destination bucket and prefix. The full list of possible canned ACLs can be found [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl). By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set `bucket-owner-full-control` to make bucket owners able to read the logs.
     */
    cannedAcl?: string;
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
    /**
     * Enable server-side encryption, false by default.
     */
    enableEncryption?: boolean;
    /**
     * The encryption type, it could be `sse-s3` or `sse-kms`. It is used only when encryption is enabled, and the default type is `sse-s3`.
     */
    encryptionType?: string;
    /**
     * S3 endpoint, e.g. <https://s3-us-west-2.amazonaws.com>. Either `region` or `endpoint` needs to be set. If both are set, the endpoint is used.
     */
    endpoint?: string;
    /**
     * KMS key used if encryption is enabled and encryption type is set to `sse-kms`.
     */
    kmsKey?: string;
    /**
     * S3 region, e.g. `us-west-2`. Either `region` or `endpoint` must be set. If both are set, the endpoint is used.
     */
    region?: string;
}

export interface ClusterClusterLogConfVolumes {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterClusterMountInfo {
    /**
     * path inside the Spark container.
     *
     * For example, you can mount Azure Data Lake Storage container using the following code:
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const storageAccount = "ewfw3ggwegwg";
     * const storageContainer = "test";
     * const withNfs = new databricks.Cluster("with_nfs", {clusterMountInfos: [{
     *     networkFilesystemInfo: {
     *         serverAddress: `${storageAccount}.blob.core.windows.net`,
     *         mountOptions: "sec=sys,vers=3,nolock,proto=tcp",
     *     },
     *     remoteMountDirPath: `${storageAccount}/${storageContainer}`,
     *     localMountDirPath: "/mnt/nfs-test",
     * }]});
     * ```
     */
    localMountDirPath: string;
    /**
     * block specifying connection. It consists of:
     */
    networkFilesystemInfo: outputs.ClusterClusterMountInfoNetworkFilesystemInfo;
    /**
     * string specifying path to mount on the remote service.
     */
    remoteMountDirPath?: string;
}

export interface ClusterClusterMountInfoNetworkFilesystemInfo {
    /**
     * string that will be passed as options passed to the `mount` command.
     */
    mountOptions?: string;
    /**
     * host name.
     */
    serverAddress: string;
}

export interface ClusterDockerImage {
    /**
     * `basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.
     *
     * Example usage with azurermContainerRegistry and docker_registry_image, that you can adapt to your specific use-case:
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     * import * as docker from "@pulumi/docker";
     *
     * const _this = new docker.index.RegistryImage("this", {
     *     build: [{}],
     *     name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,
     * });
     * const thisCluster = new databricks.Cluster("this", {dockerImage: {
     *     url: _this.name,
     *     basicAuth: {
     *         username: thisAzurermContainerRegistry.adminUsername,
     *         password: thisAzurermContainerRegistry.adminPassword,
     *     },
     * }});
     * ```
     */
    basicAuth?: outputs.ClusterDockerImageBasicAuth;
    /**
     * URL for the Docker image
     */
    url: string;
}

export interface ClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface ClusterGcpAttributes {
    /**
     * Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.
     */
    availability?: string;
    /**
     * Boot disk size in GB
     */
    bootDiskSize?: number;
    /**
     * Google Service Account email address that the cluster uses to authenticate with Google Identity. This field is used for authentication with the GCS and BigQuery data sources.
     */
    googleServiceAccount?: string;
    /**
     * Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
     */
    localSsdCount?: number;
    /**
     * if we should use preemptible executors ([GCP documentation](https://cloud.google.com/compute/docs/instances/preemptible)). *Warning: this field is deprecated in favor of `availability`, and will be removed soon.*
     */
    usePreemptibleExecutors?: boolean;
    /**
     * Identifier for the availability zone in which the cluster resides. This can be one of the following:
     * * `HA` (default): High availability, spread nodes across availability zones for a Databricks deployment region.
     * * `AUTO`: Databricks picks an availability zone to schedule the cluster on.
     * * name of a GCP availability zone: pick one of the available zones from the [list of available availability zones](https://cloud.google.com/compute/docs/regions-zones#available).
     */
    zoneId?: string;
}

export interface ClusterInitScript {
    abfss?: outputs.ClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.ClusterInitScriptDbfs;
    file?: outputs.ClusterInitScriptFile;
    gcs?: outputs.ClusterInitScriptGcs;
    s3?: outputs.ClusterInitScriptS3;
    volumes?: outputs.ClusterInitScriptVolumes;
    workspace?: outputs.ClusterInitScriptWorkspace;
}

export interface ClusterInitScriptAbfss {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterInitScriptDbfs {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterInitScriptFile {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterInitScriptGcs {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterInitScriptS3 {
    /**
     * Set canned access control list, e.g. `bucket-owner-full-control`. If `cannedCal` is set, the cluster instance profile must have `s3:PutObjectAcl` permission on the destination bucket and prefix. The full list of possible canned ACLs can be found [here](https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html#canned-acl). By default, only the object owner gets full control. If you are using a cross-account role for writing data, you may want to set `bucket-owner-full-control` to make bucket owners able to read the logs.
     */
    cannedAcl?: string;
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
    /**
     * Enable server-side encryption, false by default.
     */
    enableEncryption?: boolean;
    /**
     * The encryption type, it could be `sse-s3` or `sse-kms`. It is used only when encryption is enabled, and the default type is `sse-s3`.
     */
    encryptionType?: string;
    /**
     * S3 endpoint, e.g. <https://s3-us-west-2.amazonaws.com>. Either `region` or `endpoint` needs to be set. If both are set, the endpoint is used.
     */
    endpoint?: string;
    /**
     * KMS key used if encryption is enabled and encryption type is set to `sse-kms`.
     */
    kmsKey?: string;
    /**
     * S3 region, e.g. `us-west-2`. Either `region` or `endpoint` must be set. If both are set, the endpoint is used.
     */
    region?: string;
}

export interface ClusterInitScriptVolumes {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterInitScriptWorkspace {
    /**
     * S3 destination, e.g., `s3://my-bucket/some-prefix` You must configure the cluster with an instance profile, and the instance profile must have write access to the destination. You cannot use AWS keys.
     */
    destination: string;
}

export interface ClusterLibrary {
    cran?: outputs.ClusterLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.ClusterLibraryMaven;
    pypi?: outputs.ClusterLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface ClusterLibraryCran {
    package: string;
    repo?: string;
}

export interface ClusterLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface ClusterLibraryPypi {
    package: string;
    repo?: string;
}

export interface ClusterPolicyLibrary {
    cran?: outputs.ClusterPolicyLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.ClusterPolicyLibraryMaven;
    pypi?: outputs.ClusterPolicyLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface ClusterPolicyLibraryCran {
    package: string;
    repo?: string;
}

export interface ClusterPolicyLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface ClusterPolicyLibraryPypi {
    package: string;
    repo?: string;
}

export interface ClusterWorkloadType {
    clients: outputs.ClusterWorkloadTypeClients;
}

export interface ClusterWorkloadTypeClients {
    /**
     * boolean flag defining if it's possible to run Databricks Jobs on this cluster. Default: `true`.
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const withNfs = new databricks.Cluster("with_nfs", {workloadType: {
     *     clients: {
     *         jobs: false,
     *         notebooks: true,
     *     },
     * }});
     * ```
     */
    jobs?: boolean;
    /**
     * boolean flag defining if it's possible to run notebooks on this cluster. Default: `true`.
     */
    notebooks?: boolean;
}

export interface ComplianceSecurityProfileWorkspaceSettingComplianceSecurityProfileWorkspace {
    complianceStandards: string[];
    isEnabled: boolean;
}

export interface ConnectionProvisioningInfo {
    state?: string;
}

export interface CredentialAwsIamRole {
    externalId: string;
    /**
     * The Amazon Resource Name (ARN) of the AWS IAM role you want to use to setup the trust policy, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`
     *
     * `azureManagedIdentity` optional configuration block for using managed identity as credential details for Azure (recommended over `azureServicePrincipal`):
     */
    roleArn?: string;
    unityCatalogIamArn: string;
}

export interface CredentialAzureManagedIdentity {
    /**
     * The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.
     */
    accessConnectorId: string;
    /**
     * Unique ID of the credential.
     */
    credentialId: string;
    /**
     * The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.
     *
     * `azureServicePrincipal` optional configuration block to use service principal as credential details for Azure. Only applicable when purpose is `STORAGE` (Legacy):
     */
    managedIdentityId?: string;
}

export interface CredentialAzureServicePrincipal {
    /**
     * The application ID of the application registration within the referenced AAD tenant
     */
    applicationId: string;
    /**
     * The client secret generated for the above app ID in AAD. **This field is redacted on output**
     *
     * `databricksGcpServiceAccount` optional configuration block for creating a Databricks-managed GCP Service Account:
     */
    clientSecret: string;
    /**
     * The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application
     */
    directoryId: string;
}

export interface CredentialDatabricksGcpServiceAccount {
    /**
     * Unique ID of the credential.
     */
    credentialId: string;
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     */
    email: string;
    privateKeyId: string;
}

export interface CustomAppIntegrationTokenAccessPolicy {
    /**
     * access token time to live (TTL) in minutes.
     */
    accessTokenTtlInMinutes?: number;
    /**
     * refresh token TTL in minutes. The TTL of refresh token cannot be lower than TTL of access token.
     */
    refreshTokenTtlInMinutes?: number;
}

export interface DefaultNamespaceSettingNamespace {
    /**
     * The value for the setting.
     */
    value?: string;
}

export interface DisableLegacyAccessSettingDisableLegacyAccess {
    value: boolean;
}

export interface DisableLegacyDbfsSettingDisableLegacyDbfs {
    /**
     * The boolean value for the setting.
     */
    value: boolean;
}

export interface EnhancedSecurityMonitoringWorkspaceSettingEnhancedSecurityMonitoringWorkspace {
    isEnabled: boolean;
}

export interface ExternalLocationEncryptionDetails {
    sseEncryptionDetails?: outputs.ExternalLocationEncryptionDetailsSseEncryptionDetails;
}

export interface ExternalLocationEncryptionDetailsSseEncryptionDetails {
    algorithm?: string;
    awsKmsKeyArn?: string;
}

export interface GetAppApp {
    activeDeployment: outputs.GetAppAppActiveDeployment;
    /**
     * attribute
     */
    appStatus: outputs.GetAppAppAppStatus;
    /**
     * The Budget Policy ID set for this resource.
     */
    budgetPolicyId?: string;
    /**
     * attribute
     */
    computeStatus: outputs.GetAppAppComputeStatus;
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    /**
     * The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.
     */
    defaultSourceCodePath: string;
    /**
     * The description of the resource.
     */
    description?: string;
    /**
     * The effective budget policy ID.
     */
    effectiveBudgetPolicyId: string;
    /**
     * A list of effective api scopes granted to the user access token.
     */
    effectiveUserApiScopes: string[];
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * The name of the app.
     */
    name: string;
    oauth2AppClientId: string;
    oauth2AppIntegrationId: string;
    pendingDeployment: outputs.GetAppAppPendingDeployment;
    /**
     * A list of resources that the app have access to.
     */
    resources?: outputs.GetAppAppResource[];
    servicePrincipalClientId: string;
    /**
     * id of the app service principal
     */
    servicePrincipalId: number;
    /**
     * name of the app service principal
     */
    servicePrincipalName: string;
    /**
     * The update time of the app.
     */
    updateTime: string;
    /**
     * The email of the user that last updated the app.
     */
    updater: string;
    /**
     * The URL of the app once it is deployed.
     */
    url: string;
    userApiScopes?: string[];
}

export interface GetAppAppActiveDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.GetAppAppActiveDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.GetAppAppActiveDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface GetAppAppActiveDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface GetAppAppActiveDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppAppAppStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppAppComputeStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppAppPendingDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.GetAppAppPendingDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.GetAppAppPendingDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface GetAppAppPendingDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface GetAppAppPendingDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppAppResource {
    /**
     * The description of the resource.
     */
    description?: string;
    /**
     * attribute
     */
    job?: outputs.GetAppAppResourceJob;
    /**
     * The name of the app.
     */
    name: string;
    /**
     * attribute
     */
    secret?: outputs.GetAppAppResourceSecret;
    /**
     * attribute
     */
    servingEndpoint?: outputs.GetAppAppResourceServingEndpoint;
    /**
     * attribute
     */
    sqlWarehouse?: outputs.GetAppAppResourceSqlWarehouse;
}

export interface GetAppAppResourceJob {
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetAppAppResourceSecret {
    /**
     * Key of the secret to grant permission on.
     */
    key: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
    /**
     * Scope of the secret to grant permission on.
     */
    scope: string;
}

export interface GetAppAppResourceServingEndpoint {
    /**
     * The name of the app.
     */
    name: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetAppAppResourceSqlWarehouse {
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetAppsApp {
    activeDeployment: outputs.GetAppsAppActiveDeployment;
    /**
     * attribute
     */
    appStatus: outputs.GetAppsAppAppStatus;
    /**
     * The Budget Policy ID set for this resource.
     */
    budgetPolicyId?: string;
    /**
     * attribute
     */
    computeStatus: outputs.GetAppsAppComputeStatus;
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    /**
     * The default workspace file system path of the source code from which app deployment are created. This field tracks the workspace source code path of the last active deployment.
     */
    defaultSourceCodePath: string;
    /**
     * The description of the resource.
     */
    description?: string;
    /**
     * The effective budget policy ID.
     */
    effectiveBudgetPolicyId: string;
    /**
     * A list of effective api scopes granted to the user access token.
     */
    effectiveUserApiScopes: string[];
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Name of the serving endpoint to grant permission on.
     */
    name: string;
    oauth2AppClientId: string;
    oauth2AppIntegrationId: string;
    pendingDeployment: outputs.GetAppsAppPendingDeployment;
    /**
     * A list of resources that the app have access to.
     */
    resources?: outputs.GetAppsAppResource[];
    servicePrincipalClientId: string;
    /**
     * id of the app service principal
     */
    servicePrincipalId: number;
    /**
     * name of the app service principal
     */
    servicePrincipalName: string;
    /**
     * The update time of the app.
     */
    updateTime: string;
    /**
     * The email of the user that last updated the app.
     */
    updater: string;
    /**
     * The URL of the app once it is deployed.
     */
    url: string;
    userApiScopes?: string[];
}

export interface GetAppsAppActiveDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.GetAppsAppActiveDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.GetAppsAppActiveDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface GetAppsAppActiveDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface GetAppsAppActiveDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppsAppAppStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppsAppComputeStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppsAppPendingDeployment {
    /**
     * The creation time of the app.
     */
    createTime: string;
    /**
     * The email of the user that created the app.
     */
    creator: string;
    deploymentArtifacts: outputs.GetAppsAppPendingDeploymentDeploymentArtifacts;
    deploymentId?: string;
    mode?: string;
    sourceCodePath?: string;
    status: outputs.GetAppsAppPendingDeploymentStatus;
    /**
     * The update time of the app.
     */
    updateTime: string;
}

export interface GetAppsAppPendingDeploymentDeploymentArtifacts {
    sourceCodePath?: string;
}

export interface GetAppsAppPendingDeploymentStatus {
    /**
     * Application status message
     */
    message: string;
    /**
     * State of the application.
     */
    state: string;
}

export interface GetAppsAppResource {
    /**
     * The description of the resource.
     */
    description?: string;
    /**
     * attribute
     */
    job?: outputs.GetAppsAppResourceJob;
    /**
     * Name of the serving endpoint to grant permission on.
     */
    name: string;
    /**
     * attribute
     */
    secret?: outputs.GetAppsAppResourceSecret;
    /**
     * attribute
     */
    servingEndpoint?: outputs.GetAppsAppResourceServingEndpoint;
    /**
     * attribute
     */
    sqlWarehouse?: outputs.GetAppsAppResourceSqlWarehouse;
}

export interface GetAppsAppResourceJob {
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetAppsAppResourceSecret {
    /**
     * Key of the secret to grant permission on.
     */
    key: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
    /**
     * Scope of the secret to grant permission on.
     */
    scope: string;
}

export interface GetAppsAppResourceServingEndpoint {
    /**
     * Name of the serving endpoint to grant permission on.
     */
    name: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetAppsAppResourceSqlWarehouse {
    /**
     * Id of the job to grant permission on.
     */
    id: string;
    /**
     * Permissions to grant on the Job. Supported permissions are: `CAN_MANAGE`, `IS_OWNER`, `CAN_MANAGE_RUN`, `CAN_VIEW`.
     */
    permission: string;
}

export interface GetBudgetPoliciesBudgetPolicy {
    bindingWorkspaceIds?: number[];
    customTags?: outputs.GetBudgetPoliciesBudgetPolicyCustomTag[];
    policyId: string;
    /**
     * The partial name of policies to be filtered on. If unspecified, all policies will be returned.
     */
    policyName?: string;
}

export interface GetBudgetPoliciesBudgetPolicyCustomTag {
    key: string;
    value?: string;
}

export interface GetBudgetPolicyCustomTag {
    key: string;
    value?: string;
}

export interface GetCatalogCatalogInfo {
    browseOnly?: boolean;
    /**
     * Type of the catalog, e.g. `MANAGED_CATALOG`, `DELTASHARING_CATALOG`, `SYSTEM_CATALOG`,
     */
    catalogType?: string;
    /**
     * Free-form text description
     */
    comment?: string;
    /**
     * The name of the connection to an external data source.
     */
    connectionName?: string;
    /**
     * Time at which this catalog was created, in epoch milliseconds.
     */
    createdAt?: number;
    /**
     * Username of catalog creator.
     */
    createdBy?: string;
    /**
     * object describing applied predictive optimization flag.
     */
    effectivePredictiveOptimizationFlag?: outputs.GetCatalogCatalogInfoEffectivePredictiveOptimizationFlag;
    /**
     * Whether predictive optimization should be enabled for this object and objects under it.
     */
    enablePredictiveOptimization?: string;
    /**
     * The full name of the catalog. Corresponds with the name field.
     */
    fullName?: string;
    /**
     * Whether the current securable is accessible from all workspaces or a  specific set of workspaces.
     */
    isolationMode?: string;
    /**
     * Unique identifier of parent metastore.
     */
    metastoreId?: string;
    /**
     * name of the catalog
     */
    name?: string;
    /**
     * A map of key-value properties attached to the securable.
     */
    options?: {[key: string]: string};
    /**
     * Current owner of the catalog
     */
    owner?: string;
    /**
     * A map of key-value properties attached to the securable.
     */
    properties?: {[key: string]: string};
    /**
     * The name of delta sharing provider.
     */
    providerName?: string;
    provisioningInfo?: outputs.GetCatalogCatalogInfoProvisioningInfo;
    /**
     * Securable type.
     */
    securableType?: string;
    /**
     * The name of the share under the share provider.
     */
    shareName?: string;
    /**
     * Storage Location URL (full path) for managed tables within catalog.
     */
    storageLocation?: string;
    /**
     * Storage root URL for managed tables within catalog.
     */
    storageRoot?: string;
    /**
     * Time at which this catalog was last modified, in epoch milliseconds.
     */
    updatedAt?: number;
    /**
     * Username of user who last modified catalog.
     */
    updatedBy?: string;
}

export interface GetCatalogCatalogInfoEffectivePredictiveOptimizationFlag {
    inheritedFromName?: string;
    inheritedFromType?: string;
    value: string;
}

export interface GetCatalogCatalogInfoProvisioningInfo {
    state?: string;
}

export interface GetClusterClusterInfo {
    autoscale?: outputs.GetClusterClusterInfoAutoscale;
    /**
     * Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.
     */
    autoterminationMinutes?: number;
    awsAttributes?: outputs.GetClusterClusterInfoAwsAttributes;
    azureAttributes?: outputs.GetClusterClusterInfoAzureAttributes;
    clusterCores?: number;
    /**
     * The id of the cluster.
     */
    clusterId?: string;
    clusterLogConf?: outputs.GetClusterClusterInfoClusterLogConf;
    clusterLogStatus?: outputs.GetClusterClusterInfoClusterLogStatus;
    clusterMemoryMb?: number;
    /**
     * The exact name of the cluster to search. Can only be specified if there is exactly one cluster with the provided name.
     */
    clusterName?: string;
    clusterSource?: string;
    creatorUserName?: string;
    /**
     * Additional tags for cluster resources.
     */
    customTags?: {[key: string]: string};
    /**
     * Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.
     */
    dataSecurityMode?: string;
    defaultTags?: {[key: string]: string};
    dockerImage?: outputs.GetClusterClusterInfoDockerImage;
    driver?: outputs.GetClusterClusterInfoDriver;
    /**
     * similar to `instancePoolId`, but for driver node.
     */
    driverInstancePoolId?: string;
    /**
     * The node type of the Spark driver.
     */
    driverNodeTypeId?: string;
    /**
     * Use autoscaling local storage.
     */
    enableElasticDisk?: boolean;
    /**
     * Enable local disk encryption.
     */
    enableLocalDiskEncryption?: boolean;
    executors?: outputs.GetClusterClusterInfoExecutor[];
    gcpAttributes?: outputs.GetClusterClusterInfoGcpAttributes;
    initScripts?: outputs.GetClusterClusterInfoInitScript[];
    /**
     * The pool of idle instances the cluster is attached to.
     */
    instancePoolId?: string;
    isSingleNode?: boolean;
    jdbcPort?: number;
    kind?: string;
    lastRestartedTime?: number;
    lastStateLossTime?: number;
    /**
     * Any supported databricks.getNodeType id.
     */
    nodeTypeId?: string;
    numWorkers?: number;
    /**
     * Identifier of Cluster Policy to validate cluster and preset certain defaults.
     */
    policyId?: string;
    /**
     * The type of runtime of the cluster
     */
    runtimeEngine?: string;
    /**
     * The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     */
    singleUserName?: string;
    /**
     * Map with key-value pairs to fine-tune Spark clusters.
     */
    sparkConf?: {[key: string]: string};
    sparkContextId?: number;
    /**
     * Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
     */
    sparkEnvVars?: {[key: string]: string};
    /**
     * [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.
     */
    sparkVersion?: string;
    spec?: outputs.GetClusterClusterInfoSpec;
    /**
     * SSH public key contents that will be added to each Spark node in this cluster.
     */
    sshPublicKeys?: string[];
    startTime?: number;
    state?: string;
    stateMessage?: string;
    terminatedTime?: number;
    terminationReason?: outputs.GetClusterClusterInfoTerminationReason;
    useMlRuntime?: boolean;
    workloadType?: outputs.GetClusterClusterInfoWorkloadType;
}

export interface GetClusterClusterInfoAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetClusterClusterInfoAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetClusterClusterInfoAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.GetClusterClusterInfoAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface GetClusterClusterInfoAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface GetClusterClusterInfoClusterLogConf {
    dbfs?: outputs.GetClusterClusterInfoClusterLogConfDbfs;
    s3?: outputs.GetClusterClusterInfoClusterLogConfS3;
    volumes?: outputs.GetClusterClusterInfoClusterLogConfVolumes;
}

export interface GetClusterClusterInfoClusterLogConfDbfs {
    destination: string;
}

export interface GetClusterClusterInfoClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoClusterLogConfVolumes {
    destination: string;
}

export interface GetClusterClusterInfoClusterLogStatus {
    lastAttempted?: number;
    lastException?: string;
}

export interface GetClusterClusterInfoDockerImage {
    basicAuth?: outputs.GetClusterClusterInfoDockerImageBasicAuth;
    url?: string;
}

export interface GetClusterClusterInfoDockerImageBasicAuth {
    password?: string;
    username?: string;
}

export interface GetClusterClusterInfoDriver {
    hostPrivateIp?: string;
    instanceId?: string;
    nodeAwsAttributes?: outputs.GetClusterClusterInfoDriverNodeAwsAttributes;
    nodeId?: string;
    privateIp?: string;
    publicDns?: string;
    startTimestamp?: number;
}

export interface GetClusterClusterInfoDriverNodeAwsAttributes {
    isSpot?: boolean;
}

export interface GetClusterClusterInfoExecutor {
    hostPrivateIp?: string;
    instanceId?: string;
    nodeAwsAttributes?: outputs.GetClusterClusterInfoExecutorNodeAwsAttributes;
    nodeId?: string;
    privateIp?: string;
    publicDns?: string;
    startTimestamp?: number;
}

export interface GetClusterClusterInfoExecutorNodeAwsAttributes {
    isSpot?: boolean;
}

export interface GetClusterClusterInfoGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetClusterClusterInfoInitScript {
    abfss?: outputs.GetClusterClusterInfoInitScriptAbfss;
    dbfs?: outputs.GetClusterClusterInfoInitScriptDbfs;
    file?: outputs.GetClusterClusterInfoInitScriptFile;
    gcs?: outputs.GetClusterClusterInfoInitScriptGcs;
    s3?: outputs.GetClusterClusterInfoInitScriptS3;
    volumes?: outputs.GetClusterClusterInfoInitScriptVolumes;
    workspace?: outputs.GetClusterClusterInfoInitScriptWorkspace;
}

export interface GetClusterClusterInfoInitScriptAbfss {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptDbfs {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptFile {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptGcs {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoInitScriptVolumes {
    destination: string;
}

export interface GetClusterClusterInfoInitScriptWorkspace {
    destination: string;
}

export interface GetClusterClusterInfoSpec {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.GetClusterClusterInfoSpecAutoscale;
    awsAttributes?: outputs.GetClusterClusterInfoSpecAwsAttributes;
    azureAttributes?: outputs.GetClusterClusterInfoSpecAzureAttributes;
    /**
     * The id of the cluster.
     */
    clusterId: string;
    clusterLogConf?: outputs.GetClusterClusterInfoSpecClusterLogConf;
    clusterMountInfos?: outputs.GetClusterClusterInfoSpecClusterMountInfo[];
    /**
     * The exact name of the cluster to search. Can only be specified if there is exactly one cluster with the provided name.
     */
    clusterName?: string;
    /**
     * Additional tags for cluster resources.
     */
    customTags?: {[key: string]: string};
    /**
     * Security features of the cluster. Unity Catalog requires `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. Default to `NONE`, i.e. no security feature enabled.
     */
    dataSecurityMode?: string;
    dockerImage?: outputs.GetClusterClusterInfoSpecDockerImage;
    /**
     * similar to `instancePoolId`, but for driver node.
     */
    driverInstancePoolId: string;
    /**
     * The node type of the Spark driver.
     */
    driverNodeTypeId: string;
    /**
     * Use autoscaling local storage.
     */
    enableElasticDisk: boolean;
    /**
     * Enable local disk encryption.
     */
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.GetClusterClusterInfoSpecGcpAttributes;
    /**
     * An optional token to guarantee the idempotency of cluster creation requests.
     */
    idempotencyToken?: string;
    initScripts?: outputs.GetClusterClusterInfoSpecInitScript[];
    /**
     * The pool of idle instances the cluster is attached to.
     */
    instancePoolId?: string;
    isSingleNode?: boolean;
    kind?: string;
    libraries?: outputs.GetClusterClusterInfoSpecLibrary[];
    /**
     * Any supported databricks.getNodeType id.
     */
    nodeTypeId: string;
    numWorkers?: number;
    /**
     * Identifier of Cluster Policy to validate cluster and preset certain defaults.
     */
    policyId?: string;
    /**
     * The type of runtime of the cluster
     */
    runtimeEngine?: string;
    /**
     * The optional user name of the user to assign to an interactive cluster. This field is required when using standard AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     */
    singleUserName?: string;
    /**
     * Map with key-value pairs to fine-tune Spark clusters.
     */
    sparkConf?: {[key: string]: string};
    /**
     * Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X='Y') while launching the driver and workers.
     */
    sparkEnvVars?: {[key: string]: string};
    /**
     * [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster.
     */
    sparkVersion?: string;
    /**
     * SSH public key contents that will be added to each Spark node in this cluster.
     */
    sshPublicKeys?: string[];
    useMlRuntime?: boolean;
    workloadType?: outputs.GetClusterClusterInfoSpecWorkloadType;
}

export interface GetClusterClusterInfoSpecAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetClusterClusterInfoSpecAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetClusterClusterInfoSpecAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.GetClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface GetClusterClusterInfoSpecAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface GetClusterClusterInfoSpecClusterLogConf {
    dbfs?: outputs.GetClusterClusterInfoSpecClusterLogConfDbfs;
    s3?: outputs.GetClusterClusterInfoSpecClusterLogConfS3;
    volumes?: outputs.GetClusterClusterInfoSpecClusterLogConfVolumes;
}

export interface GetClusterClusterInfoSpecClusterLogConfDbfs {
    destination: string;
}

export interface GetClusterClusterInfoSpecClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoSpecClusterLogConfVolumes {
    destination: string;
}

export interface GetClusterClusterInfoSpecClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.GetClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetClusterClusterInfoSpecClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetClusterClusterInfoSpecDockerImage {
    basicAuth?: outputs.GetClusterClusterInfoSpecDockerImageBasicAuth;
    url: string;
}

export interface GetClusterClusterInfoSpecDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetClusterClusterInfoSpecGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetClusterClusterInfoSpecInitScript {
    abfss?: outputs.GetClusterClusterInfoSpecInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.GetClusterClusterInfoSpecInitScriptDbfs;
    file?: outputs.GetClusterClusterInfoSpecInitScriptFile;
    gcs?: outputs.GetClusterClusterInfoSpecInitScriptGcs;
    s3?: outputs.GetClusterClusterInfoSpecInitScriptS3;
    volumes?: outputs.GetClusterClusterInfoSpecInitScriptVolumes;
    workspace?: outputs.GetClusterClusterInfoSpecInitScriptWorkspace;
}

export interface GetClusterClusterInfoSpecInitScriptAbfss {
    destination: string;
}

export interface GetClusterClusterInfoSpecInitScriptDbfs {
    destination: string;
}

export interface GetClusterClusterInfoSpecInitScriptFile {
    destination: string;
}

export interface GetClusterClusterInfoSpecInitScriptGcs {
    destination: string;
}

export interface GetClusterClusterInfoSpecInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetClusterClusterInfoSpecInitScriptVolumes {
    destination: string;
}

export interface GetClusterClusterInfoSpecInitScriptWorkspace {
    destination: string;
}

export interface GetClusterClusterInfoSpecLibrary {
    cran?: outputs.GetClusterClusterInfoSpecLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.GetClusterClusterInfoSpecLibraryMaven;
    pypi?: outputs.GetClusterClusterInfoSpecLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface GetClusterClusterInfoSpecLibraryCran {
    package: string;
    repo?: string;
}

export interface GetClusterClusterInfoSpecLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetClusterClusterInfoSpecLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetClusterClusterInfoSpecWorkloadType {
    clients: outputs.GetClusterClusterInfoSpecWorkloadTypeClients;
}

export interface GetClusterClusterInfoSpecWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetClusterClusterInfoTerminationReason {
    code?: string;
    parameters?: {[key: string]: string};
    type?: string;
}

export interface GetClusterClusterInfoWorkloadType {
    clients: outputs.GetClusterClusterInfoWorkloadTypeClients;
}

export interface GetClusterClusterInfoWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetClustersFilterBy {
    /**
     * List of cluster sources to filter by. Possible values are `API`, `JOB`, `MODELS`, `PIPELINE`, `PIPELINE_MAINTENANCE`, `SQL`, and `UI`.
     */
    clusterSources?: string[];
    /**
     * List of cluster states to filter by. Possible values are `RUNNING`, `PENDING`, `RESIZING`, `RESTARTING`, `TERMINATING`, `TERMINATED`, `ERROR`, and `UNKNOWN`.
     */
    clusterStates?: string[];
    /**
     * Whether to filter by pinned clusters.
     */
    isPinned?: boolean;
    /**
     * Filter by databricks.ClusterPolicy id.
     */
    policyId?: string;
}

export interface GetCurrentMetastoreMetastoreInfo {
    cloud?: string;
    /**
     * Timestamp (in milliseconds) when the current metastore was created.
     */
    createdAt?: number;
    /**
     * the ID of the identity that created the current metastore.
     */
    createdBy?: string;
    /**
     * the ID of the default data access configuration.
     */
    defaultDataAccessConfigId?: string;
    /**
     * The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.
     */
    deltaSharingOrganizationName?: string;
    /**
     * the expiration duration in seconds on recipient data access tokens.
     */
    deltaSharingRecipientTokenLifetimeInSeconds?: number;
    /**
     * Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.
     */
    deltaSharingScope?: string;
    externalAccessEnabled?: boolean;
    /**
     * Identifier in form of `<cloud>:<region>:<metastore_id>` for use in Databricks to Databricks Delta Sharing.
     */
    globalMetastoreId?: string;
    /**
     * Metastore ID.
     */
    metastoreId?: string;
    /**
     * Name of metastore.
     */
    name?: string;
    /**
     * Username/group name/sp applicationId of the metastore owner.
     */
    owner?: string;
    /**
     * the version of the privilege model used by the metastore.
     */
    privilegeModelVersion?: string;
    /**
     * (Mandatory for account-level) The region of the metastore.
     */
    region?: string;
    /**
     * Path on cloud storage account, where managed `databricks.Table` are stored.
     */
    storageRoot?: string;
    /**
     * ID of a storage credential used for the `storageRoot`.
     */
    storageRootCredentialId?: string;
    /**
     * Name of a storage credential used for the `storageRoot`.
     */
    storageRootCredentialName?: string;
    /**
     * Timestamp (in milliseconds) when the current metastore was updated.
     */
    updatedAt?: number;
    /**
     * the ID of the identity that updated the current metastore.
     */
    updatedBy?: string;
}

export interface GetDashboardsDashboard {
    /**
     * The timestamp of when the dashboard was created.
     */
    createTime: string;
    /**
     * The unique ID of the dashboard.
     */
    dashboardId: string;
    /**
     * The display name of the dashboard.
     */
    displayName?: string;
    etag: string;
    lifecycleState: string;
    parentPath: string;
    path: string;
    serializedDashboard?: string;
    updateTime: string;
    warehouseId?: string;
}

export interface GetDbfsFilePathsPathList {
    fileSize?: number;
    /**
     * Path on DBFS for the file to perform listing
     */
    path?: string;
}

export interface GetExternalLocationExternalLocationInfo {
    /**
     * The ARN of the s3 access point to use with the external location (AWS).
     */
    accessPoint?: string;
    browseOnly?: boolean;
    /**
     * User-supplied comment.
     */
    comment?: string;
    /**
     * Time at which this catalog was created, in epoch milliseconds.
     */
    createdAt?: number;
    /**
     * Username of catalog creator.
     */
    createdBy?: string;
    /**
     * Unique ID of storage credential.
     */
    credentialId?: string;
    /**
     * Name of the databricks.StorageCredential to use with this external location.
     */
    credentialName?: string;
    /**
     * The options for Server-Side Encryption to be used by each Databricks s3 client when connecting to S3 cloud storage (AWS).
     */
    encryptionDetails?: outputs.GetExternalLocationExternalLocationInfoEncryptionDetails;
    fallback?: boolean;
    isolationMode?: string;
    /**
     * Unique identifier of the parent Metastore.
     */
    metastoreId?: string;
    /**
     * The name of the external location
     */
    name?: string;
    /**
     * Username/groupname/sp applicationId of the external location owner.
     */
    owner?: string;
    /**
     * Indicates whether the external location is read-only.
     */
    readOnly?: boolean;
    /**
     * Time at which this catalog was last modified, in epoch milliseconds.
     */
    updatedAt?: number;
    /**
     * Username of user who last modified catalog.
     */
    updatedBy?: string;
    /**
     * Path URL in cloud storage, of the form: `s3://[bucket-host]/[bucket-dir]` (AWS), `abfss://[user]@[host]/[path]` (Azure), `gs://[bucket-host]/[bucket-dir]` (GCP).
     */
    url?: string;
}

export interface GetExternalLocationExternalLocationInfoEncryptionDetails {
    sseEncryptionDetails?: outputs.GetExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails;
}

export interface GetExternalLocationExternalLocationInfoEncryptionDetailsSseEncryptionDetails {
    algorithm?: string;
    awsKmsKeyArn?: string;
}

export interface GetFunctionsFunction {
    /**
     * Indicates whether the principal is limited to retrieving metadata for the associated object through the `BROWSE` privilege when `includeBrowse` is enabled in the request.
     */
    browseOnly?: boolean;
    /**
     * Name of databricks_catalog.
     */
    catalogName?: string;
    /**
     * User-provided free-form text description.
     */
    comment?: string;
    /**
     * Time at which this function was created, in epoch milliseconds.
     */
    createdAt?: number;
    /**
     * Username of function creator.
     */
    createdBy?: string;
    /**
     * Scalar function return data type.
     */
    dataType?: string;
    /**
     * External function language.
     */
    externalLanguage?: string;
    /**
     * External function name.
     */
    externalName?: string;
    /**
     * Pretty printed function data type.
     */
    fullDataType?: string;
    /**
     * Full name of function, in form of catalog_name.schema_name.function__name
     */
    fullName?: string;
    /**
     * Id of Function, relative to parent schema.
     */
    functionId?: string;
    /**
     * object describing input parameters. Consists of the single attribute:
     */
    inputParams?: outputs.GetFunctionsFunctionInputParam[];
    /**
     * Boolean flag specifying whether the function is deterministic.
     */
    isDeterministic?: boolean;
    /**
     * Boolean flag whether function null call.
     */
    isNullCall?: boolean;
    /**
     * Unique identifier of parent metastore.
     */
    metastoreId?: string;
    /**
     * Name of parameter.
     */
    name?: string;
    /**
     * Username of current owner of function.
     */
    owner?: string;
    /**
     * Function parameter style. `S` is the value for SQL.
     */
    parameterStyle?: string;
    /**
     * JSON-serialized key-value pair map, encoded (escaped) as a string.
     */
    properties?: string;
    /**
     * Table function return parameters.  See `inputParams` for description.
     */
    returnParams?: outputs.GetFunctionsFunctionReturnParam[];
    /**
     * Function language (`SQL` or `EXTERNAL`). When `EXTERNAL` is used, the language of the routine function should be specified in the `externalLanguage` field, and the `returnParams` of the function cannot be used (as `TABLE` return type is not supported), and the `sqlDataAccess` field must be `NO_SQL`.
     */
    routineBody?: string;
    /**
     * Function body.
     */
    routineDefinition?: string;
    /**
     * Function dependencies.
     */
    routineDependencies?: outputs.GetFunctionsFunctionRoutineDependency[];
    /**
     * Name of databricks_schema.
     */
    schemaName?: string;
    /**
     * Function security type. (Enum: `DEFINER`).
     */
    securityType?: string;
    /**
     * Specific name of the function; Reserved for future use.
     */
    specificName?: string;
    /**
     * Function SQL data access (`CONTAINS_SQL`, `READS_SQL_DATA`, `NO_SQL`).
     */
    sqlDataAccess?: string;
    /**
     * List of schemes whose objects can be referenced without qualification.
     */
    sqlPath?: string;
    /**
     * Time at which this function was created, in epoch milliseconds.
     */
    updatedAt?: number;
    /**
     * Username of user who last modified function.
     */
    updatedBy?: string;
}

export interface GetFunctionsFunctionInputParam {
    /**
     * The array of definitions of the function's parameters:
     */
    parameters?: outputs.GetFunctionsFunctionInputParamParameter[];
}

export interface GetFunctionsFunctionInputParamParameter {
    /**
     * User-provided free-form text description.
     */
    comment?: string;
    /**
     * Name of parameter.
     */
    name: string;
    /**
     * Default value of the parameter.
     */
    parameterDefault?: string;
    /**
     * The mode of the function parameter.
     */
    parameterMode?: string;
    /**
     * The type of function parameter (`PARAM` or `COLUMN`).
     */
    parameterType?: string;
    /**
     * Ordinal position of column (starting at position 0).
     */
    position: number;
    /**
     * Format of IntervalType.
     */
    typeIntervalType?: string;
    /**
     * Full data type spec, JSON-serialized.
     */
    typeJson?: string;
    /**
     * Name of type (INT, STRUCT, MAP, etc.).
     */
    typeName: string;
    /**
     * Digits of precision; required on Create for DecimalTypes.
     */
    typePrecision?: number;
    /**
     * Digits to right of decimal; Required on Create for DecimalTypes.
     */
    typeScale?: number;
    /**
     * Full data type spec, SQL/catalogString text.
     */
    typeText: string;
}

export interface GetFunctionsFunctionReturnParam {
    /**
     * The array of definitions of the function's parameters:
     */
    parameters?: outputs.GetFunctionsFunctionReturnParamParameter[];
}

export interface GetFunctionsFunctionReturnParamParameter {
    /**
     * User-provided free-form text description.
     */
    comment?: string;
    /**
     * Name of parameter.
     */
    name: string;
    /**
     * Default value of the parameter.
     */
    parameterDefault?: string;
    /**
     * The mode of the function parameter.
     */
    parameterMode?: string;
    /**
     * The type of function parameter (`PARAM` or `COLUMN`).
     */
    parameterType?: string;
    /**
     * Ordinal position of column (starting at position 0).
     */
    position: number;
    /**
     * Format of IntervalType.
     */
    typeIntervalType?: string;
    /**
     * Full data type spec, JSON-serialized.
     */
    typeJson?: string;
    /**
     * Name of type (INT, STRUCT, MAP, etc.).
     */
    typeName: string;
    /**
     * Digits of precision; required on Create for DecimalTypes.
     */
    typePrecision?: number;
    /**
     * Digits to right of decimal; Required on Create for DecimalTypes.
     */
    typeScale?: number;
    /**
     * Full data type spec, SQL/catalogString text.
     */
    typeText: string;
}

export interface GetFunctionsFunctionRoutineDependency {
    dependencies?: outputs.GetFunctionsFunctionRoutineDependencyDependency[];
}

export interface GetFunctionsFunctionRoutineDependencyDependency {
    functions?: outputs.GetFunctionsFunctionRoutineDependencyDependencyFunction[];
    tables?: outputs.GetFunctionsFunctionRoutineDependencyDependencyTable[];
}

export interface GetFunctionsFunctionRoutineDependencyDependencyFunction {
    functionFullName: string;
}

export interface GetFunctionsFunctionRoutineDependencyDependencyTable {
    tableFullName: string;
}

export interface GetInstancePoolPoolInfo {
    awsAttributes?: outputs.GetInstancePoolPoolInfoAwsAttributes;
    azureAttributes?: outputs.GetInstancePoolPoolInfoAzureAttributes;
    customTags?: {[key: string]: string};
    defaultTags: {[key: string]: string};
    diskSpec?: outputs.GetInstancePoolPoolInfoDiskSpec;
    enableElasticDisk?: boolean;
    gcpAttributes?: outputs.GetInstancePoolPoolInfoGcpAttributes;
    idleInstanceAutoterminationMinutes: number;
    instancePoolFleetAttributes?: outputs.GetInstancePoolPoolInfoInstancePoolFleetAttribute[];
    instancePoolId: string;
    instancePoolName: string;
    maxCapacity?: number;
    minIdleInstances?: number;
    nodeTypeId?: string;
    preloadedDockerImages?: outputs.GetInstancePoolPoolInfoPreloadedDockerImage[];
    preloadedSparkVersions?: string[];
    state?: string;
    stats?: outputs.GetInstancePoolPoolInfoStats;
}

export interface GetInstancePoolPoolInfoAwsAttributes {
    availability?: string;
    spotBidPricePercent?: number;
    zoneId: string;
}

export interface GetInstancePoolPoolInfoAzureAttributes {
    availability?: string;
    spotBidMaxPrice?: number;
}

export interface GetInstancePoolPoolInfoDiskSpec {
    diskCount?: number;
    diskSize?: number;
    diskType?: outputs.GetInstancePoolPoolInfoDiskSpecDiskType;
}

export interface GetInstancePoolPoolInfoDiskSpecDiskType {
    azureDiskVolumeType?: string;
    ebsVolumeType?: string;
}

export interface GetInstancePoolPoolInfoGcpAttributes {
    gcpAvailability?: string;
    localSsdCount: number;
    zoneId: string;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttribute {
    fleetOnDemandOption?: outputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption;
    fleetSpotOption?: outputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption;
    launchTemplateOverrides: outputs.GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride[];
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetOnDemandOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeFleetSpotOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface GetInstancePoolPoolInfoInstancePoolFleetAttributeLaunchTemplateOverride {
    availabilityZone: string;
    instanceType: string;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImage {
    basicAuth?: outputs.GetInstancePoolPoolInfoPreloadedDockerImageBasicAuth;
    url: string;
}

export interface GetInstancePoolPoolInfoPreloadedDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetInstancePoolPoolInfoStats {
    idleCount?: number;
    pendingIdleCount?: number;
    pendingUsedCount?: number;
    usedCount?: number;
}

export interface GetInstanceProfilesInstanceProfile {
    /**
     * ARN of the instance profile.
     */
    arn: string;
    /**
     * Whether the instance profile is a meta instance profile or not.
     */
    isMeta: boolean;
    /**
     * Name of the instance profile.
     */
    name: string;
    /**
     * ARN of the role attached to the instance profile.
     */
    roleArn: string;
}

export interface GetJobJobSettings {
    createdTime?: number;
    creatorUserName?: string;
    jobId?: number;
    runAsUserName: string;
    settings?: outputs.GetJobJobSettingsSettings;
}

export interface GetJobJobSettingsSettings {
    continuous?: outputs.GetJobJobSettingsSettingsContinuous;
    dbtTask?: outputs.GetJobJobSettingsSettingsDbtTask;
    deployment?: outputs.GetJobJobSettingsSettingsDeployment;
    description?: string;
    editMode?: string;
    emailNotifications?: outputs.GetJobJobSettingsSettingsEmailNotifications;
    environments?: outputs.GetJobJobSettingsSettingsEnvironment[];
    existingClusterId?: string;
    format: string;
    gitSource?: outputs.GetJobJobSettingsSettingsGitSource;
    health?: outputs.GetJobJobSettingsSettingsHealth;
    jobClusters?: outputs.GetJobJobSettingsSettingsJobCluster[];
    libraries?: outputs.GetJobJobSettingsSettingsLibrary[];
    maxConcurrentRuns?: number;
    maxRetries?: number;
    minRetryIntervalMillis?: number;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: string;
    newCluster?: outputs.GetJobJobSettingsSettingsNewCluster;
    notebookTask?: outputs.GetJobJobSettingsSettingsNotebookTask;
    notificationSettings?: outputs.GetJobJobSettingsSettingsNotificationSettings;
    parameters?: outputs.GetJobJobSettingsSettingsParameter[];
    pipelineTask?: outputs.GetJobJobSettingsSettingsPipelineTask;
    pythonWheelTask?: outputs.GetJobJobSettingsSettingsPythonWheelTask;
    queue?: outputs.GetJobJobSettingsSettingsQueue;
    retryOnTimeout?: boolean;
    runAs: outputs.GetJobJobSettingsSettingsRunAs;
    runJobTask?: outputs.GetJobJobSettingsSettingsRunJobTask;
    schedule?: outputs.GetJobJobSettingsSettingsSchedule;
    sparkJarTask?: outputs.GetJobJobSettingsSettingsSparkJarTask;
    sparkPythonTask?: outputs.GetJobJobSettingsSettingsSparkPythonTask;
    sparkSubmitTask?: outputs.GetJobJobSettingsSettingsSparkSubmitTask;
    tags?: {[key: string]: string};
    tasks?: outputs.GetJobJobSettingsSettingsTask[];
    timeoutSeconds?: number;
    trigger?: outputs.GetJobJobSettingsSettingsTrigger;
    webhookNotifications?: outputs.GetJobJobSettingsSettingsWebhookNotifications;
}

export interface GetJobJobSettingsSettingsContinuous {
    pauseStatus?: string;
}

export interface GetJobJobSettingsSettingsDbtTask {
    catalog?: string;
    commands: string[];
    profilesDirectory?: string;
    projectDirectory?: string;
    schema?: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsDeployment {
    kind: string;
    metadataFilePath?: string;
}

export interface GetJobJobSettingsSettingsEmailNotifications {
    noAlertForSkippedRuns?: boolean;
    onDurationWarningThresholdExceededs?: string[];
    onFailures?: string[];
    onStarts?: string[];
    onStreamingBacklogExceededs?: string[];
    onSuccesses?: string[];
}

export interface GetJobJobSettingsSettingsEnvironment {
    environmentKey: string;
    spec?: outputs.GetJobJobSettingsSettingsEnvironmentSpec;
}

export interface GetJobJobSettingsSettingsEnvironmentSpec {
    client: string;
    dependencies?: string[];
    jarDependencies?: string[];
}

export interface GetJobJobSettingsSettingsGitSource {
    branch?: string;
    commit?: string;
    jobSource?: outputs.GetJobJobSettingsSettingsGitSourceJobSource;
    provider?: string;
    tag?: string;
    url: string;
}

export interface GetJobJobSettingsSettingsGitSourceJobSource {
    dirtyState?: string;
    importFromGitBranch: string;
    jobConfigPath: string;
}

export interface GetJobJobSettingsSettingsHealth {
    rules: outputs.GetJobJobSettingsSettingsHealthRule[];
}

export interface GetJobJobSettingsSettingsHealthRule {
    metric: string;
    op: string;
    value: number;
}

export interface GetJobJobSettingsSettingsJobCluster {
    jobClusterKey: string;
    newCluster: outputs.GetJobJobSettingsSettingsJobClusterNewCluster;
}

export interface GetJobJobSettingsSettingsJobClusterNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributes;
    azureAttributes?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConf;
    clusterMountInfos?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    workloadType?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConf {
    dbfs?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs;
    s3?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImage {
    basicAuth?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScript {
    abfss?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss;
    dbfs?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs;
    file?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFile;
    gcs?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs;
    s3?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3;
    volumes?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes;
    workspace?: outputs.GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptAbfss {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptFile {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptGcs {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptVolumes {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterInitScriptWorkspace {
    destination: string;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadType {
    clients: outputs.GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsJobClusterNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsLibrary {
    cran?: outputs.GetJobJobSettingsSettingsLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.GetJobJobSettingsSettingsLibraryMaven;
    pypi?: outputs.GetJobJobSettingsSettingsLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface GetJobJobSettingsSettingsLibraryCran {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetJobJobSettingsSettingsLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.GetJobJobSettingsSettingsNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: outputs.GetJobJobSettingsSettingsNewClusterAwsAttributes;
    azureAttributes?: outputs.GetJobJobSettingsSettingsNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.GetJobJobSettingsSettingsNewClusterClusterLogConf;
    clusterMountInfos?: outputs.GetJobJobSettingsSettingsNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.GetJobJobSettingsSettingsNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.GetJobJobSettingsSettingsNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.GetJobJobSettingsSettingsNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    workloadType?: outputs.GetJobJobSettingsSettingsNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConf {
    dbfs?: outputs.GetJobJobSettingsSettingsNewClusterClusterLogConfDbfs;
    s3?: outputs.GetJobJobSettingsSettingsNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImage {
    basicAuth?: outputs.GetJobJobSettingsSettingsNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScript {
    abfss?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptAbfss;
    dbfs?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptDbfs;
    file?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptFile;
    gcs?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptGcs;
    s3?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptS3;
    volumes?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptVolumes;
    workspace?: outputs.GetJobJobSettingsSettingsNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptAbfss {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptFile {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptGcs {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptVolumes {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterInitScriptWorkspace {
    destination: string;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadType {
    clients: outputs.GetJobJobSettingsSettingsNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsNotebookTask {
    baseParameters?: {[key: string]: string};
    notebookPath: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsNotificationSettings {
    noAlertForCanceledRuns?: boolean;
    noAlertForSkippedRuns?: boolean;
}

export interface GetJobJobSettingsSettingsParameter {
    default: string;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name: string;
}

export interface GetJobJobSettingsSettingsPipelineTask {
    fullRefresh?: boolean;
    pipelineId: string;
}

export interface GetJobJobSettingsSettingsPythonWheelTask {
    entryPoint?: string;
    namedParameters?: {[key: string]: string};
    packageName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsQueue {
    enabled: boolean;
}

export interface GetJobJobSettingsSettingsRunAs {
    servicePrincipalName?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsRunJobTask {
    jobId: number;
    jobParameters?: {[key: string]: string};
}

export interface GetJobJobSettingsSettingsSchedule {
    pauseStatus?: string;
    quartzCronExpression: string;
    timezoneId: string;
}

export interface GetJobJobSettingsSettingsSparkJarTask {
    jarUri?: string;
    mainClassName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsSparkPythonTask {
    parameters?: string[];
    pythonFile: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsSparkSubmitTask {
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTask {
    conditionTask?: outputs.GetJobJobSettingsSettingsTaskConditionTask;
    dashboardTask?: outputs.GetJobJobSettingsSettingsTaskDashboardTask;
    dbtTask?: outputs.GetJobJobSettingsSettingsTaskDbtTask;
    dependsOns?: outputs.GetJobJobSettingsSettingsTaskDependsOn[];
    description?: string;
    emailNotifications?: outputs.GetJobJobSettingsSettingsTaskEmailNotifications;
    environmentKey?: string;
    existingClusterId?: string;
    forEachTask?: outputs.GetJobJobSettingsSettingsTaskForEachTask;
    health?: outputs.GetJobJobSettingsSettingsTaskHealth;
    jobClusterKey?: string;
    libraries?: outputs.GetJobJobSettingsSettingsTaskLibrary[];
    maxRetries?: number;
    minRetryIntervalMillis?: number;
    newCluster?: outputs.GetJobJobSettingsSettingsTaskNewCluster;
    notebookTask?: outputs.GetJobJobSettingsSettingsTaskNotebookTask;
    notificationSettings?: outputs.GetJobJobSettingsSettingsTaskNotificationSettings;
    pipelineTask?: outputs.GetJobJobSettingsSettingsTaskPipelineTask;
    powerBiTask?: outputs.GetJobJobSettingsSettingsTaskPowerBiTask;
    pythonWheelTask?: outputs.GetJobJobSettingsSettingsTaskPythonWheelTask;
    retryOnTimeout: boolean;
    runIf?: string;
    runJobTask?: outputs.GetJobJobSettingsSettingsTaskRunJobTask;
    sparkJarTask?: outputs.GetJobJobSettingsSettingsTaskSparkJarTask;
    sparkPythonTask?: outputs.GetJobJobSettingsSettingsTaskSparkPythonTask;
    sparkSubmitTask?: outputs.GetJobJobSettingsSettingsTaskSparkSubmitTask;
    sqlTask?: outputs.GetJobJobSettingsSettingsTaskSqlTask;
    taskKey: string;
    timeoutSeconds?: number;
    webhookNotifications?: outputs.GetJobJobSettingsSettingsTaskWebhookNotifications;
}

export interface GetJobJobSettingsSettingsTaskConditionTask {
    left: string;
    op: string;
    right: string;
}

export interface GetJobJobSettingsSettingsTaskDashboardTask {
    dashboardId?: string;
    subscription?: outputs.GetJobJobSettingsSettingsTaskDashboardTaskSubscription;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskDashboardTaskSubscription {
    customSubject?: string;
    paused?: boolean;
    subscribers?: outputs.GetJobJobSettingsSettingsTaskDashboardTaskSubscriptionSubscriber[];
}

export interface GetJobJobSettingsSettingsTaskDashboardTaskSubscriptionSubscriber {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskDbtTask {
    catalog?: string;
    commands: string[];
    profilesDirectory?: string;
    projectDirectory?: string;
    schema?: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskDependsOn {
    outcome?: string;
    taskKey: string;
}

export interface GetJobJobSettingsSettingsTaskEmailNotifications {
    noAlertForSkippedRuns?: boolean;
    onDurationWarningThresholdExceededs?: string[];
    onFailures?: string[];
    onStarts?: string[];
    onStreamingBacklogExceededs?: string[];
    onSuccesses?: string[];
}

export interface GetJobJobSettingsSettingsTaskForEachTask {
    concurrency?: number;
    inputs: string;
    task: outputs.GetJobJobSettingsSettingsTaskForEachTaskTask;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTask {
    conditionTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskConditionTask;
    dashboardTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTask;
    dbtTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskDbtTask;
    dependsOns?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskDependsOn[];
    description?: string;
    emailNotifications?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications;
    environmentKey?: string;
    existingClusterId?: string;
    health?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskHealth;
    jobClusterKey?: string;
    libraries?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskLibrary[];
    maxRetries?: number;
    minRetryIntervalMillis?: number;
    newCluster?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewCluster;
    notebookTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask;
    notificationSettings?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings;
    pipelineTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask;
    powerBiTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTask;
    pythonWheelTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask;
    retryOnTimeout: boolean;
    runIf?: string;
    runJobTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask;
    sparkJarTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask;
    sparkPythonTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask;
    sparkSubmitTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask;
    sqlTask?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTask;
    taskKey: string;
    timeoutSeconds?: number;
    webhookNotifications?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskConditionTask {
    left: string;
    op: string;
    right: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTask {
    dashboardId?: string;
    subscription?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTaskSubscription;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTaskSubscription {
    customSubject?: string;
    paused?: boolean;
    subscribers?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTaskSubscriptionSubscriber[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskDashboardTaskSubscriptionSubscriber {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskDbtTask {
    catalog?: string;
    commands: string[];
    profilesDirectory?: string;
    projectDirectory?: string;
    schema?: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskDependsOn {
    outcome?: string;
    taskKey: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskEmailNotifications {
    noAlertForSkippedRuns?: boolean;
    onDurationWarningThresholdExceededs?: string[];
    onFailures?: string[];
    onStarts?: string[];
    onStreamingBacklogExceededs?: string[];
    onSuccesses?: string[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskHealth {
    rules: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskHealthRule[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskHealthRule {
    metric: string;
    op: string;
    value: number;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskLibrary {
    cran?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven;
    pypi?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryCran {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes;
    azureAttributes?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf;
    clusterMountInfos?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    workloadType?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConf {
    dbfs?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs;
    s3?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImage {
    basicAuth?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScript {
    abfss?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss;
    dbfs?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs;
    file?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile;
    gcs?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs;
    s3?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3;
    volumes?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes;
    workspace?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptAbfss {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptFile {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptGcs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptVolumes {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterInitScriptWorkspace {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadType {
    clients: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNotebookTask {
    baseParameters?: {[key: string]: string};
    notebookPath: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskNotificationSettings {
    alertOnLastAttempt?: boolean;
    noAlertForCanceledRuns?: boolean;
    noAlertForSkippedRuns?: boolean;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskPipelineTask {
    fullRefresh?: boolean;
    pipelineId: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTask {
    connectionResourceName?: string;
    powerBiModel?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTaskPowerBiModel;
    refreshAfterUpdate?: boolean;
    tables?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTaskTable[];
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTaskPowerBiModel {
    authenticationMethod?: string;
    modelName?: string;
    overwriteExisting?: boolean;
    storageMode?: string;
    workspaceName?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskPowerBiTaskTable {
    catalog?: string;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: string;
    schema?: string;
    storageMode?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskPythonWheelTask {
    entryPoint?: string;
    namedParameters?: {[key: string]: string};
    packageName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskRunJobTask {
    jobId: number;
    jobParameters?: {[key: string]: string};
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSparkJarTask {
    jarUri?: string;
    mainClassName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSparkPythonTask {
    parameters?: string[];
    pythonFile: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSparkSubmitTask {
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTask {
    alert?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert;
    dashboard?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard;
    file?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile;
    parameters?: {[key: string]: string};
    query?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery;
    warehouseId: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlert {
    alertId: string;
    pauseSubscriptions?: boolean;
    subscriptions?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskAlertSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboard {
    customSubject?: string;
    dashboardId: string;
    pauseSubscriptions?: boolean;
    subscriptions?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskDashboardSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskFile {
    path: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskSqlTaskQuery {
    queryId: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotifications {
    onDurationWarningThresholdExceededs?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded[];
    onFailures?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure[];
    onStarts?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart[];
    onStreamingBacklogExceededs?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded[];
    onSuccesses?: outputs.GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess[];
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnFailure {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStart {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskForEachTaskTaskWebhookNotificationsOnSuccess {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskHealth {
    rules: outputs.GetJobJobSettingsSettingsTaskHealthRule[];
}

export interface GetJobJobSettingsSettingsTaskHealthRule {
    metric: string;
    op: string;
    value: number;
}

export interface GetJobJobSettingsSettingsTaskLibrary {
    cran?: outputs.GetJobJobSettingsSettingsTaskLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.GetJobJobSettingsSettingsTaskLibraryMaven;
    pypi?: outputs.GetJobJobSettingsSettingsTaskLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryCran {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskLibraryPypi {
    package: string;
    repo?: string;
}

export interface GetJobJobSettingsSettingsTaskNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.GetJobJobSettingsSettingsTaskNewClusterAutoscale;
    autoterminationMinutes?: number;
    awsAttributes?: outputs.GetJobJobSettingsSettingsTaskNewClusterAwsAttributes;
    azureAttributes?: outputs.GetJobJobSettingsSettingsTaskNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConf;
    clusterMountInfos?: outputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.GetJobJobSettingsSettingsTaskNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.GetJobJobSettingsSettingsTaskNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScript[];
    instancePoolId?: string;
    nodeTypeId: string;
    numWorkers: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    workloadType?: outputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadType;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeSize?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    spotBidMaxPrice?: number;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConf {
    dbfs?: outputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs;
    s3?: outputs.GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImage {
    basicAuth?: outputs.GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth;
    url: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScript {
    abfss?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfss;
    dbfs?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfs;
    file?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptFile;
    gcs?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptGcs;
    s3?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptS3;
    volumes?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumes;
    workspace?: outputs.GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptAbfss {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptDbfs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptFile {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptGcs {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptVolumes {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterInitScriptWorkspace {
    destination: string;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadType {
    clients: outputs.GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients;
}

export interface GetJobJobSettingsSettingsTaskNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface GetJobJobSettingsSettingsTaskNotebookTask {
    baseParameters?: {[key: string]: string};
    notebookPath: string;
    source?: string;
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskNotificationSettings {
    alertOnLastAttempt?: boolean;
    noAlertForCanceledRuns?: boolean;
    noAlertForSkippedRuns?: boolean;
}

export interface GetJobJobSettingsSettingsTaskPipelineTask {
    fullRefresh?: boolean;
    pipelineId: string;
}

export interface GetJobJobSettingsSettingsTaskPowerBiTask {
    connectionResourceName?: string;
    powerBiModel?: outputs.GetJobJobSettingsSettingsTaskPowerBiTaskPowerBiModel;
    refreshAfterUpdate?: boolean;
    tables?: outputs.GetJobJobSettingsSettingsTaskPowerBiTaskTable[];
    warehouseId?: string;
}

export interface GetJobJobSettingsSettingsTaskPowerBiTaskPowerBiModel {
    authenticationMethod?: string;
    modelName?: string;
    overwriteExisting?: boolean;
    storageMode?: string;
    workspaceName?: string;
}

export interface GetJobJobSettingsSettingsTaskPowerBiTaskTable {
    catalog?: string;
    /**
     * the job name of databricks.Job if the resource was matched by id.
     */
    name?: string;
    schema?: string;
    storageMode?: string;
}

export interface GetJobJobSettingsSettingsTaskPythonWheelTask {
    entryPoint?: string;
    namedParameters?: {[key: string]: string};
    packageName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskRunJobTask {
    jobId: number;
    jobParameters?: {[key: string]: string};
}

export interface GetJobJobSettingsSettingsTaskSparkJarTask {
    jarUri?: string;
    mainClassName?: string;
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskSparkPythonTask {
    parameters?: string[];
    pythonFile: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskSparkSubmitTask {
    parameters?: string[];
}

export interface GetJobJobSettingsSettingsTaskSqlTask {
    alert?: outputs.GetJobJobSettingsSettingsTaskSqlTaskAlert;
    dashboard?: outputs.GetJobJobSettingsSettingsTaskSqlTaskDashboard;
    file?: outputs.GetJobJobSettingsSettingsTaskSqlTaskFile;
    parameters?: {[key: string]: string};
    query?: outputs.GetJobJobSettingsSettingsTaskSqlTaskQuery;
    warehouseId: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlert {
    alertId: string;
    pauseSubscriptions?: boolean;
    subscriptions?: outputs.GetJobJobSettingsSettingsTaskSqlTaskAlertSubscription[];
}

export interface GetJobJobSettingsSettingsTaskSqlTaskAlertSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboard {
    customSubject?: string;
    dashboardId: string;
    pauseSubscriptions?: boolean;
    subscriptions?: outputs.GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscription[];
}

export interface GetJobJobSettingsSettingsTaskSqlTaskDashboardSubscription {
    destinationId?: string;
    userName?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskFile {
    path: string;
    source?: string;
}

export interface GetJobJobSettingsSettingsTaskSqlTaskQuery {
    queryId: string;
}

export interface GetJobJobSettingsSettingsTaskWebhookNotifications {
    onDurationWarningThresholdExceededs?: outputs.GetJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded[];
    onFailures?: outputs.GetJobJobSettingsSettingsTaskWebhookNotificationsOnFailure[];
    onStarts?: outputs.GetJobJobSettingsSettingsTaskWebhookNotificationsOnStart[];
    onStreamingBacklogExceededs?: outputs.GetJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded[];
    onSuccesses?: outputs.GetJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess[];
}

export interface GetJobJobSettingsSettingsTaskWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskWebhookNotificationsOnFailure {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskWebhookNotificationsOnStart {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTaskWebhookNotificationsOnSuccess {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsTrigger {
    fileArrival?: outputs.GetJobJobSettingsSettingsTriggerFileArrival;
    pauseStatus?: string;
    periodic?: outputs.GetJobJobSettingsSettingsTriggerPeriodic;
    tableUpdate?: outputs.GetJobJobSettingsSettingsTriggerTableUpdate;
}

export interface GetJobJobSettingsSettingsTriggerFileArrival {
    minTimeBetweenTriggersSeconds?: number;
    url: string;
    waitAfterLastChangeSeconds?: number;
}

export interface GetJobJobSettingsSettingsTriggerPeriodic {
    interval: number;
    unit: string;
}

export interface GetJobJobSettingsSettingsTriggerTableUpdate {
    condition?: string;
    minTimeBetweenTriggersSeconds?: number;
    tableNames: string[];
    waitAfterLastChangeSeconds?: number;
}

export interface GetJobJobSettingsSettingsWebhookNotifications {
    onDurationWarningThresholdExceededs?: outputs.GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded[];
    onFailures?: outputs.GetJobJobSettingsSettingsWebhookNotificationsOnFailure[];
    onStarts?: outputs.GetJobJobSettingsSettingsWebhookNotificationsOnStart[];
    onStreamingBacklogExceededs?: outputs.GetJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded[];
    onSuccesses?: outputs.GetJobJobSettingsSettingsWebhookNotificationsOnSuccess[];
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnFailure {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnStart {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetJobJobSettingsSettingsWebhookNotificationsOnSuccess {
    /**
     * the id of databricks.Job if the resource was matched by name.
     */
    id: string;
}

export interface GetMetastoreMetastoreInfo {
    cloud?: string;
    createdAt?: number;
    createdBy?: string;
    defaultDataAccessConfigId?: string;
    /**
     * The organization name of a Delta Sharing entity. This field is used for Databricks to Databricks sharing.
     */
    deltaSharingOrganizationName?: string;
    /**
     * Used to set expiration duration in seconds on recipient data access tokens.
     */
    deltaSharingRecipientTokenLifetimeInSeconds?: number;
    /**
     * Used to enable delta sharing on the metastore. Valid values: INTERNAL, INTERNAL_AND_EXTERNAL. INTERNAL only allows sharing within the same account, and INTERNAL_AND_EXTERNAL allows cross account sharing and token based sharing.
     */
    deltaSharingScope?: string;
    externalAccessEnabled?: boolean;
    globalMetastoreId?: string;
    /**
     * ID of the metastore
     */
    metastoreId?: string;
    /**
     * Name of the metastore
     */
    name?: string;
    /**
     * Username/groupname/sp applicationId of the metastore owner.
     */
    owner?: string;
    privilegeModelVersion?: string;
    /**
     * Region of the metastore
     */
    region?: string;
    /**
     * Path on cloud storage account, where managed `databricks.Table` are stored.
     */
    storageRoot?: string;
    storageRootCredentialId?: string;
    storageRootCredentialName?: string;
    updatedAt?: number;
    updatedBy?: string;
}

export interface GetMlflowExperimentTag {
    key?: string;
    value?: string;
}

export interface GetMlflowModelLatestVersion {
    creationTimestamp?: number;
    currentStage?: string;
    /**
     * User-specified description for the object.
     */
    description?: string;
    lastUpdatedTimestamp?: number;
    /**
     * Name of the registered model.
     */
    name?: string;
    runId?: string;
    runLink?: string;
    source?: string;
    status?: string;
    statusMessage?: string;
    /**
     * Array of tags associated with the model.
     */
    tags?: outputs.GetMlflowModelLatestVersionTag[];
    /**
     * The username of the user that created the object.
     */
    userId?: string;
    version?: string;
}

export interface GetMlflowModelLatestVersionTag {
    key?: string;
    value?: string;
}

export interface GetMlflowModelTag {
    key?: string;
    value?: string;
}

export interface GetMwsNetworkConnectivityConfigEgressConfig {
    /**
     * Array of default rules.
     */
    defaultRules?: outputs.GetMwsNetworkConnectivityConfigEgressConfigDefaultRules;
    /**
     * Array of target rules.
     */
    targetRules?: outputs.GetMwsNetworkConnectivityConfigEgressConfigTargetRules;
}

export interface GetMwsNetworkConnectivityConfigEgressConfigDefaultRules {
    /**
     * The stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.
     */
    awsStableIpRule?: outputs.GetMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule;
    /**
     * Array of Azure service endpoint rules.
     */
    azureServiceEndpointRule?: outputs.GetMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule;
}

export interface GetMwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule {
    /**
     * The list of stable IP CIDR blocks from which Databricks network traffic originates when accessing your resources.
     */
    cidrBlocks?: string[];
}

export interface GetMwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule {
    /**
     * Array of strings representing the subnet IDs.
     */
    subnets?: string[];
    /**
     * The target region for the service endpoint.
     */
    targetRegion?: string;
    /**
     * Array of target services.
     */
    targetServices?: string[];
}

export interface GetMwsNetworkConnectivityConfigEgressConfigTargetRules {
    /**
     * Array of private endpoint rule objects.
     */
    azurePrivateEndpointRules?: outputs.GetMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule[];
}

export interface GetMwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule {
    /**
     * The current status of this private endpoint.
     */
    connectionState?: string;
    /**
     * Time in epoch milliseconds when this object was created.
     */
    creationTime?: number;
    /**
     * Whether this private endpoint is deactivated.
     */
    deactivated?: boolean;
    /**
     * Time in epoch milliseconds when this object was deactivated.
     */
    deactivatedAt?: number;
    /**
     * The name of the Azure private endpoint resource.
     */
    endpointName?: string;
    /**
     * The sub-resource type (group ID) of the target resource.
     */
    groupId?: string;
    /**
     * The Databricks network connectivity configuration ID.
     */
    networkConnectivityConfigId?: string;
    /**
     * The Azure resource ID of the target resource.
     */
    resourceId?: string;
    /**
     * The ID of a private endpoint rule.
     */
    ruleId?: string;
    /**
     * Time in epoch milliseconds when the network was updated.
     */
    updatedTime?: number;
}

export interface GetNotebookPathsNotebookPathList {
    language?: string;
    /**
     * Path to workspace directory
     */
    path?: string;
}

export interface GetNotificationDestinationsNotificationDestination {
    /**
     * The type of the notification destination. Possible values are `EMAIL`, `MICROSOFT_TEAMS`, `PAGERDUTY`, `SLACK`, or `WEBHOOK`.
     */
    destinationType?: string;
    /**
     * The display name of the Notification Destination.
     */
    displayName?: string;
    /**
     * The unique ID of the Notification Destination.
     */
    id?: string;
}

export interface GetRegisteredModelModelInfo {
    /**
     * the list of aliases associated with this model. Each item is object consisting of following attributes:
     */
    aliases?: outputs.GetRegisteredModelModelInfoAlias[];
    browseOnly?: boolean;
    /**
     * The name of the catalog where the schema and the registered model reside.
     */
    catalogName?: string;
    /**
     * The comment attached to the registered model.
     */
    comment?: string;
    /**
     * the Unix timestamp at the model's creation
     */
    createdAt?: number;
    /**
     * the identifier of the user who created the model
     */
    createdBy?: string;
    /**
     * The fully-qualified name of the registered model (`catalog_name.schema_name.name`).
     */
    fullName?: string;
    /**
     * the unique identifier of the metastore
     */
    metastoreId?: string;
    /**
     * The name of the registered model.
     */
    name?: string;
    /**
     * Name of the registered model owner.
     */
    owner?: string;
    /**
     * The name of the schema where the registered model resides.
     */
    schemaName?: string;
    /**
     * The storage location under which model version data files are stored.
     */
    storageLocation?: string;
    /**
     * the timestamp of the last time changes were made to the model
     */
    updatedAt?: number;
    /**
     * the identifier of the user who updated the model last time
     */
    updatedBy?: string;
}

export interface GetRegisteredModelModelInfoAlias {
    /**
     * string with the name of alias
     */
    aliasName?: string;
    /**
     * associated model version
     */
    versionNum?: number;
}

export interface GetRegisteredModelVersionsModelVersion {
    /**
     * the list of aliases associated with this model. Each item is object consisting of following attributes:
     */
    aliases?: outputs.GetRegisteredModelVersionsModelVersionAlias[];
    browseOnly?: boolean;
    /**
     * The name of the catalog where the schema and the registered model reside.
     */
    catalogName?: string;
    /**
     * The comment attached to the registered model.
     */
    comment?: string;
    /**
     * the Unix timestamp at the model's creation
     */
    createdAt?: number;
    /**
     * the identifier of the user who created the model
     */
    createdBy?: string;
    /**
     * The unique identifier of the model version
     */
    id?: string;
    /**
     * the unique identifier of the metastore
     */
    metastoreId?: string;
    modelName?: string;
    /**
     * block describing model version dependencies, for feature-store packaged models. Consists of following attributes:
     */
    modelVersionDependencies?: outputs.GetRegisteredModelVersionsModelVersionModelVersionDependency[];
    /**
     * MLflow run ID used when creating the model version, if `source` was generated by an experiment run stored in an MLflow tracking server
     */
    runId?: string;
    /**
     * ID of the Databricks workspace containing the MLflow run that generated this model version, if applicable
     */
    runWorkspaceId?: number;
    /**
     * The name of the schema where the registered model resides.
     */
    schemaName?: string;
    /**
     * URI indicating the location of the source artifacts (files) for the model version.
     */
    source?: string;
    /**
     * Current status of the model version.
     */
    status?: string;
    /**
     * The storage location under which model version data files are stored.
     */
    storageLocation?: string;
    /**
     * the timestamp of the last time changes were made to the model
     */
    updatedAt?: number;
    /**
     * the identifier of the user who updated the model last time
     */
    updatedBy?: string;
    /**
     * Integer model version number, used to reference the model version in API requests.
     */
    version?: number;
}

export interface GetRegisteredModelVersionsModelVersionAlias {
    /**
     * string with the name of alias
     */
    aliasName?: string;
    /**
     * associated model version
     */
    versionNum?: number;
}

export interface GetRegisteredModelVersionsModelVersionModelVersionDependency {
    /**
     * list of dependencies consisting of following attributes:
     */
    dependencies?: outputs.GetRegisteredModelVersionsModelVersionModelVersionDependencyDependency[];
}

export interface GetRegisteredModelVersionsModelVersionModelVersionDependencyDependency {
    /**
     * A function that is dependent on a SQL object:
     */
    functions?: outputs.GetRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction[];
    /**
     * A table that is dependent on a SQL object
     */
    tables?: outputs.GetRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable[];
}

export interface GetRegisteredModelVersionsModelVersionModelVersionDependencyDependencyFunction {
    /**
     * Full name of the dependent function
     */
    functionFullName: string;
}

export interface GetRegisteredModelVersionsModelVersionModelVersionDependencyDependencyTable {
    /**
     * Full name of the dependent table
     */
    tableFullName: string;
}

export interface GetSchemaSchemaInfo {
    /**
     * indicates whether the principal is limited to retrieving metadata for the schema through the BROWSE privilege.
     */
    browseOnly?: boolean;
    /**
     * the name of the catalog where the schema is.
     */
    catalogName?: string;
    /**
     * the type of the parent catalog.
     */
    catalogType?: string;
    /**
     * the comment attached to the volume
     */
    comment?: string;
    /**
     * time at which this schema was created, in epoch milliseconds.
     */
    createdAt?: number;
    /**
     * username of schema creator.
     */
    createdBy?: string;
    /**
     * information about actual state of predictive optimization.
     */
    effectivePredictiveOptimizationFlag?: outputs.GetSchemaSchemaInfoEffectivePredictiveOptimizationFlag;
    /**
     * whether predictive optimization should be enabled for this object and objects under it.
     */
    enablePredictiveOptimization?: string;
    /**
     * the two-level (fully qualified) name of the schema
     */
    fullName?: string;
    /**
     * the unique identifier of the metastore
     */
    metastoreId?: string;
    /**
     * a fully qualified name of databricks_schema: *`catalog`.`schema`*
     */
    name?: string;
    /**
     * the identifier of the user who owns the schema
     */
    owner?: string;
    /**
     * map of properties set on the schema
     */
    properties?: {[key: string]: string};
    /**
     * the unique identifier of the volume
     */
    schemaId?: string;
    /**
     * the storage location on the cloud.
     */
    storageLocation?: string;
    /**
     * storage root URL for managed tables within schema.
     */
    storageRoot?: string;
    /**
     * the timestamp of the last time changes were made to the schema
     */
    updatedAt?: number;
    /**
     * the identifier of the user who updated the schema last time
     */
    updatedBy?: string;
}

export interface GetSchemaSchemaInfoEffectivePredictiveOptimizationFlag {
    inheritedFromName?: string;
    inheritedFromType?: string;
    value: string;
}

export interface GetServingEndpointsEndpoint {
    /**
     * A block with AI Gateway configuration for the serving endpoint.
     */
    aiGateways?: outputs.GetServingEndpointsEndpointAiGateway[];
    budgetPolicyId?: string;
    /**
     * The model serving endpoint configuration.
     */
    configs?: outputs.GetServingEndpointsEndpointConfig[];
    creationTimestamp?: number;
    creator?: string;
    id?: string;
    lastUpdatedTimestamp?: number;
    /**
     * The name of the model serving endpoint.
     */
    name?: string;
    states?: outputs.GetServingEndpointsEndpointState[];
    /**
     * Tags to be attached to the serving endpoint and automatically propagated to billing logs.
     */
    tags?: outputs.GetServingEndpointsEndpointTag[];
    task?: string;
}

export interface GetServingEndpointsEndpointAiGateway {
    fallbackConfigs?: outputs.GetServingEndpointsEndpointAiGatewayFallbackConfig[];
    guardrails?: outputs.GetServingEndpointsEndpointAiGatewayGuardrail[];
    inferenceTableConfigs?: outputs.GetServingEndpointsEndpointAiGatewayInferenceTableConfig[];
    /**
     * A list of rate limit blocks to be applied to the serving endpoint.
     */
    rateLimits?: outputs.GetServingEndpointsEndpointAiGatewayRateLimit[];
    usageTrackingConfigs?: outputs.GetServingEndpointsEndpointAiGatewayUsageTrackingConfig[];
}

export interface GetServingEndpointsEndpointAiGatewayFallbackConfig {
    enabled: boolean;
}

export interface GetServingEndpointsEndpointAiGatewayGuardrail {
    inputProperties?: outputs.GetServingEndpointsEndpointAiGatewayGuardrailInputProperty[];
    outputs?: outputs.GetServingEndpointsEndpointAiGatewayGuardrailOutput[];
}

export interface GetServingEndpointsEndpointAiGatewayGuardrailInputProperty {
    invalidKeywords?: string[];
    piis?: outputs.GetServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii[];
    safety?: boolean;
    validTopics?: string[];
}

export interface GetServingEndpointsEndpointAiGatewayGuardrailInputPropertyPii {
    behavior?: string;
}

export interface GetServingEndpointsEndpointAiGatewayGuardrailOutput {
    invalidKeywords?: string[];
    piis?: outputs.GetServingEndpointsEndpointAiGatewayGuardrailOutputPii[];
    safety?: boolean;
    validTopics?: string[];
}

export interface GetServingEndpointsEndpointAiGatewayGuardrailOutputPii {
    behavior?: string;
}

export interface GetServingEndpointsEndpointAiGatewayInferenceTableConfig {
    catalogName?: string;
    enabled?: boolean;
    schemaName?: string;
    tableNamePrefix?: string;
}

export interface GetServingEndpointsEndpointAiGatewayRateLimit {
    calls: number;
    key?: string;
    renewalPeriod: string;
}

export interface GetServingEndpointsEndpointAiGatewayUsageTrackingConfig {
    enabled?: boolean;
}

export interface GetServingEndpointsEndpointConfig {
    servedEntities?: outputs.GetServingEndpointsEndpointConfigServedEntity[];
    servedModels?: outputs.GetServingEndpointsEndpointConfigServedModel[];
}

export interface GetServingEndpointsEndpointConfigServedEntity {
    entityName?: string;
    entityVersion?: string;
    externalModels?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModel[];
    foundationModels?: outputs.GetServingEndpointsEndpointConfigServedEntityFoundationModel[];
    /**
     * The name of the model serving endpoint.
     */
    name?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModel {
    ai21labsConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig[];
    amazonBedrockConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig[];
    anthropicConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig[];
    cohereConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig[];
    customProviderConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig[];
    databricksModelServingConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig[];
    googleCloudVertexAiConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig[];
    /**
     * The name of the model serving endpoint.
     */
    name: string;
    openaiConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig[];
    palmConfigs?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig[];
    provider: string;
    task: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelAi21labsConfig {
    ai21labsApiKey?: string;
    ai21labsApiKeyPlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelAmazonBedrockConfig {
    awsAccessKeyId?: string;
    awsAccessKeyIdPlaintext?: string;
    awsRegion: string;
    awsSecretAccessKey?: string;
    awsSecretAccessKeyPlaintext?: string;
    bedrockProvider: string;
    instanceProfileArn?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelAnthropicConfig {
    anthropicApiKey?: string;
    anthropicApiKeyPlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelCohereConfig {
    cohereApiBase?: string;
    cohereApiKey?: string;
    cohereApiKeyPlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfig {
    apiKeyAuths?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth[];
    bearerTokenAuths?: outputs.GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth[];
    customProviderUrl: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth {
    key: string;
    value?: string;
    valuePlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth {
    token?: string;
    tokenPlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelDatabricksModelServingConfig {
    databricksApiToken?: string;
    databricksApiTokenPlaintext?: string;
    databricksWorkspaceUrl: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelGoogleCloudVertexAiConfig {
    privateKey?: string;
    privateKeyPlaintext?: string;
    projectId: string;
    region: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelOpenaiConfig {
    microsoftEntraClientId?: string;
    microsoftEntraClientSecret?: string;
    microsoftEntraClientSecretPlaintext?: string;
    microsoftEntraTenantId?: string;
    openaiApiBase?: string;
    openaiApiKey?: string;
    openaiApiKeyPlaintext?: string;
    openaiApiType?: string;
    openaiApiVersion?: string;
    openaiDeploymentName?: string;
    openaiOrganization?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityExternalModelPalmConfig {
    palmApiKey?: string;
    palmApiKeyPlaintext?: string;
}

export interface GetServingEndpointsEndpointConfigServedEntityFoundationModel {
    description?: string;
    displayName?: string;
    docs?: string;
    /**
     * The name of the model serving endpoint.
     */
    name?: string;
}

export interface GetServingEndpointsEndpointConfigServedModel {
    modelName?: string;
    modelVersion?: string;
    /**
     * The name of the model serving endpoint.
     */
    name?: string;
}

export interface GetServingEndpointsEndpointState {
    configUpdate?: string;
    ready?: string;
}

export interface GetServingEndpointsEndpointTag {
    key: string;
    value?: string;
}

export interface GetShareObject {
    addedAt: number;
    addedBy: string;
    cdfEnabled?: boolean;
    /**
     * Description about the object.
     */
    comment?: string;
    content?: string;
    /**
     * Type of the object.
     */
    dataObjectType: string;
    historyDataSharingStatus?: string;
    /**
     * The name of the share
     */
    name: string;
    partitions?: outputs.GetShareObjectPartition[];
    sharedAs?: string;
    startVersion?: number;
    status: string;
    stringSharedAs?: string;
}

export interface GetShareObjectPartition {
    values?: outputs.GetShareObjectPartitionValue[];
}

export interface GetShareObjectPartitionValue {
    /**
     * The name of the share
     */
    name: string;
    op: string;
    recipientPropertyKey?: string;
    value?: string;
}

export interface GetSqlWarehouseChannel {
    dbsqlVersion?: string;
    /**
     * Name of the SQL warehouse to search (case-sensitive).
     */
    name?: string;
}

export interface GetSqlWarehouseHealth {
    details?: string;
    failureReason?: outputs.GetSqlWarehouseHealthFailureReason;
    message?: string;
    status?: string;
    summary?: string;
}

export interface GetSqlWarehouseHealthFailureReason {
    code?: string;
    parameters?: {[key: string]: string};
    type?: string;
}

export interface GetSqlWarehouseOdbcParams {
    hostname?: string;
    path?: string;
    port?: number;
    protocol?: string;
}

export interface GetSqlWarehouseTags {
    customTags?: outputs.GetSqlWarehouseTagsCustomTag[];
}

export interface GetSqlWarehouseTagsCustomTag {
    key?: string;
    value?: string;
}

export interface GetStorageCredentialStorageCredentialInfo {
    /**
     * credential details for AWS:
     */
    awsIamRole?: outputs.GetStorageCredentialStorageCredentialInfoAwsIamRole;
    /**
     * managed identity credential details for Azure
     */
    azureManagedIdentity?: outputs.GetStorageCredentialStorageCredentialInfoAzureManagedIdentity;
    /**
     * service principal credential details for Azure:
     */
    azureServicePrincipal?: outputs.GetStorageCredentialStorageCredentialInfoAzureServicePrincipal;
    cloudflareApiToken?: outputs.GetStorageCredentialStorageCredentialInfoCloudflareApiToken;
    comment?: string;
    /**
     * Time at which this catalog was created, in epoch milliseconds.
     */
    createdAt?: number;
    /**
     * Username of catalog creator.
     */
    createdBy?: string;
    /**
     * credential details for GCP:
     */
    databricksGcpServiceAccount?: outputs.GetStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount;
    fullName?: string;
    /**
     * Unique ID of storage credential.
     */
    id?: string;
    isolationMode?: string;
    /**
     * Unique identifier of the parent Metastore.
     */
    metastoreId?: string;
    /**
     * The name of the storage credential
     */
    name?: string;
    /**
     * Username/groupname/sp applicationId of the storage credential owner.
     */
    owner?: string;
    /**
     * Indicates whether the storage credential is only usable for read operations.
     */
    readOnly?: boolean;
    /**
     * Time at which this catalog was last modified, in epoch milliseconds.
     */
    updatedAt?: number;
    /**
     * Username of user who last modified catalog.
     */
    updatedBy?: string;
    usedForManagedStorage?: boolean;
}

export interface GetStorageCredentialStorageCredentialInfoAwsIamRole {
    /**
     * (output only) - The external ID used in role assumption to prevent confused deputy problem.
     */
    externalId?: string;
    /**
     * The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`
     */
    roleArn: string;
    /**
     * (output only) - The Amazon Resource Name (ARN) of the AWS IAM user managed by Databricks. This is the identity that is going to assume the AWS IAM role.
     */
    unityCatalogIamArn?: string;
}

export interface GetStorageCredentialStorageCredentialInfoAzureManagedIdentity {
    /**
     * The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.
     */
    accessConnectorId: string;
    credentialId?: string;
    /**
     * The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.
     */
    managedIdentityId?: string;
}

export interface GetStorageCredentialStorageCredentialInfoAzureServicePrincipal {
    /**
     * The application ID of the application registration within the referenced AAD tenant
     */
    applicationId: string;
    clientSecret: string;
    /**
     * The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application
     */
    directoryId: string;
}

export interface GetStorageCredentialStorageCredentialInfoCloudflareApiToken {
    accessKeyId: string;
    accountId: string;
    secretAccessKey: string;
}

export interface GetStorageCredentialStorageCredentialInfoDatabricksGcpServiceAccount {
    credentialId?: string;
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     */
    email?: string;
}

export interface GetTableTableInfo {
    accessPoint?: string;
    browseOnly?: boolean;
    /**
     * Name of parent catalog.
     */
    catalogName?: string;
    /**
     * Array of ColumnInfo objects of the table's columns
     */
    columns?: outputs.GetTableTableInfoColumn[];
    /**
     * Free-form text description
     */
    comment?: string;
    createdAt?: number;
    createdBy?: string;
    dataAccessConfigurationId?: string;
    /**
     * Table format, e.g. DELTA, CSV, JSON
     */
    dataSourceFormat?: string;
    deletedAt?: number;
    deltaRuntimePropertiesKvpairs?: outputs.GetTableTableInfoDeltaRuntimePropertiesKvpairs;
    effectivePredictiveOptimizationFlag?: outputs.GetTableTableInfoEffectivePredictiveOptimizationFlag;
    enablePredictiveOptimization?: string;
    encryptionDetails?: outputs.GetTableTableInfoEncryptionDetails;
    fullName?: string;
    metastoreId?: string;
    /**
     * Full name of the databricks_table: _`catalog`.`schema`.`table`_
     */
    name?: string;
    /**
     * Current owner of the table
     */
    owner?: string;
    pipelineId?: string;
    properties?: {[key: string]: string};
    rowFilter?: outputs.GetTableTableInfoRowFilter;
    /**
     * Name of parent schema relative to its parent catalog.
     */
    schemaName?: string;
    sqlPath?: string;
    storageCredentialName?: string;
    storageLocation?: string;
    tableConstraints?: outputs.GetTableTableInfoTableConstraint[];
    tableId?: string;
    /**
     * Table type, e.g. MANAGED, EXTERNAL, VIEW
     */
    tableType?: string;
    updatedAt?: number;
    updatedBy?: string;
    /**
     * View definition SQL (when `tableType` is VIEW, MATERIALIZED_VIEW, or STREAMING_TABLE)
     */
    viewDefinition?: string;
    /**
     * View dependencies (when `tableType` is VIEW or MATERIALIZED_VIEW, STREAMING_TABLE)
     */
    viewDependencies?: outputs.GetTableTableInfoViewDependencies;
}

export interface GetTableTableInfoColumn {
    /**
     * Free-form text description
     */
    comment?: string;
    mask?: outputs.GetTableTableInfoColumnMask;
    /**
     * Full name of the databricks_table: _`catalog`.`schema`.`table`_
     */
    name?: string;
    nullable?: boolean;
    partitionIndex?: number;
    position?: number;
    typeIntervalType?: string;
    typeJson?: string;
    typeName?: string;
    typePrecision?: number;
    typeScale?: number;
    typeText?: string;
}

export interface GetTableTableInfoColumnMask {
    functionName?: string;
    usingColumnNames?: string[];
}

export interface GetTableTableInfoDeltaRuntimePropertiesKvpairs {
    deltaRuntimeProperties: {[key: string]: string};
}

export interface GetTableTableInfoEffectivePredictiveOptimizationFlag {
    inheritedFromName?: string;
    inheritedFromType?: string;
    value: string;
}

export interface GetTableTableInfoEncryptionDetails {
    sseEncryptionDetails?: outputs.GetTableTableInfoEncryptionDetailsSseEncryptionDetails;
}

export interface GetTableTableInfoEncryptionDetailsSseEncryptionDetails {
    algorithm?: string;
    awsKmsKeyArn?: string;
}

export interface GetTableTableInfoRowFilter {
    functionName: string;
    inputColumnNames: string[];
}

export interface GetTableTableInfoTableConstraint {
    foreignKeyConstraint?: outputs.GetTableTableInfoTableConstraintForeignKeyConstraint;
    namedTableConstraint?: outputs.GetTableTableInfoTableConstraintNamedTableConstraint;
    primaryKeyConstraint?: outputs.GetTableTableInfoTableConstraintPrimaryKeyConstraint;
}

export interface GetTableTableInfoTableConstraintForeignKeyConstraint {
    childColumns: string[];
    /**
     * Full name of the databricks_table: _`catalog`.`schema`.`table`_
     */
    name: string;
    parentColumns: string[];
    parentTable: string;
}

export interface GetTableTableInfoTableConstraintNamedTableConstraint {
    /**
     * Full name of the databricks_table: _`catalog`.`schema`.`table`_
     */
    name: string;
}

export interface GetTableTableInfoTableConstraintPrimaryKeyConstraint {
    childColumns: string[];
    /**
     * Full name of the databricks_table: _`catalog`.`schema`.`table`_
     */
    name: string;
}

export interface GetTableTableInfoViewDependencies {
    dependencies?: outputs.GetTableTableInfoViewDependenciesDependency[];
}

export interface GetTableTableInfoViewDependenciesDependency {
    function?: outputs.GetTableTableInfoViewDependenciesDependencyFunction;
    table?: outputs.GetTableTableInfoViewDependenciesDependencyTable;
}

export interface GetTableTableInfoViewDependenciesDependencyFunction {
    functionFullName: string;
}

export interface GetTableTableInfoViewDependenciesDependencyTable {
    tableFullName: string;
}

export interface GetVolumeVolumeInfo {
    /**
     * the AWS access point to use when accessing s3 bucket for this volume's external location
     */
    accessPoint?: string;
    /**
     * indicates whether the principal is limited to retrieving metadata for the volume through the BROWSE privilege when includeBrowse is enabled in the request.
     */
    browseOnly?: boolean;
    /**
     * the name of the catalog where the schema and the volume are
     */
    catalogName?: string;
    /**
     * the comment attached to the volume
     */
    comment?: string;
    /**
     * the Unix timestamp at the volume's creation
     */
    createdAt?: number;
    /**
     * the identifier of the user who created the volume
     */
    createdBy?: string;
    /**
     * encryption options that apply to clients connecting to cloud storage
     */
    encryptionDetails?: outputs.GetVolumeVolumeInfoEncryptionDetails;
    /**
     * the three-level (fully qualified) name of the volume
     */
    fullName?: string;
    /**
     * the unique identifier of the metastore
     */
    metastoreId?: string;
    /**
     * a fully qualified name of databricks_volume: *`catalog`.`schema`.`volume`*
     */
    name?: string;
    /**
     * the identifier of the user who owns the volume
     */
    owner?: string;
    /**
     * the name of the schema where the volume is
     */
    schemaName?: string;
    /**
     * the storage location on the cloud
     */
    storageLocation?: string;
    /**
     * the timestamp of the last time changes were made to the volume
     */
    updatedAt?: number;
    /**
     * the identifier of the user who updated the volume last time
     */
    updatedBy?: string;
    /**
     * the unique identifier of the volume
     */
    volumeId?: string;
    /**
     * whether the volume is `MANAGED` or `EXTERNAL`
     */
    volumeType?: string;
}

export interface GetVolumeVolumeInfoEncryptionDetails {
    sseEncryptionDetails?: outputs.GetVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails;
}

export interface GetVolumeVolumeInfoEncryptionDetailsSseEncryptionDetails {
    algorithm?: string;
    awsKmsKeyArn?: string;
}

export interface GrantsGrant {
    principal: string;
    privileges: string[];
}

export interface InstancePoolAwsAttributes {
    /**
     * (String) Availability type used for all instances in the pool. Only `ON_DEMAND` and `SPOT` are supported.
     */
    availability?: string;
    /**
     * (Integer) The max price for AWS spot instances, as a percentage of the corresponding instance type’s on-demand price. For example, if this field is set to 50, and the instance pool needs a new i3.xlarge spot instance, then the max price is half of the price of on-demand i3.xlarge instances. Similarly, if this field is set to 200, the max price is twice the price of on-demand i3.xlarge instances. If not specified, the *default value is 100*. When spot instances are requested for this instance pool, only spot instances whose max price percentage matches this field are considered. *For safety, this field cannot be greater than 10000.*
     */
    spotBidPricePercent?: number;
    /**
     * (String) Identifier for the availability zone/datacenter in which the instance pool resides. This string is of the form like `"us-west-2a"`. The provided availability zone must be in the same region as the Databricks deployment. For example, `"us-west-2a"` is not a valid zone ID if the Databricks deployment resides in the `"us-east-1"` region. If not specified, a default zone is used. You can find the list of available zones as well as the default value by using the [List Zones API](https://docs.databricks.com/dev-tools/api/latest/clusters.html#clusterclusterservicelistavailablezones).
     */
    zoneId: string;
}

export interface InstancePoolAzureAttributes {
    /**
     * Availability type used for all nodes. Valid values are `SPOT_AZURE` and `ON_DEMAND_AZURE`.
     */
    availability?: string;
    /**
     * The max bid price used for Azure spot instances. You can set this to greater than or equal to the current spot price. You can also set this to `-1`, which specifies that the instance cannot be evicted on the basis of price. The price for the instance will be the current price for spot instances or the price for a standard instance.
     */
    spotBidMaxPrice?: number;
}

export interface InstancePoolDiskSpec {
    /**
     * (Integer) The number of disks to attach to each instance. This feature is only enabled for supported node types. Users can choose up to the limit of the disks supported by the node type. For node types with no local disk, at least one disk needs to be specified.
     */
    diskCount?: number;
    /**
     * (Integer) The size of each disk (in GiB) to attach.
     */
    diskSize?: number;
    diskType?: outputs.InstancePoolDiskSpecDiskType;
}

export interface InstancePoolDiskSpecDiskType {
    azureDiskVolumeType?: string;
    ebsVolumeType?: string;
}

export interface InstancePoolGcpAttributes {
    /**
     * Availability type used for all nodes. Valid values are `PREEMPTIBLE_GCP`, `PREEMPTIBLE_WITH_FALLBACK_GCP` and `ON_DEMAND_GCP`, default: `ON_DEMAND_GCP`.
     */
    gcpAvailability?: string;
    /**
     * Number of local SSD disks (each is 375GB in size) that will be attached to each node of the cluster.
     */
    localSsdCount: number;
    /**
     * Identifier for the availability zone/datacenter in which the cluster resides. This string will be of a form like `us-central1-a`. The provided availability zone must be in the same region as the Databricks workspace.
     */
    zoneId: string;
}

export interface InstancePoolInstancePoolFleetAttributes {
    fleetOnDemandOption?: outputs.InstancePoolInstancePoolFleetAttributesFleetOnDemandOption;
    fleetSpotOption?: outputs.InstancePoolInstancePoolFleetAttributesFleetSpotOption;
    launchTemplateOverrides: outputs.InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride[];
}

export interface InstancePoolInstancePoolFleetAttributesFleetOnDemandOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface InstancePoolInstancePoolFleetAttributesFleetSpotOption {
    allocationStrategy: string;
    instancePoolsToUseCount?: number;
}

export interface InstancePoolInstancePoolFleetAttributesLaunchTemplateOverride {
    availabilityZone: string;
    instanceType: string;
}

export interface InstancePoolPreloadedDockerImage {
    /**
     * `basic_auth.username` and `basic_auth.password` for Docker repository. Docker registry credentials are encrypted when they are stored in Databricks internal storage and when they are passed to a registry upon fetching Docker images at cluster launch. However, other authenticated and authorized API users of this workspace can access the username and password.
     *
     * Example usage with azurermContainerRegistry and docker_registry_image, that you can adapt to your specific use-case:
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     * import * as docker from "@pulumi/docker";
     *
     * const _this = new docker.index.RegistryImage("this", {
     *     build: [{}],
     *     name: `${thisAzurermContainerRegistry.loginServer}/sample:latest`,
     * });
     * const thisInstancePool = new databricks.InstancePool("this", {preloadedDockerImages: [{
     *     url: _this.name,
     *     basicAuth: {
     *         username: thisAzurermContainerRegistry.adminUsername,
     *         password: thisAzurermContainerRegistry.adminPassword,
     *     },
     * }]});
     * ```
     */
    basicAuth?: outputs.InstancePoolPreloadedDockerImageBasicAuth;
    /**
     * URL for the Docker image
     */
    url: string;
}

export interface InstancePoolPreloadedDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface JobContinuous {
    /**
     * Indicate whether this continuous job is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: string;
}

export interface JobDbtTask {
    /**
     * The name of the catalog to use inside Unity Catalog.
     */
    catalog?: string;
    /**
     * (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".
     */
    commands: string[];
    /**
     * The relative path to the directory in the repository specified by `gitSource` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.
     */
    profilesDirectory?: string;
    /**
     * The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.
     * * If `source` is `GIT`: Relative path to the directory in the repository specified in the `gitSource` block. Defaults to the repository's root directory when not specified.
     * * If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.
     */
    projectDirectory?: string;
    /**
     * The name of the schema dbt should run in. Defaults to `default`.
     */
    schema?: string;
    /**
     * The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `gitSource` block is present in the job definition.
     */
    source?: string;
    /**
     * The ID of the SQL warehouse that dbt should execute against.
     *
     * You also need to include a `gitSource` block to configure the repository that contains the dbt project.
     */
    warehouseId?: string;
}

export interface JobDeployment {
    kind: string;
    metadataFilePath?: string;
}

export interface JobEmailNotifications {
    /**
     * (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notificationSettings` configuration block).
     */
    noAlertForSkippedRuns?: boolean;
    /**
     * (List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: string[];
    /**
     * (List) list of emails to notify when the run fails.
     */
    onFailures?: string[];
    /**
     * (List) list of emails to notify when the run starts.
     */
    onStarts?: string[];
    /**
     * (List) list of emails to notify when any streaming backlog thresholds are exceeded for any stream.
     *
     * The following parameter is only available for the job level configuration.
     */
    onStreamingBacklogExceededs?: string[];
    /**
     * (List) list of emails to notify when the run completes successfully.
     */
    onSuccesses?: string[];
}

export interface JobEnvironment {
    /**
     * an unique identifier of the Environment.  It will be referenced from `environmentKey` attribute of corresponding task.
     */
    environmentKey: string;
    /**
     * block describing the Environment. Consists of following attributes:
     */
    spec?: outputs.JobEnvironmentSpec;
}

export interface JobEnvironmentSpec {
    /**
     * client version used by the environment.
     */
    client: string;
    /**
     * List of pip dependencies, as supported by the version of pip in this environment. Each dependency is a pip requirement file line.  See [API docs](https://docs.databricks.com/api/workspace/jobs/create#environments-spec-dependencies) for more information.
     */
    dependencies?: string[];
    jarDependencies?: string[];
}

export interface JobGitSource {
    /**
     * name of the Git branch to use. Conflicts with `tag` and `commit`.
     */
    branch?: string;
    /**
     * hash of Git commit to use. Conflicts with `branch` and `tag`.
     */
    commit?: string;
    gitSnapshot?: outputs.JobGitSourceGitSnapshot;
    jobSource?: outputs.JobGitSourceJobSource;
    /**
     * case insensitive name of the Git provider.  Following values are supported right now (could be a subject for change, consult [Repos API documentation](https://docs.databricks.com/dev-tools/api/latest/repos.html)): `gitHub`, `gitHubEnterprise`, `bitbucketCloud`, `bitbucketServer`, `azureDevOpsServices`, `gitLab`, `gitLabEnterpriseEdition`.
     */
    provider?: string;
    /**
     * name of the Git branch to use. Conflicts with `branch` and `commit`.
     */
    tag?: string;
    /**
     * URL of the Git repository to use.
     */
    url: string;
}

export interface JobGitSourceGitSnapshot {
    usedCommit?: string;
}

export interface JobGitSourceJobSource {
    dirtyState?: string;
    importFromGitBranch: string;
    jobConfigPath: string;
}

export interface JobHealth {
    /**
     * list of rules that are represented as objects with the following attributes:
     */
    rules: outputs.JobHealthRule[];
}

export interface JobHealthRule {
    /**
     * string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.
     */
    metric: string;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op: string;
    /**
     * integer value used to compare to the given metric.
     */
    value: number;
}

export interface JobJobCluster {
    /**
     * Identifier that can be referenced in `task` block, so that cluster is shared between tasks
     */
    jobClusterKey: string;
    /**
     * Block with almost the same set of parameters as for databricks.Cluster resource, except following (check the [REST API documentation for full list of supported parameters](https://docs.databricks.com/api/workspace/jobs/create#job_clusters-new_cluster)):
     */
    newCluster: outputs.JobJobClusterNewCluster;
}

export interface JobJobClusterNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.JobJobClusterNewClusterAutoscale;
    awsAttributes?: outputs.JobJobClusterNewClusterAwsAttributes;
    azureAttributes?: outputs.JobJobClusterNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.JobJobClusterNewClusterClusterLogConf;
    clusterMountInfos?: outputs.JobJobClusterNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.JobJobClusterNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.JobJobClusterNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.JobJobClusterNewClusterInitScript[];
    instancePoolId?: string;
    isSingleNode?: boolean;
    kind?: string;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: outputs.JobJobClusterNewClusterLibrary[];
    nodeTypeId: string;
    numWorkers?: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    useMlRuntime?: boolean;
    /**
     * isn't supported
     */
    workloadType?: outputs.JobJobClusterNewClusterWorkloadType;
}

export interface JobJobClusterNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface JobJobClusterNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface JobJobClusterNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface JobJobClusterNewClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface JobJobClusterNewClusterClusterLogConf {
    dbfs?: outputs.JobJobClusterNewClusterClusterLogConfDbfs;
    s3?: outputs.JobJobClusterNewClusterClusterLogConfS3;
    volumes?: outputs.JobJobClusterNewClusterClusterLogConfVolumes;
}

export interface JobJobClusterNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface JobJobClusterNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobJobClusterNewClusterClusterLogConfVolumes {
    destination: string;
}

export interface JobJobClusterNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface JobJobClusterNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface JobJobClusterNewClusterDockerImage {
    basicAuth?: outputs.JobJobClusterNewClusterDockerImageBasicAuth;
    /**
     * URL of the job on the given workspace
     */
    url: string;
}

export interface JobJobClusterNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface JobJobClusterNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface JobJobClusterNewClusterInitScript {
    abfss?: outputs.JobJobClusterNewClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.JobJobClusterNewClusterInitScriptDbfs;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobJobClusterNewClusterInitScriptFile;
    gcs?: outputs.JobJobClusterNewClusterInitScriptGcs;
    s3?: outputs.JobJobClusterNewClusterInitScriptS3;
    volumes?: outputs.JobJobClusterNewClusterInitScriptVolumes;
    workspace?: outputs.JobJobClusterNewClusterInitScriptWorkspace;
}

export interface JobJobClusterNewClusterInitScriptAbfss {
    destination: string;
}

export interface JobJobClusterNewClusterInitScriptDbfs {
    destination: string;
}

export interface JobJobClusterNewClusterInitScriptFile {
    destination: string;
}

export interface JobJobClusterNewClusterInitScriptGcs {
    destination: string;
}

export interface JobJobClusterNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobJobClusterNewClusterInitScriptVolumes {
    destination: string;
}

export interface JobJobClusterNewClusterInitScriptWorkspace {
    destination: string;
}

export interface JobJobClusterNewClusterLibrary {
    cran?: outputs.JobJobClusterNewClusterLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobJobClusterNewClusterLibraryMaven;
    pypi?: outputs.JobJobClusterNewClusterLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobJobClusterNewClusterLibraryCran {
    package: string;
    repo?: string;
}

export interface JobJobClusterNewClusterLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobJobClusterNewClusterLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobJobClusterNewClusterWorkloadType {
    clients: outputs.JobJobClusterNewClusterWorkloadTypeClients;
}

export interface JobJobClusterNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface JobLibrary {
    cran?: outputs.JobLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobLibraryMaven;
    pypi?: outputs.JobLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobLibraryCran {
    package: string;
    repo?: string;
}

export interface JobLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.JobNewClusterAutoscale;
    awsAttributes?: outputs.JobNewClusterAwsAttributes;
    azureAttributes?: outputs.JobNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.JobNewClusterClusterLogConf;
    clusterMountInfos?: outputs.JobNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.JobNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.JobNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.JobNewClusterInitScript[];
    instancePoolId?: string;
    isSingleNode?: boolean;
    kind?: string;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: outputs.JobNewClusterLibrary[];
    nodeTypeId: string;
    numWorkers?: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    useMlRuntime?: boolean;
    /**
     * isn't supported
     */
    workloadType?: outputs.JobNewClusterWorkloadType;
}

export interface JobNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface JobNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface JobNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.JobNewClusterAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface JobNewClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface JobNewClusterClusterLogConf {
    dbfs?: outputs.JobNewClusterClusterLogConfDbfs;
    s3?: outputs.JobNewClusterClusterLogConfS3;
    volumes?: outputs.JobNewClusterClusterLogConfVolumes;
}

export interface JobNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface JobNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobNewClusterClusterLogConfVolumes {
    destination: string;
}

export interface JobNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.JobNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface JobNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface JobNewClusterDockerImage {
    basicAuth?: outputs.JobNewClusterDockerImageBasicAuth;
    /**
     * URL of the job on the given workspace
     */
    url: string;
}

export interface JobNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface JobNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface JobNewClusterInitScript {
    abfss?: outputs.JobNewClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.JobNewClusterInitScriptDbfs;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobNewClusterInitScriptFile;
    gcs?: outputs.JobNewClusterInitScriptGcs;
    s3?: outputs.JobNewClusterInitScriptS3;
    volumes?: outputs.JobNewClusterInitScriptVolumes;
    workspace?: outputs.JobNewClusterInitScriptWorkspace;
}

export interface JobNewClusterInitScriptAbfss {
    destination: string;
}

export interface JobNewClusterInitScriptDbfs {
    destination: string;
}

export interface JobNewClusterInitScriptFile {
    destination: string;
}

export interface JobNewClusterInitScriptGcs {
    destination: string;
}

export interface JobNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobNewClusterInitScriptVolumes {
    destination: string;
}

export interface JobNewClusterInitScriptWorkspace {
    destination: string;
}

export interface JobNewClusterLibrary {
    cran?: outputs.JobNewClusterLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobNewClusterLibraryMaven;
    pypi?: outputs.JobNewClusterLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobNewClusterLibraryCran {
    package: string;
    repo?: string;
}

export interface JobNewClusterLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobNewClusterLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobNewClusterWorkloadType {
    clients: outputs.JobNewClusterWorkloadTypeClients;
}

export interface JobNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface JobNotebookTask {
    /**
     * (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in baseParameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s baseParameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.
     */
    baseParameters?: {[key: string]: string};
    /**
     * The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.
     */
    notebookPath: string;
    /**
     * Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `gitSource`. If the value is empty, the task will use `GIT` if `gitSource` is defined and `WORKSPACE` otherwise.
     */
    source?: string;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.
     */
    warehouseId?: string;
}

export interface JobNotificationSettings {
    /**
     * (Bool) don't send alert for cancelled runs.
     *
     * The following parameter is only available on task level.
     */
    noAlertForCanceledRuns?: boolean;
    /**
     * (Bool) don't send alert for skipped runs.
     */
    noAlertForSkippedRuns?: boolean;
}

export interface JobParameter {
    /**
     * Default value of the parameter.
     *
     * *You can use this block only together with `task` blocks, not with the legacy tasks specification!*
     */
    default: string;
    /**
     * The name of the defined parameter. May only contain alphanumeric characters, `_`, `-`, and `.`.
     */
    name: string;
}

export interface JobPipelineTask {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: boolean;
    /**
     * The pipeline's unique ID.
     */
    pipelineId: string;
}

export interface JobPythonWheelTask {
    /**
     * Python function as entry point for the task
     */
    entryPoint?: string;
    /**
     * Named parameters for the task
     */
    namedParameters?: {[key: string]: string};
    /**
     * Name of Python package
     */
    packageName?: string;
    /**
     * Parameters for the task
     */
    parameters?: string[];
}

export interface JobQueue {
    /**
     * If true, enable queueing for the job.
     */
    enabled: boolean;
}

export interface JobRunAs {
    /**
     * The application ID of an active service principal. Setting this field requires the `servicePrincipal/user` role.
     *
     * Example:
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const _this = new databricks.Job("this", {runAs: {
     *     servicePrincipalName: "8d23ae77-912e-4a19-81e4-b9c3f5cc9349",
     * }});
     * ```
     */
    servicePrincipalName?: string;
    /**
     * The email of an active workspace user. Non-admin users can only set this field to their own email.
     */
    userName?: string;
}

export interface JobRunJobTask {
    /**
     * (String) ID of the job
     */
    jobId: number;
    /**
     * (Map) Job parameters for the task
     */
    jobParameters?: {[key: string]: string};
}

export interface JobSchedule {
    /**
     * Indicate whether this schedule is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted and a schedule is provided, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: string;
    /**
     * A [Cron expression using Quartz syntax](http://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) that describes the schedule for a job. This field is required.
     */
    quartzCronExpression: string;
    /**
     * A Java timezone ID. The schedule for a job will be resolved with respect to this timezone. See Java TimeZone for details. This field is required.
     */
    timezoneId: string;
}

export interface JobSparkJarTask {
    jarUri?: string;
    /**
     * The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
     */
    mainClassName?: string;
    /**
     * (List) Parameters passed to the main method.
     */
    parameters?: string[];
}

export interface JobSparkPythonTask {
    /**
     * (List) Command line parameters passed to the Python file.
     */
    parameters?: string[];
    /**
     * The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/`. For files stored in a remote repository, the path must be relative. This field is required.
     */
    pythonFile: string;
    /**
     * Location type of the Python file. When set to `WORKSPACE` or not specified, the file will be retrieved from the local Databricks workspace or cloud location (if the pythonFile has a URI format). When set to `GIT`, the Python file will be retrieved from a Git repository defined in `gitSource`.
     * * `WORKSPACE`: The Python file is located in a Databricks workspace or at a cloud filesystem URI.
     * * `GIT`: The Python file is located in a remote Git repository.
     */
    source?: string;
}

export interface JobSparkSubmitTask {
    /**
     * (List) Command-line parameters passed to spark submit.
     */
    parameters?: string[];
}

export interface JobTask {
    cleanRoomsNotebookTask?: outputs.JobTaskCleanRoomsNotebookTask;
    conditionTask?: outputs.JobTaskConditionTask;
    dashboardTask?: outputs.JobTaskDashboardTask;
    dbtTask?: outputs.JobTaskDbtTask;
    /**
     * block specifying dependency(-ies) for a given task.
     */
    dependsOns?: outputs.JobTaskDependsOn[];
    /**
     * description for this task.
     */
    description?: string;
    /**
     * A flag to disable auto optimization in serverless tasks.
     */
    disableAutoOptimization?: boolean;
    /**
     * An optional block to specify a set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This block is documented below.
     */
    emailNotifications?: outputs.JobTaskEmailNotifications;
    /**
     * identifier of an `environment` block that is used to specify libraries.  Required for some tasks (`sparkPythonTask`, `pythonWheelTask`, ...) running on serverless compute.
     */
    environmentKey?: string;
    /**
     * Identifier of the interactive cluster to run job on.  *Note: running tasks on interactive clusters may lead to increased costs!*
     */
    existingClusterId?: string;
    forEachTask?: outputs.JobTaskForEachTask;
    genAiComputeTask?: outputs.JobTaskGenAiComputeTask;
    /**
     * block described below that specifies health conditions for a given task.
     */
    health?: outputs.JobTaskHealth;
    /**
     * Identifier of the Job cluster specified in the `jobCluster` block.
     */
    jobClusterKey?: string;
    /**
     * (Set) An optional list of libraries to be installed on the cluster that will execute the job.
     */
    libraries?: outputs.JobTaskLibrary[];
    /**
     * (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.
     */
    maxRetries?: number;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     */
    minRetryIntervalMillis?: number;
    /**
     * Task will run on a dedicated cluster.  See databricks.Cluster documentation for specification. *Some parameters, such as `autoterminationMinutes`, `isPinned`, `workloadType` aren't supported!*
     */
    newCluster?: outputs.JobTaskNewCluster;
    notebookTask?: outputs.JobTaskNotebookTask;
    /**
     * An optional block controlling the notification settings on the job level documented below.
     */
    notificationSettings?: outputs.JobTaskNotificationSettings;
    pipelineTask?: outputs.JobTaskPipelineTask;
    powerBiTask?: outputs.JobTaskPowerBiTask;
    pythonWheelTask?: outputs.JobTaskPythonWheelTask;
    /**
     * (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
     */
    retryOnTimeout: boolean;
    /**
     * An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of `ALL_SUCCESS`, `AT_LEAST_ONE_SUCCESS`, `NONE_FAILED`, `ALL_DONE`, `AT_LEAST_ONE_FAILED` or `ALL_FAILED`. When omitted, defaults to `ALL_SUCCESS`.
     */
    runIf?: string;
    runJobTask?: outputs.JobTaskRunJobTask;
    sparkJarTask?: outputs.JobTaskSparkJarTask;
    sparkPythonTask?: outputs.JobTaskSparkPythonTask;
    sparkSubmitTask?: outputs.JobTaskSparkSubmitTask;
    sqlTask?: outputs.JobTaskSqlTask;
    /**
     * string specifying an unique key for a given task.
     * * `*_task` - (Required) one of the specific task blocks described below:
     */
    taskKey: string;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    timeoutSeconds?: number;
    /**
     * (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
     *
     * > If no `jobClusterKey`, `existingClusterId`, or `newCluster` were specified in task definition, then task will executed using serverless compute.
     */
    webhookNotifications?: outputs.JobTaskWebhookNotifications;
}

export interface JobTaskCleanRoomsNotebookTask {
    /**
     * The clean room that the notebook belongs to.
     */
    cleanRoomName: string;
    /**
     * Checksum to validate the freshness of the notebook resource.
     */
    etag?: string;
    /**
     * Base parameters to be used for the clean room notebook job.
     */
    notebookBaseParameters?: {[key: string]: string};
    /**
     * Name of the notebook being run.
     */
    notebookName: string;
}

export interface JobTaskConditionTask {
    /**
     * The left operand of the condition task. It could be a string value, job state, or a parameter reference.
     */
    left: string;
    /**
     * The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).
     *
     * This task does not require a cluster to execute and does not support retries or notifications.
     */
    op: string;
    /**
     * The right operand of the condition task. It could be a string value, job state, or parameter reference.
     */
    right: string;
}

export interface JobTaskDashboardTask {
    /**
     * The identifier of the dashboard to refresh
     */
    dashboardId?: string;
    /**
     * Represents a subscription configuration for scheduled dashboard snapshots.
     */
    subscription?: outputs.JobTaskDashboardTaskSubscription;
    /**
     * The warehouse id to execute the dashboard with for the schedule. If not specified, will use the default warehouse of dashboard
     */
    warehouseId?: string;
}

export interface JobTaskDashboardTaskSubscription {
    /**
     * Allows users to specify a custom subject line on the email sent to subscribers.
     */
    customSubject?: string;
    /**
     * When true, the subscription will not send emails.
     */
    paused?: boolean;
    /**
     * The list of subscribers to send the snapshot of the dashboard to.
     */
    subscribers?: outputs.JobTaskDashboardTaskSubscriptionSubscriber[];
}

export interface JobTaskDashboardTaskSubscriptionSubscriber {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    /**
     * A snapshot of the dashboard will be sent to the user's email when the `userName` field is present.
     */
    userName?: string;
}

export interface JobTaskDbtTask {
    /**
     * The name of the catalog to use inside Unity Catalog.
     */
    catalog?: string;
    /**
     * (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".
     */
    commands: string[];
    /**
     * The relative path to the directory in the repository specified by `gitSource` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.
     */
    profilesDirectory?: string;
    /**
     * The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.
     * * If `source` is `GIT`: Relative path to the directory in the repository specified in the `gitSource` block. Defaults to the repository's root directory when not specified.
     * * If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.
     */
    projectDirectory?: string;
    /**
     * The name of the schema dbt should run in. Defaults to `default`.
     */
    schema?: string;
    /**
     * The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `gitSource` block is present in the job definition.
     */
    source?: string;
    /**
     * The ID of the SQL warehouse that dbt should execute against.
     *
     * You also need to include a `gitSource` block to configure the repository that contains the dbt project.
     */
    warehouseId?: string;
}

export interface JobTaskDependsOn {
    /**
     * Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `"true"` or `"false"`.
     *
     * > Similar to the tasks themselves, each dependency inside the task need to be declared in alphabetical order with respect to taskKey in order to get consistent Pulumi diffs.
     */
    outcome?: string;
    /**
     * The name of the task this task depends on.
     */
    taskKey: string;
}

export interface JobTaskEmailNotifications {
    /**
     * (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notificationSettings` configuration block).
     */
    noAlertForSkippedRuns?: boolean;
    /**
     * (List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: string[];
    /**
     * (List) list of emails to notify when the run fails.
     */
    onFailures?: string[];
    /**
     * (List) list of emails to notify when the run starts.
     */
    onStarts?: string[];
    /**
     * (List) list of emails to notify when any streaming backlog thresholds are exceeded for any stream.
     *
     * The following parameter is only available for the job level configuration.
     */
    onStreamingBacklogExceededs?: string[];
    /**
     * (List) list of emails to notify when the run completes successfully.
     */
    onSuccesses?: string[];
}

export interface JobTaskForEachTask {
    /**
     * Controls the number of active iteration task runs. Default is 20, maximum allowed is 100.
     */
    concurrency?: number;
    /**
     * (String) Array for task to iterate on. This can be a JSON string or a reference to an array parameter.
     */
    inputs: string;
    /**
     * Task to run against the `inputs` list.
     */
    task: outputs.JobTaskForEachTaskTask;
}

export interface JobTaskForEachTaskTask {
    cleanRoomsNotebookTask?: outputs.JobTaskForEachTaskTaskCleanRoomsNotebookTask;
    conditionTask?: outputs.JobTaskForEachTaskTaskConditionTask;
    dashboardTask?: outputs.JobTaskForEachTaskTaskDashboardTask;
    dbtTask?: outputs.JobTaskForEachTaskTaskDbtTask;
    /**
     * block specifying dependency(-ies) for a given task.
     */
    dependsOns?: outputs.JobTaskForEachTaskTaskDependsOn[];
    /**
     * description for this task.
     */
    description?: string;
    /**
     * A flag to disable auto optimization in serverless tasks.
     */
    disableAutoOptimization?: boolean;
    /**
     * An optional block to specify a set of email addresses notified when this task begins, completes or fails. The default behavior is to not send any emails. This block is documented below.
     */
    emailNotifications?: outputs.JobTaskForEachTaskTaskEmailNotifications;
    /**
     * identifier of an `environment` block that is used to specify libraries.  Required for some tasks (`sparkPythonTask`, `pythonWheelTask`, ...) running on serverless compute.
     */
    environmentKey?: string;
    /**
     * Identifier of the interactive cluster to run job on.  *Note: running tasks on interactive clusters may lead to increased costs!*
     */
    existingClusterId?: string;
    genAiComputeTask?: outputs.JobTaskForEachTaskTaskGenAiComputeTask;
    /**
     * block described below that specifies health conditions for a given task.
     */
    health?: outputs.JobTaskForEachTaskTaskHealth;
    /**
     * Identifier of the Job cluster specified in the `jobCluster` block.
     */
    jobClusterKey?: string;
    /**
     * (Set) An optional list of libraries to be installed on the cluster that will execute the job.
     */
    libraries?: outputs.JobTaskForEachTaskTaskLibrary[];
    /**
     * (Integer) An optional maximum number of times to retry an unsuccessful run. A run is considered to be unsuccessful if it completes with a `FAILED` or `INTERNAL_ERROR` lifecycle state. The value -1 means to retry indefinitely and the value 0 means to never retry. The default behavior is to never retry. A run can have the following lifecycle state: `PENDING`, `RUNNING`, `TERMINATING`, `TERMINATED`, `SKIPPED` or `INTERNAL_ERROR`.
     */
    maxRetries?: number;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     */
    minRetryIntervalMillis?: number;
    /**
     * Task will run on a dedicated cluster.  See databricks.Cluster documentation for specification. *Some parameters, such as `autoterminationMinutes`, `isPinned`, `workloadType` aren't supported!*
     */
    newCluster?: outputs.JobTaskForEachTaskTaskNewCluster;
    notebookTask?: outputs.JobTaskForEachTaskTaskNotebookTask;
    /**
     * An optional block controlling the notification settings on the job level documented below.
     */
    notificationSettings?: outputs.JobTaskForEachTaskTaskNotificationSettings;
    pipelineTask?: outputs.JobTaskForEachTaskTaskPipelineTask;
    powerBiTask?: outputs.JobTaskForEachTaskTaskPowerBiTask;
    pythonWheelTask?: outputs.JobTaskForEachTaskTaskPythonWheelTask;
    /**
     * (Bool) An optional policy to specify whether to retry a job when it times out. The default behavior is to not retry on timeout.
     */
    retryOnTimeout: boolean;
    /**
     * An optional value indicating the condition that determines whether the task should be run once its dependencies have been completed. One of `ALL_SUCCESS`, `AT_LEAST_ONE_SUCCESS`, `NONE_FAILED`, `ALL_DONE`, `AT_LEAST_ONE_FAILED` or `ALL_FAILED`. When omitted, defaults to `ALL_SUCCESS`.
     */
    runIf?: string;
    runJobTask?: outputs.JobTaskForEachTaskTaskRunJobTask;
    sparkJarTask?: outputs.JobTaskForEachTaskTaskSparkJarTask;
    sparkPythonTask?: outputs.JobTaskForEachTaskTaskSparkPythonTask;
    sparkSubmitTask?: outputs.JobTaskForEachTaskTaskSparkSubmitTask;
    sqlTask?: outputs.JobTaskForEachTaskTaskSqlTask;
    /**
     * string specifying an unique key for a given task.
     * * `*_task` - (Required) one of the specific task blocks described below:
     */
    taskKey: string;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    timeoutSeconds?: number;
    /**
     * (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this task begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
     *
     * > If no `jobClusterKey`, `existingClusterId`, or `newCluster` were specified in task definition, then task will executed using serverless compute.
     */
    webhookNotifications?: outputs.JobTaskForEachTaskTaskWebhookNotifications;
}

export interface JobTaskForEachTaskTaskCleanRoomsNotebookTask {
    /**
     * The clean room that the notebook belongs to.
     */
    cleanRoomName: string;
    /**
     * Checksum to validate the freshness of the notebook resource.
     */
    etag?: string;
    /**
     * Base parameters to be used for the clean room notebook job.
     */
    notebookBaseParameters?: {[key: string]: string};
    /**
     * Name of the notebook being run.
     */
    notebookName: string;
}

export interface JobTaskForEachTaskTaskConditionTask {
    /**
     * The left operand of the condition task. It could be a string value, job state, or a parameter reference.
     */
    left: string;
    /**
     * The string specifying the operation used to compare operands.  Currently, following operators are supported: `EQUAL_TO`, `GREATER_THAN`, `GREATER_THAN_OR_EQUAL`, `LESS_THAN`, `LESS_THAN_OR_EQUAL`, `NOT_EQUAL`. (Check the [API docs](https://docs.databricks.com/api/workspace/jobs/create) for the latest information).
     *
     * This task does not require a cluster to execute and does not support retries or notifications.
     */
    op: string;
    /**
     * The right operand of the condition task. It could be a string value, job state, or parameter reference.
     */
    right: string;
}

export interface JobTaskForEachTaskTaskDashboardTask {
    /**
     * The identifier of the dashboard to refresh
     */
    dashboardId?: string;
    /**
     * Represents a subscription configuration for scheduled dashboard snapshots.
     */
    subscription?: outputs.JobTaskForEachTaskTaskDashboardTaskSubscription;
    /**
     * The warehouse id to execute the dashboard with for the schedule. If not specified, will use the default warehouse of dashboard
     */
    warehouseId?: string;
}

export interface JobTaskForEachTaskTaskDashboardTaskSubscription {
    /**
     * Allows users to specify a custom subject line on the email sent to subscribers.
     */
    customSubject?: string;
    /**
     * When true, the subscription will not send emails.
     */
    paused?: boolean;
    /**
     * The list of subscribers to send the snapshot of the dashboard to.
     */
    subscribers?: outputs.JobTaskForEachTaskTaskDashboardTaskSubscriptionSubscriber[];
}

export interface JobTaskForEachTaskTaskDashboardTaskSubscriptionSubscriber {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    /**
     * A snapshot of the dashboard will be sent to the user's email when the `userName` field is present.
     */
    userName?: string;
}

export interface JobTaskForEachTaskTaskDbtTask {
    /**
     * The name of the catalog to use inside Unity Catalog.
     */
    catalog?: string;
    /**
     * (Array) Series of dbt commands to execute in sequence. Every command must start with "dbt".
     */
    commands: string[];
    /**
     * The relative path to the directory in the repository specified by `gitSource` where dbt should look in for the `profiles.yml` file. If not specified, defaults to the repository's root directory. Equivalent to passing `--profile-dir` to a dbt command.
     */
    profilesDirectory?: string;
    /**
     * The path where dbt should look for `dbt_project.yml`. Equivalent to passing `--project-dir` to the dbt CLI.
     * * If `source` is `GIT`: Relative path to the directory in the repository specified in the `gitSource` block. Defaults to the repository's root directory when not specified.
     * * If `source` is `WORKSPACE`: Absolute path to the folder in the workspace.
     */
    projectDirectory?: string;
    /**
     * The name of the schema dbt should run in. Defaults to `default`.
     */
    schema?: string;
    /**
     * The source of the project. Possible values are `WORKSPACE` and `GIT`.  Defaults to `GIT` if a `gitSource` block is present in the job definition.
     */
    source?: string;
    /**
     * The ID of the SQL warehouse that dbt should execute against.
     *
     * You also need to include a `gitSource` block to configure the repository that contains the dbt project.
     */
    warehouseId?: string;
}

export interface JobTaskForEachTaskTaskDependsOn {
    /**
     * Can only be specified on condition task dependencies. The outcome of the dependent task that must be met for this task to run. Possible values are `"true"` or `"false"`.
     *
     * > Similar to the tasks themselves, each dependency inside the task need to be declared in alphabetical order with respect to taskKey in order to get consistent Pulumi diffs.
     */
    outcome?: string;
    /**
     * The name of the task this task depends on.
     */
    taskKey: string;
}

export interface JobTaskForEachTaskTaskEmailNotifications {
    /**
     * (Bool) don't send alert for skipped runs. (It's recommended to use the corresponding setting in the `notificationSettings` configuration block).
     */
    noAlertForSkippedRuns?: boolean;
    /**
     * (List) list of emails to notify when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: string[];
    /**
     * (List) list of emails to notify when the run fails.
     */
    onFailures?: string[];
    /**
     * (List) list of emails to notify when the run starts.
     */
    onStarts?: string[];
    /**
     * (List) list of emails to notify when any streaming backlog thresholds are exceeded for any stream.
     *
     * The following parameter is only available for the job level configuration.
     */
    onStreamingBacklogExceededs?: string[];
    /**
     * (List) list of emails to notify when the run completes successfully.
     */
    onSuccesses?: string[];
}

export interface JobTaskForEachTaskTaskGenAiComputeTask {
    command?: string;
    compute?: outputs.JobTaskForEachTaskTaskGenAiComputeTaskCompute;
    dlRuntimeImage: string;
    mlflowExperimentName?: string;
    source?: string;
    trainingScriptPath?: string;
    yamlParameters?: string;
    yamlParametersFilePath?: string;
}

export interface JobTaskForEachTaskTaskGenAiComputeTaskCompute {
    gpuNodePoolId: string;
    gpuType?: string;
    numGpus: number;
}

export interface JobTaskForEachTaskTaskHealth {
    /**
     * list of rules that are represented as objects with the following attributes:
     */
    rules: outputs.JobTaskForEachTaskTaskHealthRule[];
}

export interface JobTaskForEachTaskTaskHealthRule {
    /**
     * string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.
     */
    metric: string;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op: string;
    /**
     * integer value used to compare to the given metric.
     */
    value: number;
}

export interface JobTaskForEachTaskTaskLibrary {
    cran?: outputs.JobTaskForEachTaskTaskLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobTaskForEachTaskTaskLibraryMaven;
    pypi?: outputs.JobTaskForEachTaskTaskLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobTaskForEachTaskTaskLibraryCran {
    package: string;
    repo?: string;
}

export interface JobTaskForEachTaskTaskLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobTaskForEachTaskTaskLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobTaskForEachTaskTaskNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.JobTaskForEachTaskTaskNewClusterAutoscale;
    awsAttributes?: outputs.JobTaskForEachTaskTaskNewClusterAwsAttributes;
    azureAttributes?: outputs.JobTaskForEachTaskTaskNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.JobTaskForEachTaskTaskNewClusterClusterLogConf;
    clusterMountInfos?: outputs.JobTaskForEachTaskTaskNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.JobTaskForEachTaskTaskNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.JobTaskForEachTaskTaskNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.JobTaskForEachTaskTaskNewClusterInitScript[];
    instancePoolId?: string;
    isSingleNode?: boolean;
    kind?: string;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: outputs.JobTaskForEachTaskTaskNewClusterLibrary[];
    nodeTypeId: string;
    numWorkers?: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    useMlRuntime?: boolean;
    /**
     * isn't supported
     */
    workloadType?: outputs.JobTaskForEachTaskTaskNewClusterWorkloadType;
}

export interface JobTaskForEachTaskTaskNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface JobTaskForEachTaskTaskNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface JobTaskForEachTaskTaskNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface JobTaskForEachTaskTaskNewClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface JobTaskForEachTaskTaskNewClusterClusterLogConf {
    dbfs?: outputs.JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs;
    s3?: outputs.JobTaskForEachTaskTaskNewClusterClusterLogConfS3;
    volumes?: outputs.JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes;
}

export interface JobTaskForEachTaskTaskNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobTaskForEachTaskTaskNewClusterClusterLogConfVolumes {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface JobTaskForEachTaskTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface JobTaskForEachTaskTaskNewClusterDockerImage {
    basicAuth?: outputs.JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth;
    /**
     * URL of the job on the given workspace
     */
    url: string;
}

export interface JobTaskForEachTaskTaskNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface JobTaskForEachTaskTaskNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScript {
    abfss?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptDbfs;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptFile;
    gcs?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptGcs;
    s3?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptS3;
    volumes?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptVolumes;
    workspace?: outputs.JobTaskForEachTaskTaskNewClusterInitScriptWorkspace;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptAbfss {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptDbfs {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptFile {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptGcs {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptVolumes {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterInitScriptWorkspace {
    destination: string;
}

export interface JobTaskForEachTaskTaskNewClusterLibrary {
    cran?: outputs.JobTaskForEachTaskTaskNewClusterLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobTaskForEachTaskTaskNewClusterLibraryMaven;
    pypi?: outputs.JobTaskForEachTaskTaskNewClusterLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobTaskForEachTaskTaskNewClusterLibraryCran {
    package: string;
    repo?: string;
}

export interface JobTaskForEachTaskTaskNewClusterLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobTaskForEachTaskTaskNewClusterLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobTaskForEachTaskTaskNewClusterWorkloadType {
    clients: outputs.JobTaskForEachTaskTaskNewClusterWorkloadTypeClients;
}

export interface JobTaskForEachTaskTaskNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface JobTaskForEachTaskTaskNotebookTask {
    /**
     * (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in baseParameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s baseParameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.
     */
    baseParameters?: {[key: string]: string};
    /**
     * The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.
     */
    notebookPath: string;
    /**
     * Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `gitSource`. If the value is empty, the task will use `GIT` if `gitSource` is defined and `WORKSPACE` otherwise.
     */
    source?: string;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.
     */
    warehouseId?: string;
}

export interface JobTaskForEachTaskTaskNotificationSettings {
    /**
     * (Bool) do not send notifications to recipients specified in `onStart` for the retried runs and do not send notifications to recipients specified in `onFailure` until the last retry of the run.
     */
    alertOnLastAttempt?: boolean;
    /**
     * (Bool) don't send alert for cancelled runs.
     *
     * The following parameter is only available on task level.
     */
    noAlertForCanceledRuns?: boolean;
    /**
     * (Bool) don't send alert for skipped runs.
     */
    noAlertForSkippedRuns?: boolean;
}

export interface JobTaskForEachTaskTaskPipelineTask {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: boolean;
    /**
     * The pipeline's unique ID.
     */
    pipelineId: string;
}

export interface JobTaskForEachTaskTaskPowerBiTask {
    /**
     * The resource name of the UC connection to authenticate from Databricks to Power BI
     */
    connectionResourceName?: string;
    /**
     * The semantic model to update. Block consists of following fields:
     */
    powerBiModel?: outputs.JobTaskForEachTaskTaskPowerBiTaskPowerBiModel;
    /**
     * Whether the model should be refreshed after the update. Default is false
     */
    refreshAfterUpdate?: boolean;
    /**
     * The tables to be exported to Power BI. Block consists of following fields:
     */
    tables?: outputs.JobTaskForEachTaskTaskPowerBiTaskTable[];
    /**
     * The SQL warehouse ID to use as the Power BI data source
     */
    warehouseId?: string;
}

export interface JobTaskForEachTaskTaskPowerBiTaskPowerBiModel {
    /**
     * How the published Power BI model authenticates to Databricks
     */
    authenticationMethod?: string;
    /**
     * The name of the Power BI model
     */
    modelName?: string;
    /**
     * Whether to overwrite existing Power BI models. Default is false
     */
    overwriteExisting?: boolean;
    /**
     * The default storage mode of the Power BI model
     */
    storageMode?: string;
    /**
     * The name of the Power BI workspace of the model
     */
    workspaceName?: string;
}

export interface JobTaskForEachTaskTaskPowerBiTaskTable {
    /**
     * The catalog name in Databricks
     */
    catalog?: string;
    /**
     * The table name in Databricks. If empty, all tables under the schema are selected.
     */
    name?: string;
    /**
     * The schema name in Databricks
     */
    schema?: string;
    /**
     * The Power BI storage mode of the table
     */
    storageMode?: string;
}

export interface JobTaskForEachTaskTaskPythonWheelTask {
    /**
     * Python function as entry point for the task
     */
    entryPoint?: string;
    /**
     * Named parameters for the task
     */
    namedParameters?: {[key: string]: string};
    /**
     * Name of Python package
     */
    packageName?: string;
    /**
     * Parameters for the task
     */
    parameters?: string[];
}

export interface JobTaskForEachTaskTaskRunJobTask {
    dbtCommands?: string[];
    jarParams?: string[];
    /**
     * (String) ID of the job
     */
    jobId: number;
    /**
     * (Map) Job parameters for the task
     */
    jobParameters?: {[key: string]: string};
    notebookParams?: {[key: string]: string};
    pipelineParams?: outputs.JobTaskForEachTaskTaskRunJobTaskPipelineParams;
    pythonNamedParams?: {[key: string]: string};
    pythonParams?: string[];
    sparkSubmitParams?: string[];
    sqlParams?: {[key: string]: string};
}

export interface JobTaskForEachTaskTaskRunJobTaskPipelineParams {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: boolean;
}

export interface JobTaskForEachTaskTaskSparkJarTask {
    jarUri?: string;
    /**
     * The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
     */
    mainClassName?: string;
    /**
     * (List) Parameters passed to the main method.
     */
    parameters?: string[];
    runAsRepl?: boolean;
}

export interface JobTaskForEachTaskTaskSparkPythonTask {
    /**
     * (List) Command line parameters passed to the Python file.
     */
    parameters?: string[];
    /**
     * The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/`. For files stored in a remote repository, the path must be relative. This field is required.
     */
    pythonFile: string;
    /**
     * Location type of the Python file. When set to `WORKSPACE` or not specified, the file will be retrieved from the local Databricks workspace or cloud location (if the pythonFile has a URI format). When set to `GIT`, the Python file will be retrieved from a Git repository defined in `gitSource`.
     * * `WORKSPACE`: The Python file is located in a Databricks workspace or at a cloud filesystem URI.
     * * `GIT`: The Python file is located in a remote Git repository.
     */
    source?: string;
}

export interface JobTaskForEachTaskTaskSparkSubmitTask {
    /**
     * (List) Command-line parameters passed to spark submit.
     */
    parameters?: string[];
}

export interface JobTaskForEachTaskTaskSqlTask {
    /**
     * block consisting of following fields:
     */
    alert?: outputs.JobTaskForEachTaskTaskSqlTaskAlert;
    /**
     * block consisting of following fields:
     */
    dashboard?: outputs.JobTaskForEachTaskTaskSqlTaskDashboard;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobTaskForEachTaskTaskSqlTaskFile;
    /**
     * (Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.
     */
    parameters?: {[key: string]: string};
    /**
     * block consisting of single string field: `queryId` - identifier of the Databricks Query (databricks_query).
     */
    query?: outputs.JobTaskForEachTaskTaskSqlTaskQuery;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless & Pro warehouses are supported right now.
     */
    warehouseId: string;
}

export interface JobTaskForEachTaskTaskSqlTaskAlert {
    /**
     * (String) identifier of the Databricks Alert (databricks_alert).
     */
    alertId: string;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: boolean;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions?: outputs.JobTaskForEachTaskTaskSqlTaskAlertSubscription[];
}

export interface JobTaskForEachTaskTaskSqlTaskAlertSubscription {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    userName?: string;
}

export interface JobTaskForEachTaskTaskSqlTaskDashboard {
    /**
     * string specifying a custom subject of email sent.
     */
    customSubject?: string;
    /**
     * (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.
     */
    dashboardId: string;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: boolean;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions?: outputs.JobTaskForEachTaskTaskSqlTaskDashboardSubscription[];
}

export interface JobTaskForEachTaskTaskSqlTaskDashboardSubscription {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    userName?: string;
}

export interface JobTaskForEachTaskTaskSqlTaskFile {
    /**
     * If `source` is `GIT`: Relative path to the file in the repository specified in the `gitSource` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sql_aggregation_job", {
     *     name: "Example SQL Job",
     *     tasks: [
     *         {
     *             taskKey: "run_agg_query",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 query: {
     *                     queryId: aggQuery.id,
     *                 },
     *             },
     *         },
     *         {
     *             taskKey: "run_dashboard",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 dashboard: {
     *                     dashboardId: dash.id,
     *                     subscriptions: [{
     *                         userName: "user@domain.com",
     *                     }],
     *                 },
     *             },
     *         },
     *         {
     *             taskKey: "run_alert",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 alert: {
     *                     alertId: alert.id,
     *                     subscriptions: [{
     *                         userName: "user@domain.com",
     *                     }],
     *                 },
     *             },
     *         },
     *     ],
     * });
     * ```
     */
    path: string;
    /**
     * The source of the project. Possible values are `WORKSPACE` and `GIT`.
     */
    source?: string;
}

export interface JobTaskForEachTaskTaskSqlTaskQuery {
    queryId: string;
}

export interface JobTaskForEachTaskTaskWebhookNotifications {
    /**
     * (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: outputs.JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded[];
    /**
     * (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.
     */
    onFailures?: outputs.JobTaskForEachTaskTaskWebhookNotificationsOnFailure[];
    /**
     * (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.
     */
    onStarts?: outputs.JobTaskForEachTaskTaskWebhookNotificationsOnStart[];
    /**
     * (List) list of notification IDs to call when any streaming backlog thresholds are exceeded for any stream.
     *
     * Note that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://<workspace host>/sql/destinations/<notification id>?o=<workspace id>`
     *
     * Example
     */
    onStreamingBacklogExceededs?: outputs.JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded[];
    /**
     * (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.
     */
    onSuccesses?: outputs.JobTaskForEachTaskTaskWebhookNotificationsOnSuccess[];
}

export interface JobTaskForEachTaskTaskWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskForEachTaskTaskWebhookNotificationsOnFailure {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskForEachTaskTaskWebhookNotificationsOnStart {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskForEachTaskTaskWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskForEachTaskTaskWebhookNotificationsOnSuccess {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskGenAiComputeTask {
    command?: string;
    compute?: outputs.JobTaskGenAiComputeTaskCompute;
    dlRuntimeImage: string;
    mlflowExperimentName?: string;
    source?: string;
    trainingScriptPath?: string;
    yamlParameters?: string;
    yamlParametersFilePath?: string;
}

export interface JobTaskGenAiComputeTaskCompute {
    gpuNodePoolId: string;
    gpuType?: string;
    numGpus: number;
}

export interface JobTaskHealth {
    /**
     * list of rules that are represented as objects with the following attributes:
     */
    rules: outputs.JobTaskHealthRule[];
}

export interface JobTaskHealthRule {
    /**
     * string specifying the metric to check, like `RUN_DURATION_SECONDS`, `STREAMING_BACKLOG_FILES`, etc. - check the [Jobs REST API documentation](https://docs.databricks.com/api/workspace/jobs/create#health-rules-metric) for the full list of supported metrics.
     */
    metric: string;
    /**
     * string specifying the operation used to evaluate the given metric. The only supported operation is `GREATER_THAN`.
     */
    op: string;
    /**
     * integer value used to compare to the given metric.
     */
    value: number;
}

export interface JobTaskLibrary {
    cran?: outputs.JobTaskLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobTaskLibraryMaven;
    pypi?: outputs.JobTaskLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobTaskLibraryCran {
    package: string;
    repo?: string;
}

export interface JobTaskLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobTaskLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobTaskNewCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.JobTaskNewClusterAutoscale;
    awsAttributes?: outputs.JobTaskNewClusterAwsAttributes;
    azureAttributes?: outputs.JobTaskNewClusterAzureAttributes;
    clusterId?: string;
    clusterLogConf?: outputs.JobTaskNewClusterClusterLogConf;
    clusterMountInfos?: outputs.JobTaskNewClusterClusterMountInfo[];
    clusterName?: string;
    customTags?: {[key: string]: string};
    dataSecurityMode?: string;
    dockerImage?: outputs.JobTaskNewClusterDockerImage;
    driverInstancePoolId: string;
    driverNodeTypeId: string;
    enableElasticDisk: boolean;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.JobTaskNewClusterGcpAttributes;
    idempotencyToken?: string;
    initScripts?: outputs.JobTaskNewClusterInitScript[];
    instancePoolId?: string;
    isSingleNode?: boolean;
    kind?: string;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: outputs.JobTaskNewClusterLibrary[];
    nodeTypeId: string;
    numWorkers?: number;
    policyId?: string;
    runtimeEngine?: string;
    singleUserName?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sparkVersion?: string;
    sshPublicKeys?: string[];
    useMlRuntime?: boolean;
    /**
     * isn't supported
     */
    workloadType?: outputs.JobTaskNewClusterWorkloadType;
}

export interface JobTaskNewClusterAutoscale {
    maxWorkers?: number;
    minWorkers?: number;
}

export interface JobTaskNewClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface JobTaskNewClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.JobTaskNewClusterAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface JobTaskNewClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface JobTaskNewClusterClusterLogConf {
    dbfs?: outputs.JobTaskNewClusterClusterLogConfDbfs;
    s3?: outputs.JobTaskNewClusterClusterLogConfS3;
    volumes?: outputs.JobTaskNewClusterClusterLogConfVolumes;
}

export interface JobTaskNewClusterClusterLogConfDbfs {
    destination: string;
}

export interface JobTaskNewClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobTaskNewClusterClusterLogConfVolumes {
    destination: string;
}

export interface JobTaskNewClusterClusterMountInfo {
    localMountDirPath: string;
    networkFilesystemInfo: outputs.JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo;
    remoteMountDirPath?: string;
}

export interface JobTaskNewClusterClusterMountInfoNetworkFilesystemInfo {
    mountOptions?: string;
    serverAddress: string;
}

export interface JobTaskNewClusterDockerImage {
    basicAuth?: outputs.JobTaskNewClusterDockerImageBasicAuth;
    /**
     * URL of the job on the given workspace
     */
    url: string;
}

export interface JobTaskNewClusterDockerImageBasicAuth {
    password: string;
    username: string;
}

export interface JobTaskNewClusterGcpAttributes {
    availability?: string;
    bootDiskSize?: number;
    googleServiceAccount?: string;
    localSsdCount?: number;
    usePreemptibleExecutors?: boolean;
    zoneId?: string;
}

export interface JobTaskNewClusterInitScript {
    abfss?: outputs.JobTaskNewClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.JobTaskNewClusterInitScriptDbfs;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobTaskNewClusterInitScriptFile;
    gcs?: outputs.JobTaskNewClusterInitScriptGcs;
    s3?: outputs.JobTaskNewClusterInitScriptS3;
    volumes?: outputs.JobTaskNewClusterInitScriptVolumes;
    workspace?: outputs.JobTaskNewClusterInitScriptWorkspace;
}

export interface JobTaskNewClusterInitScriptAbfss {
    destination: string;
}

export interface JobTaskNewClusterInitScriptDbfs {
    destination: string;
}

export interface JobTaskNewClusterInitScriptFile {
    destination: string;
}

export interface JobTaskNewClusterInitScriptGcs {
    destination: string;
}

export interface JobTaskNewClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface JobTaskNewClusterInitScriptVolumes {
    destination: string;
}

export interface JobTaskNewClusterInitScriptWorkspace {
    destination: string;
}

export interface JobTaskNewClusterLibrary {
    cran?: outputs.JobTaskNewClusterLibraryCran;
    egg?: string;
    jar?: string;
    maven?: outputs.JobTaskNewClusterLibraryMaven;
    pypi?: outputs.JobTaskNewClusterLibraryPypi;
    requirements?: string;
    whl?: string;
}

export interface JobTaskNewClusterLibraryCran {
    package: string;
    repo?: string;
}

export interface JobTaskNewClusterLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface JobTaskNewClusterLibraryPypi {
    package: string;
    repo?: string;
}

export interface JobTaskNewClusterWorkloadType {
    clients: outputs.JobTaskNewClusterWorkloadTypeClients;
}

export interface JobTaskNewClusterWorkloadTypeClients {
    jobs?: boolean;
    notebooks?: boolean;
}

export interface JobTaskNotebookTask {
    /**
     * (Map) Base parameters to be used for each run of this job. If the run is initiated by a call to run-now with parameters specified, the two parameters maps will be merged. If the same key is specified in baseParameters and in run-now, the value from run-now will be used. If the notebook takes a parameter that is not specified in the job’s baseParameters or the run-now override parameters, the default value from the notebook will be used. Retrieve these parameters in a notebook using `dbutils.widgets.get`.
     */
    baseParameters?: {[key: string]: string};
    /**
     * The path of the databricks.Notebook to be run in the Databricks workspace or remote repository. For notebooks stored in the Databricks workspace, the path must be absolute and begin with a slash. For notebooks stored in a remote repository, the path must be relative. This field is required.
     */
    notebookPath: string;
    /**
     * Location type of the notebook, can only be `WORKSPACE` or `GIT`. When set to `WORKSPACE`, the notebook will be retrieved from the local Databricks workspace. When set to `GIT`, the notebook will be retrieved from a Git repository defined in `gitSource`. If the value is empty, the task will use `GIT` if `gitSource` is defined and `WORKSPACE` otherwise.
     */
    source?: string;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task with SQL notebook.
     */
    warehouseId?: string;
}

export interface JobTaskNotificationSettings {
    /**
     * (Bool) do not send notifications to recipients specified in `onStart` for the retried runs and do not send notifications to recipients specified in `onFailure` until the last retry of the run.
     */
    alertOnLastAttempt?: boolean;
    /**
     * (Bool) don't send alert for cancelled runs.
     *
     * The following parameter is only available on task level.
     */
    noAlertForCanceledRuns?: boolean;
    /**
     * (Bool) don't send alert for skipped runs.
     */
    noAlertForSkippedRuns?: boolean;
}

export interface JobTaskPipelineTask {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: boolean;
    /**
     * The pipeline's unique ID.
     */
    pipelineId: string;
}

export interface JobTaskPowerBiTask {
    /**
     * The resource name of the UC connection to authenticate from Databricks to Power BI
     */
    connectionResourceName?: string;
    /**
     * The semantic model to update. Block consists of following fields:
     */
    powerBiModel?: outputs.JobTaskPowerBiTaskPowerBiModel;
    /**
     * Whether the model should be refreshed after the update. Default is false
     */
    refreshAfterUpdate?: boolean;
    /**
     * The tables to be exported to Power BI. Block consists of following fields:
     */
    tables?: outputs.JobTaskPowerBiTaskTable[];
    /**
     * The SQL warehouse ID to use as the Power BI data source
     */
    warehouseId?: string;
}

export interface JobTaskPowerBiTaskPowerBiModel {
    /**
     * How the published Power BI model authenticates to Databricks
     */
    authenticationMethod?: string;
    /**
     * The name of the Power BI model
     */
    modelName?: string;
    /**
     * Whether to overwrite existing Power BI models. Default is false
     */
    overwriteExisting?: boolean;
    /**
     * The default storage mode of the Power BI model
     */
    storageMode?: string;
    /**
     * The name of the Power BI workspace of the model
     */
    workspaceName?: string;
}

export interface JobTaskPowerBiTaskTable {
    /**
     * The catalog name in Databricks
     */
    catalog?: string;
    /**
     * The table name in Databricks. If empty, all tables under the schema are selected.
     */
    name?: string;
    /**
     * The schema name in Databricks
     */
    schema?: string;
    /**
     * The Power BI storage mode of the table
     */
    storageMode?: string;
}

export interface JobTaskPythonWheelTask {
    /**
     * Python function as entry point for the task
     */
    entryPoint?: string;
    /**
     * Named parameters for the task
     */
    namedParameters?: {[key: string]: string};
    /**
     * Name of Python package
     */
    packageName?: string;
    /**
     * Parameters for the task
     */
    parameters?: string[];
}

export interface JobTaskRunJobTask {
    dbtCommands?: string[];
    jarParams?: string[];
    /**
     * (String) ID of the job
     */
    jobId: number;
    /**
     * (Map) Job parameters for the task
     */
    jobParameters?: {[key: string]: string};
    notebookParams?: {[key: string]: string};
    pipelineParams?: outputs.JobTaskRunJobTaskPipelineParams;
    pythonNamedParams?: {[key: string]: string};
    pythonParams?: string[];
    sparkSubmitParams?: string[];
    sqlParams?: {[key: string]: string};
}

export interface JobTaskRunJobTaskPipelineParams {
    /**
     * (Bool) Specifies if there should be full refresh of the pipeline.
     *
     * > The following configuration blocks are only supported inside a `task` block
     */
    fullRefresh?: boolean;
}

export interface JobTaskSparkJarTask {
    jarUri?: string;
    /**
     * The full name of the class containing the main method to be executed. This class must be contained in a JAR provided as a library. The code should use `SparkContext.getOrCreate` to obtain a Spark context; otherwise, runs of the job will fail.
     */
    mainClassName?: string;
    /**
     * (List) Parameters passed to the main method.
     */
    parameters?: string[];
    runAsRepl: boolean;
}

export interface JobTaskSparkPythonTask {
    /**
     * (List) Command line parameters passed to the Python file.
     */
    parameters?: string[];
    /**
     * The URI of the Python file to be executed. databricks_dbfs_file, cloud file URIs (e.g. `s3:/`, `abfss:/`, `gs:/`), workspace paths and remote repository are supported. For Python files stored in the Databricks workspace, the path must be absolute and begin with `/`. For files stored in a remote repository, the path must be relative. This field is required.
     */
    pythonFile: string;
    /**
     * Location type of the Python file. When set to `WORKSPACE` or not specified, the file will be retrieved from the local Databricks workspace or cloud location (if the pythonFile has a URI format). When set to `GIT`, the Python file will be retrieved from a Git repository defined in `gitSource`.
     * * `WORKSPACE`: The Python file is located in a Databricks workspace or at a cloud filesystem URI.
     * * `GIT`: The Python file is located in a remote Git repository.
     */
    source?: string;
}

export interface JobTaskSparkSubmitTask {
    /**
     * (List) Command-line parameters passed to spark submit.
     */
    parameters?: string[];
}

export interface JobTaskSqlTask {
    /**
     * block consisting of following fields:
     */
    alert?: outputs.JobTaskSqlTaskAlert;
    /**
     * block consisting of following fields:
     */
    dashboard?: outputs.JobTaskSqlTaskDashboard;
    /**
     * block consisting of single string fields:
     */
    file?: outputs.JobTaskSqlTaskFile;
    /**
     * (Map) parameters to be used for each run of this task. The SQL alert task does not support custom parameters.
     */
    parameters?: {[key: string]: string};
    /**
     * block consisting of single string field: `queryId` - identifier of the Databricks Query (databricks_query).
     */
    query?: outputs.JobTaskSqlTaskQuery;
    /**
     * ID of the (the databricks_sql_endpoint) that will be used to execute the task.  Only Serverless & Pro warehouses are supported right now.
     */
    warehouseId: string;
}

export interface JobTaskSqlTaskAlert {
    /**
     * (String) identifier of the Databricks Alert (databricks_alert).
     */
    alertId: string;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: boolean;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions?: outputs.JobTaskSqlTaskAlertSubscription[];
}

export interface JobTaskSqlTaskAlertSubscription {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    userName?: string;
}

export interface JobTaskSqlTaskDashboard {
    /**
     * string specifying a custom subject of email sent.
     */
    customSubject?: string;
    /**
     * (String) identifier of the Databricks SQL Dashboard databricks_sql_dashboard.
     */
    dashboardId: string;
    /**
     * flag that specifies if subscriptions are paused or not.
     */
    pauseSubscriptions?: boolean;
    /**
     * a list of subscription blocks consisting out of one of the required fields: `userName` for user emails or `destinationId` - for Alert destination's identifier.
     */
    subscriptions?: outputs.JobTaskSqlTaskDashboardSubscription[];
}

export interface JobTaskSqlTaskDashboardSubscription {
    /**
     * A snapshot of the dashboard will be sent to the destination when the `destinationId` field is present.
     */
    destinationId?: string;
    userName?: string;
}

export interface JobTaskSqlTaskFile {
    /**
     * If `source` is `GIT`: Relative path to the file in the repository specified in the `gitSource` block with SQL commands to execute. If `source` is `WORKSPACE`: Absolute path to the file in the workspace with SQL commands to execute.
     *
     * Example
     *
     * ```typescript
     * import * as pulumi from "@pulumi/pulumi";
     * import * as databricks from "@pulumi/databricks";
     *
     * const sqlAggregationJob = new databricks.Job("sql_aggregation_job", {
     *     name: "Example SQL Job",
     *     tasks: [
     *         {
     *             taskKey: "run_agg_query",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 query: {
     *                     queryId: aggQuery.id,
     *                 },
     *             },
     *         },
     *         {
     *             taskKey: "run_dashboard",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 dashboard: {
     *                     dashboardId: dash.id,
     *                     subscriptions: [{
     *                         userName: "user@domain.com",
     *                     }],
     *                 },
     *             },
     *         },
     *         {
     *             taskKey: "run_alert",
     *             sqlTask: {
     *                 warehouseId: sqlJobWarehouse.id,
     *                 alert: {
     *                     alertId: alert.id,
     *                     subscriptions: [{
     *                         userName: "user@domain.com",
     *                     }],
     *                 },
     *             },
     *         },
     *     ],
     * });
     * ```
     */
    path: string;
    /**
     * The source of the project. Possible values are `WORKSPACE` and `GIT`.
     */
    source?: string;
}

export interface JobTaskSqlTaskQuery {
    queryId: string;
}

export interface JobTaskWebhookNotifications {
    /**
     * (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: outputs.JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded[];
    /**
     * (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.
     */
    onFailures?: outputs.JobTaskWebhookNotificationsOnFailure[];
    /**
     * (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.
     */
    onStarts?: outputs.JobTaskWebhookNotificationsOnStart[];
    /**
     * (List) list of notification IDs to call when any streaming backlog thresholds are exceeded for any stream.
     *
     * Note that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://<workspace host>/sql/destinations/<notification id>?o=<workspace id>`
     *
     * Example
     */
    onStreamingBacklogExceededs?: outputs.JobTaskWebhookNotificationsOnStreamingBacklogExceeded[];
    /**
     * (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.
     */
    onSuccesses?: outputs.JobTaskWebhookNotificationsOnSuccess[];
}

export interface JobTaskWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskWebhookNotificationsOnFailure {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskWebhookNotificationsOnStart {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTaskWebhookNotificationsOnSuccess {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobTrigger {
    /**
     * configuration block to define a trigger for [File Arrival events](https://learn.microsoft.com/en-us/azure/databricks/workflows/jobs/file-arrival-triggers) consisting of following attributes:
     */
    fileArrival?: outputs.JobTriggerFileArrival;
    /**
     * Indicate whether this trigger is paused or not. Either `PAUSED` or `UNPAUSED`. When the `pauseStatus` field is omitted in the block, the server will default to using `UNPAUSED` as a value for `pauseStatus`.
     */
    pauseStatus?: string;
    /**
     * configuration block to define a trigger for Periodic Triggers consisting of the following attributes:
     */
    periodic?: outputs.JobTriggerPeriodic;
    table?: outputs.JobTriggerTable;
    tableUpdate?: outputs.JobTriggerTableUpdate;
}

export interface JobTriggerFileArrival {
    /**
     * If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.
     */
    minTimeBetweenTriggersSeconds?: number;
    /**
     * URL to be monitored for file arrivals. The path must point to the root or a subpath of the external location. Please note that the URL must have a trailing slash character (`/`).
     */
    url: string;
    /**
     * If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.
     */
    waitAfterLastChangeSeconds?: number;
}

export interface JobTriggerPeriodic {
    /**
     * Specifies the interval at which the job should run. This value is required.
     */
    interval: number;
    /**
     * Options are {"DAYS", "HOURS", "WEEKS"}.
     */
    unit: string;
}

export interface JobTriggerTable {
    condition?: string;
    /**
     * If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.
     */
    minTimeBetweenTriggersSeconds?: number;
    tableNames?: string[];
    /**
     * If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.
     */
    waitAfterLastChangeSeconds?: number;
}

export interface JobTriggerTableUpdate {
    condition?: string;
    /**
     * If set, the trigger starts a run only after the specified amount of time passed since the last time the trigger fired. The minimum allowed value is 60 seconds.
     */
    minTimeBetweenTriggersSeconds?: number;
    tableNames: string[];
    /**
     * If set, the trigger starts a run only after no file activity has occurred for the specified amount of time. This makes it possible to wait for a batch of incoming files to arrive before triggering a run. The minimum allowed value is 60 seconds.
     */
    waitAfterLastChangeSeconds?: number;
}

export interface JobWebhookNotifications {
    /**
     * (List) list of notification IDs to call when the duration of a run exceeds the threshold specified by the `RUN_DURATION_SECONDS` metric in the `health` block.
     */
    onDurationWarningThresholdExceededs?: outputs.JobWebhookNotificationsOnDurationWarningThresholdExceeded[];
    /**
     * (List) list of notification IDs to call when the run fails. A maximum of 3 destinations can be specified.
     */
    onFailures?: outputs.JobWebhookNotificationsOnFailure[];
    /**
     * (List) list of notification IDs to call when the run starts. A maximum of 3 destinations can be specified.
     */
    onStarts?: outputs.JobWebhookNotificationsOnStart[];
    /**
     * (List) list of notification IDs to call when any streaming backlog thresholds are exceeded for any stream.
     *
     * Note that the `id` is not to be confused with the name of the alert destination. The `id` can be retrieved through the API or the URL of Databricks UI `https://<workspace host>/sql/destinations/<notification id>?o=<workspace id>`
     *
     * Example
     */
    onStreamingBacklogExceededs?: outputs.JobWebhookNotificationsOnStreamingBacklogExceeded[];
    /**
     * (List) list of notification IDs to call when the run completes successfully. A maximum of 3 destinations can be specified.
     */
    onSuccesses?: outputs.JobWebhookNotificationsOnSuccess[];
}

export interface JobWebhookNotificationsOnDurationWarningThresholdExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobWebhookNotificationsOnFailure {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobWebhookNotificationsOnStart {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobWebhookNotificationsOnStreamingBacklogExceeded {
    /**
     * ID of the job
     */
    id: string;
}

export interface JobWebhookNotificationsOnSuccess {
    /**
     * ID of the job
     */
    id: string;
}

export interface LakehouseMonitorCustomMetric {
    /**
     * [create metric definition](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html#create-definition)
     */
    definition: string;
    /**
     * Columns on the monitored table to apply the custom metrics to.
     */
    inputColumns: string[];
    /**
     * Name of the custom metric.
     */
    name: string;
    /**
     * The output type of the custom metric.
     */
    outputDataType: string;
    /**
     * The type of the custom metric.
     */
    type: string;
}

export interface LakehouseMonitorDataClassificationConfig {
    enabled?: boolean;
}

export interface LakehouseMonitorInferenceLog {
    /**
     * List of granularities to use when aggregating data into time windows based on their timestamp.
     */
    granularities: string[];
    /**
     * Column of the model label
     */
    labelCol?: string;
    /**
     * Column of the model id or version
     */
    modelIdCol: string;
    /**
     * Column of the model prediction
     */
    predictionCol: string;
    /**
     * Column of the model prediction probabilities
     */
    predictionProbaCol?: string;
    /**
     * Problem type the model aims to solve. Either `PROBLEM_TYPE_CLASSIFICATION` or `PROBLEM_TYPE_REGRESSION`
     */
    problemType: string;
    /**
     * Column of the timestamp of predictions
     */
    timestampCol: string;
}

export interface LakehouseMonitorNotifications {
    /**
     * who to send notifications to on monitor failure.
     */
    onFailure?: outputs.LakehouseMonitorNotificationsOnFailure;
    /**
     * Who to send notifications to when new data classification tags are detected.
     */
    onNewClassificationTagDetected?: outputs.LakehouseMonitorNotificationsOnNewClassificationTagDetected;
}

export interface LakehouseMonitorNotificationsOnFailure {
    emailAddresses?: string[];
}

export interface LakehouseMonitorNotificationsOnNewClassificationTagDetected {
    emailAddresses?: string[];
}

export interface LakehouseMonitorSchedule {
    /**
     * optional string field that indicates whether a schedule is paused (`PAUSED`) or not (`UNPAUSED`).
     */
    pauseStatus: string;
    /**
     * string expression that determines when to run the monitor. See [Quartz documentation](https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for examples.
     */
    quartzCronExpression: string;
    /**
     * string with timezone id (e.g., `PST`) in which to evaluate the Quartz expression.
     */
    timezoneId: string;
}

export interface LakehouseMonitorSnapshot {
}

export interface LakehouseMonitorTimeSeries {
    /**
     * List of granularities to use when aggregating data into time windows based on their timestamp.
     */
    granularities: string[];
    /**
     * Column of the timestamp of predictions
     */
    timestampCol: string;
}

export interface LibraryCran {
    package: string;
    repo?: string;
}

export interface LibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface LibraryPypi {
    package: string;
    repo?: string;
}

export interface MetastoreDataAccessAwsIamRole {
    externalId: string;
    roleArn: string;
    unityCatalogIamArn: string;
}

export interface MetastoreDataAccessAzureManagedIdentity {
    accessConnectorId: string;
    credentialId: string;
    managedIdentityId?: string;
}

export interface MetastoreDataAccessAzureServicePrincipal {
    applicationId: string;
    clientSecret: string;
    directoryId: string;
}

export interface MetastoreDataAccessCloudflareApiToken {
    accessKeyId: string;
    accountId: string;
    secretAccessKey: string;
}

export interface MetastoreDataAccessDatabricksGcpServiceAccount {
    credentialId: string;
    email: string;
}

export interface MetastoreDataAccessGcpServiceAccountKey {
    email: string;
    privateKey: string;
    privateKeyId: string;
}

export interface MlflowExperimentTag {
    key?: string;
    value?: string;
}

export interface MlflowModelTag {
    key?: string;
    value?: string;
}

export interface MlflowWebhookHttpUrlSpec {
    /**
     * Value of the authorization header that should be sent in the request sent by the wehbook.  It should be of the form `<auth type> <credentials>`, e.g. `Bearer <access_token>`. If set to an empty string, no authorization header will be included in the request.
     */
    authorization?: string;
    /**
     * Enable/disable SSL certificate validation. Default is `true`. For self-signed certificates, this field must be `false` AND the destination server must disable certificate validation as well. For security purposes, it is encouraged to perform secret validation with the HMAC-encoded portion of the payload and acknowledge the risk associated with disabling hostname validation whereby it becomes more likely that requests can be maliciously routed to an unintended host.
     */
    enableSslVerification?: boolean;
    /**
     * Shared secret required for HMAC encoding payload. The HMAC-encoded payload will be sent in the header as `X-Databricks-Signature: encodedPayload`.
     */
    secret?: string;
    /**
     * External HTTPS URL called on event trigger (by using a POST request). Structure of payload depends on the event type, refer to [documentation](https://docs.databricks.com/applications/mlflow/model-registry-webhooks.html) for more details.
     */
    url: string;
}

export interface MlflowWebhookJobSpec {
    /**
     * The personal access token used to authorize webhook's job runs.
     */
    accessToken: string;
    /**
     * ID of the Databricks job that the webhook runs.
     */
    jobId: string;
    /**
     * URL of the workspace containing the job that this webhook runs. If not specified, the job’s workspace URL is assumed to be the same as the workspace where the webhook is created.
     */
    workspaceUrl?: string;
}

export interface ModelServingAiGateway {
    /**
     * block with configuration for traffic fallback which auto fallbacks to other served entities if the request to a served entity fails with certain error codes, to increase availability.
     */
    fallbackConfig?: outputs.ModelServingAiGatewayFallbackConfig;
    /**
     * Block with configuration for AI Guardrails to prevent unwanted data and unsafe data in requests and responses. Consists of the following attributes:
     */
    guardrails?: outputs.ModelServingAiGatewayGuardrails;
    /**
     * Block describing the configuration of usage tracking. Consists of the following attributes:
     */
    inferenceTableConfig?: outputs.ModelServingAiGatewayInferenceTableConfig;
    /**
     * Block describing rate limits for AI gateway. For details see the description of `rateLimits` block above.
     */
    rateLimits?: outputs.ModelServingAiGatewayRateLimit[];
    /**
     * Block with configuration for payload logging using inference tables. For details see the description of `autoCaptureConfig` block above.
     */
    usageTrackingConfig?: outputs.ModelServingAiGatewayUsageTrackingConfig;
}

export interface ModelServingAiGatewayFallbackConfig {
    /**
     * Whether to enable traffic fallback. When a served entity in the serving endpoint returns specific error codes (e.g. 500), the request will automatically be round-robin attempted with other served entities in the same endpoint, following the order of served entity list, until a successful response is returned.
     */
    enabled: boolean;
}

export interface ModelServingAiGatewayGuardrails {
    /**
     * A block with configuration for input guardrail filters:
     */
    input?: outputs.ModelServingAiGatewayGuardrailsInput;
    /**
     * A block with configuration for output guardrail filters.  Has the same structure as `input` block.
     */
    output?: outputs.ModelServingAiGatewayGuardrailsOutput;
}

export interface ModelServingAiGatewayGuardrailsInput {
    /**
     * List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
     */
    invalidKeywords?: string[];
    /**
     * Block with configuration for guardrail PII filter:
     */
    pii?: outputs.ModelServingAiGatewayGuardrailsInputPii;
    /**
     * the boolean flag that indicates whether the safety filter is enabled.
     */
    safety?: boolean;
    /**
     * The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
     */
    validTopics?: string[];
}

export interface ModelServingAiGatewayGuardrailsInputPii {
    /**
     * a string that describes the behavior for PII filter. Currently only `BLOCK` value is supported.
     */
    behavior?: string;
}

export interface ModelServingAiGatewayGuardrailsOutput {
    /**
     * List of invalid keywords. AI guardrail uses keyword or string matching to decide if the keyword exists in the request or response content.
     */
    invalidKeywords?: string[];
    /**
     * Block with configuration for guardrail PII filter:
     */
    pii?: outputs.ModelServingAiGatewayGuardrailsOutputPii;
    /**
     * the boolean flag that indicates whether the safety filter is enabled.
     */
    safety?: boolean;
    /**
     * The list of allowed topics. Given a chat request, this guardrail flags the request if its topic is not in the allowed topics.
     */
    validTopics?: string[];
}

export interface ModelServingAiGatewayGuardrailsOutputPii {
    /**
     * a string that describes the behavior for PII filter. Currently only `BLOCK` value is supported.
     */
    behavior?: string;
}

export interface ModelServingAiGatewayInferenceTableConfig {
    /**
     * The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
     */
    catalogName?: string;
    /**
     * boolean flag specifying if usage tracking is enabled.
     */
    enabled?: boolean;
    /**
     * The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
     */
    schemaName?: string;
    /**
     * The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
     */
    tableNamePrefix?: string;
}

export interface ModelServingAiGatewayRateLimit {
    /**
     * Used to specify how many calls are allowed for a key within the renewal_period.
     */
    calls: number;
    /**
     * Key field for a serving endpoint rate limit. Currently, only `user` and `endpoint` are supported, with `endpoint` being the default if not specified.
     */
    key?: string;
    /**
     * Renewal period field for a serving endpoint rate limit. Currently, only `minute` is supported.
     */
    renewalPeriod: string;
}

export interface ModelServingAiGatewayUsageTrackingConfig {
    enabled?: boolean;
}

export interface ModelServingConfig {
    /**
     * Configuration for Inference Tables which automatically logs requests and responses to Unity Catalog.
     */
    autoCaptureConfig?: outputs.ModelServingConfigAutoCaptureConfig;
    /**
     * A list of served entities for the endpoint to serve. A serving endpoint can have up to 10 served entities.
     */
    servedEntities?: outputs.ModelServingConfigServedEntity[];
    /**
     * Each block represents a served model for the endpoint to serve. A model serving endpoint can have up to 10 served models.
     *
     * @deprecated Please use 'config.served_entities' instead of 'config.served_models'.
     */
    servedModels?: outputs.ModelServingConfigServedModel[];
    /**
     * A single block represents the traffic split configuration amongst the served models.
     */
    trafficConfig: outputs.ModelServingConfigTrafficConfig;
}

export interface ModelServingConfigAutoCaptureConfig {
    /**
     * The name of the catalog in Unity Catalog. NOTE: On update, you cannot change the catalog name if it was already set.
     */
    catalogName?: string;
    /**
     * If inference tables are enabled or not. NOTE: If you have already disabled payload logging once, you cannot enable it again.
     */
    enabled: boolean;
    /**
     * The name of the schema in Unity Catalog. NOTE: On update, you cannot change the schema name if it was already set.
     */
    schemaName?: string;
    /**
     * The prefix of the table in Unity Catalog. NOTE: On update, you cannot change the prefix name if it was already set.
     */
    tableNamePrefix: string;
}

export interface ModelServingConfigServedEntity {
    /**
     * The name of the entity to be served. The entity may be a model in the Databricks Model Registry, a model in the Unity Catalog (UC), or a function of type `FEATURE_SPEC` in the UC. If it is a UC object, the full name of the object should be given in the form of `catalog_name.schema_name.model_name`.
     */
    entityName?: string;
    /**
     * The version of the model in Databricks Model Registry to be served or empty if the entity is a `FEATURE_SPEC`.
     */
    entityVersion?: string;
    /**
     * An object containing a set of optional, user-specified environment variable key-value pairs used for serving this entity. Note: this is an experimental feature and is subject to change. Example entity environment variables that refer to Databricks secrets: ```{"OPENAI_API_KEY": "{{secrets/my_scope/my_key}}", "DATABRICKS_TOKEN": "{{secrets/my_scope2/my_key2}}"}```
     */
    environmentVars?: {[key: string]: string};
    /**
     * The external model to be served. NOTE: Only one of `externalModel` and (`entityName`, `entityVersion`, `workloadSize`, `workloadType`, and `scaleToZeroEnabled`) can be specified with the latter set being used for custom model serving for a Databricks registered model. When an `externalModel` is present, the served entities list can only have one `servedEntity` object. An existing endpoint with `externalModel` can not be updated to an endpoint without `externalModel`. If the endpoint is created without `externalModel`, users cannot update it to add `externalModel` later.
     */
    externalModel?: outputs.ModelServingConfigServedEntityExternalModel;
    /**
     * ARN of the instance profile that the served entity uses to access AWS resources.
     */
    instanceProfileArn?: string;
    /**
     * The maximum tokens per second that the endpoint can scale up to.
     */
    maxProvisionedThroughput?: number;
    /**
     * The minimum tokens per second that the endpoint can scale down to.
     */
    minProvisionedThroughput?: number;
    /**
     * The name of a served entity. It must be unique across an endpoint. A served entity name can consist of alphanumeric characters, dashes, and underscores. If not specified for an external model, this field defaults to `external_model.name`, with '.' and ':' replaced with '-', and if not specified for other entities, it defaults to -.
     */
    name: string;
    /**
     * Whether the compute resources for the served entity should scale down to zero.
     */
    scaleToZeroEnabled?: boolean;
    /**
     * The workload size of the served entity. The workload size corresponds to a range of provisioned concurrency that the compute autoscales between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency). If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size is 0.
     */
    workloadSize: string;
    /**
     * The workload type of the served entity. The workload type selects which type of compute to use in the endpoint. The default value for this parameter is `CPU`. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See the available [GPU types](https://docs.databricks.com/machine-learning/model-serving/create-manage-serving-endpoints.html#gpu-workload-types).
     */
    workloadType: string;
}

export interface ModelServingConfigServedEntityExternalModel {
    /**
     * AI21Labs Config
     */
    ai21labsConfig?: outputs.ModelServingConfigServedEntityExternalModelAi21labsConfig;
    /**
     * Amazon Bedrock Config
     */
    amazonBedrockConfig?: outputs.ModelServingConfigServedEntityExternalModelAmazonBedrockConfig;
    /**
     * Anthropic Config
     */
    anthropicConfig?: outputs.ModelServingConfigServedEntityExternalModelAnthropicConfig;
    /**
     * Cohere Config
     */
    cohereConfig?: outputs.ModelServingConfigServedEntityExternalModelCohereConfig;
    /**
     * Custom Provider Config. Only required if the provider is 'custom'.
     */
    customProviderConfig?: outputs.ModelServingConfigServedEntityExternalModelCustomProviderConfig;
    /**
     * Databricks Model Serving Config
     */
    databricksModelServingConfig?: outputs.ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig;
    /**
     * Google Cloud Vertex AI Config.
     */
    googleCloudVertexAiConfig?: outputs.ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig;
    /**
     * The name of the external model.
     */
    name: string;
    /**
     * OpenAI Config
     */
    openaiConfig?: outputs.ModelServingConfigServedEntityExternalModelOpenaiConfig;
    /**
     * PaLM Config
     */
    palmConfig?: outputs.ModelServingConfigServedEntityExternalModelPalmConfig;
    /**
     * The name of the provider for the external model. Currently, the supported providers are `ai21labs`, `anthropic`, `amazon-bedrock`, `cohere`, `databricks-model-serving`, `google-cloud-vertex-ai`, `openai`, and `palm`.
     */
    provider: string;
    /**
     * The task type of the external model.
     */
    task: string;
}

export interface ModelServingConfigServedEntityExternalModelAi21labsConfig {
    /**
     * The Databricks secret key reference for an AI21Labs API key.
     */
    ai21labsApiKey?: string;
    /**
     * An AI21 Labs API key provided as a plaintext string.
     */
    ai21labsApiKeyPlaintext?: string;
}

export interface ModelServingConfigServedEntityExternalModelAmazonBedrockConfig {
    /**
     * The Databricks secret key reference for an AWS Access Key ID with permissions to interact with Bedrock services.
     */
    awsAccessKeyId?: string;
    /**
     * An AWS access key ID with permissions to interact with Bedrock services provided as a plaintext string.
     */
    awsAccessKeyIdPlaintext?: string;
    /**
     * The AWS region to use. Bedrock has to be enabled there.
     */
    awsRegion: string;
    /**
     * The Databricks secret key reference for an AWS Secret Access Key paired with the access key ID, with permissions to interact with Bedrock services.
     */
    awsSecretAccessKey?: string;
    /**
     * An AWS secret access key paired with the access key ID, with permissions to interact with Bedrock services provided as a plaintext string.
     */
    awsSecretAccessKeyPlaintext?: string;
    /**
     * The underlying provider in Amazon Bedrock. Supported values (case insensitive) include: `Anthropic`, `Cohere`, `AI21Labs`, `Amazon`.
     */
    bedrockProvider: string;
    instanceProfileArn?: string;
}

export interface ModelServingConfigServedEntityExternalModelAnthropicConfig {
    /**
     * The Databricks secret key reference for an Anthropic API key.
     */
    anthropicApiKey?: string;
    /**
     * The Anthropic API key provided as a plaintext string.
     */
    anthropicApiKeyPlaintext?: string;
}

export interface ModelServingConfigServedEntityExternalModelCohereConfig {
    cohereApiBase?: string;
    /**
     * The Databricks secret key reference for a Cohere API key.
     */
    cohereApiKey?: string;
    /**
     * The Cohere API key provided as a plaintext string.
     */
    cohereApiKeyPlaintext?: string;
}

export interface ModelServingConfigServedEntityExternalModelCustomProviderConfig {
    /**
     * API key authentication for the custom provider API. Conflicts with `bearerTokenAuth`.
     */
    apiKeyAuth?: outputs.ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth;
    /**
     * bearer token authentication for the custom provider API.  Conflicts with `apiKeyAuth`.
     */
    bearerTokenAuth?: outputs.ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth;
    /**
     * URL of the custom provider API.
     */
    customProviderUrl: string;
}

export interface ModelServingConfigServedEntityExternalModelCustomProviderConfigApiKeyAuth {
    key: string;
    value?: string;
    /**
     * The API Key provided as a plaintext string.
     */
    valuePlaintext?: string;
}

export interface ModelServingConfigServedEntityExternalModelCustomProviderConfigBearerTokenAuth {
    /**
     * The Databricks secret key reference for a token.
     */
    token?: string;
    /**
     * The token provided as a plaintext string.
     */
    tokenPlaintext?: string;
}

export interface ModelServingConfigServedEntityExternalModelDatabricksModelServingConfig {
    /**
     * The Databricks secret key reference for a Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model.
     */
    databricksApiToken?: string;
    /**
     * The Databricks API token that corresponds to a user or service principal with Can Query access to the model serving endpoint pointed to by this external model provided as a plaintext string.
     */
    databricksApiTokenPlaintext?: string;
    /**
     * The URL of the Databricks workspace containing the model serving endpoint pointed to by this external model.
     */
    databricksWorkspaceUrl: string;
}

export interface ModelServingConfigServedEntityExternalModelGoogleCloudVertexAiConfig {
    /**
     * The Databricks secret key reference for a private key for the service account that has access to the Google Cloud Vertex AI Service.
     */
    privateKey?: string;
    /**
     * The private key for the service account that has access to the Google Cloud Vertex AI Service is provided as a plaintext secret.
     */
    privateKeyPlaintext?: string;
    /**
     * This is the Google Cloud project id that the service account is associated with.
     */
    projectId: string;
    /**
     * This is the region for the Google Cloud Vertex AI Service.
     */
    region: string;
}

export interface ModelServingConfigServedEntityExternalModelOpenaiConfig {
    /**
     * This field is only required for Azure AD OpenAI and is the Microsoft Entra Client ID.
     */
    microsoftEntraClientId?: string;
    /**
     * The Databricks secret key reference for a client secret used for Microsoft Entra ID authentication.
     */
    microsoftEntraClientSecret?: string;
    /**
     * The client secret used for Microsoft Entra ID authentication provided as a plaintext string.
     */
    microsoftEntraClientSecretPlaintext?: string;
    /**
     * This field is only required for Azure AD OpenAI and is the Microsoft Entra Tenant ID.
     */
    microsoftEntraTenantId?: string;
    /**
     * This is the base URL for the OpenAI API (default: "<https://api.openai.com/v1>"). For Azure OpenAI, this field is required and is the base URL for the Azure OpenAI API service provided by Azure.
     */
    openaiApiBase?: string;
    /**
     * The Databricks secret key reference for an OpenAI or Azure OpenAI API key.
     */
    openaiApiKey?: string;
    /**
     * The OpenAI API key using the OpenAI or Azure service provided as a plaintext string.
     */
    openaiApiKeyPlaintext?: string;
    /**
     * This is an optional field to specify the type of OpenAI API to use. For Azure OpenAI, this field is required, and this parameter represents the preferred security access validation protocol. For access token validation, use `azure`. For authentication using Azure Active Directory (Azure AD) use, `azuread`.
     */
    openaiApiType?: string;
    /**
     * This is an optional field to specify the OpenAI API version. For Azure OpenAI, this field is required and is the version of the Azure OpenAI service to utilize, specified by a date.
     */
    openaiApiVersion?: string;
    /**
     * This field is only required for Azure OpenAI and is the name of the deployment resource for the Azure OpenAI service.
     */
    openaiDeploymentName?: string;
    /**
     * This is an optional field to specify the organization in OpenAI or Azure OpenAI.
     */
    openaiOrganization?: string;
}

export interface ModelServingConfigServedEntityExternalModelPalmConfig {
    /**
     * The Databricks secret key reference for a PaLM API key.
     */
    palmApiKey?: string;
    /**
     * The PaLM API key provided as a plaintext string.
     */
    palmApiKeyPlaintext?: string;
}

export interface ModelServingConfigServedModel {
    /**
     * a map of environment variable names/values that will be used for serving this model.  Environment variables may refer to Databricks secrets using the standard syntax: `{{secrets/secret_scope/secret_key}}`.
     */
    environmentVars?: {[key: string]: string};
    /**
     * ARN of the instance profile that the served model will use to access AWS resources.
     */
    instanceProfileArn?: string;
    /**
     * The maximum tokens per second that the endpoint can scale up to.
     */
    maxProvisionedThroughput?: number;
    /**
     * The minimum tokens per second that the endpoint can scale down to.
     */
    minProvisionedThroughput?: number;
    /**
     * The name of the model in Databricks Model Registry to be served.
     */
    modelName: string;
    /**
     * The version of the model in Databricks Model Registry to be served.
     */
    modelVersion: string;
    /**
     * The name of a served model. It must be unique across an endpoint. If not specified, this field will default to `modelname-modelversion`. A served model name can consist of alphanumeric characters, dashes, and underscores.
     */
    name: string;
    /**
     * Whether the compute resources for the served model should scale down to zero. If `scale-to-zero` is enabled, the lower bound of the provisioned concurrency for each workload size will be 0. The default value is `true`.
     */
    scaleToZeroEnabled?: boolean;
    /**
     * The workload size of the served model. The workload size corresponds to a range of provisioned concurrency that the compute will autoscale between. A single unit of provisioned concurrency can process one request at a time. Valid workload sizes are `Small` (4 - 4 provisioned concurrency), `Medium` (8 - 16 provisioned concurrency), and `Large` (16 - 64 provisioned concurrency).
     */
    workloadSize?: string;
    /**
     * The workload type of the served model. The workload type selects which type of compute to use in the endpoint. For deep learning workloads, GPU acceleration is available by selecting workload types like `GPU_SMALL` and others. See the documentation for all options. The default value is `CPU`.
     */
    workloadType: string;
}

export interface ModelServingConfigTrafficConfig {
    /**
     * Each block represents a route that defines traffic to each served entity. Each `servedEntity` block needs to have a corresponding `routes` block.
     */
    routes?: outputs.ModelServingConfigTrafficConfigRoute[];
}

export interface ModelServingConfigTrafficConfigRoute {
    servedModelName: string;
    /**
     * The percentage of endpoint traffic to send to this route. It must be an integer between 0 and 100 inclusive.
     */
    trafficPercentage: number;
}

export interface ModelServingRateLimit {
    /**
     * Used to specify how many calls are allowed for a key within the renewal_period.
     */
    calls: number;
    /**
     * Key field for a serving endpoint rate limit. Currently, only `user` and `endpoint` are supported, with `endpoint` being the default if not specified.
     */
    key?: string;
    /**
     * Renewal period field for a serving endpoint rate limit. Currently, only `minute` is supported.
     */
    renewalPeriod: string;
}

export interface ModelServingTag {
    /**
     * The key field for a tag.
     */
    key: string;
    /**
     * The value field for a tag.
     */
    value?: string;
}

export interface MountAbfs {
    clientId: string;
    clientSecretKey: string;
    clientSecretScope: string;
    containerName: string;
    directory?: string;
    initializeFileSystem: boolean;
    storageAccountName: string;
    tenantId: string;
}

export interface MountAdl {
    clientId: string;
    clientSecretKey: string;
    clientSecretScope: string;
    directory?: string;
    sparkConfPrefix?: string;
    storageResourceName: string;
    tenantId: string;
}

export interface MountGs {
    bucketName: string;
    serviceAccount?: string;
}

export interface MountS3 {
    bucketName: string;
    instanceProfile?: string;
}

export interface MountWasb {
    authType: string;
    containerName: string;
    directory?: string;
    storageAccountName: string;
    tokenSecretKey: string;
    tokenSecretScope: string;
}

export interface MwsCustomerManagedKeysAwsKeyInfo {
    /**
     * The AWS KMS key alias.
     */
    keyAlias?: string;
    /**
     * The AWS KMS key's Amazon Resource Name (ARN).
     */
    keyArn: string;
    /**
     * (Computed) The AWS region in which KMS key is deployed to. This is not required.
     */
    keyRegion: string;
}

export interface MwsCustomerManagedKeysGcpKeyInfo {
    /**
     * The GCP KMS key's resource name.
     */
    kmsKeyId: string;
}

export interface MwsNetworkConnectivityConfigEgressConfig {
    /**
     * block describing network connectivity rules that are applied by default without resource specific configurations.  Consists of the following fields:
     */
    defaultRules?: outputs.MwsNetworkConnectivityConfigEgressConfigDefaultRules;
    /**
     * block describing network connectivity rules that configured for each destinations. These rules override default rules.  Consists of the following fields:
     */
    targetRules?: outputs.MwsNetworkConnectivityConfigEgressConfigTargetRules;
}

export interface MwsNetworkConnectivityConfigEgressConfigDefaultRules {
    /**
     * (AWS only) - block with information about stable AWS IP CIDR blocks. You can use these to configure the firewall of your resources to allow traffic from your Databricks workspace.  Consists of the following fields:
     */
    awsStableIpRule?: outputs.MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule;
    /**
     * (Azure only) - block with information about stable Azure service endpoints. You can configure the firewall of your Azure resources to allow traffic from your Databricks serverless compute resources.  Consists of the following fields:
     */
    azureServiceEndpointRule?: outputs.MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule;
}

export interface MwsNetworkConnectivityConfigEgressConfigDefaultRulesAwsStableIpRule {
    /**
     * list of IP CIDR blocks.
     */
    cidrBlocks?: string[];
}

export interface MwsNetworkConnectivityConfigEgressConfigDefaultRulesAzureServiceEndpointRule {
    /**
     * list of subnets from which Databricks network traffic originates when accessing your Azure resources.
     */
    subnets?: string[];
    /**
     * the Azure region in which this service endpoint rule applies.
     */
    targetRegion?: string;
    /**
     * the Azure services to which this service endpoint rule applies to.
     */
    targetServices?: string[];
}

export interface MwsNetworkConnectivityConfigEgressConfigTargetRules {
    /**
     * (Azure only) - list containing information about configure Azure Private Endpoints.
     */
    azurePrivateEndpointRules?: outputs.MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule[];
}

export interface MwsNetworkConnectivityConfigEgressConfigTargetRulesAzurePrivateEndpointRule {
    connectionState?: string;
    creationTime?: number;
    deactivated?: boolean;
    deactivatedAt?: number;
    endpointName?: string;
    groupId?: string;
    /**
     * Canonical unique identifier of Network Connectivity Config in Databricks Account
     */
    networkConnectivityConfigId?: string;
    resourceId?: string;
    ruleId?: string;
    updatedTime?: number;
}

export interface MwsNetworksErrorMessage {
    errorMessage?: string;
    errorType?: string;
}

export interface MwsNetworksGcpNetworkInfo {
    /**
     * The Google Cloud project ID of the VPC network.
     */
    networkProjectId: string;
    /**
     * The name of the secondary IP range for pods. A Databricks-managed GKE cluster uses this IP range for its pods. This secondary IP range can only be used by one workspace.
     *
     * @deprecated gcp_network_info.pod_ip_range_name is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.75.0/docs/guides/gcp-workspace#creating-a-vpc
     */
    podIpRangeName?: string;
    /**
     * The name of the secondary IP range for services. A Databricks-managed GKE cluster uses this IP range for its services. This secondary IP range can only be used by one workspace.
     *
     * @deprecated gcp_network_info.service_ip_range_name is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.75.0/docs/guides/gcp-workspace#creating-a-vpc
     */
    serviceIpRangeName?: string;
    /**
     * The ID of the subnet associated with this network.
     */
    subnetId: string;
    /**
     * The Google Cloud region of the workspace data plane. For example, `us-east4`.
     */
    subnetRegion: string;
    /**
     * The ID of the VPC associated with this network. VPC IDs can be used in multiple network configurations.
     */
    vpcId: string;
}

export interface MwsNetworksVpcEndpoints {
    dataplaneRelays: string[];
    restApis: string[];
}

export interface MwsVpcEndpointGcpVpcEndpointInfo {
    /**
     * Region of the PSC endpoint.
     */
    endpointRegion: string;
    /**
     * The Google Cloud project ID of the VPC network where the PSC connection resides.
     */
    projectId: string;
    /**
     * The unique ID of this PSC connection.
     */
    pscConnectionId: string;
    /**
     * The name of the PSC endpoint in the Google Cloud project.
     */
    pscEndpointName: string;
    /**
     * The service attachment this PSC connection connects to.
     */
    serviceAttachmentId: string;
}

export interface MwsWorkspacesCloudResourceContainer {
    /**
     * A block that consists of the following field:
     */
    gcp: outputs.MwsWorkspacesCloudResourceContainerGcp;
}

export interface MwsWorkspacesCloudResourceContainerGcp {
    /**
     * The Google Cloud project ID, which the workspace uses to instantiate cloud resources for your workspace.
     */
    projectId: string;
}

export interface MwsWorkspacesExternalCustomerInfo {
    authoritativeUserEmail: string;
    authoritativeUserFullName: string;
    customerName: string;
}

export interface MwsWorkspacesGcpManagedNetworkConfig {
    /**
     * @deprecated gcp_managed_network_config.gke_cluster_pod_ip_range is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.75.0/docs/guides/gcp-workspace#creating-a-databricks-workspace
     */
    gkeClusterPodIpRange?: string;
    /**
     * @deprecated gcp_managed_network_config.gke_cluster_service_ip_range is deprecated and will be removed in a future release. For more information, review the documentation at https://registry.terraform.io/providers/databricks/databricks/1.75.0/docs/guides/gcp-workspace#creating-a-databricks-workspace
     */
    gkeClusterServiceIpRange?: string;
    subnetCidr: string;
}

export interface MwsWorkspacesGkeConfig {
    /**
     * Specifies the network connectivity types for the GKE nodes and the GKE master network. Possible values are: `PRIVATE_NODE_PUBLIC_MASTER`, `PUBLIC_NODE_PUBLIC_MASTER`.
     */
    connectivityType?: string;
    /**
     * The IP range from which to allocate GKE cluster master resources. This field will be ignored if GKE private cluster is not enabled. It must be exactly as big as `/28`.
     */
    masterIpRange?: string;
}

export interface MwsWorkspacesToken {
    /**
     * Comment, that will appear in "User Settings / Access Tokens" page on Workspace UI. By default it's "Pulumi PAT".
     */
    comment?: string;
    /**
     * Token expiry lifetime. By default its 2592000 (30 days).
     */
    lifetimeSeconds?: number;
    tokenId: string;
    tokenValue: string;
}

export interface NotificationDestinationConfig {
    /**
     * The email configuration of the Notification Destination. It must contain the following:
     */
    email?: outputs.NotificationDestinationConfigEmail;
    /**
     * The Generic Webhook configuration of the Notification Destination. It must contain the following:
     */
    genericWebhook?: outputs.NotificationDestinationConfigGenericWebhook;
    /**
     * The Microsoft Teams configuration of the Notification Destination. It must contain the following:
     */
    microsoftTeams?: outputs.NotificationDestinationConfigMicrosoftTeams;
    /**
     * The PagerDuty configuration of the Notification Destination. It must contain the following:
     */
    pagerduty?: outputs.NotificationDestinationConfigPagerduty;
    /**
     * The Slack configuration of the Notification Destination. It must contain the following:
     */
    slack?: outputs.NotificationDestinationConfigSlack;
}

export interface NotificationDestinationConfigEmail {
    /**
     * The list of email addresses to send notifications to.
     */
    addresses?: string[];
}

export interface NotificationDestinationConfigGenericWebhook {
    /**
     * The password for basic authentication.
     *
     * > **NOTE** If the type of notification destination is changed, the existing notification destination will be deleted and a new notification destination will be created with the new type.
     */
    password?: string;
    passwordSet: boolean;
    /**
     * The Generic Webhook URL.
     */
    url?: string;
    urlSet: boolean;
    /**
     * The username for basic authentication.
     */
    username?: string;
    usernameSet: boolean;
}

export interface NotificationDestinationConfigMicrosoftTeams {
    /**
     * The Microsoft Teams webhook URL.
     */
    url?: string;
    urlSet: boolean;
}

export interface NotificationDestinationConfigPagerduty {
    /**
     * The PagerDuty integration key.
     */
    integrationKey?: string;
    integrationKeySet: boolean;
}

export interface NotificationDestinationConfigSlack {
    /**
     * The Slack webhook URL.
     */
    url?: string;
    urlSet: boolean;
}

export interface OnlineTableSpec {
    /**
     * Whether to create a full-copy pipeline -- a pipeline that stops after creates a full copy of the source table upon initialization and does not process any change data feeds (CDFs) afterwards. The pipeline can still be manually triggered afterwards, but it always perform a full copy of the source table and there are no incremental updates. This mode is useful for syncing views or tables without CDFs to online tables. Note that the full-copy pipeline only supports "triggered" scheduling policy.
     */
    performFullCopy?: boolean;
    /**
     * ID of the associated Delta Live Table pipeline.
     */
    pipelineId: string;
    /**
     * list of the columns comprising the primary key.
     */
    primaryKeyColumns?: string[];
    /**
     * empty block that specifies that pipeline runs continuously after generating the initial data.  Conflicts with `runTriggered`.
     */
    runContinuously?: outputs.OnlineTableSpecRunContinuously;
    /**
     * empty block that specifies that pipeline stops after generating the initial data and can be triggered later (manually, through a cron job or through data triggers).
     */
    runTriggered?: outputs.OnlineTableSpecRunTriggered;
    /**
     * full name of the source table.
     */
    sourceTableFullName?: string;
    /**
     * Time series key to deduplicate (tie-break) rows with the same primary key.
     */
    timeseriesKey?: string;
}

export interface OnlineTableSpecRunContinuously {
}

export interface OnlineTableSpecRunTriggered {
}

export interface OnlineTableStatus {
    continuousUpdateStatus?: outputs.OnlineTableStatusContinuousUpdateStatus;
    /**
     * The state of the online table.
     */
    detailedState?: string;
    failedStatus?: outputs.OnlineTableStatusFailedStatus;
    /**
     * A text description of the current state of the online table.
     */
    message?: string;
    provisioningStatus?: outputs.OnlineTableStatusProvisioningStatus;
    triggeredUpdateStatus?: outputs.OnlineTableStatusTriggeredUpdateStatus;
}

export interface OnlineTableStatusContinuousUpdateStatus {
    initialPipelineSyncProgress?: outputs.OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress;
    lastProcessedCommitVersion?: number;
    timestamp?: string;
}

export interface OnlineTableStatusContinuousUpdateStatusInitialPipelineSyncProgress {
    estimatedCompletionTimeSeconds?: number;
    latestVersionCurrentlyProcessing?: number;
    syncProgressCompletion?: number;
    syncedRowCount?: number;
    totalRowCount?: number;
}

export interface OnlineTableStatusFailedStatus {
    lastProcessedCommitVersion?: number;
    timestamp?: string;
}

export interface OnlineTableStatusProvisioningStatus {
    initialPipelineSyncProgress?: outputs.OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress;
}

export interface OnlineTableStatusProvisioningStatusInitialPipelineSyncProgress {
    estimatedCompletionTimeSeconds?: number;
    latestVersionCurrentlyProcessing?: number;
    syncProgressCompletion?: number;
    syncedRowCount?: number;
    totalRowCount?: number;
}

export interface OnlineTableStatusTriggeredUpdateStatus {
    lastProcessedCommitVersion?: number;
    timestamp?: string;
    triggeredUpdateProgress?: outputs.OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress;
}

export interface OnlineTableStatusTriggeredUpdateStatusTriggeredUpdateProgress {
    estimatedCompletionTimeSeconds?: number;
    latestVersionCurrentlyProcessing?: number;
    syncProgressCompletion?: number;
    syncedRowCount?: number;
    totalRowCount?: number;
}

export interface PermissionsAccessControl {
    /**
     * name of the group. We recommend setting permissions on groups.
     */
    groupName?: string;
    /**
     * permission level according to specific resource. See examples above for the reference.
     *
     * Exactly one of the below arguments is required:
     */
    permissionLevel?: string;
    /**
     * Application ID of the service_principal.
     */
    servicePrincipalName?: string;
    /**
     * name of the user.
     */
    userName?: string;
}

export interface PipelineCluster {
    applyPolicyDefaultValues?: boolean;
    autoscale?: outputs.PipelineClusterAutoscale;
    awsAttributes?: outputs.PipelineClusterAwsAttributes;
    azureAttributes?: outputs.PipelineClusterAzureAttributes;
    clusterLogConf?: outputs.PipelineClusterClusterLogConf;
    customTags?: {[key: string]: string};
    driverInstancePoolId?: string;
    driverNodeTypeId: string;
    enableLocalDiskEncryption: boolean;
    gcpAttributes?: outputs.PipelineClusterGcpAttributes;
    initScripts?: outputs.PipelineClusterInitScript[];
    instancePoolId?: string;
    label?: string;
    nodeTypeId: string;
    numWorkers?: number;
    policyId?: string;
    sparkConf?: {[key: string]: string};
    sparkEnvVars?: {[key: string]: string};
    sshPublicKeys?: string[];
}

export interface PipelineClusterAutoscale {
    maxWorkers: number;
    minWorkers: number;
    mode?: string;
}

export interface PipelineClusterAwsAttributes {
    availability?: string;
    ebsVolumeCount?: number;
    ebsVolumeIops?: number;
    ebsVolumeSize?: number;
    ebsVolumeThroughput?: number;
    ebsVolumeType?: string;
    firstOnDemand?: number;
    instanceProfileArn?: string;
    spotBidPricePercent?: number;
    zoneId?: string;
}

export interface PipelineClusterAzureAttributes {
    availability?: string;
    firstOnDemand?: number;
    logAnalyticsInfo?: outputs.PipelineClusterAzureAttributesLogAnalyticsInfo;
    spotBidMaxPrice?: number;
}

export interface PipelineClusterAzureAttributesLogAnalyticsInfo {
    logAnalyticsPrimaryKey?: string;
    logAnalyticsWorkspaceId?: string;
}

export interface PipelineClusterClusterLogConf {
    dbfs?: outputs.PipelineClusterClusterLogConfDbfs;
    s3?: outputs.PipelineClusterClusterLogConfS3;
    volumes?: outputs.PipelineClusterClusterLogConfVolumes;
}

export interface PipelineClusterClusterLogConfDbfs {
    destination: string;
}

export interface PipelineClusterClusterLogConfS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface PipelineClusterClusterLogConfVolumes {
    destination: string;
}

export interface PipelineClusterGcpAttributes {
    availability?: string;
    googleServiceAccount?: string;
    localSsdCount?: number;
    zoneId?: string;
}

export interface PipelineClusterInitScript {
    abfss?: outputs.PipelineClusterInitScriptAbfss;
    /**
     * @deprecated For init scripts use 'volumes', 'workspace' or cloud storage location instead of 'dbfs'.
     */
    dbfs?: outputs.PipelineClusterInitScriptDbfs;
    file?: outputs.PipelineClusterInitScriptFile;
    gcs?: outputs.PipelineClusterInitScriptGcs;
    s3?: outputs.PipelineClusterInitScriptS3;
    volumes?: outputs.PipelineClusterInitScriptVolumes;
    workspace?: outputs.PipelineClusterInitScriptWorkspace;
}

export interface PipelineClusterInitScriptAbfss {
    destination: string;
}

export interface PipelineClusterInitScriptDbfs {
    destination: string;
}

export interface PipelineClusterInitScriptFile {
    destination: string;
}

export interface PipelineClusterInitScriptGcs {
    destination: string;
}

export interface PipelineClusterInitScriptS3 {
    cannedAcl?: string;
    destination: string;
    enableEncryption?: boolean;
    encryptionType?: string;
    endpoint?: string;
    kmsKey?: string;
    region?: string;
}

export interface PipelineClusterInitScriptVolumes {
    destination: string;
}

export interface PipelineClusterInitScriptWorkspace {
    destination: string;
}

export interface PipelineDeployment {
    /**
     * The deployment method that manages the pipeline.
     */
    kind?: string;
    /**
     * The path to the file containing metadata about the deployment.
     */
    metadataFilePath?: string;
}

export interface PipelineEventLog {
    /**
     * The UC catalog the event log is published under.
     */
    catalog: string;
    /**
     * The table name the event log is published to in UC.
     */
    name: string;
    /**
     * The UC schema the event log is published under.
     */
    schema: string;
}

export interface PipelineFilters {
    /**
     * Paths to exclude.
     */
    excludes?: string[];
    /**
     * Paths to include.
     */
    includes?: string[];
}

export interface PipelineGatewayDefinition {
    /**
     * Immutable. The Unity Catalog connection this gateway pipeline uses to communicate with the source.
     */
    connectionId?: string;
    connectionName?: string;
    /**
     * Required, Immutable. The name of the catalog for the gateway pipeline's storage location.
     */
    gatewayStorageCatalog?: string;
    /**
     * Required. The Unity Catalog-compatible naming for the gateway storage location. This is the destination to use for the data that is extracted by the gateway. Delta Live Tables system will automatically create the storage location under the catalog and schema.
     */
    gatewayStorageName?: string;
    /**
     * Required, Immutable. The name of the schema for the gateway pipelines's storage location.
     */
    gatewayStorageSchema?: string;
}

export interface PipelineIngestionDefinition {
    connectionName?: string;
    ingestionGatewayId?: string;
    objects?: outputs.PipelineIngestionDefinitionObject[];
    tableConfiguration?: outputs.PipelineIngestionDefinitionTableConfiguration;
}

export interface PipelineIngestionDefinitionObject {
    report?: outputs.PipelineIngestionDefinitionObjectReport;
    /**
     * The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
     */
    schema?: outputs.PipelineIngestionDefinitionObjectSchema;
    table?: outputs.PipelineIngestionDefinitionObjectTable;
}

export interface PipelineIngestionDefinitionObjectReport {
    destinationCatalog?: string;
    destinationSchema?: string;
    destinationTable?: string;
    sourceUrl?: string;
    tableConfiguration?: outputs.PipelineIngestionDefinitionObjectReportTableConfiguration;
}

export interface PipelineIngestionDefinitionObjectReportTableConfiguration {
    primaryKeys?: string[];
    salesforceIncludeFormulaFields?: boolean;
    scdType?: string;
    sequenceBies?: string[];
}

export interface PipelineIngestionDefinitionObjectSchema {
    destinationCatalog?: string;
    destinationSchema?: string;
    sourceCatalog?: string;
    sourceSchema?: string;
    tableConfiguration?: outputs.PipelineIngestionDefinitionObjectSchemaTableConfiguration;
}

export interface PipelineIngestionDefinitionObjectSchemaTableConfiguration {
    primaryKeys?: string[];
    salesforceIncludeFormulaFields?: boolean;
    scdType?: string;
    sequenceBies?: string[];
}

export interface PipelineIngestionDefinitionObjectTable {
    destinationCatalog?: string;
    destinationSchema?: string;
    destinationTable?: string;
    sourceCatalog?: string;
    sourceSchema?: string;
    sourceTable?: string;
    tableConfiguration?: outputs.PipelineIngestionDefinitionObjectTableTableConfiguration;
}

export interface PipelineIngestionDefinitionObjectTableTableConfiguration {
    primaryKeys?: string[];
    salesforceIncludeFormulaFields?: boolean;
    scdType?: string;
    sequenceBies?: string[];
}

export interface PipelineIngestionDefinitionTableConfiguration {
    primaryKeys?: string[];
    salesforceIncludeFormulaFields?: boolean;
    scdType?: string;
    sequenceBies?: string[];
}

export interface PipelineLatestUpdate {
    creationTime?: string;
    state?: string;
    updateId?: string;
}

export interface PipelineLibrary {
    file?: outputs.PipelineLibraryFile;
    jar?: string;
    maven?: outputs.PipelineLibraryMaven;
    notebook?: outputs.PipelineLibraryNotebook;
    /**
     * @deprecated The 'whl' field is deprecated
     */
    whl?: string;
}

export interface PipelineLibraryFile {
    path?: string;
}

export interface PipelineLibraryMaven {
    coordinates: string;
    exclusions?: string[];
    repo?: string;
}

export interface PipelineLibraryNotebook {
    path?: string;
}

export interface PipelineNotification {
    /**
     * non-empty list of alert types. Right now following alert types are supported, consult documentation for actual list
     * * `on-update-success` - a pipeline update completes successfully.
     * * `on-update-failure` - a pipeline update fails with a retryable error.
     * * `on-update-fatal-failure` - a pipeline update fails with a non-retryable (fatal) error.
     * * `on-flow-failure` - a single data flow fails.
     */
    alerts?: string[];
    /**
     * non-empty list of emails to notify.
     */
    emailRecipients?: string[];
}

export interface PipelineRestartWindow {
    daysOfWeeks?: string[];
    startHour: number;
    timeZoneId?: string;
}

export interface PipelineRunAs {
    servicePrincipalName?: string;
    userName?: string;
}

export interface PipelineTrigger {
    cron?: outputs.PipelineTriggerCron;
    manual?: outputs.PipelineTriggerManual;
}

export interface PipelineTriggerCron {
    quartzCronSchedule?: string;
    timezoneId?: string;
}

export interface PipelineTriggerManual {
}

export interface QualityMonitorCustomMetric {
    /**
     * [create metric definition](https://docs.databricks.com/en/lakehouse-monitoring/custom-metrics.html#create-definition)
     */
    definition: string;
    /**
     * Columns on the monitored table to apply the custom metrics to.
     */
    inputColumns: string[];
    /**
     * Name of the custom metric.
     */
    name: string;
    /**
     * The output type of the custom metric.
     */
    outputDataType: string;
    /**
     * The type of the custom metric.
     */
    type: string;
}

export interface QualityMonitorDataClassificationConfig {
    enabled?: boolean;
}

export interface QualityMonitorInferenceLog {
    /**
     * List of granularities to use when aggregating data into time windows based on their timestamp.
     */
    granularities: string[];
    /**
     * Column of the model label
     */
    labelCol?: string;
    /**
     * Column of the model id or version
     */
    modelIdCol: string;
    /**
     * Column of the model prediction
     */
    predictionCol: string;
    /**
     * Column of the model prediction probabilities
     */
    predictionProbaCol?: string;
    /**
     * Problem type the model aims to solve. Either `PROBLEM_TYPE_CLASSIFICATION` or `PROBLEM_TYPE_REGRESSION`
     */
    problemType: string;
    /**
     * Column of the timestamp of predictions
     */
    timestampCol: string;
}

export interface QualityMonitorNotifications {
    /**
     * who to send notifications to on monitor failure.
     */
    onFailure?: outputs.QualityMonitorNotificationsOnFailure;
    /**
     * Who to send notifications to when new data classification tags are detected.
     */
    onNewClassificationTagDetected?: outputs.QualityMonitorNotificationsOnNewClassificationTagDetected;
}

export interface QualityMonitorNotificationsOnFailure {
    emailAddresses?: string[];
}

export interface QualityMonitorNotificationsOnNewClassificationTagDetected {
    emailAddresses?: string[];
}

export interface QualityMonitorSchedule {
    pauseStatus: string;
    /**
     * string expression that determines when to run the monitor. See [Quartz documentation](https://www.quartz-scheduler.org/documentation/quartz-2.3.0/tutorials/crontrigger.html) for examples.
     */
    quartzCronExpression: string;
    /**
     * string with timezone id (e.g., `PST`) in which to evaluate the Quartz expression.
     */
    timezoneId: string;
}

export interface QualityMonitorSnapshot {
}

export interface QualityMonitorTimeSeries {
    /**
     * List of granularities to use when aggregating data into time windows based on their timestamp.
     */
    granularities: string[];
    /**
     * Column of the timestamp of predictions
     */
    timestampCol: string;
}

export interface QueryParameter {
    /**
     * Date-range query parameter value. Consists of following attributes (Can only specify one of `dynamicDateRangeValue` or `dateRangeValue`):
     */
    dateRangeValue?: outputs.QueryParameterDateRangeValue;
    /**
     * Date query parameter value. Consists of following attributes (Can only specify one of `dynamicDateValue` or `dateValue`):
     */
    dateValue?: outputs.QueryParameterDateValue;
    /**
     * Dropdown parameter value. Consists of following attributes:
     */
    enumValue?: outputs.QueryParameterEnumValue;
    /**
     * Literal parameter marker that appears between double curly braces in the query text.
     */
    name: string;
    /**
     * Numeric parameter value. Consists of following attributes:
     */
    numericValue?: outputs.QueryParameterNumericValue;
    /**
     * Query-based dropdown parameter value. Consists of following attributes:
     */
    queryBackedValue?: outputs.QueryParameterQueryBackedValue;
    /**
     * Text parameter value. Consists of following attributes:
     */
    textValue?: outputs.QueryParameterTextValue;
    /**
     * Text displayed in the user-facing parameter widget in the UI.
     */
    title?: string;
}

export interface QueryParameterDateRangeValue {
    /**
     * Manually specified date-time range value.  Consists of the following attributes:
     */
    dateRangeValue?: outputs.QueryParameterDateRangeValueDateRangeValue;
    /**
     * Dynamic date-time range value based on current date-time.  Possible values are `TODAY`, `YESTERDAY`, `THIS_WEEK`, `THIS_MONTH`, `THIS_YEAR`, `LAST_WEEK`, `LAST_MONTH`, `LAST_YEAR`, `LAST_HOUR`, `LAST_8_HOURS`, `LAST_24_HOURS`, `LAST_7_DAYS`, `LAST_14_DAYS`, `LAST_30_DAYS`, `LAST_60_DAYS`, `LAST_90_DAYS`, `LAST_12_MONTHS`.
     */
    dynamicDateRangeValue?: string;
    /**
     * Date-time precision to format the value into when the query is run.  Possible values are `DAY_PRECISION`, `MINUTE_PRECISION`, `SECOND_PRECISION`.  Defaults to `DAY_PRECISION` (`YYYY-MM-DD`).
     */
    precision?: string;
    /**
     * Specify what day that starts the week.
     */
    startDayOfWeek?: number;
}

export interface QueryParameterDateRangeValueDateRangeValue {
    /**
     * end of the date range.
     */
    end: string;
    /**
     * begin of the date range.
     */
    start: string;
}

export interface QueryParameterDateValue {
    /**
     * Manually specified date-time value
     */
    dateValue?: string;
    /**
     * Dynamic date-time value based on current date-time.  Possible values are `NOW`, `YESTERDAY`.
     */
    dynamicDateValue?: string;
    /**
     * Date-time precision to format the value into when the query is run.  Possible values are `DAY_PRECISION`, `MINUTE_PRECISION`, `SECOND_PRECISION`.  Defaults to `DAY_PRECISION` (`YYYY-MM-DD`).
     */
    precision?: string;
}

export interface QueryParameterEnumValue {
    /**
     * List of valid query parameter values, newline delimited.
     */
    enumOptions?: string;
    /**
     * If specified, allows multiple values to be selected for this parameter. Consists of following attributes:
     */
    multiValuesOptions?: outputs.QueryParameterEnumValueMultiValuesOptions;
    /**
     * List of selected query parameter values.
     */
    values?: string[];
}

export interface QueryParameterEnumValueMultiValuesOptions {
    /**
     * Character that prefixes each selected parameter value.
     */
    prefix?: string;
    /**
     * Character that separates each selected parameter value. Defaults to a comma.
     */
    separator?: string;
    /**
     * Character that suffixes each selected parameter value.
     */
    suffix?: string;
}

export interface QueryParameterNumericValue {
    /**
     * actual numeric value.
     */
    value: number;
}

export interface QueryParameterQueryBackedValue {
    /**
     * If specified, allows multiple values to be selected for this parameter. Consists of following attributes:
     */
    multiValuesOptions?: outputs.QueryParameterQueryBackedValueMultiValuesOptions;
    /**
     * ID of the query that provides the parameter values.
     */
    queryId: string;
    /**
     * List of selected query parameter values.
     */
    values?: string[];
}

export interface QueryParameterQueryBackedValueMultiValuesOptions {
    /**
     * Character that prefixes each selected parameter value.
     */
    prefix?: string;
    /**
     * Character that separates each selected parameter value. Defaults to a comma.
     */
    separator?: string;
    /**
     * Character that suffixes each selected parameter value.
     */
    suffix?: string;
}

export interface QueryParameterTextValue {
    /**
     * actual text value.
     */
    value: string;
}

export interface RecipientIpAccessList {
    /**
     * Allowed IP Addresses in CIDR notation. Limit of 100.
     */
    allowedIpAddresses?: string[];
}

export interface RecipientPropertiesKvpairs {
    /**
     * a map of string key-value pairs with recipient's properties.  Properties with name starting with `databricks.` are reserved.
     */
    properties: {[key: string]: string};
}

export interface RecipientToken {
    /**
     * Full activation URL to retrieve the access token. It will be empty if the token is already retrieved.
     */
    activationUrl: string;
    /**
     * Time at which this recipient was created, in epoch milliseconds.
     */
    createdAt: number;
    /**
     * Username of recipient creator.
     */
    createdBy: string;
    /**
     * Expiration timestamp of the token in epoch milliseconds.
     */
    expirationTime: number;
    /**
     * Unique ID of the recipient token.
     */
    id: string;
    /**
     * Time at which this recipient was updated, in epoch milliseconds.
     */
    updatedAt: number;
    /**
     * Username of recipient Token updater.
     */
    updatedBy: string;
}

export interface RepoSparseCheckout {
    /**
     * array of paths (directories) that will be used for sparse checkout.  List of patterns could be updated in-place.
     *
     * Addition or removal of the `sparseCheckout` configuration block will lead to recreation of the Git folder.
     */
    patterns: string[];
}

export interface RestrictWorkspaceAdminsSettingRestrictWorkspaceAdmins {
    /**
     * The restrict workspace admins status for the workspace.
     */
    status: string;
}

export interface SecretScopeKeyvaultMetadata {
    dnsName: string;
    resourceId: string;
}

export interface ShareObject {
    addedAt: number;
    addedBy: string;
    /**
     * Whether to enable Change Data Feed (cdf) on the shared object. When this field is set, field `historyDataSharingStatus` can not be set.
     */
    cdfEnabled?: boolean;
    /**
     * Description about the object.
     */
    comment?: string;
    content?: string;
    /**
     * Type of the data object, currently `TABLE`, `VIEW`, `SCHEMA`, `VOLUME`, and `MODEL` are supported.
     */
    dataObjectType: string;
    /**
     * Whether to enable history sharing, one of: `ENABLED`, `DISABLED`. When a table has history sharing enabled, recipients can query table data by version, starting from the current table version. If not specified, clients can only query starting from the version of the object at the time it was added to the share. *NOTE*: The startVersion should be less than or equal the current version of the object. When this field is set, field `cdfEnabled` can not be set.
     *
     * To share only part of a table when you add the table to a share, you can provide partition specifications. This is specified by a number of `partition` blocks. Each entry in `partition` block takes a list of `value` blocks. The field is documented below.
     */
    historyDataSharingStatus?: string;
    /**
     * Full name of the object, e.g. `catalog.schema.name` for a tables, views, volumes and models, or `catalog.schema` for schemas.
     */
    name: string;
    partitions?: outputs.ShareObjectPartition[];
    /**
     * A user-provided new name for the data object within the share. If this new name is not provided, the object's original name will be used as the `sharedAs` name. The `sharedAs` name must be unique within a Share. Change forces creation of a new resource.
     */
    sharedAs?: string;
    /**
     * The start version associated with the object for cdf. This allows data providers to control the lowest object version that is accessible by clients.
     */
    startVersion?: number;
    /**
     * Status of the object, one of: `ACTIVE`, `PERMISSION_DENIED`.
     */
    status: string;
    stringSharedAs?: string;
}

export interface ShareObjectPartition {
    /**
     * The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipientPropertyKey` can not be set.
     */
    values?: outputs.ShareObjectPartitionValue[];
}

export interface ShareObjectPartitionValue {
    /**
     * The name of the partition column.
     */
    name: string;
    /**
     * The operator to apply for the value, one of: `EQUAL`, `LIKE`
     */
    op: string;
    /**
     * The key of a Delta Sharing recipient's property. For example `databricks-account-id`. When this field is set, field `value` can not be set.
     */
    recipientPropertyKey?: string;
    /**
     * The value of the partition column. When this value is not set, it means null value. When this field is set, field `recipientPropertyKey` can not be set.
     */
    value?: string;
}

export interface SqlAlertOptions {
    /**
     * Name of column in the query result to compare in alert evaluation.
     */
    column: string;
    /**
     * Custom body of alert notification, if it exists. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.
     */
    customBody?: string;
    /**
     * Custom subject of alert notification, if it exists. This includes email subject, Slack notification header, etc. See [Alerts API reference](https://docs.databricks.com/sql/user/alerts/index.html) for custom templating instructions.
     */
    customSubject?: string;
    /**
     * State that alert evaluates to when query result is empty.  Currently supported values are `unknown`, `triggered`, `ok` - check [API documentation](https://docs.databricks.com/api/workspace/alerts/create) for full list of supported values.
     */
    emptyResultState?: string;
    /**
     * Whether or not the alert is muted. If an alert is muted, it will not notify users and alert destinations when triggered.
     */
    muted?: boolean;
    /**
     * Operator used to compare in alert evaluation. (Enum: `>`, `>=`, `<`, `<=`, `==`, `!=`)
     */
    op: string;
    /**
     * Value used to compare in alert evaluation.
     */
    value: string;
}

export interface SqlEndpointChannel {
    dbsqlVersion?: string;
    /**
     * Name of the Databricks SQL release channel. Possible values are: `CHANNEL_NAME_PREVIEW` and `CHANNEL_NAME_CURRENT`. Default is `CHANNEL_NAME_CURRENT`.
     */
    name?: string;
}

export interface SqlEndpointHealth {
    details?: string;
    failureReason?: outputs.SqlEndpointHealthFailureReason;
    message?: string;
    status?: string;
    summary?: string;
}

export interface SqlEndpointHealthFailureReason {
    code?: string;
    parameters?: {[key: string]: string};
    type?: string;
}

export interface SqlEndpointOdbcParams {
    hostname?: string;
    path?: string;
    port?: number;
    protocol?: string;
}

export interface SqlEndpointTags {
    customTags?: outputs.SqlEndpointTagsCustomTag[];
}

export interface SqlEndpointTagsCustomTag {
    key: string;
    value: string;
}

export interface SqlPermissionsPrivilegeAssignment {
    /**
     * `displayName` for a databricks.Group or databricks_user, `applicationId` for a databricks_service_principal.
     */
    principal: string;
    privileges: string[];
}

export interface SqlQueryParameter {
    date?: outputs.SqlQueryParameterDate;
    dateRange?: outputs.SqlQueryParameterDateRange;
    datetime?: outputs.SqlQueryParameterDatetime;
    datetimeRange?: outputs.SqlQueryParameterDatetimeRange;
    datetimesec?: outputs.SqlQueryParameterDatetimesec;
    datetimesecRange?: outputs.SqlQueryParameterDatetimesecRange;
    enum?: outputs.SqlQueryParameterEnum;
    /**
     * The literal parameter marker that appears between double curly braces in the query text.
     * Parameters can have several different types. Type is specified using one of the following configuration blocks: `text`, `number`, `enum`, `query`, `date`, `datetime`, `datetimesec`, `dateRange`, `datetimeRange`, `datetimesecRange`.
     *
     * For `text`, `number`, `date`, `datetime`, `datetimesec` block
     */
    name: string;
    number?: outputs.SqlQueryParameterNumber;
    /**
     * The text of the query to be run.
     */
    query?: outputs.SqlQueryParameterQuery;
    text?: outputs.SqlQueryParameterText;
    /**
     * The text displayed in a parameter picking widget.
     */
    title?: string;
}

export interface SqlQueryParameterDate {
    /**
     * The default value for this parameter.
     */
    value: string;
}

export interface SqlQueryParameterDateRange {
    range?: outputs.SqlQueryParameterDateRangeRange;
    /**
     * The default value for this parameter.
     */
    value?: string;
}

export interface SqlQueryParameterDateRangeRange {
    end: string;
    start: string;
}

export interface SqlQueryParameterDatetime {
    /**
     * The default value for this parameter.
     */
    value: string;
}

export interface SqlQueryParameterDatetimeRange {
    range?: outputs.SqlQueryParameterDatetimeRangeRange;
    /**
     * The default value for this parameter.
     */
    value?: string;
}

export interface SqlQueryParameterDatetimeRangeRange {
    end: string;
    start: string;
}

export interface SqlQueryParameterDatetimesec {
    /**
     * The default value for this parameter.
     */
    value: string;
}

export interface SqlQueryParameterDatetimesecRange {
    range?: outputs.SqlQueryParameterDatetimesecRangeRange;
    /**
     * The default value for this parameter.
     */
    value?: string;
}

export interface SqlQueryParameterDatetimesecRangeRange {
    end: string;
    start: string;
}

export interface SqlQueryParameterEnum {
    multiple?: outputs.SqlQueryParameterEnumMultiple;
    options: string[];
    /**
     * The default value for this parameter.
     */
    value?: string;
    values?: string[];
}

export interface SqlQueryParameterEnumMultiple {
    prefix?: string;
    separator: string;
    suffix?: string;
}

export interface SqlQueryParameterNumber {
    /**
     * The default value for this parameter.
     */
    value: number;
}

export interface SqlQueryParameterQuery {
    multiple?: outputs.SqlQueryParameterQueryMultiple;
    queryId: string;
    /**
     * The default value for this parameter.
     */
    value?: string;
    values?: string[];
}

export interface SqlQueryParameterQueryMultiple {
    prefix?: string;
    separator: string;
    suffix?: string;
}

export interface SqlQueryParameterText {
    /**
     * The default value for this parameter.
     */
    value: string;
}

export interface SqlQuerySchedule {
    continuous?: outputs.SqlQueryScheduleContinuous;
    daily?: outputs.SqlQueryScheduleDaily;
    weekly?: outputs.SqlQueryScheduleWeekly;
}

export interface SqlQueryScheduleContinuous {
    intervalSeconds: number;
    untilDate?: string;
}

export interface SqlQueryScheduleDaily {
    intervalDays: number;
    timeOfDay: string;
    untilDate?: string;
}

export interface SqlQueryScheduleWeekly {
    dayOfWeek: string;
    intervalWeeks: number;
    timeOfDay: string;
    untilDate?: string;
}

export interface SqlTableColumn {
    /**
     * User-supplied free-form text.
     */
    comment?: string;
    /**
     * Whether the field is an identity column. Can be `default`, `always`, or unset. It is unset by default.
     */
    identity?: string;
    /**
     * User-visible name of column
     */
    name: string;
    /**
     * Whether field is nullable (Default: `true`)
     */
    nullable?: boolean;
    /**
     * Column type spec (with metadata) as SQL text. Not supported for `VIEW` table_type.
     */
    type: string;
    typeJson: string;
}

export interface SqlWidgetParameter {
    mapTo?: string;
    name: string;
    title?: string;
    type: string;
    value?: string;
    values?: string[];
}

export interface SqlWidgetPosition {
    autoHeight?: boolean;
    posX?: number;
    posY?: number;
    sizeX: number;
    sizeY: number;
}

export interface StorageCredentialAwsIamRole {
    externalId: string;
    /**
     * The Amazon Resource Name (ARN) of the AWS IAM role for S3 data access, of the form `arn:aws:iam::1234567890:role/MyRole-AJJHDSKSDF`
     *
     * `azureManagedIdentity` optional configuration block for using managed identity as credential details for Azure (recommended over service principal):
     */
    roleArn: string;
    unityCatalogIamArn: string;
}

export interface StorageCredentialAzureManagedIdentity {
    /**
     * The Resource ID of the Azure Databricks Access Connector resource, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.Databricks/accessConnectors/connector-name`.
     */
    accessConnectorId: string;
    credentialId: string;
    /**
     * The Resource ID of the Azure User Assigned Managed Identity associated with Azure Databricks Access Connector, of the form `/subscriptions/00000000-0000-0000-0000-000000000000/resourceGroups/rg-name/providers/Microsoft.ManagedIdentity/userAssignedIdentities/user-managed-identity-name`.
     *
     * `databricksGcpServiceAccount` optional configuration block for creating a Databricks-managed GCP Service Account:
     */
    managedIdentityId?: string;
}

export interface StorageCredentialAzureServicePrincipal {
    /**
     * The application ID of the application registration within the referenced AAD tenant
     */
    applicationId: string;
    /**
     * The client secret generated for the above app ID in AAD. **This field is redacted on output**
     */
    clientSecret: string;
    /**
     * The directory ID corresponding to the Azure Active Directory (AAD) tenant of the application
     */
    directoryId: string;
}

export interface StorageCredentialCloudflareApiToken {
    /**
     * R2 API token access key ID
     */
    accessKeyId: string;
    /**
     * R2 account ID
     */
    accountId: string;
    /**
     * R2 API token secret access key
     *
     * `azureServicePrincipal` optional configuration block to use service principal as credential details for Azure (Legacy):
     */
    secretAccessKey: string;
}

export interface StorageCredentialDatabricksGcpServiceAccount {
    credentialId: string;
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     *
     * `cloudflareApiToken` optional configuration block for using a Cloudflare API Token as credential details. This requires account admin access:
     */
    email: string;
}

export interface StorageCredentialGcpServiceAccountKey {
    /**
     * The email of the GCP service account created, to be granted access to relevant buckets.
     *
     * `cloudflareApiToken` optional configuration block for using a Cloudflare API Token as credential details. This requires account admin access:
     */
    email: string;
    privateKey: string;
    privateKeyId: string;
}

export interface TableColumn {
    comment?: string;
    name: string;
    nullable?: boolean;
    partitionIndex?: number;
    position: number;
    typeIntervalType?: string;
    typeJson?: string;
    typeName: string;
    typePrecision?: number;
    typeScale?: number;
    typeText: string;
}

export interface VectorSearchEndpointEndpointStatus {
    /**
     * Additional status message.
     */
    message?: string;
    /**
     * Current state of the endpoint. Currently following values are supported: `PROVISIONING`, `ONLINE`, and `OFFLINE`.
     */
    state?: string;
}

export interface VectorSearchIndexDeltaSyncIndexSpec {
    /**
     * array of objects representing columns that contain the embedding source.  Each entry consists of:
     */
    embeddingSourceColumns?: outputs.VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn[];
    /**
     * array of objects representing columns that contain the embedding vectors. Each entry consists of:
     */
    embeddingVectorColumns?: outputs.VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn[];
    /**
     * Automatically sync the vector index contents and computed embeddings to the specified Delta table. The only supported table name is the index name with the suffix `_writeback_table`.
     */
    embeddingWritebackTable?: string;
    /**
     * ID of the associated Delta Live Table pipeline.
     */
    pipelineId: string;
    /**
     * Pipeline execution mode. Possible values are:
     * * `TRIGGERED`: If the pipeline uses the triggered execution mode, the system stops processing after successfully refreshing the source table in the pipeline once, ensuring the table is updated based on the data available when the update started.
     * * `CONTINUOUS`: If the pipeline uses continuous execution, the pipeline processes new data as it arrives in the source table to keep the vector index fresh.
     */
    pipelineType?: string;
    /**
     * The name of the source table.
     */
    sourceTable?: string;
}

export interface VectorSearchIndexDeltaSyncIndexSpecEmbeddingSourceColumn {
    /**
     * The name of the embedding model endpoint
     */
    embeddingModelEndpointName?: string;
    /**
     * The name of the column
     */
    name?: string;
}

export interface VectorSearchIndexDeltaSyncIndexSpecEmbeddingVectorColumn {
    /**
     * Dimension of the embedding vector.
     */
    embeddingDimension?: number;
    /**
     * The name of the column.
     */
    name?: string;
}

export interface VectorSearchIndexDirectAccessIndexSpec {
    /**
     * array of objects representing columns that contain the embedding source.  Each entry consists of:
     */
    embeddingSourceColumns?: outputs.VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn[];
    /**
     * array of objects representing columns that contain the embedding vectors. Each entry consists of:
     */
    embeddingVectorColumns?: outputs.VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn[];
    /**
     * The schema of the index in JSON format.  Check the [API documentation](https://docs.databricks.com/api/workspace/vectorsearchindexes/createindex#direct_access_index_spec-schema_json) for a list of supported data types.
     */
    schemaJson?: string;
}

export interface VectorSearchIndexDirectAccessIndexSpecEmbeddingSourceColumn {
    /**
     * The name of the embedding model endpoint
     */
    embeddingModelEndpointName?: string;
    /**
     * The name of the column
     */
    name?: string;
}

export interface VectorSearchIndexDirectAccessIndexSpecEmbeddingVectorColumn {
    /**
     * Dimension of the embedding vector.
     */
    embeddingDimension?: number;
    /**
     * The name of the column.
     */
    name?: string;
}

export interface VectorSearchIndexStatus {
    /**
     * Index API Url to be used to perform operations on the index
     */
    indexUrl?: string;
    /**
     * Number of rows indexed
     */
    indexedRowCount?: number;
    /**
     * Message associated with the index status
     */
    message?: string;
    /**
     * Whether the index is ready for search
     */
    ready?: boolean;
}

