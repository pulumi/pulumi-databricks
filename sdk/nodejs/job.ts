// *** WARNING: this file was generated by pulumi-language-nodejs. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as inputs from "./types/input";
import * as outputs from "./types/output";
import * as utilities from "./utilities";

/**
 * The `databricks.Job` resource allows you to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
 *
 * > This resource can only be used with a workspace-level provider!
 *
 * ## Example Usage
 *
 * > In Pulumi configuration, it is recommended to define tasks in alphabetical order of their `taskKey` arguments, so that you get consistent and readable diff. Whenever tasks are added or removed, or `taskKey` is renamed, you'll observe a change in the majority of tasks. It's related to the fact that the current version of the provider treats `task` blocks as an ordered list. Alternatively, `task` block could have been an unordered set, though end-users would see the entire block replaced upon a change in single property of the task.
 *
 * It is possible to create [a Databricks job](https://docs.databricks.com/aws/en/jobs/) using `task` blocks. A single task is defined with the `task` block containing one of the `*_task` blocks, `taskKey`, and additional arguments described below.
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const _this = new databricks.Job("this", {
 *     name: "Job with multiple tasks",
 *     description: "This job executes multiple tasks on a shared job cluster, which will be provisioned as part of execution, and terminated once all tasks are finished.",
 *     jobClusters: [{
 *         jobClusterKey: "j",
 *         newCluster: {
 *             numWorkers: 2,
 *             sparkVersion: latest.id,
 *             nodeTypeId: smallest.id,
 *         },
 *     }],
 *     tasks: [
 *         {
 *             taskKey: "a",
 *             newCluster: {
 *                 numWorkers: 1,
 *                 sparkVersion: latest.id,
 *                 nodeTypeId: smallest.id,
 *             },
 *             notebookTask: {
 *                 notebookPath: thisDatabricksNotebook.path,
 *             },
 *         },
 *         {
 *             taskKey: "b",
 *             dependsOns: [{
 *                 taskKey: "a",
 *             }],
 *             existingClusterId: shared.id,
 *             sparkJarTask: {
 *                 mainClassName: "com.acme.data.Main",
 *             },
 *         },
 *         {
 *             taskKey: "c",
 *             jobClusterKey: "j",
 *             notebookTask: {
 *                 notebookPath: thisDatabricksNotebook.path,
 *             },
 *         },
 *         {
 *             taskKey: "d",
 *             pipelineTask: {
 *                 pipelineId: thisDatabricksPipeline.id,
 *             },
 *         },
 *     ],
 * });
 * ```
 *
 * ## Access Control
 *
 * By default, all users can create and modify jobs unless an administrator [enables jobs access control](https://docs.databricks.com/administration-guide/access-control/jobs-acl.html). With jobs access control, individual permissions determine a user's abilities.
 *
 * * databricks.Permissions can control which groups or individual users can *Can View*, *Can Manage Run*, and *Can Manage*.
 * * databricks.ClusterPolicy can control which kinds of clusters users can create for jobs.
 *
 * ## Import
 *
 * The resource job can be imported using the id of the job:
 *
 * hcl
 *
 * import {
 *
 *   to = databricks_job.this
 *
 *   id = "<job-id>"
 *
 * }
 *
 * Alternatively, when using `terraform` version 1.4 or earlier, import using the `pulumi import` command:
 *
 * bash
 *
 * ```sh
 * $ pulumi import databricks:index/job:Job this <job-id>
 * ```
 */
export class Job extends pulumi.CustomResource {
    /**
     * Get an existing Job resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state Any extra arguments used during the lookup.
     * @param opts Optional settings to control the behavior of the CustomResource.
     */
    public static get(name: string, id: pulumi.Input<pulumi.ID>, state?: JobState, opts?: pulumi.CustomResourceOptions): Job {
        return new Job(name, <any>state, { ...opts, id: id });
    }

    /** @internal */
    public static readonly __pulumiType = 'databricks:index/job:Job';

    /**
     * Returns true if the given object is an instance of Job.  This is designed to work even
     * when multiple copies of the Pulumi SDK have been loaded into the same process.
     */
    public static isInstance(obj: any): obj is Job {
        if (obj === undefined || obj === null) {
            return false;
        }
        return obj['__pulumiType'] === Job.__pulumiType;
    }

    /**
     * (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `sparkJarTask` or `sparkSubmitTask` or `sparkPythonTask` or `notebookTask` blocks.
     *
     * @deprecated always_running will be replaced by controlRunState in the next major release.
     */
    declare public readonly alwaysRunning: pulumi.Output<boolean | undefined>;
    /**
     * The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.
     */
    declare public readonly budgetPolicyId: pulumi.Output<string | undefined>;
    /**
     * Configuration block to configure pause status. See continuous Configuration Block.
     */
    declare public readonly continuous: pulumi.Output<outputs.JobContinuous | undefined>;
    /**
     * (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pauseStatus` by stopping the current active run. This flag cannot be set for non-continuous jobs.
     *
     * When migrating from `alwaysRunning` to `controlRunState`, set `continuous` as follows:
     */
    declare public readonly controlRunState: pulumi.Output<boolean | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly dbtTask: pulumi.Output<outputs.JobDbtTask | undefined>;
    declare public readonly deployment: pulumi.Output<outputs.JobDeployment | undefined>;
    /**
     * An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.
     */
    declare public readonly description: pulumi.Output<string | undefined>;
    /**
     * If `"UI_LOCKED"`, the user interface for the job will be locked. If `"EDITABLE"` (the default), the user interface will be editable.
     */
    declare public readonly editMode: pulumi.Output<string | undefined>;
    /**
     * (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.
     */
    declare public readonly emailNotifications: pulumi.Output<outputs.JobEmailNotifications | undefined>;
    declare public readonly environments: pulumi.Output<outputs.JobEnvironment[] | undefined>;
    declare public readonly existingClusterId: pulumi.Output<string | undefined>;
    declare public readonly format: pulumi.Output<string>;
    /**
     * Specifies the a Git repository for task source code. See gitSource Configuration Block below.
     */
    declare public readonly gitSource: pulumi.Output<outputs.JobGitSource | undefined>;
    /**
     * An optional block that specifies the health conditions for the job documented below.
     */
    declare public readonly health: pulumi.Output<outputs.JobHealth | undefined>;
    /**
     * A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
     */
    declare public readonly jobClusters: pulumi.Output<outputs.JobJobCluster[] | undefined>;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    declare public readonly libraries: pulumi.Output<outputs.JobLibrary[] | undefined>;
    /**
     * (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
     */
    declare public readonly maxConcurrentRuns: pulumi.Output<number | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly maxRetries: pulumi.Output<number | undefined>;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     *
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly minRetryIntervalMillis: pulumi.Output<number | undefined>;
    /**
     * An optional name for the job. The default value is Untitled.
     */
    declare public readonly name: pulumi.Output<string>;
    declare public readonly newCluster: pulumi.Output<outputs.JobNewCluster | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly notebookTask: pulumi.Output<outputs.JobNotebookTask | undefined>;
    /**
     * An optional block controlling the notification settings on the job level documented below.
     */
    declare public readonly notificationSettings: pulumi.Output<outputs.JobNotificationSettings | undefined>;
    /**
     * Specifies job parameter for the job. See parameter Configuration Block
     */
    declare public readonly parameters: pulumi.Output<outputs.JobParameter[] | undefined>;
    /**
     * The performance mode on a serverless job. The performance target determines the level of compute performance or cost-efficiency for the run.  Supported values are:
     * * `PERFORMANCE_OPTIMIZED`: (default value) Prioritizes fast startup and execution times through rapid scaling and optimized cluster performance.
     * * `STANDARD`: Enables cost-efficient execution of serverless workloads.
     */
    declare public readonly performanceTarget: pulumi.Output<string | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly pipelineTask: pulumi.Output<outputs.JobPipelineTask | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly pythonWheelTask: pulumi.Output<outputs.JobPythonWheelTask | undefined>;
    /**
     * The queue status for the job. See queue Configuration Block below.
     */
    declare public readonly queue: pulumi.Output<outputs.JobQueue | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly retryOnTimeout: pulumi.Output<boolean | undefined>;
    /**
     * The user or the service principal the job runs as. See runAs Configuration Block below.
     */
    declare public readonly runAs: pulumi.Output<outputs.JobRunAs>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly runJobTask: pulumi.Output<outputs.JobRunJobTask | undefined>;
    /**
     * An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.
     */
    declare public readonly schedule: pulumi.Output<outputs.JobSchedule | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly sparkJarTask: pulumi.Output<outputs.JobSparkJarTask | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly sparkPythonTask: pulumi.Output<outputs.JobSparkPythonTask | undefined>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    declare public readonly sparkSubmitTask: pulumi.Output<outputs.JobSparkSubmitTask | undefined>;
    /**
     * An optional map of the tags associated with the job. See tags Configuration Map
     */
    declare public readonly tags: pulumi.Output<{[key: string]: string} | undefined>;
    /**
     * A list of task specification that the job will execute. See task Configuration Block below.
     */
    declare public readonly tasks: pulumi.Output<outputs.JobTask[] | undefined>;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    declare public readonly timeoutSeconds: pulumi.Output<number | undefined>;
    /**
     * The conditions that triggers the job to start. See trigger Configuration Block below.
     */
    declare public readonly trigger: pulumi.Output<outputs.JobTrigger | undefined>;
    /**
     * URL of the job on the given workspace
     */
    declare public /*out*/ readonly url: pulumi.Output<string>;
    declare public readonly usagePolicyId: pulumi.Output<string | undefined>;
    /**
     * (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
     */
    declare public readonly webhookNotifications: pulumi.Output<outputs.JobWebhookNotifications | undefined>;

    /**
     * Create a Job resource with the given unique name, arguments, and options.
     *
     * @param name The _unique_ name of the resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param opts A bag of options that control this resource's behavior.
     */
    constructor(name: string, args?: JobArgs, opts?: pulumi.CustomResourceOptions)
    constructor(name: string, argsOrState?: JobArgs | JobState, opts?: pulumi.CustomResourceOptions) {
        let resourceInputs: pulumi.Inputs = {};
        opts = opts || {};
        if (opts.id) {
            const state = argsOrState as JobState | undefined;
            resourceInputs["alwaysRunning"] = state?.alwaysRunning;
            resourceInputs["budgetPolicyId"] = state?.budgetPolicyId;
            resourceInputs["continuous"] = state?.continuous;
            resourceInputs["controlRunState"] = state?.controlRunState;
            resourceInputs["dbtTask"] = state?.dbtTask;
            resourceInputs["deployment"] = state?.deployment;
            resourceInputs["description"] = state?.description;
            resourceInputs["editMode"] = state?.editMode;
            resourceInputs["emailNotifications"] = state?.emailNotifications;
            resourceInputs["environments"] = state?.environments;
            resourceInputs["existingClusterId"] = state?.existingClusterId;
            resourceInputs["format"] = state?.format;
            resourceInputs["gitSource"] = state?.gitSource;
            resourceInputs["health"] = state?.health;
            resourceInputs["jobClusters"] = state?.jobClusters;
            resourceInputs["libraries"] = state?.libraries;
            resourceInputs["maxConcurrentRuns"] = state?.maxConcurrentRuns;
            resourceInputs["maxRetries"] = state?.maxRetries;
            resourceInputs["minRetryIntervalMillis"] = state?.minRetryIntervalMillis;
            resourceInputs["name"] = state?.name;
            resourceInputs["newCluster"] = state?.newCluster;
            resourceInputs["notebookTask"] = state?.notebookTask;
            resourceInputs["notificationSettings"] = state?.notificationSettings;
            resourceInputs["parameters"] = state?.parameters;
            resourceInputs["performanceTarget"] = state?.performanceTarget;
            resourceInputs["pipelineTask"] = state?.pipelineTask;
            resourceInputs["pythonWheelTask"] = state?.pythonWheelTask;
            resourceInputs["queue"] = state?.queue;
            resourceInputs["retryOnTimeout"] = state?.retryOnTimeout;
            resourceInputs["runAs"] = state?.runAs;
            resourceInputs["runJobTask"] = state?.runJobTask;
            resourceInputs["schedule"] = state?.schedule;
            resourceInputs["sparkJarTask"] = state?.sparkJarTask;
            resourceInputs["sparkPythonTask"] = state?.sparkPythonTask;
            resourceInputs["sparkSubmitTask"] = state?.sparkSubmitTask;
            resourceInputs["tags"] = state?.tags;
            resourceInputs["tasks"] = state?.tasks;
            resourceInputs["timeoutSeconds"] = state?.timeoutSeconds;
            resourceInputs["trigger"] = state?.trigger;
            resourceInputs["url"] = state?.url;
            resourceInputs["usagePolicyId"] = state?.usagePolicyId;
            resourceInputs["webhookNotifications"] = state?.webhookNotifications;
        } else {
            const args = argsOrState as JobArgs | undefined;
            resourceInputs["alwaysRunning"] = args?.alwaysRunning;
            resourceInputs["budgetPolicyId"] = args?.budgetPolicyId;
            resourceInputs["continuous"] = args?.continuous;
            resourceInputs["controlRunState"] = args?.controlRunState;
            resourceInputs["dbtTask"] = args?.dbtTask;
            resourceInputs["deployment"] = args?.deployment;
            resourceInputs["description"] = args?.description;
            resourceInputs["editMode"] = args?.editMode;
            resourceInputs["emailNotifications"] = args?.emailNotifications;
            resourceInputs["environments"] = args?.environments;
            resourceInputs["existingClusterId"] = args?.existingClusterId;
            resourceInputs["format"] = args?.format;
            resourceInputs["gitSource"] = args?.gitSource;
            resourceInputs["health"] = args?.health;
            resourceInputs["jobClusters"] = args?.jobClusters;
            resourceInputs["libraries"] = args?.libraries;
            resourceInputs["maxConcurrentRuns"] = args?.maxConcurrentRuns;
            resourceInputs["maxRetries"] = args?.maxRetries;
            resourceInputs["minRetryIntervalMillis"] = args?.minRetryIntervalMillis;
            resourceInputs["name"] = args?.name;
            resourceInputs["newCluster"] = args?.newCluster;
            resourceInputs["notebookTask"] = args?.notebookTask;
            resourceInputs["notificationSettings"] = args?.notificationSettings;
            resourceInputs["parameters"] = args?.parameters;
            resourceInputs["performanceTarget"] = args?.performanceTarget;
            resourceInputs["pipelineTask"] = args?.pipelineTask;
            resourceInputs["pythonWheelTask"] = args?.pythonWheelTask;
            resourceInputs["queue"] = args?.queue;
            resourceInputs["retryOnTimeout"] = args?.retryOnTimeout;
            resourceInputs["runAs"] = args?.runAs;
            resourceInputs["runJobTask"] = args?.runJobTask;
            resourceInputs["schedule"] = args?.schedule;
            resourceInputs["sparkJarTask"] = args?.sparkJarTask;
            resourceInputs["sparkPythonTask"] = args?.sparkPythonTask;
            resourceInputs["sparkSubmitTask"] = args?.sparkSubmitTask;
            resourceInputs["tags"] = args?.tags;
            resourceInputs["tasks"] = args?.tasks;
            resourceInputs["timeoutSeconds"] = args?.timeoutSeconds;
            resourceInputs["trigger"] = args?.trigger;
            resourceInputs["usagePolicyId"] = args?.usagePolicyId;
            resourceInputs["webhookNotifications"] = args?.webhookNotifications;
            resourceInputs["url"] = undefined /*out*/;
        }
        opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts);
        super(Job.__pulumiType, name, resourceInputs, opts);
    }
}

/**
 * Input properties used for looking up and filtering Job resources.
 */
export interface JobState {
    /**
     * (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `sparkJarTask` or `sparkSubmitTask` or `sparkPythonTask` or `notebookTask` blocks.
     *
     * @deprecated always_running will be replaced by controlRunState in the next major release.
     */
    alwaysRunning?: pulumi.Input<boolean>;
    /**
     * The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.
     */
    budgetPolicyId?: pulumi.Input<string>;
    /**
     * Configuration block to configure pause status. See continuous Configuration Block.
     */
    continuous?: pulumi.Input<inputs.JobContinuous>;
    /**
     * (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pauseStatus` by stopping the current active run. This flag cannot be set for non-continuous jobs.
     *
     * When migrating from `alwaysRunning` to `controlRunState`, set `continuous` as follows:
     */
    controlRunState?: pulumi.Input<boolean>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    dbtTask?: pulumi.Input<inputs.JobDbtTask>;
    deployment?: pulumi.Input<inputs.JobDeployment>;
    /**
     * An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.
     */
    description?: pulumi.Input<string>;
    /**
     * If `"UI_LOCKED"`, the user interface for the job will be locked. If `"EDITABLE"` (the default), the user interface will be editable.
     */
    editMode?: pulumi.Input<string>;
    /**
     * (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.
     */
    emailNotifications?: pulumi.Input<inputs.JobEmailNotifications>;
    environments?: pulumi.Input<pulumi.Input<inputs.JobEnvironment>[]>;
    existingClusterId?: pulumi.Input<string>;
    format?: pulumi.Input<string>;
    /**
     * Specifies the a Git repository for task source code. See gitSource Configuration Block below.
     */
    gitSource?: pulumi.Input<inputs.JobGitSource>;
    /**
     * An optional block that specifies the health conditions for the job documented below.
     */
    health?: pulumi.Input<inputs.JobHealth>;
    /**
     * A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
     */
    jobClusters?: pulumi.Input<pulumi.Input<inputs.JobJobCluster>[]>;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: pulumi.Input<pulumi.Input<inputs.JobLibrary>[]>;
    /**
     * (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
     */
    maxConcurrentRuns?: pulumi.Input<number>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    maxRetries?: pulumi.Input<number>;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     *
     * @deprecated should be used inside a task block and not inside a job block
     */
    minRetryIntervalMillis?: pulumi.Input<number>;
    /**
     * An optional name for the job. The default value is Untitled.
     */
    name?: pulumi.Input<string>;
    newCluster?: pulumi.Input<inputs.JobNewCluster>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    notebookTask?: pulumi.Input<inputs.JobNotebookTask>;
    /**
     * An optional block controlling the notification settings on the job level documented below.
     */
    notificationSettings?: pulumi.Input<inputs.JobNotificationSettings>;
    /**
     * Specifies job parameter for the job. See parameter Configuration Block
     */
    parameters?: pulumi.Input<pulumi.Input<inputs.JobParameter>[]>;
    /**
     * The performance mode on a serverless job. The performance target determines the level of compute performance or cost-efficiency for the run.  Supported values are:
     * * `PERFORMANCE_OPTIMIZED`: (default value) Prioritizes fast startup and execution times through rapid scaling and optimized cluster performance.
     * * `STANDARD`: Enables cost-efficient execution of serverless workloads.
     */
    performanceTarget?: pulumi.Input<string>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    pipelineTask?: pulumi.Input<inputs.JobPipelineTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    pythonWheelTask?: pulumi.Input<inputs.JobPythonWheelTask>;
    /**
     * The queue status for the job. See queue Configuration Block below.
     */
    queue?: pulumi.Input<inputs.JobQueue>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    retryOnTimeout?: pulumi.Input<boolean>;
    /**
     * The user or the service principal the job runs as. See runAs Configuration Block below.
     */
    runAs?: pulumi.Input<inputs.JobRunAs>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    runJobTask?: pulumi.Input<inputs.JobRunJobTask>;
    /**
     * An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.
     */
    schedule?: pulumi.Input<inputs.JobSchedule>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkJarTask?: pulumi.Input<inputs.JobSparkJarTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkPythonTask?: pulumi.Input<inputs.JobSparkPythonTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkSubmitTask?: pulumi.Input<inputs.JobSparkSubmitTask>;
    /**
     * An optional map of the tags associated with the job. See tags Configuration Map
     */
    tags?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * A list of task specification that the job will execute. See task Configuration Block below.
     */
    tasks?: pulumi.Input<pulumi.Input<inputs.JobTask>[]>;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    timeoutSeconds?: pulumi.Input<number>;
    /**
     * The conditions that triggers the job to start. See trigger Configuration Block below.
     */
    trigger?: pulumi.Input<inputs.JobTrigger>;
    /**
     * URL of the job on the given workspace
     */
    url?: pulumi.Input<string>;
    usagePolicyId?: pulumi.Input<string>;
    /**
     * (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
     */
    webhookNotifications?: pulumi.Input<inputs.JobWebhookNotifications>;
}

/**
 * The set of arguments for constructing a Job resource.
 */
export interface JobArgs {
    /**
     * (Bool) Whenever the job is always running, like a Spark Streaming application, on every update restart the current active run or start it again, if nothing it is not running. False by default. Any job runs are started with `parameters` specified in `sparkJarTask` or `sparkSubmitTask` or `sparkPythonTask` or `notebookTask` blocks.
     *
     * @deprecated always_running will be replaced by controlRunState in the next major release.
     */
    alwaysRunning?: pulumi.Input<boolean>;
    /**
     * The ID of the user-specified budget policy to use for this job. If not specified, a default budget policy may be applied when creating or modifying the job.
     */
    budgetPolicyId?: pulumi.Input<string>;
    /**
     * Configuration block to configure pause status. See continuous Configuration Block.
     */
    continuous?: pulumi.Input<inputs.JobContinuous>;
    /**
     * (Bool) If true, the Databricks provider will stop and start the job as needed to ensure that the active run for the job reflects the deployed configuration. For continuous jobs, the provider respects the `pauseStatus` by stopping the current active run. This flag cannot be set for non-continuous jobs.
     *
     * When migrating from `alwaysRunning` to `controlRunState`, set `continuous` as follows:
     */
    controlRunState?: pulumi.Input<boolean>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    dbtTask?: pulumi.Input<inputs.JobDbtTask>;
    deployment?: pulumi.Input<inputs.JobDeployment>;
    /**
     * An optional description for the job. The maximum length is 1024 characters in UTF-8 encoding.
     */
    description?: pulumi.Input<string>;
    /**
     * If `"UI_LOCKED"`, the user interface for the job will be locked. If `"EDITABLE"` (the default), the user interface will be editable.
     */
    editMode?: pulumi.Input<string>;
    /**
     * (List) An optional set of email addresses notified when runs of this job begins, completes or fails. The default behavior is to not send any emails. This field is a block and is documented below.
     */
    emailNotifications?: pulumi.Input<inputs.JobEmailNotifications>;
    environments?: pulumi.Input<pulumi.Input<inputs.JobEnvironment>[]>;
    existingClusterId?: pulumi.Input<string>;
    format?: pulumi.Input<string>;
    /**
     * Specifies the a Git repository for task source code. See gitSource Configuration Block below.
     */
    gitSource?: pulumi.Input<inputs.JobGitSource>;
    /**
     * An optional block that specifies the health conditions for the job documented below.
     */
    health?: pulumi.Input<inputs.JobHealth>;
    /**
     * A list of job databricks.Cluster specifications that can be shared and reused by tasks of this job. Libraries cannot be declared in a shared job cluster. You must declare dependent libraries in task settings. *Multi-task syntax*
     */
    jobClusters?: pulumi.Input<pulumi.Input<inputs.JobJobCluster>[]>;
    /**
     * (List) An optional list of libraries to be installed on the cluster that will execute the job. See library Configuration Block below.
     */
    libraries?: pulumi.Input<pulumi.Input<inputs.JobLibrary>[]>;
    /**
     * (Integer) An optional maximum allowed number of concurrent runs of the job. Defaults to *1*.
     */
    maxConcurrentRuns?: pulumi.Input<number>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    maxRetries?: pulumi.Input<number>;
    /**
     * (Integer) An optional minimal interval in milliseconds between the start of the failed run and the subsequent retry run. The default behavior is that unsuccessful runs are immediately retried.
     *
     * @deprecated should be used inside a task block and not inside a job block
     */
    minRetryIntervalMillis?: pulumi.Input<number>;
    /**
     * An optional name for the job. The default value is Untitled.
     */
    name?: pulumi.Input<string>;
    newCluster?: pulumi.Input<inputs.JobNewCluster>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    notebookTask?: pulumi.Input<inputs.JobNotebookTask>;
    /**
     * An optional block controlling the notification settings on the job level documented below.
     */
    notificationSettings?: pulumi.Input<inputs.JobNotificationSettings>;
    /**
     * Specifies job parameter for the job. See parameter Configuration Block
     */
    parameters?: pulumi.Input<pulumi.Input<inputs.JobParameter>[]>;
    /**
     * The performance mode on a serverless job. The performance target determines the level of compute performance or cost-efficiency for the run.  Supported values are:
     * * `PERFORMANCE_OPTIMIZED`: (default value) Prioritizes fast startup and execution times through rapid scaling and optimized cluster performance.
     * * `STANDARD`: Enables cost-efficient execution of serverless workloads.
     */
    performanceTarget?: pulumi.Input<string>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    pipelineTask?: pulumi.Input<inputs.JobPipelineTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    pythonWheelTask?: pulumi.Input<inputs.JobPythonWheelTask>;
    /**
     * The queue status for the job. See queue Configuration Block below.
     */
    queue?: pulumi.Input<inputs.JobQueue>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    retryOnTimeout?: pulumi.Input<boolean>;
    /**
     * The user or the service principal the job runs as. See runAs Configuration Block below.
     */
    runAs?: pulumi.Input<inputs.JobRunAs>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    runJobTask?: pulumi.Input<inputs.JobRunJobTask>;
    /**
     * An optional periodic schedule for this job. The default behavior is that the job runs when triggered by clicking Run Now in the Jobs UI or sending an API request to runNow. See schedule Configuration Block below.
     */
    schedule?: pulumi.Input<inputs.JobSchedule>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkJarTask?: pulumi.Input<inputs.JobSparkJarTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkPythonTask?: pulumi.Input<inputs.JobSparkPythonTask>;
    /**
     * @deprecated should be used inside a task block and not inside a job block
     */
    sparkSubmitTask?: pulumi.Input<inputs.JobSparkSubmitTask>;
    /**
     * An optional map of the tags associated with the job. See tags Configuration Map
     */
    tags?: pulumi.Input<{[key: string]: pulumi.Input<string>}>;
    /**
     * A list of task specification that the job will execute. See task Configuration Block below.
     */
    tasks?: pulumi.Input<pulumi.Input<inputs.JobTask>[]>;
    /**
     * (Integer) An optional timeout applied to each run of this job. The default behavior is to have no timeout.
     */
    timeoutSeconds?: pulumi.Input<number>;
    /**
     * The conditions that triggers the job to start. See trigger Configuration Block below.
     */
    trigger?: pulumi.Input<inputs.JobTrigger>;
    usagePolicyId?: pulumi.Input<string>;
    /**
     * (List) An optional set of system destinations (for example, webhook destinations or Slack) to be notified when runs of this job begins, completes or fails. The default behavior is to not send any notifications. This field is a block and is documented below.
     */
    webhookNotifications?: pulumi.Input<inputs.JobWebhookNotifications>;
}
