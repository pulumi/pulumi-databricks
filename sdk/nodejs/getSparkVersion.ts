// *** WARNING: this file was generated by the Pulumi Terraform Bridge (tfgen) Tool. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

import * as pulumi from "@pulumi/pulumi";
import * as utilities from "./utilities";

/**
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const withGpu = databricks.getNodeType({
 *     localDisk: true,
 *     minCores: 16,
 *     gbPerCore: 1,
 *     minGpus: 1,
 * });
 * const gpuMl = databricks.getSparkVersion({
 *     gpu: true,
 *     ml: true,
 * });
 * const research = new databricks.Cluster("research", {
 *     clusterName: "Research Cluster",
 *     sparkVersion: gpuMl.then(gpuMl => gpuMl.id),
 *     nodeTypeId: withGpu.then(withGpu => withGpu.id),
 *     autoterminationMinutes: 20,
 *     autoscale: {
 *         minWorkers: 1,
 *         maxWorkers: 50,
 *     },
 * });
 * ```
 * ## Related Resources
 *
 * The following resources are used in the same context:
 *
 * * End to end workspace management guide
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
 * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
 * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
 */
export function getSparkVersion(args?: GetSparkVersionArgs, opts?: pulumi.InvokeOptions): Promise<GetSparkVersionResult> {
    args = args || {};

    opts = pulumi.mergeOptions(utilities.resourceOptsDefaults(), opts || {});
    return pulumi.runtime.invoke("databricks:index/getSparkVersion:getSparkVersion", {
        "beta": args.beta,
        "genomics": args.genomics,
        "gpu": args.gpu,
        "graviton": args.graviton,
        "latest": args.latest,
        "longTermSupport": args.longTermSupport,
        "ml": args.ml,
        "photon": args.photon,
        "scala": args.scala,
        "sparkVersion": args.sparkVersion,
    }, opts);
}

/**
 * A collection of arguments for invoking getSparkVersion.
 */
export interface GetSparkVersionArgs {
    /**
     * if we should limit the search only to runtimes that are in Beta stage. Default to `false`.
     */
    beta?: boolean;
    /**
     * if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.
     */
    genomics?: boolean;
    /**
     * if we should limit the search only to runtimes that support GPUs. Default to `false`.
     */
    gpu?: boolean;
    /**
     * if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`.
     */
    graviton?: boolean;
    /**
     * if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.
     */
    latest?: boolean;
    /**
     * if we should limit the search only to LTS (long term support) & ESR (extended support) versions. Default to `false`.
     */
    longTermSupport?: boolean;
    /**
     * if we should limit the search only to ML runtimes. Default to `false`.
     */
    ml?: boolean;
    /**
     * if we should limit the search only to Photon runtimes. Default to `false`.
     */
    photon?: boolean;
    /**
     * if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.
     */
    scala?: string;
    /**
     * if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.
     */
    sparkVersion?: string;
}

/**
 * A collection of values returned by getSparkVersion.
 */
export interface GetSparkVersionResult {
    readonly beta?: boolean;
    readonly genomics?: boolean;
    readonly gpu?: boolean;
    readonly graviton?: boolean;
    /**
     * The provider-assigned unique ID for this managed resource.
     */
    readonly id: string;
    readonly latest?: boolean;
    readonly longTermSupport?: boolean;
    readonly ml?: boolean;
    readonly photon?: boolean;
    readonly scala?: string;
    readonly sparkVersion?: string;
}
/**
 * ## Example Usage
 *
 * ```typescript
 * import * as pulumi from "@pulumi/pulumi";
 * import * as databricks from "@pulumi/databricks";
 *
 * const withGpu = databricks.getNodeType({
 *     localDisk: true,
 *     minCores: 16,
 *     gbPerCore: 1,
 *     minGpus: 1,
 * });
 * const gpuMl = databricks.getSparkVersion({
 *     gpu: true,
 *     ml: true,
 * });
 * const research = new databricks.Cluster("research", {
 *     clusterName: "Research Cluster",
 *     sparkVersion: gpuMl.then(gpuMl => gpuMl.id),
 *     nodeTypeId: withGpu.then(withGpu => withGpu.id),
 *     autoterminationMinutes: 20,
 *     autoscale: {
 *         minWorkers: 1,
 *         maxWorkers: 50,
 *     },
 * });
 * ```
 * ## Related Resources
 *
 * The following resources are used in the same context:
 *
 * * End to end workspace management guide
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
 * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
 * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
 */
export function getSparkVersionOutput(args?: GetSparkVersionOutputArgs, opts?: pulumi.InvokeOptions): pulumi.Output<GetSparkVersionResult> {
    return pulumi.output(args).apply((a: any) => getSparkVersion(a, opts))
}

/**
 * A collection of arguments for invoking getSparkVersion.
 */
export interface GetSparkVersionOutputArgs {
    /**
     * if we should limit the search only to runtimes that are in Beta stage. Default to `false`.
     */
    beta?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to Genomics (HLS) runtimes. Default to `false`.
     */
    genomics?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to runtimes that support GPUs. Default to `false`.
     */
    gpu?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to runtimes supporting AWS Graviton CPUs. Default to `false`.
     */
    graviton?: pulumi.Input<boolean>;
    /**
     * if we should return only the latest version if there is more than one result.  Default to `true`. If set to `false` and multiple versions are matching, throws an error.
     */
    latest?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to LTS (long term support) & ESR (extended support) versions. Default to `false`.
     */
    longTermSupport?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to ML runtimes. Default to `false`.
     */
    ml?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to Photon runtimes. Default to `false`.
     */
    photon?: pulumi.Input<boolean>;
    /**
     * if we should limit the search only to runtimes that are based on specific Scala version. Default to `2.12`.
     */
    scala?: pulumi.Input<string>;
    /**
     * if we should limit the search only to runtimes that are based on specific Spark version. Default to empty string.  It could be specified as `3`, or `3.0`, or full version, like, `3.0.1`.
     */
    sparkVersion?: pulumi.Input<string>;
}
