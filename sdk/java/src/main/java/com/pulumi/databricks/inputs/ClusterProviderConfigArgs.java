// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.String;
import java.util.Objects;


public final class ClusterProviderConfigArgs extends com.pulumi.resources.ResourceArgs {

    public static final ClusterProviderConfigArgs Empty = new ClusterProviderConfigArgs();

    /**
     * Workspace ID which the resource belongs to. This workspace must be part of the account which the provider is configured with.
     * 
     * The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .build());
     * 
     *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .longTermSupport(true)
     *             .build());
     * 
     *         var sharedAutoscaling = new Cluster("sharedAutoscaling", ClusterArgs.builder()
     *             .clusterName("Shared Autoscaling")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .sparkConf(Map.ofEntries(
     *                 Map.entry("spark.databricks.io.cache.enabled", "true"),
     *                 Map.entry("spark.databricks.io.cache.maxDiskUsage", "50g"),
     *                 Map.entry("spark.databricks.io.cache.maxMetaDataCache", "1g")
     *             ))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    @Import(name="workspaceId", required=true)
    private Output<String> workspaceId;

    /**
     * @return Workspace ID which the resource belongs to. This workspace must be part of the account which the provider is configured with.
     * 
     * The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .build());
     * 
     *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .longTermSupport(true)
     *             .build());
     * 
     *         var sharedAutoscaling = new Cluster("sharedAutoscaling", ClusterArgs.builder()
     *             .clusterName("Shared Autoscaling")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .sparkConf(Map.ofEntries(
     *                 Map.entry("spark.databricks.io.cache.enabled", "true"),
     *                 Map.entry("spark.databricks.io.cache.maxDiskUsage", "50g"),
     *                 Map.entry("spark.databricks.io.cache.maxMetaDataCache", "1g")
     *             ))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public Output<String> workspaceId() {
        return this.workspaceId;
    }

    private ClusterProviderConfigArgs() {}

    private ClusterProviderConfigArgs(ClusterProviderConfigArgs $) {
        this.workspaceId = $.workspaceId;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ClusterProviderConfigArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ClusterProviderConfigArgs $;

        public Builder() {
            $ = new ClusterProviderConfigArgs();
        }

        public Builder(ClusterProviderConfigArgs defaults) {
            $ = new ClusterProviderConfigArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param workspaceId Workspace ID which the resource belongs to. This workspace must be part of the account which the provider is configured with.
         * 
         * The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
         * 
         * <pre>
         * {@code
         * package generated_program;
         * 
         * import com.pulumi.Context;
         * import com.pulumi.Pulumi;
         * import com.pulumi.core.Output;
         * import com.pulumi.databricks.DatabricksFunctions;
         * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
         * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
         * import com.pulumi.databricks.Cluster;
         * import com.pulumi.databricks.ClusterArgs;
         * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
         * import java.util.List;
         * import java.util.ArrayList;
         * import java.util.Map;
         * import java.io.File;
         * import java.nio.file.Files;
         * import java.nio.file.Paths;
         * 
         * public class App {
         *     public static void main(String[] args) {
         *         Pulumi.run(App::stack);
         *     }
         * 
         *     public static void stack(Context ctx) {
         *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
         *             .localDisk(true)
         *             .build());
         * 
         *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
         *             .longTermSupport(true)
         *             .build());
         * 
         *         var sharedAutoscaling = new Cluster("sharedAutoscaling", ClusterArgs.builder()
         *             .clusterName("Shared Autoscaling")
         *             .sparkVersion(latestLts.id())
         *             .nodeTypeId(smallest.id())
         *             .autoterminationMinutes(20)
         *             .autoscale(ClusterAutoscaleArgs.builder()
         *                 .minWorkers(1)
         *                 .maxWorkers(50)
         *                 .build())
         *             .sparkConf(Map.ofEntries(
         *                 Map.entry("spark.databricks.io.cache.enabled", "true"),
         *                 Map.entry("spark.databricks.io.cache.maxDiskUsage", "50g"),
         *                 Map.entry("spark.databricks.io.cache.maxMetaDataCache", "1g")
         *             ))
         *             .build());
         * 
         *     }
         * }
         * }
         * </pre>
         * 
         * @return builder
         * 
         */
        public Builder workspaceId(Output<String> workspaceId) {
            $.workspaceId = workspaceId;
            return this;
        }

        /**
         * @param workspaceId Workspace ID which the resource belongs to. This workspace must be part of the account which the provider is configured with.
         * 
         * The following example demonstrates how to create an autoscaling cluster with [Delta Cache](https://docs.databricks.com/delta/optimizations/delta-cache.html) enabled:
         * 
         * <pre>
         * {@code
         * package generated_program;
         * 
         * import com.pulumi.Context;
         * import com.pulumi.Pulumi;
         * import com.pulumi.core.Output;
         * import com.pulumi.databricks.DatabricksFunctions;
         * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
         * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
         * import com.pulumi.databricks.Cluster;
         * import com.pulumi.databricks.ClusterArgs;
         * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
         * import java.util.List;
         * import java.util.ArrayList;
         * import java.util.Map;
         * import java.io.File;
         * import java.nio.file.Files;
         * import java.nio.file.Paths;
         * 
         * public class App {
         *     public static void main(String[] args) {
         *         Pulumi.run(App::stack);
         *     }
         * 
         *     public static void stack(Context ctx) {
         *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
         *             .localDisk(true)
         *             .build());
         * 
         *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
         *             .longTermSupport(true)
         *             .build());
         * 
         *         var sharedAutoscaling = new Cluster("sharedAutoscaling", ClusterArgs.builder()
         *             .clusterName("Shared Autoscaling")
         *             .sparkVersion(latestLts.id())
         *             .nodeTypeId(smallest.id())
         *             .autoterminationMinutes(20)
         *             .autoscale(ClusterAutoscaleArgs.builder()
         *                 .minWorkers(1)
         *                 .maxWorkers(50)
         *                 .build())
         *             .sparkConf(Map.ofEntries(
         *                 Map.entry("spark.databricks.io.cache.enabled", "true"),
         *                 Map.entry("spark.databricks.io.cache.maxDiskUsage", "50g"),
         *                 Map.entry("spark.databricks.io.cache.maxMetaDataCache", "1g")
         *             ))
         *             .build());
         * 
         *     }
         * }
         * }
         * </pre>
         * 
         * @return builder
         * 
         */
        public Builder workspaceId(String workspaceId) {
            return workspaceId(Output.of(workspaceId));
        }

        public ClusterProviderConfigArgs build() {
            if ($.workspaceId == null) {
                throw new MissingRequiredPropertyException("ClusterProviderConfigArgs", "workspaceId");
            }
            return $;
        }
    }

}
