// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks.inputs;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.databricks.inputs.FeatureEngineeringKafkaConfigAuthConfigArgs;
import com.pulumi.databricks.inputs.FeatureEngineeringKafkaConfigBackfillSourceArgs;
import com.pulumi.databricks.inputs.FeatureEngineeringKafkaConfigKeySchemaArgs;
import com.pulumi.databricks.inputs.FeatureEngineeringKafkaConfigSubscriptionModeArgs;
import com.pulumi.databricks.inputs.FeatureEngineeringKafkaConfigValueSchemaArgs;
import java.lang.String;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class FeatureEngineeringKafkaConfigState extends com.pulumi.resources.ResourceArgs {

    public static final FeatureEngineeringKafkaConfigState Empty = new FeatureEngineeringKafkaConfigState();

    /**
     * Authentication configuration for connection to topics
     * 
     */
    @Import(name="authConfig")
    private @Nullable Output<FeatureEngineeringKafkaConfigAuthConfigArgs> authConfig;

    /**
     * @return Authentication configuration for connection to topics
     * 
     */
    public Optional<Output<FeatureEngineeringKafkaConfigAuthConfigArgs>> authConfig() {
        return Optional.ofNullable(this.authConfig);
    }

    /**
     * A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
     * In the future, a separate table will be maintained by Databricks for forward filling data.
     * The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
     * 
     */
    @Import(name="backfillSource")
    private @Nullable Output<FeatureEngineeringKafkaConfigBackfillSourceArgs> backfillSource;

    /**
     * @return A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
     * In the future, a separate table will be maintained by Databricks for forward filling data.
     * The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
     * 
     */
    public Optional<Output<FeatureEngineeringKafkaConfigBackfillSourceArgs>> backfillSource() {
        return Optional.ofNullable(this.backfillSource);
    }

    /**
     * A comma-separated list of host/port pairs pointing to Kafka cluster
     * 
     */
    @Import(name="bootstrapServers")
    private @Nullable Output<String> bootstrapServers;

    /**
     * @return A comma-separated list of host/port pairs pointing to Kafka cluster
     * 
     */
    public Optional<Output<String>> bootstrapServers() {
        return Optional.ofNullable(this.bootstrapServers);
    }

    /**
     * Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
     * 
     */
    @Import(name="extraOptions")
    private @Nullable Output<Map<String,String>> extraOptions;

    /**
     * @return Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
     * 
     */
    public Optional<Output<Map<String,String>>> extraOptions() {
        return Optional.ofNullable(this.extraOptions);
    }

    /**
     * Schema configuration for extracting message keys from topics. At least one of keySchema and valueSchema must be provided
     * 
     */
    @Import(name="keySchema")
    private @Nullable Output<FeatureEngineeringKafkaConfigKeySchemaArgs> keySchema;

    /**
     * @return Schema configuration for extracting message keys from topics. At least one of keySchema and valueSchema must be provided
     * 
     */
    public Optional<Output<FeatureEngineeringKafkaConfigKeySchemaArgs>> keySchema() {
        return Optional.ofNullable(this.keySchema);
    }

    /**
     * (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
     * Can be distinct from topic name
     * 
     */
    @Import(name="name")
    private @Nullable Output<String> name;

    /**
     * @return (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
     * Can be distinct from topic name
     * 
     */
    public Optional<Output<String>> name() {
        return Optional.ofNullable(this.name);
    }

    /**
     * Options to configure which Kafka topics to pull data from
     * 
     */
    @Import(name="subscriptionMode")
    private @Nullable Output<FeatureEngineeringKafkaConfigSubscriptionModeArgs> subscriptionMode;

    /**
     * @return Options to configure which Kafka topics to pull data from
     * 
     */
    public Optional<Output<FeatureEngineeringKafkaConfigSubscriptionModeArgs>> subscriptionMode() {
        return Optional.ofNullable(this.subscriptionMode);
    }

    /**
     * Schema configuration for extracting message values from topics. At least one of keySchema and valueSchema must be provided
     * 
     */
    @Import(name="valueSchema")
    private @Nullable Output<FeatureEngineeringKafkaConfigValueSchemaArgs> valueSchema;

    /**
     * @return Schema configuration for extracting message values from topics. At least one of keySchema and valueSchema must be provided
     * 
     */
    public Optional<Output<FeatureEngineeringKafkaConfigValueSchemaArgs>> valueSchema() {
        return Optional.ofNullable(this.valueSchema);
    }

    private FeatureEngineeringKafkaConfigState() {}

    private FeatureEngineeringKafkaConfigState(FeatureEngineeringKafkaConfigState $) {
        this.authConfig = $.authConfig;
        this.backfillSource = $.backfillSource;
        this.bootstrapServers = $.bootstrapServers;
        this.extraOptions = $.extraOptions;
        this.keySchema = $.keySchema;
        this.name = $.name;
        this.subscriptionMode = $.subscriptionMode;
        this.valueSchema = $.valueSchema;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(FeatureEngineeringKafkaConfigState defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private FeatureEngineeringKafkaConfigState $;

        public Builder() {
            $ = new FeatureEngineeringKafkaConfigState();
        }

        public Builder(FeatureEngineeringKafkaConfigState defaults) {
            $ = new FeatureEngineeringKafkaConfigState(Objects.requireNonNull(defaults));
        }

        /**
         * @param authConfig Authentication configuration for connection to topics
         * 
         * @return builder
         * 
         */
        public Builder authConfig(@Nullable Output<FeatureEngineeringKafkaConfigAuthConfigArgs> authConfig) {
            $.authConfig = authConfig;
            return this;
        }

        /**
         * @param authConfig Authentication configuration for connection to topics
         * 
         * @return builder
         * 
         */
        public Builder authConfig(FeatureEngineeringKafkaConfigAuthConfigArgs authConfig) {
            return authConfig(Output.of(authConfig));
        }

        /**
         * @param backfillSource A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
         * In the future, a separate table will be maintained by Databricks for forward filling data.
         * The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
         * 
         * @return builder
         * 
         */
        public Builder backfillSource(@Nullable Output<FeatureEngineeringKafkaConfigBackfillSourceArgs> backfillSource) {
            $.backfillSource = backfillSource;
            return this;
        }

        /**
         * @param backfillSource A user-provided and managed source for backfilling data. Historical data is used when creating a training set from streaming features linked to this Kafka config.
         * In the future, a separate table will be maintained by Databricks for forward filling data.
         * The schema for this source must match exactly that of the key and value schemas specified for this Kafka config
         * 
         * @return builder
         * 
         */
        public Builder backfillSource(FeatureEngineeringKafkaConfigBackfillSourceArgs backfillSource) {
            return backfillSource(Output.of(backfillSource));
        }

        /**
         * @param bootstrapServers A comma-separated list of host/port pairs pointing to Kafka cluster
         * 
         * @return builder
         * 
         */
        public Builder bootstrapServers(@Nullable Output<String> bootstrapServers) {
            $.bootstrapServers = bootstrapServers;
            return this;
        }

        /**
         * @param bootstrapServers A comma-separated list of host/port pairs pointing to Kafka cluster
         * 
         * @return builder
         * 
         */
        public Builder bootstrapServers(String bootstrapServers) {
            return bootstrapServers(Output.of(bootstrapServers));
        }

        /**
         * @param extraOptions Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
         * 
         * @return builder
         * 
         */
        public Builder extraOptions(@Nullable Output<Map<String,String>> extraOptions) {
            $.extraOptions = extraOptions;
            return this;
        }

        /**
         * @param extraOptions Catch-all for miscellaneous options. Keys should be source options or Kafka consumer options (kafka.*)
         * 
         * @return builder
         * 
         */
        public Builder extraOptions(Map<String,String> extraOptions) {
            return extraOptions(Output.of(extraOptions));
        }

        /**
         * @param keySchema Schema configuration for extracting message keys from topics. At least one of keySchema and valueSchema must be provided
         * 
         * @return builder
         * 
         */
        public Builder keySchema(@Nullable Output<FeatureEngineeringKafkaConfigKeySchemaArgs> keySchema) {
            $.keySchema = keySchema;
            return this;
        }

        /**
         * @param keySchema Schema configuration for extracting message keys from topics. At least one of keySchema and valueSchema must be provided
         * 
         * @return builder
         * 
         */
        public Builder keySchema(FeatureEngineeringKafkaConfigKeySchemaArgs keySchema) {
            return keySchema(Output.of(keySchema));
        }

        /**
         * @param name (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
         * Can be distinct from topic name
         * 
         * @return builder
         * 
         */
        public Builder name(@Nullable Output<String> name) {
            $.name = name;
            return this;
        }

        /**
         * @param name (string) - Name that uniquely identifies this Kafka config within the metastore. This will be the identifier used from the Feature object to reference these configs for a feature.
         * Can be distinct from topic name
         * 
         * @return builder
         * 
         */
        public Builder name(String name) {
            return name(Output.of(name));
        }

        /**
         * @param subscriptionMode Options to configure which Kafka topics to pull data from
         * 
         * @return builder
         * 
         */
        public Builder subscriptionMode(@Nullable Output<FeatureEngineeringKafkaConfigSubscriptionModeArgs> subscriptionMode) {
            $.subscriptionMode = subscriptionMode;
            return this;
        }

        /**
         * @param subscriptionMode Options to configure which Kafka topics to pull data from
         * 
         * @return builder
         * 
         */
        public Builder subscriptionMode(FeatureEngineeringKafkaConfigSubscriptionModeArgs subscriptionMode) {
            return subscriptionMode(Output.of(subscriptionMode));
        }

        /**
         * @param valueSchema Schema configuration for extracting message values from topics. At least one of keySchema and valueSchema must be provided
         * 
         * @return builder
         * 
         */
        public Builder valueSchema(@Nullable Output<FeatureEngineeringKafkaConfigValueSchemaArgs> valueSchema) {
            $.valueSchema = valueSchema;
            return this;
        }

        /**
         * @param valueSchema Schema configuration for extracting message values from topics. At least one of keySchema and valueSchema must be provided
         * 
         * @return builder
         * 
         */
        public Builder valueSchema(FeatureEngineeringKafkaConfigValueSchemaArgs valueSchema) {
            return valueSchema(Output.of(valueSchema));
        }

        public FeatureEngineeringKafkaConfigState build() {
            return $;
        }
    }

}
