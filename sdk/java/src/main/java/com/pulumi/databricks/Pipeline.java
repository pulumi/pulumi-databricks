// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.databricks.PipelineArgs;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.PipelineState;
import com.pulumi.databricks.outputs.PipelineCluster;
import com.pulumi.databricks.outputs.PipelineDeployment;
import com.pulumi.databricks.outputs.PipelineEventLog;
import com.pulumi.databricks.outputs.PipelineFilters;
import com.pulumi.databricks.outputs.PipelineGatewayDefinition;
import com.pulumi.databricks.outputs.PipelineIngestionDefinition;
import com.pulumi.databricks.outputs.PipelineLatestUpdate;
import com.pulumi.databricks.outputs.PipelineLibrary;
import com.pulumi.databricks.outputs.PipelineNotification;
import com.pulumi.databricks.outputs.PipelineRestartWindow;
import com.pulumi.databricks.outputs.PipelineRunAs;
import com.pulumi.databricks.outputs.PipelineTrigger;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Use `databricks.Pipeline` to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
 * 
 * ## Example Usage
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Notebook;
 * import com.pulumi.databricks.Repo;
 * import com.pulumi.databricks.Pipeline;
 * import com.pulumi.databricks.PipelineArgs;
 * import com.pulumi.databricks.inputs.PipelineClusterArgs;
 * import com.pulumi.databricks.inputs.PipelineLibraryArgs;
 * import com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;
 * import com.pulumi.databricks.inputs.PipelineLibraryFileArgs;
 * import com.pulumi.databricks.inputs.PipelineNotificationArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         var dltDemo = new Notebook("dltDemo");
 * 
 *         var dltDemoRepo = new Repo("dltDemoRepo");
 * 
 *         var this_ = new Pipeline("this", PipelineArgs.builder()
 *             .name("Pipeline Name")
 *             .storage("/test/first-pipeline")
 *             .configuration(Map.ofEntries(
 *                 Map.entry("key1", "value1"),
 *                 Map.entry("key2", "value2")
 *             ))
 *             .clusters(            
 *                 PipelineClusterArgs.builder()
 *                     .label("default")
 *                     .numWorkers(2)
 *                     .customTags(Map.of("cluster_type", "default"))
 *                     .build(),
 *                 PipelineClusterArgs.builder()
 *                     .label("maintenance")
 *                     .numWorkers(1)
 *                     .customTags(Map.of("cluster_type", "maintenance"))
 *                     .build())
 *             .libraries(            
 *                 PipelineLibraryArgs.builder()
 *                     .notebook(PipelineLibraryNotebookArgs.builder()
 *                         .path(dltDemo.id())
 *                         .build())
 *                     .build(),
 *                 PipelineLibraryArgs.builder()
 *                     .file(PipelineLibraryFileArgs.builder()
 *                         .path(dltDemoRepo.path().applyValue(_path -> String.format("%s/pipeline.sql", _path)))
 *                         .build())
 *                     .build())
 *             .continuous(false)
 *             .notifications(PipelineNotificationArgs.builder()
 *                 .emailRecipients(                
 *                     "user}{@literal @}{@code domain.com",
 *                     "user1}{@literal @}{@code domain.com")
 *                 .alerts(                
 *                     "on-update-failure",
 *                     "on-update-fatal-failure",
 *                     "on-update-success",
 *                     "on-flow-failure")
 *                 .build())
 *             .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Related Resources
 * 
 * The following resources are often used in the same context:
 * 
 * * End to end workspace management guide.
 * * databricks.getPipelines to retrieve [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html) pipeline data.
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
 * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
 * 
 * ## Import
 * 
 * The resource job can be imported using the id of the pipeline
 * 
 * bash
 * 
 * ```sh
 * $ pulumi import databricks:index/pipeline:Pipeline this &lt;pipeline-id&gt;
 * ```
 * 
 */
@ResourceType(type="databricks:index/pipeline:Pipeline")
public class Pipeline extends com.pulumi.resources.CustomResource {
    /**
     * Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
     * 
     */
    @Export(name="allowDuplicateNames", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> allowDuplicateNames;

    /**
     * @return Optional boolean flag. If false, deployment will fail if name conflicts with that of another pipeline. default is `false`.
     * 
     */
    public Output<Optional<Boolean>> allowDuplicateNames() {
        return Codegen.optional(this.allowDuplicateNames);
    }
    /**
     * optional string specifying ID of the budget policy for this DLT pipeline.
     * 
     */
    @Export(name="budgetPolicyId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> budgetPolicyId;

    /**
     * @return optional string specifying ID of the budget policy for this DLT pipeline.
     * 
     */
    public Output<Optional<String>> budgetPolicyId() {
        return Codegen.optional(this.budgetPolicyId);
    }
    /**
     * The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
     * 
     */
    @Export(name="catalog", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> catalog;

    /**
     * @return The name of catalog in Unity Catalog. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `storage`).
     * 
     */
    public Output<Optional<String>> catalog() {
        return Codegen.optional(this.catalog);
    }
    @Export(name="cause", refs={String.class}, tree="[0]")
    private Output<String> cause;

    public Output<String> cause() {
        return this.cause;
    }
    /**
     * optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
     * 
     */
    @Export(name="channel", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> channel;

    /**
     * @return optional name of the release channel for Spark version used by DLT pipeline.  Supported values are: `CURRENT` (default) and `PREVIEW`.
     * 
     */
    public Output<Optional<String>> channel() {
        return Codegen.optional(this.channel);
    }
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output<String> clusterId;

    public Output<String> clusterId() {
        return this.clusterId;
    }
    /**
     * blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
     * 
     */
    @Export(name="clusters", refs={List.class,PipelineCluster.class}, tree="[0,1]")
    private Output</* @Nullable */ List<PipelineCluster>> clusters;

    /**
     * @return blocks - Clusters to run the pipeline. If none is specified, pipelines will automatically select a default cluster configuration for the pipeline. *Please note that DLT pipeline clusters are supporting only subset of attributes as described in [documentation](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-api-guide.html#pipelinesnewcluster).*  Also, note that `autoscale` block is extended with the `mode` parameter that controls the autoscaling algorithm (possible values are `ENHANCED` for new, enhanced autoscaling algorithm, or `LEGACY` for old algorithm).
     * 
     */
    public Output<Optional<List<PipelineCluster>>> clusters() {
        return Codegen.optional(this.clusters);
    }
    /**
     * An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
     * 
     */
    @Export(name="configuration", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> configuration;

    /**
     * @return An optional list of values to apply to the entire pipeline. Elements must be formatted as key:value pairs.
     * 
     */
    public Output<Optional<Map<String,String>>> configuration() {
        return Codegen.optional(this.configuration);
    }
    /**
     * A flag indicating whether to run the pipeline continuously. The default value is `false`.
     * 
     */
    @Export(name="continuous", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> continuous;

    /**
     * @return A flag indicating whether to run the pipeline continuously. The default value is `false`.
     * 
     */
    public Output<Optional<Boolean>> continuous() {
        return Codegen.optional(this.continuous);
    }
    @Export(name="creatorUserName", refs={String.class}, tree="[0]")
    private Output<String> creatorUserName;

    public Output<String> creatorUserName() {
        return this.creatorUserName;
    }
    /**
     * Deployment type of this pipeline. Supports following attributes:
     * 
     */
    @Export(name="deployment", refs={PipelineDeployment.class}, tree="[0]")
    private Output</* @Nullable */ PipelineDeployment> deployment;

    /**
     * @return Deployment type of this pipeline. Supports following attributes:
     * 
     */
    public Output<Optional<PipelineDeployment>> deployment() {
        return Codegen.optional(this.deployment);
    }
    /**
     * A flag indicating whether to run the pipeline in development mode. The default value is `false`.
     * 
     */
    @Export(name="development", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> development;

    /**
     * @return A flag indicating whether to run the pipeline in development mode. The default value is `false`.
     * 
     */
    public Output<Optional<Boolean>> development() {
        return Codegen.optional(this.development);
    }
    /**
     * optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
     * 
     */
    @Export(name="edition", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> edition;

    /**
     * @return optional name of the [product edition](https://docs.databricks.com/data-engineering/delta-live-tables/delta-live-tables-concepts.html#editions). Supported values are: `CORE`, `PRO`, `ADVANCED` (default).  Not required when `serverless` is set to `true`.
     * 
     */
    public Output<Optional<String>> edition() {
        return Codegen.optional(this.edition);
    }
    @Export(name="eventLog", refs={PipelineEventLog.class}, tree="[0]")
    private Output</* @Nullable */ PipelineEventLog> eventLog;

    public Output<Optional<PipelineEventLog>> eventLog() {
        return Codegen.optional(this.eventLog);
    }
    @Export(name="expectedLastModified", refs={Integer.class}, tree="[0]")
    private Output</* @Nullable */ Integer> expectedLastModified;

    public Output<Optional<Integer>> expectedLastModified() {
        return Codegen.optional(this.expectedLastModified);
    }
    /**
     * Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
     * 
     */
    @Export(name="filters", refs={PipelineFilters.class}, tree="[0]")
    private Output</* @Nullable */ PipelineFilters> filters;

    /**
     * @return Filters on which Pipeline packages to include in the deployed graph.  This block consists of following attributes:
     * 
     */
    public Output<Optional<PipelineFilters>> filters() {
        return Codegen.optional(this.filters);
    }
    /**
     * The definition of a gateway pipeline to support CDC. Consists of following attributes:
     * 
     */
    @Export(name="gatewayDefinition", refs={PipelineGatewayDefinition.class}, tree="[0]")
    private Output</* @Nullable */ PipelineGatewayDefinition> gatewayDefinition;

    /**
     * @return The definition of a gateway pipeline to support CDC. Consists of following attributes:
     * 
     */
    public Output<Optional<PipelineGatewayDefinition>> gatewayDefinition() {
        return Codegen.optional(this.gatewayDefinition);
    }
    @Export(name="health", refs={String.class}, tree="[0]")
    private Output<String> health;

    public Output<String> health() {
        return this.health;
    }
    @Export(name="ingestionDefinition", refs={PipelineIngestionDefinition.class}, tree="[0]")
    private Output</* @Nullable */ PipelineIngestionDefinition> ingestionDefinition;

    public Output<Optional<PipelineIngestionDefinition>> ingestionDefinition() {
        return Codegen.optional(this.ingestionDefinition);
    }
    @Export(name="lastModified", refs={Integer.class}, tree="[0]")
    private Output<Integer> lastModified;

    public Output<Integer> lastModified() {
        return this.lastModified;
    }
    @Export(name="latestUpdates", refs={List.class,PipelineLatestUpdate.class}, tree="[0,1]")
    private Output<List<PipelineLatestUpdate>> latestUpdates;

    public Output<List<PipelineLatestUpdate>> latestUpdates() {
        return this.latestUpdates;
    }
    /**
     * blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` &amp; `file` library types that should have the `path` attribute. *Right now only the `notebook` &amp; `file` types are supported.*
     * 
     */
    @Export(name="libraries", refs={List.class,PipelineLibrary.class}, tree="[0,1]")
    private Output</* @Nullable */ List<PipelineLibrary>> libraries;

    /**
     * @return blocks - Specifies pipeline code and required artifacts. Syntax resembles library configuration block with the addition of a special `notebook` &amp; `file` library types that should have the `path` attribute. *Right now only the `notebook` &amp; `file` types are supported.*
     * 
     */
    public Output<Optional<List<PipelineLibrary>>> libraries() {
        return Codegen.optional(this.libraries);
    }
    /**
     * A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
     * 
     */
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    /**
     * @return A user-friendly name for this pipeline. The name can be used to identify pipeline jobs in the UI.
     * 
     */
    public Output<String> name() {
        return this.name;
    }
    @Export(name="notifications", refs={List.class,PipelineNotification.class}, tree="[0,1]")
    private Output</* @Nullable */ List<PipelineNotification>> notifications;

    public Output<Optional<List<PipelineNotification>>> notifications() {
        return Codegen.optional(this.notifications);
    }
    /**
     * A flag indicating whether to use Photon engine. The default value is `false`.
     * 
     */
    @Export(name="photon", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> photon;

    /**
     * @return A flag indicating whether to use Photon engine. The default value is `false`.
     * 
     */
    public Output<Optional<Boolean>> photon() {
        return Codegen.optional(this.photon);
    }
    @Export(name="restartWindow", refs={PipelineRestartWindow.class}, tree="[0]")
    private Output</* @Nullable */ PipelineRestartWindow> restartWindow;

    public Output<Optional<PipelineRestartWindow>> restartWindow() {
        return Codegen.optional(this.restartWindow);
    }
    @Export(name="runAs", refs={PipelineRunAs.class}, tree="[0]")
    private Output<PipelineRunAs> runAs;

    public Output<PipelineRunAs> runAs() {
        return this.runAs;
    }
    @Export(name="runAsUserName", refs={String.class}, tree="[0]")
    private Output<String> runAsUserName;

    public Output<String> runAsUserName() {
        return this.runAsUserName;
    }
    /**
     * The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
     * 
     */
    @Export(name="schema", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> schema;

    /**
     * @return The default schema (database) where tables are read from or published to. The presence of this attribute implies that the pipeline is in direct publishing mode.
     * 
     */
    public Output<Optional<String>> schema() {
        return Codegen.optional(this.schema);
    }
    /**
     * An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
     * 
     */
    @Export(name="serverless", refs={Boolean.class}, tree="[0]")
    private Output</* @Nullable */ Boolean> serverless;

    /**
     * @return An optional flag indicating if serverless compute should be used for this DLT pipeline.  Requires `catalog` to be set, as it could be used only with Unity Catalog.
     * 
     */
    public Output<Optional<Boolean>> serverless() {
        return Codegen.optional(this.serverless);
    }
    @Export(name="state", refs={String.class}, tree="[0]")
    private Output<String> state;

    public Output<String> state() {
        return this.state;
    }
    /**
     * A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
     * 
     */
    @Export(name="storage", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> storage;

    /**
     * @return A location on DBFS or cloud storage where output data and metadata required for pipeline execution are stored. By default, tables are stored in a subdirectory of this location. *Change of this parameter forces recreation of the pipeline.* (Conflicts with `catalog`).
     * 
     */
    public Output<Optional<String>> storage() {
        return Codegen.optional(this.storage);
    }
    /**
     * The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
     * 
     */
    @Export(name="target", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> target;

    /**
     * @return The name of a database (in either the Hive metastore or in a UC catalog) for persisting pipeline output data. Configuring the target setting allows you to view and query the pipeline output data from the Databricks UI.
     * 
     */
    public Output<Optional<String>> target() {
        return Codegen.optional(this.target);
    }
    @Export(name="trigger", refs={PipelineTrigger.class}, tree="[0]")
    private Output</* @Nullable */ PipelineTrigger> trigger;

    public Output<Optional<PipelineTrigger>> trigger() {
        return Codegen.optional(this.trigger);
    }
    /**
     * URL of the DLT pipeline on the given workspace.
     * 
     */
    @Export(name="url", refs={String.class}, tree="[0]")
    private Output<String> url;

    /**
     * @return URL of the DLT pipeline on the given workspace.
     * 
     */
    public Output<String> url() {
        return this.url;
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Pipeline(java.lang.String name) {
        this(name, PipelineArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Pipeline(java.lang.String name, @Nullable PipelineArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Pipeline(java.lang.String name, @Nullable PipelineArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/pipeline:Pipeline", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Pipeline(java.lang.String name, Output<java.lang.String> id, @Nullable PipelineState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/pipeline:Pipeline", name, state, makeResourceOptions(options, id), false);
    }

    private static PipelineArgs makeArgs(@Nullable PipelineArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? PipelineArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Pipeline get(java.lang.String name, Output<java.lang.String> id, @Nullable PipelineState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Pipeline(name, id, state, options);
    }
}
