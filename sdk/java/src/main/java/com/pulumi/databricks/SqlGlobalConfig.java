// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.databricks.SqlGlobalConfigArgs;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.SqlGlobalConfigState;
import java.lang.Boolean;
import java.lang.String;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * This resource configures the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.SqlEndpoint of workspace. *Please note that changing parameters of this resource will restart all running databricks_sql_endpoint.*  To use this resource you need to be an administrator.
 * 
 * &gt; This resource can only be used with a workspace-level provider!
 * 
 * ## Example Usage
 * 
 * ### AWS example
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.SqlGlobalConfig;
 * import com.pulumi.databricks.SqlGlobalConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new SqlGlobalConfig("this", SqlGlobalConfigArgs.builder()
 *             .securityPolicy("DATA_ACCESS_CONTROL")
 *             .instanceProfileArn("arn:....")
 *             .dataAccessConfig(Map.of("spark.sql.session.timeZone", "UTC"))
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ### Azure example
 * 
 * For Azure you should use the `data_access_config` to provide the service principal configuration. You can use the Databricks SQL Admin Console UI to help you generate the right configuration values.
 * 
 * &lt;!--Start PulumiCodeChooser --&gt;
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.SqlGlobalConfig;
 * import com.pulumi.databricks.SqlGlobalConfigArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new SqlGlobalConfig("this", SqlGlobalConfigArgs.builder()
 *             .securityPolicy("DATA_ACCESS_CONTROL")
 *             .dataAccessConfig(Map.ofEntries(
 *                 Map.entry("spark.hadoop.fs.azure.account.auth.type", "OAuth"),
 *                 Map.entry("spark.hadoop.fs.azure.account.oauth.provider.type", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"),
 *                 Map.entry("spark.hadoop.fs.azure.account.oauth2.client.id", applicationId),
 *                 Map.entry("spark.hadoop.fs.azure.account.oauth2.client.secret", String.format("{{{{secrets/%s/%s}}}}", secretScope,secretKey)),
 *                 Map.entry("spark.hadoop.fs.azure.account.oauth2.client.endpoint", String.format("https://login.microsoftonline.com/%s/oauth2/token", tenantId))
 *             ))
 *             .sqlConfigParams(Map.of("ANSI_MODE", "true"))
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * &lt;!--End PulumiCodeChooser --&gt;
 * 
 * ## Related Resources
 * 
 * The following resources are often used in the same context:
 * 
 * * End to end workspace management guide.
 * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
 * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
 * * databricks.SqlEndpoint to manage Databricks SQL [Endpoints](https://docs.databricks.com/sql/admin/sql-endpoints.html).
 * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
 * 
 * ## Import
 * 
 * You can import a `databricks_sql_global_config` resource with command like the following (you need to use `global` as ID):
 * 
 * hcl
 * 
 * import {
 * 
 *   to = databricks_sql_global_config.this
 * 
 *   id = &#34;global&#34;
 * 
 * }
 * 
 * Alternatively, when using `terraform` version 1.4 or earlier, import using the `pulumi import` command:
 * 
 * bash
 * 
 * ```sh
 * $ pulumi import databricks:index/sqlGlobalConfig:SqlGlobalConfig this global
 * ```
 * 
 */
@ResourceType(type="databricks:index/sqlGlobalConfig:SqlGlobalConfig")
public class SqlGlobalConfig extends com.pulumi.resources.CustomResource {
    /**
     * Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you&#39;re specifying not permitted configuration.
     * 
     */
    @Export(name="dataAccessConfig", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> dataAccessConfig;

    /**
     * @return Data access configuration for databricks_sql_endpoint, such as configuration for an external Hive metastore, Hadoop Filesystem configuration, etc.  Please note that the list of supported configuration properties is limited, so refer to the [documentation](https://docs.databricks.com/sql/admin/data-access-configuration.html#supported-properties) for a full list.  Apply will fail if you&#39;re specifying not permitted configuration.
     * 
     */
    public Output<Optional<Map<String,String>>> dataAccessConfig() {
        return Codegen.optional(this.dataAccessConfig);
    }
    /**
     * @deprecated
     * This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future
     * 
     */
    @Deprecated /* This field is intended as an internal API and may be removed from the Databricks Terraform provider in the future */
    @Export(name="enableServerlessCompute", refs={Boolean.class}, tree="[0]")
    private Output<Boolean> enableServerlessCompute;

    public Output<Boolean> enableServerlessCompute() {
        return this.enableServerlessCompute;
    }
    /**
     * used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
     * 
     */
    @Export(name="googleServiceAccount", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> googleServiceAccount;

    /**
     * @return used to access GCP services, such as Cloud Storage, from databricks_sql_endpoint. Please note that this parameter is only for GCP, and will generate an error if used on other clouds.
     * 
     */
    public Output<Optional<String>> googleServiceAccount() {
        return Codegen.optional(this.googleServiceAccount);
    }
    /**
     * databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
     * 
     */
    @Export(name="instanceProfileArn", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> instanceProfileArn;

    /**
     * @return databricks_instance_profile used to access storage from databricks_sql_endpoint. Please note that this parameter is only for AWS, and will generate an error if used on other clouds.
     * 
     */
    public Output<Optional<String>> instanceProfileArn() {
        return Codegen.optional(this.instanceProfileArn);
    }
    /**
     * The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values
     * 
     */
    @Export(name="securityPolicy", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> securityPolicy;

    /**
     * @return The policy for controlling access to datasets. Default value: `DATA_ACCESS_CONTROL`, consult documentation for list of possible values
     * 
     */
    public Output<Optional<String>> securityPolicy() {
        return Codegen.optional(this.securityPolicy);
    }
    /**
     * SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
     * 
     */
    @Export(name="sqlConfigParams", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> sqlConfigParams;

    /**
     * @return SQL Configuration Parameters let you override the default behavior for all sessions with all endpoints.
     * 
     */
    public Output<Optional<Map<String,String>>> sqlConfigParams() {
        return Codegen.optional(this.sqlConfigParams);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public SqlGlobalConfig(java.lang.String name) {
        this(name, SqlGlobalConfigArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public SqlGlobalConfig(java.lang.String name, @Nullable SqlGlobalConfigArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public SqlGlobalConfig(java.lang.String name, @Nullable SqlGlobalConfigArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/sqlGlobalConfig:SqlGlobalConfig", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private SqlGlobalConfig(java.lang.String name, Output<java.lang.String> id, @Nullable SqlGlobalConfigState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/sqlGlobalConfig:SqlGlobalConfig", name, state, makeResourceOptions(options, id), false);
    }

    private static SqlGlobalConfigArgs makeArgs(@Nullable SqlGlobalConfigArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? SqlGlobalConfigArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static SqlGlobalConfig get(java.lang.String name, Output<java.lang.String> id, @Nullable SqlGlobalConfigState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new SqlGlobalConfig(name, id, state, options);
    }
}
