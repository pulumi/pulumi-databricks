// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.databricks.LibraryArgs;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.LibraryState;
import com.pulumi.databricks.outputs.LibraryCran;
import com.pulumi.databricks.outputs.LibraryMaven;
import com.pulumi.databricks.outputs.LibraryProviderConfig;
import com.pulumi.databricks.outputs.LibraryPypi;
import java.lang.String;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * Installs a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster. Each different type of library has a slightly different syntax. It&#39;s possible to set only one type of library within one resource. Otherwise, the plan will fail with an error.
 * 
 * &gt; This resource can only be used with a workspace-level provider!
 * 
 * &gt; `databricks.Library` resource would always start the associated cluster if it&#39;s not running, so make sure to have auto-termination configured. It&#39;s not possible to atomically change the version of the same library without cluster restart. Libraries are fully removed from the cluster only after restart.
 * 
 * ## Plugin Framework Migration
 * 
 * The library resource has been migrated from sdkv2 to plugin frameworkã€‚ If you encounter any problem with this resource and suspect it is due to the migration, you can fallback to sdkv2 by setting the environment variable in the following way `export USE_SDK_V2_RESOURCES=&#34;databricks.Library&#34;`.
 * 
 * ## Installing library on all clusters
 * 
 * You can install libraries on all clusters with the help of databricks.getClusters data resource:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.inputs.GetClustersArgs;
 * import com.pulumi.databricks.Library;
 * import com.pulumi.databricks.LibraryArgs;
 * import com.pulumi.databricks.inputs.LibraryPypiArgs;
 * import com.pulumi.codegen.internal.KeyedValue;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         final var all = DatabricksFunctions.getClusters(GetClustersArgs.builder()
 *             .build());
 * 
 *         final var cli = all.applyValue(getClustersResult -> {
 *             final var resources = new ArrayList<Library>();
 *             for (var range : KeyedValue.of(getClustersResult.ids())) {
 *                 var resource = new Library("cli-" + range.key(), LibraryArgs.builder()
 *                     .clusterId(range.key())
 *                     .pypi(LibraryPypiArgs.builder()
 *                         .package_("databricks-cli")
 *                         .build())
 *                     .build());
 * 
 *                 resources.add(resource);
 *             }
 * 
 *             return resources;
 *         });
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Java/Scala Maven
 * 
 * Installing artifacts from Maven repository. You can also optionally specify a `repo` parameter for a custom Maven-style repository, that should be accessible without any authentication. Maven libraries are resolved in Databricks Control Plane, so repo should be accessible from it. It can even be properly configured [maven s3 wagon](https://github.com/seahen/maven-s3-wagon), [AWS CodeArtifact](https://aws.amazon.com/codeartifact/) or [Azure Artifacts](https://azure.microsoft.com/en-us/services/devops/artifacts/).
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Library;
 * import com.pulumi.databricks.LibraryArgs;
 * import com.pulumi.databricks.inputs.LibraryMavenArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var deequ = new Library("deequ", LibraryArgs.builder()
 *             .clusterId(this_.id())
 *             .maven(LibraryMavenArgs.builder()
 *                 .coordinates("com.amazon.deequ:deequ:1.0.4")
 *                 .exclusions("org.apache.avro:avro")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Python PyPI
 * 
 * Installing Python PyPI artifacts. You can optionally also specify the `repo` parameter for a custom PyPI mirror, which should be accessible without any authentication for the network that cluster runs in.
 * 
 * &gt; `repo` host should be accessible from the Internet by Databricks control plane. If connectivity to custom PyPI repositories is required, please modify cluster-node `/etc/pip.conf` through databricks_global_init_script.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Library;
 * import com.pulumi.databricks.LibraryArgs;
 * import com.pulumi.databricks.inputs.LibraryPypiArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var fbprophet = new Library("fbprophet", LibraryArgs.builder()
 *             .clusterId(this_.id())
 *             .pypi(LibraryPypiArgs.builder()
 *                 .package_("fbprophet==0.6")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Python requirements files
 * 
 * Installing Python libraries listed in the `requirements.txt` file.  Only Workspace paths and Unity Catalog Volumes paths are supported.  Requires a cluster with DBR 15.0+.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Library;
 * import com.pulumi.databricks.LibraryArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var libraries = new Library("libraries", LibraryArgs.builder()
 *             .clusterId(this_.id())
 *             .requirements("/Workspace/path/to/requirements.txt")
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## R CRan
 * 
 * Installing artifacts from CRan. You can also optionally specify a `repo` parameter for a custom cran mirror.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Library;
 * import com.pulumi.databricks.LibraryArgs;
 * import com.pulumi.databricks.inputs.LibraryCranArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var rkeops = new Library("rkeops", LibraryArgs.builder()
 *             .clusterId(this_.id())
 *             .cran(LibraryCranArgs.builder()
 *                 .package_("rkeops")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Related Resources
 * 
 * The following resources are often used in the same context:
 * 
 * * End to end workspace management guide.
 * * databricks.getClusters data to retrieve a list of databricks.Cluster ids.
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
 * * databricks.GlobalInitScript to manage [global init scripts](https://docs.databricks.com/clusters/init-scripts.html#global-init-scripts), which are run on all databricks.Cluster and databricks_job.
 * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
 * * databricks.Pipeline to deploy [Lakeflow Declarative Pipelines](https://docs.databricks.com/aws/en/dlt).
 * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
 * 
 * ## Import
 * 
 * !&gt; Importing this resource is not currently supported.
 * 
 */
@ResourceType(type="databricks:index/library:Library")
public class Library extends com.pulumi.resources.CustomResource {
    /**
     * ID of the databricks.Cluster to install the library on.
     * 
     * You must specify exactly **one** of the following library types:
     * 
     */
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output<String> clusterId;

    /**
     * @return ID of the databricks.Cluster to install the library on.
     * 
     * You must specify exactly **one** of the following library types:
     * 
     */
    public Output<String> clusterId() {
        return this.clusterId;
    }
    /**
     * Configuration block for a CRAN library. The block consists of the following fields:
     * 
     */
    @Export(name="cran", refs={LibraryCran.class}, tree="[0]")
    private Output</* @Nullable */ LibraryCran> cran;

    /**
     * @return Configuration block for a CRAN library. The block consists of the following fields:
     * 
     */
    public Output<Optional<LibraryCran>> cran() {
        return Codegen.optional(this.cran);
    }
    /**
     * Path to the EGG library. Installing Python egg files is deprecated and is not supported in Databricks Runtime 14.0 and above. Use `whl` or `pypi` instead.
     * 
     * @deprecated
     * The `egg` library type is deprecated. Please use `whl` or `pypi` instead.
     * 
     */
    @Deprecated /* The `egg` library type is deprecated. Please use `whl` or `pypi` instead. */
    @Export(name="egg", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> egg;

    /**
     * @return Path to the EGG library. Installing Python egg files is deprecated and is not supported in Databricks Runtime 14.0 and above. Use `whl` or `pypi` instead.
     * 
     */
    public Output<Optional<String>> egg() {
        return Codegen.optional(this.egg);
    }
    /**
     * Path to the JAR library. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `/Workspace/path/to/library.jar`, `/Volumes/path/to/library.jar` or `s3://my-bucket/library.jar`. If S3 is used, make sure the cluster has read access to the library. You may need to launch the cluster with an IAM role to access the S3 URI.
     * 
     */
    @Export(name="jar", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> jar;

    /**
     * @return Path to the JAR library. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `/Workspace/path/to/library.jar`, `/Volumes/path/to/library.jar` or `s3://my-bucket/library.jar`. If S3 is used, make sure the cluster has read access to the library. You may need to launch the cluster with an IAM role to access the S3 URI.
     * 
     */
    public Output<Optional<String>> jar() {
        return Codegen.optional(this.jar);
    }
    @Export(name="libraryId", refs={String.class}, tree="[0]")
    private Output<String> libraryId;

    public Output<String> libraryId() {
        return this.libraryId;
    }
    /**
     * Configuration block for a Maven library. The block consists of the following fields:
     * 
     */
    @Export(name="maven", refs={LibraryMaven.class}, tree="[0]")
    private Output</* @Nullable */ LibraryMaven> maven;

    /**
     * @return Configuration block for a Maven library. The block consists of the following fields:
     * 
     */
    public Output<Optional<LibraryMaven>> maven() {
        return Codegen.optional(this.maven);
    }
    /**
     * Configuration block for management through the account provider. This block consists of the following fields:
     * 
     */
    @Export(name="providerConfig", refs={LibraryProviderConfig.class}, tree="[0]")
    private Output</* @Nullable */ LibraryProviderConfig> providerConfig;

    /**
     * @return Configuration block for management through the account provider. This block consists of the following fields:
     * 
     */
    public Output<Optional<LibraryProviderConfig>> providerConfig() {
        return Codegen.optional(this.providerConfig);
    }
    /**
     * Configuration block for a PyPI library. The block consists of the following fields:
     * 
     */
    @Export(name="pypi", refs={LibraryPypi.class}, tree="[0]")
    private Output</* @Nullable */ LibraryPypi> pypi;

    /**
     * @return Configuration block for a PyPI library. The block consists of the following fields:
     * 
     */
    public Output<Optional<LibraryPypi>> pypi() {
        return Codegen.optional(this.pypi);
    }
    /**
     * Path to the requirements.txt file. Only Workspace paths and Unity Catalog Volumes paths are supported. For example: `/Workspace/path/to/requirements.txt` or `/Volumes/path/to/requirements.txt`. Requires a cluster with DBR 15.0+.
     * 
     */
    @Export(name="requirements", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> requirements;

    /**
     * @return Path to the requirements.txt file. Only Workspace paths and Unity Catalog Volumes paths are supported. For example: `/Workspace/path/to/requirements.txt` or `/Volumes/path/to/requirements.txt`. Requires a cluster with DBR 15.0+.
     * 
     */
    public Output<Optional<String>> requirements() {
        return Codegen.optional(this.requirements);
    }
    /**
     * Path to the wheel library. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `/Workspace/path/to/library.whl`, `/Volumes/path/to/library.whl` or `s3://my-bucket/library.whl`. If S3 is used, make sure the cluster has read access to the library. You may need to launch the cluster with an IAM role to access the S3 URI.
     * 
     */
    @Export(name="whl", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> whl;

    /**
     * @return Path to the wheel library. Supported URIs include Workspace paths, Unity Catalog Volumes paths, and S3 URIs. For example: `/Workspace/path/to/library.whl`, `/Volumes/path/to/library.whl` or `s3://my-bucket/library.whl`. If S3 is used, make sure the cluster has read access to the library. You may need to launch the cluster with an IAM role to access the S3 URI.
     * 
     */
    public Output<Optional<String>> whl() {
        return Codegen.optional(this.whl);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Library(java.lang.String name) {
        this(name, LibraryArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Library(java.lang.String name, LibraryArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Library(java.lang.String name, LibraryArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/library:Library", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Library(java.lang.String name, Output<java.lang.String> id, @Nullable LibraryState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/library:Library", name, state, makeResourceOptions(options, id), false);
    }

    private static LibraryArgs makeArgs(LibraryArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? LibraryArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Library get(java.lang.String name, Output<java.lang.String> id, @Nullable LibraryState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Library(name, id, state, options);
    }
}
