// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.databricks.MountArgs;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.MountState;
import com.pulumi.databricks.outputs.MountAbfs;
import com.pulumi.databricks.outputs.MountAdl;
import com.pulumi.databricks.outputs.MountGs;
import com.pulumi.databricks.outputs.MountProviderConfig;
import com.pulumi.databricks.outputs.MountS3;
import com.pulumi.databricks.outputs.MountWasb;
import java.lang.String;
import java.util.Map;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * &gt; Please switch to databricks_volume. DBFS mounts are deprecated.
 * 
 * &gt; This resource can only be used with a workspace-level provider!
 * 
 * &gt; When `clusterId` is not specified, it will create the smallest possible cluster in the default availability zone with name equal to or starting with `terraform-mount` for the shortest possible amount of time. To avoid mount failure due to potentially quota or capacity issues with the default cluster, we recommend specifying a cluster to use for mounting.
 * 
 * &gt; CRUD operations on a databricks mount require a running cluster. Due to limitations of terraform and the databricks mounts APIs, if the cluster the mount was most recently created / updated using no longer exists AND the mount is destroyed as a part of a pulumi up, we mark it as deleted without cleaning it up from the workspace.
 * 
 * This resource will [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`. Right now it supports mounting AWS S3, Azure (Blob Storage, ADLS Gen1 &amp; Gen2), Google Cloud Storage.  It is important to understand that this will start up the cluster if the cluster is terminated. The read and refresh terraform command will require a cluster and may take some time to validate the mount.
 * 
 * This resource provides two ways of mounting a storage account:
 * 
 * 1. Use a storage-specific configuration block - this could be used for the most cases, as it will fill most of the necessary details. Currently we support following configuration blocks:
 * 
 * * `s3` - to [mount AWS S3](https://docs.databricks.com/data/data-sources/aws/amazon-s3.html)
 * * `gs` - to [mount Google Cloud Storage](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html)
 * * `abfs` - to [mount ADLS Gen2](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/) using Azure Blob Filesystem (ABFS) driver
 * * `adl` - to [mount ADLS Gen1](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake) using Azure Data Lake (ADL) driver
 * * `wasb`  - to [mount Azure Blob Storage](https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-storage) using Windows Azure Storage Blob (WASB) driver
 * 
 * 1. Use generic arguments - you have a responsibility for providing all necessary parameters that are required to mount specific storage. This is most flexible option
 * 
 * ## Common arguments
 * 
 * * `clusterId` - (Optional, String) Cluster to use for mounting. If no cluster is specified, a new cluster will be created and will mount the bucket for all of the clusters in this workspace. If the cluster is not running - it&#39;s going to be started, so be aware to set auto-termination rules on it.
 * * `name` - (Optional, String) Name, under which mount will be accessible in `dbfs:/mnt/&lt;MOUNT_NAME&gt;`. If not specified, provider will try to infer it from depending on the resource type:
 *   * `bucketName` for AWS S3 and Google Cloud Storage
 *   * `containerName` for ADLS Gen2 and Azure Blob Storage
 *   * `storageResourceName` for ADLS Gen1
 * * `uri` - (Optional, String) the URI for accessing specific storage (`s3a://....`, `abfss://....`, `gs://....`, etc.)
 * * `extraConfigs` - (Optional, String map) configuration parameters that are necessary for mounting of specific storage
 * * `resourceId` - (Optional, String) resource ID for a given storage account. Could be used to fill defaults, such as storage account &amp; container names on Azure.
 * * `encryptionType` - (Optional, String) encryption type. Currently used only for [AWS S3 mounts](https://docs.databricks.com/data/data-sources/aws/amazon-s3.html#encrypt-data-in-s3-buckets)
 * 
 * ### Example mounting ADLS Gen2 using uri and extraConfigs
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         final var tenantId = "00000000-1111-2222-3333-444444444444";
 * 
 *         final var clientId = "55555555-6666-7777-8888-999999999999";
 * 
 *         final var secretScope = "some-kv";
 * 
 *         final var secretKey = "some-sp-secret";
 * 
 *         final var container = "test";
 * 
 *         final var storageAcc = "lrs";
 * 
 *         var this_ = new Mount("this", MountArgs.builder()
 *             .name("tf-abfss")
 *             .uri(String.format("abfss://%s}{@literal @}{@code %s.dfs.core.windows.net", container,storageAcc))
 *             .extraConfigs(Map.ofEntries(
 *                 Map.entry("fs.azure.account.auth.type", "OAuth"),
 *                 Map.entry("fs.azure.account.oauth.provider.type", "org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider"),
 *                 Map.entry("fs.azure.account.oauth2.client.id", clientId),
 *                 Map.entry("fs.azure.account.oauth2.client.secret", String.format("}{{{{{@code secrets/%s/%s}}}}}{@code ", secretScope,secretKey)),
 *                 Map.entry("fs.azure.account.oauth2.client.endpoint", String.format("https://login.microsoftonline.com/%s/oauth2/token", tenantId)),
 *                 Map.entry("fs.azure.createRemoteFileSystemDuringInitialization", "false")
 *             ))
 *             .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * 
 * ### Example mounting ADLS Gen2 with AAD passthrough
 * 
 * &gt; AAD passthrough is considered a legacy data access pattern. Use Unity Catalog for fine-grained data access control.
 * 
 * &gt; Mounts using AAD passthrough cannot be created using a service principal.
 * 
 * To mount ALDS Gen2 with Azure Active Directory Credentials passthrough we need to execute the mount commands using the cluster configured with AAD Credentials passthrough &amp; provide necessary configuration parameters (see [documentation](https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough#--mount-azure-data-lake-storage-to-dbfs-using-credential-passthrough) for more details).
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.azurerm.AzurermFunctions;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
 * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
 * import com.pulumi.databricks.Cluster;
 * import com.pulumi.databricks.ClusterArgs;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         final var config = ctx.config();
 *         final var resourceGroup = config.require("resourceGroup");
 *         final var workspaceName = config.require("workspaceName");
 *         final var this = AzurermFunctions.DatabricksWorkspace(Map.ofEntries(
 *             Map.entry("name", workspaceName),
 *             Map.entry("resourceGroupName", resourceGroup)
 *         ));
 * 
 *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
 *             .localDisk(true)
 *             .build());
 * 
 *         final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
 *             .build());
 * 
 *         var sharedPassthrough = new Cluster("sharedPassthrough", ClusterArgs.builder()
 *             .clusterName("Shared Passthrough for mount")
 *             .sparkVersion(latest.id())
 *             .nodeTypeId(smallest.id())
 *             .autoterminationMinutes(10)
 *             .numWorkers(1)
 *             .sparkConf(Map.ofEntries(
 *                 Map.entry("spark.databricks.cluster.profile", "serverless"),
 *                 Map.entry("spark.databricks.repl.allowedLanguages", "python,sql"),
 *                 Map.entry("spark.databricks.passthrough.enabled", "true"),
 *                 Map.entry("spark.databricks.pyspark.enableProcessIsolation", "true")
 *             ))
 *             .customTags(Map.of("ResourceClass", "Serverless"))
 *             .build());
 * 
 *         final var storageAcc = config.require("storageAcc");
 *         final var container = config.require("container");
 *         var passthrough = new Mount("passthrough", MountArgs.builder()
 *             .name("passthrough-test")
 *             .clusterId(sharedPassthrough.id())
 *             .uri(String.format("abfss://%s}{@literal @}{@code %s.dfs.core.windows.net", container,storageAcc))
 *             .extraConfigs(Map.ofEntries(
 *                 Map.entry("fs.azure.account.auth.type", "CustomAccessToken"),
 *                 Map.entry("fs.azure.account.custom.token.provider.class", "}{{{@code sparkconf/spark.databricks.passthrough.adls.gen2.tokenProviderClassName}}}{@code ")
 *             ))
 *             .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * 
 * ## s3 block
 * 
 * This block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `s3` block:
 * 
 * * `instanceProfile` - (Optional) (String) ARN of registered instance profile for data access.  If it&#39;s not specified, then the `clusterId` should be provided, and the cluster should have an instance profile attached to it. If both `clusterId` &amp; `instanceProfile` are specified, then `clusterId` takes precedence.
 * * `bucketName` - (Required) (String) S3 bucket name to be mounted.
 * 
 * ### Example of mounting S3
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import com.pulumi.databricks.inputs.MountS3Args;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         // now you can do `%fs ls /mnt/experiments` in notebooks
 *         var this_ = new Mount("this", MountArgs.builder()
 *             .name("experiments")
 *             .s3(MountS3Args.builder()
 *                 .instanceProfile(ds.id())
 *                 .bucketName(thisAwsS3Bucket.bucket())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## abfs block
 * 
 * This block allows specifying parameters for mounting of the ADLS Gen2. The following arguments are required inside the `abfs` block:
 * 
 * * `clientId` - (Required) (String) This is the clientId (Application Object ID) for the enterprise application for the service principal.
 * * `tenantId` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenantId` from it).
 * * `clientSecretKey` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.
 * * `clientSecretScope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.
 * * `containerName` - (Required) (String) ADLS gen2 container name. (Could be omitted if `resourceId` is provided)
 * * `storageAccountName` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resourceId` is provided)
 * * `directory` - (Computed) (String) This is optional if you don&#39;t want to add an additional directory that you wish to mount. This must start with a &#34;/&#34;.
 * * `initializeFileSystem` - (Required) (Bool) either or not initialize FS for the first use
 * 
 * ### Creating mount for ADLS Gen2 using abfs block
 * 
 * In this example, we&#39;re using Azure authentication, so we can omit some parameters (`tenantId`, `storageAccountName`, and `containerName`) that will be detected automatically.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.SecretScope;
 * import com.pulumi.databricks.SecretScopeArgs;
 * import com.pulumi.databricks.Secret;
 * import com.pulumi.databricks.SecretArgs;
 * import com.pulumi.azurerm.StorageAccount;
 * import com.pulumi.azurerm.StorageAccountArgs;
 * import com.pulumi.azurerm.RoleAssignment;
 * import com.pulumi.azurerm.RoleAssignmentArgs;
 * import com.pulumi.azurerm.StorageContainer;
 * import com.pulumi.azurerm.StorageContainerArgs;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import com.pulumi.databricks.inputs.MountAbfsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var terraform = new SecretScope("terraform", SecretScopeArgs.builder()
 *             .name("application")
 *             .initialManagePrincipal("users")
 *             .build());
 * 
 *         var servicePrincipalKey = new Secret("servicePrincipalKey", SecretArgs.builder()
 *             .key("service_principal_key")
 *             .stringValue(ARM_CLIENT_SECRET)
 *             .scope(terraform.name())
 *             .build());
 * 
 *         var this_ = new StorageAccount("this", StorageAccountArgs.builder()
 *             .name(String.format("%sdatalake", prefix))
 *             .resourceGroupName(resourceGroupName)
 *             .location(resourceGroupLocation)
 *             .accountTier("Standard")
 *             .accountReplicationType("GRS")
 *             .accountKind("StorageV2")
 *             .isHnsEnabled(true)
 *             .build());
 * 
 *         var thisRoleAssignment = new RoleAssignment("thisRoleAssignment", RoleAssignmentArgs.builder()
 *             .scope(this_.id())
 *             .roleDefinitionName("Storage Blob Data Contributor")
 *             .principalId(current.objectId())
 *             .build());
 * 
 *         var thisStorageContainer = new StorageContainer("thisStorageContainer", StorageContainerArgs.builder()
 *             .name("marketing")
 *             .storageAccountName(this_.name())
 *             .containerAccessType("private")
 *             .build());
 * 
 *         var marketing = new Mount("marketing", MountArgs.builder()
 *             .name("marketing")
 *             .resourceId(thisStorageContainer.resourceManagerId())
 *             .abfs(MountAbfsArgs.builder()
 *                 .clientId(current.clientId())
 *                 .clientSecretScope(terraform.name())
 *                 .clientSecretKey(servicePrincipalKey.key())
 *                 .initializeFileSystem(true)
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## gs block
 * 
 * This block allows specifying parameters for mounting of the Google Cloud Storage. The following arguments are required inside the `gs` block:
 * 
 * * `serviceAccount` - (Optional) (String) email of registered [Google Service Account](https://docs.gcp.databricks.com/data/data-sources/google/gcs.html#step-1-set-up-google-cloud-service-account-using-google-cloud-console) for data access.  If it&#39;s not specified, then the `clusterId` should be provided, and the cluster should have a Google service account attached to it.
 * * `bucketName` - (Required) (String) GCS bucket name to be mounted.
 * 
 * ### Example mounting Google Cloud Storage
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import com.pulumi.databricks.inputs.MountGsArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         var thisGs = new Mount("thisGs", MountArgs.builder()
 *             .name("gs-mount")
 *             .gs(MountGsArgs.builder()
 *                 .serviceAccount("acc}{@literal @}{@code company.iam.gserviceaccount.com")
 *                 .bucketName("mybucket")
 *                 .build())
 *             .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * 
 * ## adl block
 * 
 * This block allows specifying parameters for mounting of the ADLS Gen1. The following arguments are required inside the `adl` block:
 * 
 * * `clientId` - (Required) (String) This is the clientId for the enterprise application for the service principal.
 * * `tenantId` - (Optional) (String) This is your azure directory tenant id. It is required for creating the mount. (Could be omitted if Azure authentication is used, and we can extract `tenantId` from it)
 * * `clientSecretKey` - (Required) (String) This is the secret key in which your service principal/enterprise app client secret will be stored.
 * * `clientSecretScope` - (Required) (String) This is the secret scope in which your service principal/enterprise app client secret will be stored.
 * 
 * * `storageResourceName` - (Required) (String) The name of the storage resource in which the data is for ADLS gen 1. This is what you are trying to mount. (Could be omitted if `resourceId` is provided)
 * * `sparkConfPrefix` - (Optional) (String) This is the spark configuration prefix for adls gen 1 mount. The options are `fs.adl`, `dfs.adls`. Use `fs.adl` for runtime 6.0 and above for the clusters. Otherwise use `dfs.adls`. The default value is: `fs.adl`.
 * * `directory` - (Computed) (String) This is optional if you don&#39;t want to add an additional directory that you wish to mount. This must start with a &#34;/&#34;.
 * 
 * ### Example mounting ADLS Gen1
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import com.pulumi.databricks.inputs.MountAdlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var mount = new Mount("mount", MountArgs.builder()
 *             .name("{var.RANDOM}")
 *             .adl(MountAdlArgs.builder()
 *                 .storageResourceName("{env.TEST_STORAGE_ACCOUNT_NAME}")
 *                 .tenantId(current.tenantId())
 *                 .clientId(current.clientId())
 *                 .clientSecretScope(terraform.name())
 *                 .clientSecretKey(servicePrincipalKey.key())
 *                 .sparkConfPrefix("fs.adl")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## wasb block
 * 
 * This block allows specifying parameters for mounting of the Azure Blob Storage. The following arguments are required inside the `wasb` block:
 * 
 * * `authType` - (Required) (String) This is the auth type for blob storage. This can either be SAS tokens (`SAS`) or account access keys (`ACCESS_KEY`).
 * * `tokenSecretScope` - (Required) (String) This is the secret scope in which your auth type token is stored.
 * * `tokenSecretKey` - (Required) (String) This is the secret key in which your auth type token is stored.
 * * `containerName` - (Required) (String) The container in which the data is. This is what you are trying to mount. (Could be omitted if `resourceId` is provided)
 * * `storageAccountName` - (Required) (String) The name of the storage resource in which the data is. (Could be omitted if `resourceId` is provided)
 * * `directory` - (Computed) (String) This is optional if you don&#39;t want to add an additional directory that you wish to mount. This must start with a &#34;/&#34;.
 * 
 * ### Example mounting Azure Blob Storage
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.azurerm.StorageAccount;
 * import com.pulumi.azurerm.StorageAccountArgs;
 * import com.pulumi.azurerm.StorageContainer;
 * import com.pulumi.azurerm.StorageContainerArgs;
 * import com.pulumi.databricks.SecretScope;
 * import com.pulumi.databricks.SecretScopeArgs;
 * import com.pulumi.databricks.Secret;
 * import com.pulumi.databricks.SecretArgs;
 * import com.pulumi.databricks.Mount;
 * import com.pulumi.databricks.MountArgs;
 * import com.pulumi.databricks.inputs.MountWasbArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var blobaccount = new StorageAccount("blobaccount", StorageAccountArgs.builder()
 *             .name(String.format("%sblob", prefix))
 *             .resourceGroupName(resourceGroupName)
 *             .location(resourceGroupLocation)
 *             .accountTier("Standard")
 *             .accountReplicationType("LRS")
 *             .accountKind("StorageV2")
 *             .build());
 * 
 *         var marketing = new StorageContainer("marketing", StorageContainerArgs.builder()
 *             .name("marketing")
 *             .storageAccountName(blobaccount.name())
 *             .containerAccessType("private")
 *             .build());
 * 
 *         var terraform = new SecretScope("terraform", SecretScopeArgs.builder()
 *             .name("application")
 *             .initialManagePrincipal("users")
 *             .build());
 * 
 *         var storageKey = new Secret("storageKey", SecretArgs.builder()
 *             .key("blob_storage_key")
 *             .stringValue(blobaccount.primaryAccessKey())
 *             .scope(terraform.name())
 *             .build());
 * 
 *         var marketingMount = new Mount("marketingMount", MountArgs.builder()
 *             .name("marketing")
 *             .wasb(MountWasbArgs.builder()
 *                 .containerName(marketing.name())
 *                 .storageAccountName(blobaccount.name())
 *                 .authType("ACCESS_KEY")
 *                 .tokenSecretScope(terraform.name())
 *                 .tokenSecretKey(storageKey.key())
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * * `providerConfig` - (Optional) Configure the provider for management through account provider. This block consists of the following fields:
 *   * `workspaceId` - (Required) Workspace ID which the resource belongs to. This workspace must be part of the account which the provider is configured with.
 * 
 * ## Migration from other mount resources
 * 
 * Migration from the specific mount resource is straightforward:
 * 
 * * rename `mountName` to `name`
 * * wrap storage-specific settings (`containerName`, ...) into corresponding block (`adl`, `abfs`, `s3`, `wasbs`)
 * * for S3 mounts, rename `s3BucketName` to `bucketName`
 * 
 * ## Related Resources
 * 
 * The following resources are often used in the same context:
 * 
 * * End to end workspace management guide.
 * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
 * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
 * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
 * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
 * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
 * 
 * ## Import
 * 
 * !&gt; Importing this resource is not currently supported.
 * 
 */
@ResourceType(type="databricks:index/mount:Mount")
public class Mount extends com.pulumi.resources.CustomResource {
    @Export(name="abfs", refs={MountAbfs.class}, tree="[0]")
    private Output</* @Nullable */ MountAbfs> abfs;

    public Output<Optional<MountAbfs>> abfs() {
        return Codegen.optional(this.abfs);
    }
    @Export(name="adl", refs={MountAdl.class}, tree="[0]")
    private Output</* @Nullable */ MountAdl> adl;

    public Output<Optional<MountAdl>> adl() {
        return Codegen.optional(this.adl);
    }
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output<String> clusterId;

    public Output<String> clusterId() {
        return this.clusterId;
    }
    @Export(name="encryptionType", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> encryptionType;

    public Output<Optional<String>> encryptionType() {
        return Codegen.optional(this.encryptionType);
    }
    @Export(name="extraConfigs", refs={Map.class,String.class}, tree="[0,1,1]")
    private Output</* @Nullable */ Map<String,String>> extraConfigs;

    public Output<Optional<Map<String,String>>> extraConfigs() {
        return Codegen.optional(this.extraConfigs);
    }
    @Export(name="gs", refs={MountGs.class}, tree="[0]")
    private Output</* @Nullable */ MountGs> gs;

    public Output<Optional<MountGs>> gs() {
        return Codegen.optional(this.gs);
    }
    @Export(name="name", refs={String.class}, tree="[0]")
    private Output<String> name;

    public Output<String> name() {
        return this.name;
    }
    @Export(name="providerConfig", refs={MountProviderConfig.class}, tree="[0]")
    private Output</* @Nullable */ MountProviderConfig> providerConfig;

    public Output<Optional<MountProviderConfig>> providerConfig() {
        return Codegen.optional(this.providerConfig);
    }
    @Export(name="resourceId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> resourceId;

    public Output<Optional<String>> resourceId() {
        return Codegen.optional(this.resourceId);
    }
    @Export(name="s3", refs={MountS3.class}, tree="[0]")
    private Output</* @Nullable */ MountS3> s3;

    public Output<Optional<MountS3>> s3() {
        return Codegen.optional(this.s3);
    }
    /**
     * (String) HDFS-compatible url
     * 
     */
    @Export(name="source", refs={String.class}, tree="[0]")
    private Output<String> source;

    /**
     * @return (String) HDFS-compatible url
     * 
     */
    public Output<String> source() {
        return this.source;
    }
    @Export(name="uri", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> uri;

    public Output<Optional<String>> uri() {
        return Codegen.optional(this.uri);
    }
    @Export(name="wasb", refs={MountWasb.class}, tree="[0]")
    private Output</* @Nullable */ MountWasb> wasb;

    public Output<Optional<MountWasb>> wasb() {
        return Codegen.optional(this.wasb);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Mount(java.lang.String name) {
        this(name, MountArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Mount(java.lang.String name, @Nullable MountArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Mount(java.lang.String name, @Nullable MountArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/mount:Mount", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Mount(java.lang.String name, Output<java.lang.String> id, @Nullable MountState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/mount:Mount", name, state, makeResourceOptions(options, id), false);
    }

    private static MountArgs makeArgs(@Nullable MountArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? MountArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Mount get(java.lang.String name, Output<java.lang.String> id, @Nullable MountState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Mount(name, id, state, options);
    }
}
