// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Export;
import com.pulumi.core.annotations.ResourceType;
import com.pulumi.core.internal.Codegen;
import com.pulumi.databricks.PermissionsArgs;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.PermissionsState;
import com.pulumi.databricks.outputs.PermissionsAccessControl;
import java.lang.String;
import java.util.List;
import java.util.Optional;
import javax.annotation.Nullable;

/**
 * This resource allows you to generically manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspaces. It ensures that only _admins_, _authenticated principal_ and those declared within `accessControl` blocks would have specified access. It is not possible to remove management rights from _admins_ group.
 * 
 * &gt; This resource can only be used with a workspace-level provider!
 * 
 * &gt; This resource is _authoritative_ for permissions on objects. Configuring this resource for an object will **OVERWRITE** any existing permissions of the same type unless imported, and changes made outside of Pulumi will be reset.
 * 
 * &gt; It is not possible to lower permissions for `admins`, so Databricks Pulumi Provider removes those `accessControl` blocks automatically.
 * 
 * &gt; If multiple permission levels are specified for an identity (e.g. `CAN_RESTART` and `CAN_MANAGE` for a cluster), only the highest level permission is returned and will cause permanent drift.
 * 
 * &gt; To manage access control on service principals, use databricks_access_control_rule_set.
 * 
 * ## Cluster usage
 * 
 * It&#39;s possible to separate [cluster access control](https://docs.databricks.com/security/access-control/cluster-acl.html) to three different permission levels: `CAN_ATTACH_TO`, `CAN_RESTART` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
 * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
 * import com.pulumi.databricks.Cluster;
 * import com.pulumi.databricks.ClusterArgs;
 * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var ds = new Group("ds", GroupArgs.builder()
 *             .displayName("Data Science")
 *             .build());
 * 
 *         final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
 *             .build());
 * 
 *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
 *             .localDisk(true)
 *             .build());
 * 
 *         var sharedAutoscaling = new Cluster("sharedAutoscaling", ClusterArgs.builder()
 *             .clusterName("Shared Autoscaling")
 *             .sparkVersion(latest.id())
 *             .nodeTypeId(smallest.id())
 *             .autoterminationMinutes(60)
 *             .autoscale(ClusterAutoscaleArgs.builder()
 *                 .minWorkers(1)
 *                 .maxWorkers(10)
 *                 .build())
 *             .build());
 * 
 *         var clusterUsage = new Permissions("clusterUsage", PermissionsArgs.builder()
 *             .clusterId(sharedAutoscaling.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_ATTACH_TO")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_RESTART")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(ds.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Cluster Policy usage
 * 
 * Cluster policies allow creation of clusters, that match [given policy](https://docs.databricks.com/administration-guide/clusters/policies.html). It&#39;s possible to assign `CAN_USE` permission to users and groups:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.ClusterPolicy;
 * import com.pulumi.databricks.ClusterPolicyArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import static com.pulumi.codegen.internal.Serialization.*;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var ds = new Group("ds", GroupArgs.builder()
 *             .displayName("Data Science")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var somethingSimple = new ClusterPolicy("somethingSimple", ClusterPolicyArgs.builder()
 *             .name("Some simple policy")
 *             .definition(serializeJson(
 *                 jsonObject(
 *                     jsonProperty("spark_conf.spark.hadoop.javax.jdo.option.ConnectionURL", jsonObject(
 *                         jsonProperty("type", "forbidden")
 *                     )),
 *                     jsonProperty("spark_conf.spark.secondkey", jsonObject(
 *                         jsonProperty("type", "forbidden")
 *                     ))
 *                 )))
 *             .build());
 * 
 *         var policyUsage = new Permissions("policyUsage", PermissionsArgs.builder()
 *             .clusterPolicyId(somethingSimple.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(ds.displayName())
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_USE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Instance Pool usage
 * 
 * Instance Pools access control [allows to](https://docs.databricks.com/security/access-control/pool-acl.html) assign `CAN_ATTACH_TO` and `CAN_MANAGE` permissions to users, service principals, and groups. It&#39;s also possible to grant creation of Instance Pools to individual groups and users, service principals.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
 * import com.pulumi.databricks.InstancePool;
 * import com.pulumi.databricks.InstancePoolArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
 *             .localDisk(true)
 *             .build());
 * 
 *         var this_ = new InstancePool("this", InstancePoolArgs.builder()
 *             .instancePoolName("Reserved Instances")
 *             .idleInstanceAutoterminationMinutes(60)
 *             .nodeTypeId(smallest.id())
 *             .minIdleInstances(0)
 *             .maxCapacity(10)
 *             .build());
 * 
 *         var poolUsage = new Permissions("poolUsage", PermissionsArgs.builder()
 *             .instancePoolId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_ATTACH_TO")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Job usage
 * 
 * There are four assignable [permission levels](https://docs.databricks.com/security/access-control/jobs-acl.html#job-permissions) for databricks_job: `CAN_VIEW`, `CAN_MANAGE_RUN`, `IS_OWNER`, and `CAN_MANAGE`. Admins are granted the `CAN_MANAGE` permission by default, and they can assign that permission to non-admin users, and service principals.
 * 
 * - The creator of a job has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a job would revert ownership to the creator.
 * - A job must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the job. Nothing would change, per se, if the job was created through Pulumi.
 * - A job cannot have a group as an owner.
 * - Jobs triggered through _Run Now_ assume the permissions of the job owner and not the user, and service principal who issued Run Now.
 * - Read [main documentation](https://docs.databricks.com/security/access-control/jobs-acl.html) for additional detail.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.ServicePrincipal;
 * import com.pulumi.databricks.ServicePrincipalArgs;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
 * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
 * import com.pulumi.databricks.Job;
 * import com.pulumi.databricks.JobArgs;
 * import com.pulumi.databricks.inputs.JobTaskArgs;
 * import com.pulumi.databricks.inputs.JobTaskNewClusterArgs;
 * import com.pulumi.databricks.inputs.JobTaskNotebookTaskArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var awsPrincipal = new ServicePrincipal("awsPrincipal", ServicePrincipalArgs.builder()
 *             .displayName("main")
 *             .build());
 * 
 *         final var latest = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
 *             .build());
 * 
 *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
 *             .localDisk(true)
 *             .build());
 * 
 *         var this_ = new Job("this", JobArgs.builder()
 *             .name("Featurization")
 *             .maxConcurrentRuns(1)
 *             .tasks(JobTaskArgs.builder()
 *                 .taskKey("task1")
 *                 .newCluster(JobTaskNewClusterArgs.builder()
 *                     .numWorkers(300)
 *                     .sparkVersion(latest.id())
 *                     .nodeTypeId(smallest.id())
 *                     .build())
 *                 .notebookTask(JobTaskNotebookTaskArgs.builder()
 *                     .notebookPath("/Production/MakeFeatures")
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         var jobUsage = new Permissions("jobUsage", PermissionsArgs.builder()
 *             .jobId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_VIEW")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_MANAGE_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .servicePrincipalName(awsPrincipal.applicationId())
 *                     .permissionLevel("IS_OWNER")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Lakeflow Declarative Pipelines usage
 * 
 * There are four assignable [permission levels](https://docs.databricks.com/aws/en/security/auth/access-control#lakeflow-declarative-pipelines-acls) for databricks_pipeline: `CAN_VIEW`, `CAN_RUN`, `CAN_MANAGE`, and `IS_OWNER`. Admins are granted the `CAN_MANAGE` permission by default, and they can assign that permission to non-admin users, and service principals.
 * 
 * - The creator of a Lakeflow Declarative Pipeline has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a pipeline would revert ownership to the creator.
 * - A Lakeflow Declarative Pipeline must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the pipeline. Nothing would change, per se, if the pipeline was created through Pulumi.
 * - A Lakeflow Declarative Pipeline cannot have a group as an owner.
 * - Lakeflow Declarative Pipelines triggered through _Start_ assume the permissions of the pipeline owner and not the user, and service principal who issued Run Now.
 * - Read [main documentation](https://docs.databricks.com/aws/en/security/auth/access-control#lakeflow-declarative-pipelines-acls) for additional detail.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Notebook;
 * import com.pulumi.databricks.NotebookArgs;
 * import com.pulumi.std.StdFunctions;
 * import com.pulumi.std.inputs.Base64encodeArgs;
 * import com.pulumi.databricks.Pipeline;
 * import com.pulumi.databricks.PipelineArgs;
 * import com.pulumi.databricks.inputs.PipelineLibraryArgs;
 * import com.pulumi.databricks.inputs.PipelineLibraryNotebookArgs;
 * import com.pulumi.databricks.inputs.PipelineFiltersArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App }{{@code
 *     public static void main(String[] args) }{{@code
 *         Pulumi.run(App::stack);
 *     }}{@code
 * 
 *     public static void stack(Context ctx) }{{@code
 *         final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var ldpDemo = new Notebook("ldpDemo", NotebookArgs.builder()
 *             .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()
 *                 .input("""
 * import dlt
 * json_path = \"/databricks-datasets/wikipedia-datasets/data-001/clickstream/raw-uncompressed-json/2015_2_clickstream.json\"
 * }{@literal @}{@code dlt.table(
 *    comment=\"The raw wikipedia clickstream dataset, ingested from /databricks-datasets.\"
 * )
 * def clickstream_raw():
 *     return (spark.read.format(\"json\").load(json_path))
 *                 """)
 *                 .build()).result())
 *             .language("PYTHON")
 *             .path(String.format("%s/ldp_demo", me.home()))
 *             .build());
 * 
 *         var this_ = new Pipeline("this", PipelineArgs.builder()
 *             .name(String.format("LDP Demo Pipeline (%s)", me.alphanumeric()))
 *             .storage("/test/tf-pipeline")
 *             .configuration(Map.ofEntries(
 *                 Map.entry("key1", "value1"),
 *                 Map.entry("key2", "value2")
 *             ))
 *             .libraries(PipelineLibraryArgs.builder()
 *                 .notebook(PipelineLibraryNotebookArgs.builder()
 *                     .path(ldpDemo.id())
 *                     .build())
 *                 .build())
 *             .continuous(false)
 *             .filters(PipelineFiltersArgs.builder()
 *                 .includes("com.databricks.include")
 *                 .excludes("com.databricks.exclude")
 *                 .build())
 *             .build());
 * 
 *         var ldpUsage = new Permissions("ldpUsage", PermissionsArgs.builder()
 *             .pipelineId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_VIEW")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }}{@code
 * }}{@code
 * }
 * </pre>
 * 
 * ## Notebook usage
 * 
 * Valid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#notebook-permissions) for databricks.Notebook are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.
 * 
 * A notebook could be specified by using either `notebookPath` or `notebookId` attribute.  The value for the `notebookId` is the object ID of the resource in the Databricks Workspace that is exposed as `objectId` attribute of the `databricks.Notebook` resource as shown below.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Notebook;
 * import com.pulumi.databricks.NotebookArgs;
 * import com.pulumi.std.StdFunctions;
 * import com.pulumi.std.inputs.Base64encodeArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var this_ = new Notebook("this", NotebookArgs.builder()
 *             .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()
 *                 .input("# Welcome to your Python notebook")
 *                 .build()).result())
 *             .path("/Production/ETL/Features")
 *             .language("PYTHON")
 *             .build());
 * 
 *         var notebookUsageByPath = new Permissions("notebookUsageByPath", PermissionsArgs.builder()
 *             .notebookPath(this_.path())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *         var notebookUsageById = new Permissions("notebookUsageById", PermissionsArgs.builder()
 *             .notebookId(this_.objectId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * &gt; when importing a permissions resource, only the `notebookId` is filled!
 * 
 * ## Workspace file usage
 * 
 * Valid permission levels for databricks.WorkspaceFile are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.
 * 
 * A workspace file could be specified by using either `workspaceFilePath` or `workspaceFileId` attribute.  The value for the `workspaceFileId` is the object ID of the resource in the Databricks Workspace that is exposed as `objectId` attribute of the `databricks.WorkspaceFile` resource as shown below.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.WorkspaceFile;
 * import com.pulumi.databricks.WorkspaceFileArgs;
 * import com.pulumi.std.StdFunctions;
 * import com.pulumi.std.inputs.Base64encodeArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var this_ = new WorkspaceFile("this", WorkspaceFileArgs.builder()
 *             .contentBase64(StdFunctions.base64encode(Base64encodeArgs.builder()
 *                 .input("print('Hello World')")
 *                 .build()).result())
 *             .path("/Production/ETL/Features.py")
 *             .build());
 * 
 *         var workspaceFileUsageByPath = new Permissions("workspaceFileUsageByPath", PermissionsArgs.builder()
 *             .workspaceFilePath(this_.path())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *         var workspaceFileUsageById = new Permissions("workspaceFileUsageById", PermissionsArgs.builder()
 *             .workspaceFileId(this_.objectId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * &gt; when importing a permissions resource, only the `workspaceFileId` is filled!
 * 
 * ## Folder usage
 * 
 * Valid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#folder-permissions) for folders of databricks.Directory are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`. Notebooks and experiments in a folder inherit all permissions settings of that folder. For example, a user (or service principal) that has `CAN_RUN` permission on a folder has `CAN_RUN` permission on the notebooks in that folder.
 * 
 * - All users can list items in the folder without any permissions.
 * - All users (or service principals) have `CAN_MANAGE` permission for items in the Workspace &gt; Shared Icon Shared folder. You can grant `CAN_MANAGE` permission to notebooks and folders by moving them to the Shared Icon Shared folder.
 * - All users (or service principals) have `CAN_MANAGE` permission for objects the user creates.
 * - User home directory - The user (or service principal) has `CAN_MANAGE` permission. All other users (or service principals) can list their directories.
 * 
 * A folder could be specified by using either `directoryPath` or `directoryId` attribute.  The value for the `directoryId` is the object ID of the resource in the Databricks Workspace that is exposed as `objectId` attribute of the `databricks.Directory` resource as shown below.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Directory;
 * import com.pulumi.databricks.DirectoryArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var this_ = new Directory("this", DirectoryArgs.builder()
 *             .path("/Production/ETL")
 *             .build());
 * 
 *         var folderUsageByPath = new Permissions("folderUsageByPath", PermissionsArgs.builder()
 *             .directoryPath(this_.path())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *         var folderUsageById = new Permissions("folderUsageById", PermissionsArgs.builder()
 *             .directoryId(this_.objectId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * &gt; when importing a permissions resource, only the `directoryId` is filled!
 * 
 * ## Repos usage
 * 
 * Valid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html) for databricks.Repo are: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Repo;
 * import com.pulumi.databricks.RepoArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var this_ = new Repo("this", RepoArgs.builder()
 *             .url("https://github.com/user/demo.git")
 *             .build());
 * 
 *         var repoUsage = new Permissions("repoUsage", PermissionsArgs.builder()
 *             .repoId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## MLflow Experiment usage
 * 
 * Valid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#mlflow-experiment-permissions-1) for databricks.MlflowExperiment are: `CAN_READ`, `CAN_EDIT`, and `CAN_MANAGE`.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.MlflowExperiment;
 * import com.pulumi.databricks.MlflowExperimentArgs;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
 * 
 *         var this_ = new MlflowExperiment("this", MlflowExperimentArgs.builder()
 *             .name(String.format("%s/Sample", me.home()))
 *             .artifactLocation("s3://bucket/my-experiment")
 *             .description("My MLflow experiment description")
 *             .build());
 * 
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var experimentUsage = new Permissions("experimentUsage", PermissionsArgs.builder()
 *             .experimentId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## MLflow Model usage
 * 
 * Valid [permission levels](https://docs.databricks.com/security/access-control/workspace-acl.html#mlflow-model-permissions-1) for databricks.MlflowModel are: `CAN_READ`, `CAN_EDIT`, `CAN_MANAGE_STAGING_VERSIONS`, `CAN_MANAGE_PRODUCTION_VERSIONS`, and `CAN_MANAGE`. You can also manage permissions for all MLflow models by `registeredModelId = &#34;root&#34;`.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.MlflowModel;
 * import com.pulumi.databricks.MlflowModelArgs;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new MlflowModel("this", MlflowModelArgs.builder()
 *             .name("SomePredictions")
 *             .build());
 * 
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var modelUsage = new Permissions("modelUsage", PermissionsArgs.builder()
 *             .registeredModelId(this_.registeredModelId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_READ")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_MANAGE_PRODUCTION_VERSIONS")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE_STAGING_VERSIONS")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Model serving usage
 * 
 * Valid permission levels for databricks.ModelServing are: `CAN_VIEW`, `CAN_QUERY`, and `CAN_MANAGE`.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.ModelServing;
 * import com.pulumi.databricks.ModelServingArgs;
 * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new ModelServing("this", ModelServingArgs.builder()
 *             .name("tf-test")
 *             .config(ModelServingConfigArgs.builder()
 *                 .servedModels(ModelServingConfigServedModelArgs.builder()
 *                     .name("prod_model")
 *                     .modelName("test")
 *                     .modelVersion("1")
 *                     .workloadSize("Small")
 *                     .scaleToZeroEnabled(true)
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var mlServingUsage = new Permissions("mlServingUsage", PermissionsArgs.builder()
 *             .servingEndpointId(this_.servingEndpointId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_VIEW")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_QUERY")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Mosaic AI Vector Search usage
 * 
 * Valid permission levels for databricks.VectorSearchEndpoint are: `CAN_USE` and `CAN_MANAGE`.
 * 
 * &gt; You need to use the `endpointId` attribute of `databricks.VectorSearchEndpoint` as value for `vectorSearchEndpointId`, not the `id`!
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.VectorSearchEndpoint;
 * import com.pulumi.databricks.VectorSearchEndpointArgs;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var this_ = new VectorSearchEndpoint("this", VectorSearchEndpointArgs.builder()
 *             .name("vector-search-test")
 *             .endpointType("STANDARD")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var vectorSearchEndpointUsage = new Permissions("vectorSearchEndpointUsage", PermissionsArgs.builder()
 *             .vectorSearchEndpointId(this_.endpointId())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Passwords usage
 * 
 * By default on AWS deployments, all admin users can sign in to Databricks using either SSO or their username and password, and all API users can authenticate to the Databricks REST APIs using their username and password. As an admin, you [can limit](https://docs.databricks.com/administration-guide/users-groups/single-sign-on/index.html#optional-configure-password-access-control) admin users&#39; and API users&#39; ability to authenticate with their username and password by configuring `CAN_USE` permissions using password access control.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var guests = new Group("guests", GroupArgs.builder()
 *             .displayName("Guest Users")
 *             .build());
 * 
 *         var passwordUsage = new Permissions("passwordUsage", PermissionsArgs.builder()
 *             .authorization("passwords")
 *             .accessControls(PermissionsAccessControlArgs.builder()
 *                 .groupName(guests.displayName())
 *                 .permissionLevel("CAN_USE")
 *                 .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Token usage
 * 
 * It is required to have at least 1 personal access token in the workspace before you can manage tokens permissions.
 * 
 * !&gt; **Warning** There can be only one `authorization = &#34;tokens&#34;` permissions resource per workspace, otherwise there&#39;ll be a permanent configuration drift. After applying changes, users who previously had either `CAN_USE` or `CAN_MANAGE` permission but no longer have either permission have their access to token-based authentication revoked. Their active tokens are immediately deleted (revoked).
 * 
 * Only [possible permission](https://docs.databricks.com/administration-guide/access-control/tokens.html) to assign to non-admin group is `CAN_USE`, where _admins_ `CAN_MANAGE` all tokens:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var tokenUsage = new Permissions("tokenUsage", PermissionsArgs.builder()
 *             .authorization("tokens")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_USE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## SQL warehouse usage
 * 
 * [SQL warehouses](https://docs.databricks.com/sql/user/security/access-control/sql-endpoint-acl.html) have five possible permissions: `CAN_USE`, `CAN_MONITOR`, `CAN_MANAGE`, `CAN_VIEW` and `IS_OWNER`:
 * 
 * - The creator of a warehouse has `IS_OWNER` permission. Destroying `databricks.Permissions` resource for a warehouse would revert ownership to the creator.
 * - A warehouse must have exactly one owner. If a resource is changed and no owner is specified, the currently authenticated principal would become the new owner of the warehouse. Nothing would change, per se, if the warehouse was created through Pulumi.
 * - A warehouse cannot have a group as an owner.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.DatabricksFunctions;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.SqlEndpoint;
 * import com.pulumi.databricks.SqlEndpointArgs;
 * import com.pulumi.databricks.inputs.SqlEndpointTagsArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         final var me = DatabricksFunctions.getCurrentUser(%!v(PANIC=Format method: runtime error: invalid memory address or nil pointer dereference);
 * 
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var this_ = new SqlEndpoint("this", SqlEndpointArgs.builder()
 *             .name(String.format("Endpoint of %s", me.alphanumeric()))
 *             .clusterSize("Small")
 *             .maxNumClusters(1)
 *             .tags(SqlEndpointTagsArgs.builder()
 *                 .customTags(SqlEndpointTagsCustomTagArgs.builder()
 *                     .key("City")
 *                     .value("Amsterdam")
 *                     .build())
 *                 .build())
 *             .build());
 * 
 *         var endpointUsage = new Permissions("endpointUsage", PermissionsArgs.builder()
 *             .sqlEndpointId(this_.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Dashboard usage
 * 
 * [Dashboards](https://docs.databricks.com/en/dashboards/tutorials/manage-permissions.html) have four possible permissions: `CAN_READ`, `CAN_RUN`, `CAN_EDIT` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Dashboard;
 * import com.pulumi.databricks.DashboardArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var dashboard = new Dashboard("dashboard", DashboardArgs.builder()
 *             .displayName("TF New Dashboard")
 *             .build());
 * 
 *         var dashboardUsage = new Permissions("dashboardUsage", PermissionsArgs.builder()
 *             .dashboardId(dashboard.id())
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Legacy SQL Dashboard usage
 * 
 * [Legacy SQL dashboards](https://docs.databricks.com/sql/user/security/access-control/dashboard-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var sqlDashboardUsage = new Permissions("sqlDashboardUsage", PermissionsArgs.builder()
 *             .sqlDashboardId("3244325")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## SQL Query usage
 * 
 * [SQL queries](https://docs.databricks.com/sql/user/security/access-control/query-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:
 * 
 * &gt; If you do not define an `accessControl` block granting `CAN_MANAGE` explictly for the user calling this provider, Databricks Pulumi Provider will add `CAN_MANAGE` permission for the caller. This is a failsafe to prevent situations where the caller is locked out from making changes to the targeted `databricks.SqlQuery` resource when backend API do not apply permission inheritance correctly.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var queryUsage = new Permissions("queryUsage", PermissionsArgs.builder()
 *             .sqlQueryId("3244325")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## SQL Alert (AlertV2) usage
 * 
 * [Alert V2](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) which is the new version of SQL Alert have 4 possible permission levels: `CAN_READ`, `CAN_RUN`, `CAN_EDIT`, and `CAN_MANAGE`.
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var appUsage = new Permissions("appUsage", PermissionsArgs.builder()
 *             .alertV2Id("12345")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_EDIT")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## SQL Alert (legacy) usage
 * 
 * [SQL alerts](https://docs.databricks.com/sql/user/security/access-control/alert-acl.html) have three possible permissions: `CAN_VIEW`, `CAN_RUN` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var auto = new Group("auto", GroupArgs.builder()
 *             .displayName("Automation")
 *             .build());
 * 
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var alertUsage = new Permissions("alertUsage", PermissionsArgs.builder()
 *             .sqlAlertId("3244325")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(auto.displayName())
 *                     .permissionLevel("CAN_RUN")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Databricks Apps usage
 * 
 * [Databricks Apps](https://docs.databricks.com/en/dev-tools/databricks-apps/index.html) have two possible permissions: `CAN_USE` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var appUsage = new Permissions("appUsage", PermissionsArgs.builder()
 *             .appName("myapp")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Lakebase Database Instances usage
 * 
 * [Databricks Lakebase](https://docs.databricks.com/aws/en/oltp/) have two possible permissions: `CAN_USE` and `CAN_MANAGE`:
 * 
 * <pre>
 * {@code
 * package generated_program;
 * 
 * import com.pulumi.Context;
 * import com.pulumi.Pulumi;
 * import com.pulumi.core.Output;
 * import com.pulumi.databricks.Group;
 * import com.pulumi.databricks.GroupArgs;
 * import com.pulumi.databricks.Permissions;
 * import com.pulumi.databricks.PermissionsArgs;
 * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
 * import java.util.List;
 * import java.util.ArrayList;
 * import java.util.Map;
 * import java.io.File;
 * import java.nio.file.Files;
 * import java.nio.file.Paths;
 * 
 * public class App {
 *     public static void main(String[] args) {
 *         Pulumi.run(App::stack);
 *     }
 * 
 *     public static void stack(Context ctx) {
 *         var eng = new Group("eng", GroupArgs.builder()
 *             .displayName("Engineering")
 *             .build());
 * 
 *         var appUsage = new Permissions("appUsage", PermissionsArgs.builder()
 *             .databaseInstanceName("my_database")
 *             .accessControls(            
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName("users")
 *                     .permissionLevel("CAN_USE")
 *                     .build(),
 *                 PermissionsAccessControlArgs.builder()
 *                     .groupName(eng.displayName())
 *                     .permissionLevel("CAN_MANAGE")
 *                     .build())
 *             .build());
 * 
 *     }
 * }
 * }
 * </pre>
 * 
 * ## Instance Profiles
 * 
 * Instance Profiles are not managed by General Permissions API and therefore databricks.GroupInstanceProfile and databricks.UserInstanceProfile should be used to allow usage of specific AWS EC2 IAM roles to users or groups.
 * 
 * ## Secrets
 * 
 * One can control access to databricks.Secret through `initialManagePrincipal` argument on databricks.SecretScope or databricks_secret_acl, so that users (or service principals) can `READ`, `WRITE` or `MANAGE` entries within secret scope.
 * 
 * ## Tables, Views and Databases
 * 
 * General Permissions API does not apply to access control for tables and they have to be managed separately using the databricks.SqlPermissions resource, though you&#39;re encouraged to use Unity Catalog or migrate to it.
 * 
 * ## Data Access with Unity Catalog
 * 
 * Initially in Unity Catalog all users have no access to data, which has to be later assigned through databricks.Grants or databricks.Grant resource.
 * 
 * ## Import
 * 
 * The resource permissions can be imported using the object id
 * 
 * ```sh
 * $ pulumi import databricks:index/permissions:Permissions databricks_permissions &lt;object type&gt;/&lt;object id&gt;
 * ```
 * 
 */
@ResourceType(type="databricks:index/permissions:Permissions")
public class Permissions extends com.pulumi.resources.CustomResource {
    @Export(name="accessControls", refs={List.class,PermissionsAccessControl.class}, tree="[0,1]")
    private Output<List<PermissionsAccessControl>> accessControls;

    public Output<List<PermissionsAccessControl>> accessControls() {
        return this.accessControls;
    }
    @Export(name="alertV2Id", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> alertV2Id;

    public Output<Optional<String>> alertV2Id() {
        return Codegen.optional(this.alertV2Id);
    }
    @Export(name="appName", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> appName;

    public Output<Optional<String>> appName() {
        return Codegen.optional(this.appName);
    }
    @Export(name="authorization", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> authorization;

    public Output<Optional<String>> authorization() {
        return Codegen.optional(this.authorization);
    }
    @Export(name="clusterId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> clusterId;

    public Output<Optional<String>> clusterId() {
        return Codegen.optional(this.clusterId);
    }
    @Export(name="clusterPolicyId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> clusterPolicyId;

    public Output<Optional<String>> clusterPolicyId() {
        return Codegen.optional(this.clusterPolicyId);
    }
    @Export(name="dashboardId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> dashboardId;

    public Output<Optional<String>> dashboardId() {
        return Codegen.optional(this.dashboardId);
    }
    @Export(name="databaseInstanceName", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> databaseInstanceName;

    public Output<Optional<String>> databaseInstanceName() {
        return Codegen.optional(this.databaseInstanceName);
    }
    @Export(name="directoryId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> directoryId;

    public Output<Optional<String>> directoryId() {
        return Codegen.optional(this.directoryId);
    }
    @Export(name="directoryPath", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> directoryPath;

    public Output<Optional<String>> directoryPath() {
        return Codegen.optional(this.directoryPath);
    }
    @Export(name="experimentId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> experimentId;

    public Output<Optional<String>> experimentId() {
        return Codegen.optional(this.experimentId);
    }
    @Export(name="instancePoolId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> instancePoolId;

    public Output<Optional<String>> instancePoolId() {
        return Codegen.optional(this.instancePoolId);
    }
    @Export(name="jobId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> jobId;

    public Output<Optional<String>> jobId() {
        return Codegen.optional(this.jobId);
    }
    @Export(name="notebookId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> notebookId;

    public Output<Optional<String>> notebookId() {
        return Codegen.optional(this.notebookId);
    }
    @Export(name="notebookPath", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> notebookPath;

    public Output<Optional<String>> notebookPath() {
        return Codegen.optional(this.notebookPath);
    }
    /**
     * type of permissions.
     * 
     */
    @Export(name="objectType", refs={String.class}, tree="[0]")
    private Output<String> objectType;

    /**
     * @return type of permissions.
     * 
     */
    public Output<String> objectType() {
        return this.objectType;
    }
    @Export(name="pipelineId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> pipelineId;

    public Output<Optional<String>> pipelineId() {
        return Codegen.optional(this.pipelineId);
    }
    @Export(name="registeredModelId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> registeredModelId;

    public Output<Optional<String>> registeredModelId() {
        return Codegen.optional(this.registeredModelId);
    }
    @Export(name="repoId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> repoId;

    public Output<Optional<String>> repoId() {
        return Codegen.optional(this.repoId);
    }
    @Export(name="repoPath", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> repoPath;

    public Output<Optional<String>> repoPath() {
        return Codegen.optional(this.repoPath);
    }
    @Export(name="servingEndpointId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> servingEndpointId;

    public Output<Optional<String>> servingEndpointId() {
        return Codegen.optional(this.servingEndpointId);
    }
    @Export(name="sqlAlertId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sqlAlertId;

    public Output<Optional<String>> sqlAlertId() {
        return Codegen.optional(this.sqlAlertId);
    }
    @Export(name="sqlDashboardId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sqlDashboardId;

    public Output<Optional<String>> sqlDashboardId() {
        return Codegen.optional(this.sqlDashboardId);
    }
    @Export(name="sqlEndpointId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sqlEndpointId;

    public Output<Optional<String>> sqlEndpointId() {
        return Codegen.optional(this.sqlEndpointId);
    }
    @Export(name="sqlQueryId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> sqlQueryId;

    public Output<Optional<String>> sqlQueryId() {
        return Codegen.optional(this.sqlQueryId);
    }
    @Export(name="vectorSearchEndpointId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> vectorSearchEndpointId;

    public Output<Optional<String>> vectorSearchEndpointId() {
        return Codegen.optional(this.vectorSearchEndpointId);
    }
    @Export(name="workspaceFileId", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> workspaceFileId;

    public Output<Optional<String>> workspaceFileId() {
        return Codegen.optional(this.workspaceFileId);
    }
    @Export(name="workspaceFilePath", refs={String.class}, tree="[0]")
    private Output</* @Nullable */ String> workspaceFilePath;

    public Output<Optional<String>> workspaceFilePath() {
        return Codegen.optional(this.workspaceFilePath);
    }

    /**
     *
     * @param name The _unique_ name of the resulting resource.
     */
    public Permissions(java.lang.String name) {
        this(name, PermissionsArgs.Empty);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     */
    public Permissions(java.lang.String name, PermissionsArgs args) {
        this(name, args, null);
    }
    /**
     *
     * @param name The _unique_ name of the resulting resource.
     * @param args The arguments to use to populate this resource's properties.
     * @param options A bag of options that control this resource's behavior.
     */
    public Permissions(java.lang.String name, PermissionsArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/permissions:Permissions", name, makeArgs(args, options), makeResourceOptions(options, Codegen.empty()), false);
    }

    private Permissions(java.lang.String name, Output<java.lang.String> id, @Nullable PermissionsState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        super("databricks:index/permissions:Permissions", name, state, makeResourceOptions(options, id), false);
    }

    private static PermissionsArgs makeArgs(PermissionsArgs args, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        if (options != null && options.getUrn().isPresent()) {
            return null;
        }
        return args == null ? PermissionsArgs.Empty : args;
    }

    private static com.pulumi.resources.CustomResourceOptions makeResourceOptions(@Nullable com.pulumi.resources.CustomResourceOptions options, @Nullable Output<java.lang.String> id) {
        var defaultOptions = com.pulumi.resources.CustomResourceOptions.builder()
            .version(Utilities.getVersion())
            .build();
        return com.pulumi.resources.CustomResourceOptions.merge(defaultOptions, options, id);
    }

    /**
     * Get an existing Host resource's state with the given name, ID, and optional extra
     * properties used to qualify the lookup.
     *
     * @param name The _unique_ name of the resulting resource.
     * @param id The _unique_ provider ID of the resource to lookup.
     * @param state
     * @param options Optional settings to control the behavior of the CustomResource.
     */
    public static Permissions get(java.lang.String name, Output<java.lang.String> id, @Nullable PermissionsState state, @Nullable com.pulumi.resources.CustomResourceOptions options) {
        return new Permissions(name, id, state, options);
    }
}
