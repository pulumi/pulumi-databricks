// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.annotations.Import;
import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
import com.pulumi.databricks.inputs.ClusterAwsAttributesArgs;
import com.pulumi.databricks.inputs.ClusterAzureAttributesArgs;
import com.pulumi.databricks.inputs.ClusterClusterLogConfArgs;
import com.pulumi.databricks.inputs.ClusterClusterMountInfoArgs;
import com.pulumi.databricks.inputs.ClusterDockerImageArgs;
import com.pulumi.databricks.inputs.ClusterDriverNodeTypeFlexibilityArgs;
import com.pulumi.databricks.inputs.ClusterGcpAttributesArgs;
import com.pulumi.databricks.inputs.ClusterInitScriptArgs;
import com.pulumi.databricks.inputs.ClusterLibraryArgs;
import com.pulumi.databricks.inputs.ClusterProviderConfigArgs;
import com.pulumi.databricks.inputs.ClusterWorkerNodeTypeFlexibilityArgs;
import com.pulumi.databricks.inputs.ClusterWorkloadTypeArgs;
import com.pulumi.exceptions.MissingRequiredPropertyException;
import java.lang.Boolean;
import java.lang.Integer;
import java.lang.String;
import java.util.List;
import java.util.Map;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;


public final class ClusterArgs extends com.pulumi.resources.ResourceArgs {

    public static final ClusterArgs Empty = new ClusterArgs();

    /**
     * Whether to use policy default values for missing cluster attributes.
     * 
     */
    @Import(name="applyPolicyDefaultValues")
    private @Nullable Output<Boolean> applyPolicyDefaultValues;

    /**
     * @return Whether to use policy default values for missing cluster attributes.
     * 
     */
    public Optional<Output<Boolean>> applyPolicyDefaultValues() {
        return Optional.ofNullable(this.applyPolicyDefaultValues);
    }

    @Import(name="autoscale")
    private @Nullable Output<ClusterAutoscaleArgs> autoscale;

    public Optional<Output<ClusterAutoscaleArgs>> autoscale() {
        return Optional.ofNullable(this.autoscale);
    }

    /**
     * Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
     * 
     */
    @Import(name="autoterminationMinutes")
    private @Nullable Output<Integer> autoterminationMinutes;

    /**
     * @return Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
     * 
     */
    public Optional<Output<Integer>> autoterminationMinutes() {
        return Optional.ofNullable(this.autoterminationMinutes);
    }

    @Import(name="awsAttributes")
    private @Nullable Output<ClusterAwsAttributesArgs> awsAttributes;

    public Optional<Output<ClusterAwsAttributesArgs>> awsAttributes() {
        return Optional.ofNullable(this.awsAttributes);
    }

    @Import(name="azureAttributes")
    private @Nullable Output<ClusterAzureAttributesArgs> azureAttributes;

    public Optional<Output<ClusterAzureAttributesArgs>> azureAttributes() {
        return Optional.ofNullable(this.azureAttributes);
    }

    @Import(name="clusterLogConf")
    private @Nullable Output<ClusterClusterLogConfArgs> clusterLogConf;

    public Optional<Output<ClusterClusterLogConfArgs>> clusterLogConf() {
        return Optional.ofNullable(this.clusterLogConf);
    }

    @Import(name="clusterMountInfos")
    private @Nullable Output<List<ClusterClusterMountInfoArgs>> clusterMountInfos;

    public Optional<Output<List<ClusterClusterMountInfoArgs>>> clusterMountInfos() {
        return Optional.ofNullable(this.clusterMountInfos);
    }

    /**
     * Cluster name, which doesn&#39;t have to be unique. If not specified at creation, the cluster name will be an empty string.
     * 
     */
    @Import(name="clusterName")
    private @Nullable Output<String> clusterName;

    /**
     * @return Cluster name, which doesn&#39;t have to be unique. If not specified at creation, the cluster name will be an empty string.
     * 
     */
    public Optional<Output<String>> clusterName() {
        return Optional.ofNullable(this.clusterName);
    }

    /**
     * should have tag `ResourceClass` set to value `Serverless`
     * 
     * For example:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var clusterWithTableAccessControl = new Cluster("clusterWithTableAccessControl", ClusterArgs.builder()
     *             .clusterName("Shared High-Concurrency")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .sparkConf(Map.ofEntries(
     *                 Map.entry("spark.databricks.repl.allowedLanguages", "python,sql"),
     *                 Map.entry("spark.databricks.cluster.profile", "serverless")
     *             ))
     *             .customTags(Map.of("ResourceClass", "Serverless"))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    @Import(name="customTags")
    private @Nullable Output<Map<String,String>> customTags;

    /**
     * @return should have tag `ResourceClass` set to value `Serverless`
     * 
     * For example:
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var clusterWithTableAccessControl = new Cluster("clusterWithTableAccessControl", ClusterArgs.builder()
     *             .clusterName("Shared High-Concurrency")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .sparkConf(Map.ofEntries(
     *                 Map.entry("spark.databricks.repl.allowedLanguages", "python,sql"),
     *                 Map.entry("spark.databricks.cluster.profile", "serverless")
     *             ))
     *             .customTags(Map.of("ResourceClass", "Serverless"))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public Optional<Output<Map<String,String>>> customTags() {
        return Optional.ofNullable(this.customTags);
    }

    /**
     * Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
     * * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
     * * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
     * * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
     * 
     */
    @Import(name="dataSecurityMode")
    private @Nullable Output<String> dataSecurityMode;

    /**
     * @return Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
     * * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
     * * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
     * * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
     * 
     */
    public Optional<Output<String>> dataSecurityMode() {
        return Optional.ofNullable(this.dataSecurityMode);
    }

    @Import(name="dockerImage")
    private @Nullable Output<ClusterDockerImageArgs> dockerImage;

    public Optional<Output<ClusterDockerImageArgs>> dockerImage() {
        return Optional.ofNullable(this.dockerImage);
    }

    /**
     * similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
     * 
     */
    @Import(name="driverInstancePoolId")
    private @Nullable Output<String> driverInstancePoolId;

    /**
     * @return similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
     * 
     */
    public Optional<Output<String>> driverInstancePoolId() {
        return Optional.ofNullable(this.driverInstancePoolId);
    }

    /**
     * a block describing the alternative driver node types if `driverNodeTypeId` isn&#39;t available.
     * 
     */
    @Import(name="driverNodeTypeFlexibility")
    private @Nullable Output<ClusterDriverNodeTypeFlexibilityArgs> driverNodeTypeFlexibility;

    /**
     * @return a block describing the alternative driver node types if `driverNodeTypeId` isn&#39;t available.
     * 
     */
    public Optional<Output<ClusterDriverNodeTypeFlexibilityArgs>> driverNodeTypeFlexibility() {
        return Optional.ofNullable(this.driverNodeTypeFlexibility);
    }

    /**
     * The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
     * 
     */
    @Import(name="driverNodeTypeId")
    private @Nullable Output<String> driverNodeTypeId;

    /**
     * @return The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
     * 
     */
    public Optional<Output<String>> driverNodeTypeId() {
        return Optional.ofNullable(this.driverNodeTypeId);
    }

    /**
     * If you don&#39;t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster&#39;s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance&#39;s local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
     * 
     */
    @Import(name="enableElasticDisk")
    private @Nullable Output<Boolean> enableElasticDisk;

    /**
     * @return If you don&#39;t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster&#39;s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance&#39;s local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
     * 
     */
    public Optional<Output<Boolean>> enableElasticDisk() {
        return Optional.ofNullable(this.enableElasticDisk);
    }

    /**
     * Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster&#39;s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
     * 
     */
    @Import(name="enableLocalDiskEncryption")
    private @Nullable Output<Boolean> enableLocalDiskEncryption;

    /**
     * @return Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster&#39;s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
     * 
     */
    public Optional<Output<Boolean>> enableLocalDiskEncryption() {
        return Optional.ofNullable(this.enableLocalDiskEncryption);
    }

    @Import(name="gcpAttributes")
    private @Nullable Output<ClusterGcpAttributesArgs> gcpAttributes;

    public Optional<Output<ClusterGcpAttributesArgs>> gcpAttributes() {
        return Optional.ofNullable(this.gcpAttributes);
    }

    /**
     * An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster&#39;s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
     * 
     */
    @Import(name="idempotencyToken")
    private @Nullable Output<String> idempotencyToken;

    /**
     * @return An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster&#39;s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
     * 
     */
    public Optional<Output<String>> idempotencyToken() {
        return Optional.ofNullable(this.idempotencyToken);
    }

    @Import(name="initScripts")
    private @Nullable Output<List<ClusterInitScriptArgs>> initScripts;

    public Optional<Output<List<ClusterInitScriptArgs>>> initScripts() {
        return Optional.ofNullable(this.initScripts);
    }

    /**
     * To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster&#39;s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
     * 
     */
    @Import(name="instancePoolId")
    private @Nullable Output<String> instancePoolId;

    /**
     * @return To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster&#39;s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
     * 
     */
    public Optional<Output<String>> instancePoolId() {
        return Optional.ofNullable(this.instancePoolId);
    }

    /**
     * boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters&#39; maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
     * 
     */
    @Import(name="isPinned")
    private @Nullable Output<Boolean> isPinned;

    /**
     * @return boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters&#39; maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
     * 
     */
    public Optional<Output<Boolean>> isPinned() {
        return Optional.ofNullable(this.isPinned);
    }

    /**
     * When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
     * 
     */
    @Import(name="isSingleNode")
    private @Nullable Output<Boolean> isSingleNode;

    /**
     * @return When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
     * 
     */
    public Optional<Output<Boolean>> isSingleNode() {
        return Optional.ofNullable(this.isSingleNode);
    }

    /**
     * The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
     * 
     */
    @Import(name="kind")
    private @Nullable Output<String> kind;

    /**
     * @return The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
     * 
     */
    public Optional<Output<String>> kind() {
        return Optional.ofNullable(this.kind);
    }

    @Import(name="libraries")
    private @Nullable Output<List<ClusterLibraryArgs>> libraries;

    public Optional<Output<List<ClusterLibraryArgs>>> libraries() {
        return Optional.ofNullable(this.libraries);
    }

    /**
     * If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
     * 
     */
    @Import(name="noWait")
    private @Nullable Output<Boolean> noWait;

    /**
     * @return If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
     * 
     */
    public Optional<Output<Boolean>> noWait() {
        return Optional.ofNullable(this.noWait);
    }

    /**
     * Any supported databricks.getNodeType id. If `instancePoolId` is specified, this field is not needed.
     * 
     */
    @Import(name="nodeTypeId")
    private @Nullable Output<String> nodeTypeId;

    /**
     * @return Any supported databricks.getNodeType id. If `instancePoolId` is specified, this field is not needed.
     * 
     */
    public Optional<Output<String>> nodeTypeId() {
        return Optional.ofNullable(this.nodeTypeId);
    }

    /**
     * Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
     * 
     */
    @Import(name="numWorkers")
    private @Nullable Output<Integer> numWorkers;

    /**
     * @return Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
     * 
     */
    public Optional<Output<Integer>> numWorkers() {
        return Optional.ofNullable(this.numWorkers);
    }

    /**
     * Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren&#39;t filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
     * 
     */
    @Import(name="policyId")
    private @Nullable Output<String> policyId;

    /**
     * @return Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren&#39;t filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
     * 
     */
    public Optional<Output<String>> policyId() {
        return Optional.ofNullable(this.policyId);
    }

    /**
     * Configure the provider for management through account provider. This block consists of the following fields:
     * 
     */
    @Import(name="providerConfig")
    private @Nullable Output<ClusterProviderConfigArgs> providerConfig;

    /**
     * @return Configure the provider for management through account provider. This block consists of the following fields:
     * 
     */
    public Optional<Output<ClusterProviderConfigArgs>> providerConfig() {
        return Optional.ofNullable(this.providerConfig);
    }

    @Import(name="remoteDiskThroughput")
    private @Nullable Output<Integer> remoteDiskThroughput;

    public Optional<Output<Integer>> remoteDiskThroughput() {
        return Optional.ofNullable(this.remoteDiskThroughput);
    }

    /**
     * The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
     * 
     */
    @Import(name="runtimeEngine")
    private @Nullable Output<String> runtimeEngine;

    /**
     * @return The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
     * 
     */
    public Optional<Output<String>> runtimeEngine() {
        return Optional.ofNullable(this.runtimeEngine);
    }

    /**
     * The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     * 
     */
    @Import(name="singleUserName")
    private @Nullable Output<String> singleUserName;

    /**
     * @return The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
     * 
     */
    public Optional<Output<String>> singleUserName() {
        return Optional.ofNullable(this.singleUserName);
    }

    /**
     * should have following items:
     * * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
     * * `spark.databricks.cluster.profile` set to `serverless`
     * 
     */
    @Import(name="sparkConf")
    private @Nullable Output<Map<String,String>> sparkConf;

    /**
     * @return should have following items:
     * * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
     * * `spark.databricks.cluster.profile` set to `serverless`
     * 
     */
    public Optional<Output<Map<String,String>>> sparkConf() {
        return Optional.ofNullable(this.sparkConf);
    }

    /**
     * Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=&#39;Y&#39;) while launching the driver and workers.
     * 
     */
    @Import(name="sparkEnvVars")
    private @Nullable Output<Map<String,String>> sparkEnvVars;

    /**
     * @return Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=&#39;Y&#39;) while launching the driver and workers.
     * 
     */
    public Optional<Output<Map<String,String>>> sparkEnvVars() {
        return Optional.ofNullable(this.sparkEnvVars);
    }

    /**
     * [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
     * 
     */
    @Import(name="sparkVersion", required=true)
    private Output<String> sparkVersion;

    /**
     * @return [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
     * 
     */
    public Output<String> sparkVersion() {
        return this.sparkVersion;
    }

    /**
     * SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
     * 
     */
    @Import(name="sshPublicKeys")
    private @Nullable Output<List<String>> sshPublicKeys;

    /**
     * @return SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
     * 
     */
    public Optional<Output<List<String>>> sshPublicKeys() {
        return Optional.ofNullable(this.sshPublicKeys);
    }

    @Import(name="totalInitialRemoteDiskSize")
    private @Nullable Output<Integer> totalInitialRemoteDiskSize;

    public Optional<Output<Integer>> totalInitialRemoteDiskSize() {
        return Optional.ofNullable(this.totalInitialRemoteDiskSize);
    }

    /**
     * Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
     * 
     */
    @Import(name="useMlRuntime")
    private @Nullable Output<Boolean> useMlRuntime;

    /**
     * @return Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
     * 
     */
    public Optional<Output<Boolean>> useMlRuntime() {
        return Optional.ofNullable(this.useMlRuntime);
    }

    /**
     * a block describing the alternative driver node types if `nodeTypeId` isn&#39;t available.
     * 
     */
    @Import(name="workerNodeTypeFlexibility")
    private @Nullable Output<ClusterWorkerNodeTypeFlexibilityArgs> workerNodeTypeFlexibility;

    /**
     * @return a block describing the alternative driver node types if `nodeTypeId` isn&#39;t available.
     * 
     */
    public Optional<Output<ClusterWorkerNodeTypeFlexibilityArgs>> workerNodeTypeFlexibility() {
        return Optional.ofNullable(this.workerNodeTypeFlexibility);
    }

    @Import(name="workloadType")
    private @Nullable Output<ClusterWorkloadTypeArgs> workloadType;

    public Optional<Output<ClusterWorkloadTypeArgs>> workloadType() {
        return Optional.ofNullable(this.workloadType);
    }

    private ClusterArgs() {}

    private ClusterArgs(ClusterArgs $) {
        this.applyPolicyDefaultValues = $.applyPolicyDefaultValues;
        this.autoscale = $.autoscale;
        this.autoterminationMinutes = $.autoterminationMinutes;
        this.awsAttributes = $.awsAttributes;
        this.azureAttributes = $.azureAttributes;
        this.clusterLogConf = $.clusterLogConf;
        this.clusterMountInfos = $.clusterMountInfos;
        this.clusterName = $.clusterName;
        this.customTags = $.customTags;
        this.dataSecurityMode = $.dataSecurityMode;
        this.dockerImage = $.dockerImage;
        this.driverInstancePoolId = $.driverInstancePoolId;
        this.driverNodeTypeFlexibility = $.driverNodeTypeFlexibility;
        this.driverNodeTypeId = $.driverNodeTypeId;
        this.enableElasticDisk = $.enableElasticDisk;
        this.enableLocalDiskEncryption = $.enableLocalDiskEncryption;
        this.gcpAttributes = $.gcpAttributes;
        this.idempotencyToken = $.idempotencyToken;
        this.initScripts = $.initScripts;
        this.instancePoolId = $.instancePoolId;
        this.isPinned = $.isPinned;
        this.isSingleNode = $.isSingleNode;
        this.kind = $.kind;
        this.libraries = $.libraries;
        this.noWait = $.noWait;
        this.nodeTypeId = $.nodeTypeId;
        this.numWorkers = $.numWorkers;
        this.policyId = $.policyId;
        this.providerConfig = $.providerConfig;
        this.remoteDiskThroughput = $.remoteDiskThroughput;
        this.runtimeEngine = $.runtimeEngine;
        this.singleUserName = $.singleUserName;
        this.sparkConf = $.sparkConf;
        this.sparkEnvVars = $.sparkEnvVars;
        this.sparkVersion = $.sparkVersion;
        this.sshPublicKeys = $.sshPublicKeys;
        this.totalInitialRemoteDiskSize = $.totalInitialRemoteDiskSize;
        this.useMlRuntime = $.useMlRuntime;
        this.workerNodeTypeFlexibility = $.workerNodeTypeFlexibility;
        this.workloadType = $.workloadType;
    }

    public static Builder builder() {
        return new Builder();
    }
    public static Builder builder(ClusterArgs defaults) {
        return new Builder(defaults);
    }

    public static final class Builder {
        private ClusterArgs $;

        public Builder() {
            $ = new ClusterArgs();
        }

        public Builder(ClusterArgs defaults) {
            $ = new ClusterArgs(Objects.requireNonNull(defaults));
        }

        /**
         * @param applyPolicyDefaultValues Whether to use policy default values for missing cluster attributes.
         * 
         * @return builder
         * 
         */
        public Builder applyPolicyDefaultValues(@Nullable Output<Boolean> applyPolicyDefaultValues) {
            $.applyPolicyDefaultValues = applyPolicyDefaultValues;
            return this;
        }

        /**
         * @param applyPolicyDefaultValues Whether to use policy default values for missing cluster attributes.
         * 
         * @return builder
         * 
         */
        public Builder applyPolicyDefaultValues(Boolean applyPolicyDefaultValues) {
            return applyPolicyDefaultValues(Output.of(applyPolicyDefaultValues));
        }

        public Builder autoscale(@Nullable Output<ClusterAutoscaleArgs> autoscale) {
            $.autoscale = autoscale;
            return this;
        }

        public Builder autoscale(ClusterAutoscaleArgs autoscale) {
            return autoscale(Output.of(autoscale));
        }

        /**
         * @param autoterminationMinutes Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
         * 
         * @return builder
         * 
         */
        public Builder autoterminationMinutes(@Nullable Output<Integer> autoterminationMinutes) {
            $.autoterminationMinutes = autoterminationMinutes;
            return this;
        }

        /**
         * @param autoterminationMinutes Automatically terminate the cluster after being inactive for this time in minutes. If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination. Defaults to `60`.  *We highly recommend having this setting present for Interactive/BI clusters.*
         * 
         * @return builder
         * 
         */
        public Builder autoterminationMinutes(Integer autoterminationMinutes) {
            return autoterminationMinutes(Output.of(autoterminationMinutes));
        }

        public Builder awsAttributes(@Nullable Output<ClusterAwsAttributesArgs> awsAttributes) {
            $.awsAttributes = awsAttributes;
            return this;
        }

        public Builder awsAttributes(ClusterAwsAttributesArgs awsAttributes) {
            return awsAttributes(Output.of(awsAttributes));
        }

        public Builder azureAttributes(@Nullable Output<ClusterAzureAttributesArgs> azureAttributes) {
            $.azureAttributes = azureAttributes;
            return this;
        }

        public Builder azureAttributes(ClusterAzureAttributesArgs azureAttributes) {
            return azureAttributes(Output.of(azureAttributes));
        }

        public Builder clusterLogConf(@Nullable Output<ClusterClusterLogConfArgs> clusterLogConf) {
            $.clusterLogConf = clusterLogConf;
            return this;
        }

        public Builder clusterLogConf(ClusterClusterLogConfArgs clusterLogConf) {
            return clusterLogConf(Output.of(clusterLogConf));
        }

        public Builder clusterMountInfos(@Nullable Output<List<ClusterClusterMountInfoArgs>> clusterMountInfos) {
            $.clusterMountInfos = clusterMountInfos;
            return this;
        }

        public Builder clusterMountInfos(List<ClusterClusterMountInfoArgs> clusterMountInfos) {
            return clusterMountInfos(Output.of(clusterMountInfos));
        }

        public Builder clusterMountInfos(ClusterClusterMountInfoArgs... clusterMountInfos) {
            return clusterMountInfos(List.of(clusterMountInfos));
        }

        /**
         * @param clusterName Cluster name, which doesn&#39;t have to be unique. If not specified at creation, the cluster name will be an empty string.
         * 
         * @return builder
         * 
         */
        public Builder clusterName(@Nullable Output<String> clusterName) {
            $.clusterName = clusterName;
            return this;
        }

        /**
         * @param clusterName Cluster name, which doesn&#39;t have to be unique. If not specified at creation, the cluster name will be an empty string.
         * 
         * @return builder
         * 
         */
        public Builder clusterName(String clusterName) {
            return clusterName(Output.of(clusterName));
        }

        /**
         * @param customTags should have tag `ResourceClass` set to value `Serverless`
         * 
         * For example:
         * 
         * <pre>
         * {@code
         * package generated_program;
         * 
         * import com.pulumi.Context;
         * import com.pulumi.Pulumi;
         * import com.pulumi.core.Output;
         * import com.pulumi.databricks.Cluster;
         * import com.pulumi.databricks.ClusterArgs;
         * import java.util.List;
         * import java.util.ArrayList;
         * import java.util.Map;
         * import java.io.File;
         * import java.nio.file.Files;
         * import java.nio.file.Paths;
         * 
         * public class App {
         *     public static void main(String[] args) {
         *         Pulumi.run(App::stack);
         *     }
         * 
         *     public static void stack(Context ctx) {
         *         var clusterWithTableAccessControl = new Cluster("clusterWithTableAccessControl", ClusterArgs.builder()
         *             .clusterName("Shared High-Concurrency")
         *             .sparkVersion(latestLts.id())
         *             .nodeTypeId(smallest.id())
         *             .autoterminationMinutes(20)
         *             .sparkConf(Map.ofEntries(
         *                 Map.entry("spark.databricks.repl.allowedLanguages", "python,sql"),
         *                 Map.entry("spark.databricks.cluster.profile", "serverless")
         *             ))
         *             .customTags(Map.of("ResourceClass", "Serverless"))
         *             .build());
         * 
         *     }
         * }
         * }
         * </pre>
         * 
         * @return builder
         * 
         */
        public Builder customTags(@Nullable Output<Map<String,String>> customTags) {
            $.customTags = customTags;
            return this;
        }

        /**
         * @param customTags should have tag `ResourceClass` set to value `Serverless`
         * 
         * For example:
         * 
         * <pre>
         * {@code
         * package generated_program;
         * 
         * import com.pulumi.Context;
         * import com.pulumi.Pulumi;
         * import com.pulumi.core.Output;
         * import com.pulumi.databricks.Cluster;
         * import com.pulumi.databricks.ClusterArgs;
         * import java.util.List;
         * import java.util.ArrayList;
         * import java.util.Map;
         * import java.io.File;
         * import java.nio.file.Files;
         * import java.nio.file.Paths;
         * 
         * public class App {
         *     public static void main(String[] args) {
         *         Pulumi.run(App::stack);
         *     }
         * 
         *     public static void stack(Context ctx) {
         *         var clusterWithTableAccessControl = new Cluster("clusterWithTableAccessControl", ClusterArgs.builder()
         *             .clusterName("Shared High-Concurrency")
         *             .sparkVersion(latestLts.id())
         *             .nodeTypeId(smallest.id())
         *             .autoterminationMinutes(20)
         *             .sparkConf(Map.ofEntries(
         *                 Map.entry("spark.databricks.repl.allowedLanguages", "python,sql"),
         *                 Map.entry("spark.databricks.cluster.profile", "serverless")
         *             ))
         *             .customTags(Map.of("ResourceClass", "Serverless"))
         *             .build());
         * 
         *     }
         * }
         * }
         * </pre>
         * 
         * @return builder
         * 
         */
        public Builder customTags(Map<String,String> customTags) {
            return customTags(Output.of(customTags));
        }

        /**
         * @param dataSecurityMode Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
         * * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
         * * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
         * * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
         * 
         * @return builder
         * 
         */
        public Builder dataSecurityMode(@Nullable Output<String> dataSecurityMode) {
            $.dataSecurityMode = dataSecurityMode;
            return this;
        }

        /**
         * @param dataSecurityMode Select the security features of the cluster (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#data_security_mode) for full list of values). [Unity Catalog requires](https://docs.databricks.com/data-governance/unity-catalog/compute.html#create-clusters--sql-warehouses-with-unity-catalog-access) `SINGLE_USER` or `USER_ISOLATION` mode. `LEGACY_PASSTHROUGH` for passthrough cluster and `LEGACY_TABLE_ACL` for Table ACL cluster. If omitted, default security features are enabled. To disable security features use `NONE` or legacy mode `NO_ISOLATION`.  If `kind` is specified, then the following options are available:
         * * `DATA_SECURITY_MODE_AUTO`: Databricks will choose the most appropriate access mode depending on your compute configuration.
         * * `DATA_SECURITY_MODE_STANDARD`: Alias for `USER_ISOLATION`.
         * * `DATA_SECURITY_MODE_DEDICATED`: Alias for `SINGLE_USER`.
         * 
         * @return builder
         * 
         */
        public Builder dataSecurityMode(String dataSecurityMode) {
            return dataSecurityMode(Output.of(dataSecurityMode));
        }

        public Builder dockerImage(@Nullable Output<ClusterDockerImageArgs> dockerImage) {
            $.dockerImage = dockerImage;
            return this;
        }

        public Builder dockerImage(ClusterDockerImageArgs dockerImage) {
            return dockerImage(Output.of(dockerImage));
        }

        /**
         * @param driverInstancePoolId similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
         * 
         * @return builder
         * 
         */
        public Builder driverInstancePoolId(@Nullable Output<String> driverInstancePoolId) {
            $.driverInstancePoolId = driverInstancePoolId;
            return this;
        }

        /**
         * @param driverInstancePoolId similar to `instancePoolId`, but for driver node. If omitted, and `instancePoolId` is specified, then the driver will be allocated from that pool.
         * 
         * @return builder
         * 
         */
        public Builder driverInstancePoolId(String driverInstancePoolId) {
            return driverInstancePoolId(Output.of(driverInstancePoolId));
        }

        /**
         * @param driverNodeTypeFlexibility a block describing the alternative driver node types if `driverNodeTypeId` isn&#39;t available.
         * 
         * @return builder
         * 
         */
        public Builder driverNodeTypeFlexibility(@Nullable Output<ClusterDriverNodeTypeFlexibilityArgs> driverNodeTypeFlexibility) {
            $.driverNodeTypeFlexibility = driverNodeTypeFlexibility;
            return this;
        }

        /**
         * @param driverNodeTypeFlexibility a block describing the alternative driver node types if `driverNodeTypeId` isn&#39;t available.
         * 
         * @return builder
         * 
         */
        public Builder driverNodeTypeFlexibility(ClusterDriverNodeTypeFlexibilityArgs driverNodeTypeFlexibility) {
            return driverNodeTypeFlexibility(Output.of(driverNodeTypeFlexibility));
        }

        /**
         * @param driverNodeTypeId The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
         * 
         * @return builder
         * 
         */
        public Builder driverNodeTypeId(@Nullable Output<String> driverNodeTypeId) {
            $.driverNodeTypeId = driverNodeTypeId;
            return this;
        }

        /**
         * @param driverNodeTypeId The node type of the Spark driver. This field is optional; if unset, API will set the driver node type to the same value as `nodeTypeId` defined above.
         * 
         * @return builder
         * 
         */
        public Builder driverNodeTypeId(String driverNodeTypeId) {
            return driverNodeTypeId(Output.of(driverNodeTypeId));
        }

        /**
         * @param enableElasticDisk If you don&#39;t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster&#39;s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance&#39;s local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
         * 
         * @return builder
         * 
         */
        public Builder enableElasticDisk(@Nullable Output<Boolean> enableElasticDisk) {
            $.enableElasticDisk = enableElasticDisk;
            return this;
        }

        /**
         * @param enableElasticDisk If you don&#39;t want to allocate a fixed number of EBS volumes at cluster creation time, use autoscaling local storage. With autoscaling local storage, Databricks monitors the amount of free disk space available on your cluster&#39;s Spark workers. If a worker begins to run too low on disk, Databricks automatically attaches a new EBS volume to the worker before it runs out of disk space. EBS volumes are attached up to a limit of 5 TB of total disk space per instance (including the instance&#39;s local storage). To scale down EBS usage, make sure you have `autoterminationMinutes` and `autoscale` attributes set. More documentation available at [cluster configuration page](https://docs.databricks.com/clusters/configure.html#autoscaling-local-storage-1).
         * 
         * @return builder
         * 
         */
        public Builder enableElasticDisk(Boolean enableElasticDisk) {
            return enableElasticDisk(Output.of(enableElasticDisk));
        }

        /**
         * @param enableLocalDiskEncryption Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster&#39;s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
         * 
         * @return builder
         * 
         */
        public Builder enableLocalDiskEncryption(@Nullable Output<Boolean> enableLocalDiskEncryption) {
            $.enableLocalDiskEncryption = enableLocalDiskEncryption;
            return this;
        }

        /**
         * @param enableLocalDiskEncryption Some instance types you use to run clusters may have locally attached disks. Databricks may store shuffle data or temporary data on these locally attached disks. To ensure that all data at rest is encrypted for all storage types, including shuffle data stored temporarily on your cluster&#39;s local disks, you can enable local disk encryption. When local disk encryption is enabled, Databricks generates an encryption key locally unique to each cluster node and uses it to encrypt all data stored on local disks. The scope of the key is local to each cluster node and is destroyed along with the cluster node itself. During its lifetime, the key resides in memory for encryption and decryption and is stored encrypted on the disk. *Your workloads may run more slowly because of the performance impact of reading and writing encrypted data to and from local volumes. This feature is not available for all Azure Databricks subscriptions. Contact your Microsoft or Databricks account representative to request access.*
         * 
         * @return builder
         * 
         */
        public Builder enableLocalDiskEncryption(Boolean enableLocalDiskEncryption) {
            return enableLocalDiskEncryption(Output.of(enableLocalDiskEncryption));
        }

        public Builder gcpAttributes(@Nullable Output<ClusterGcpAttributesArgs> gcpAttributes) {
            $.gcpAttributes = gcpAttributes;
            return this;
        }

        public Builder gcpAttributes(ClusterGcpAttributesArgs gcpAttributes) {
            return gcpAttributes(Output.of(gcpAttributes));
        }

        /**
         * @param idempotencyToken An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster&#39;s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
         * 
         * @return builder
         * 
         */
        public Builder idempotencyToken(@Nullable Output<String> idempotencyToken) {
            $.idempotencyToken = idempotencyToken;
            return this;
        }

        /**
         * @param idempotencyToken An optional token to guarantee the idempotency of cluster creation requests. If an active cluster with the provided token already exists, the request will not create a new cluster, but it will return the existing running cluster&#39;s ID instead. If you specify the idempotency token, upon failure, you can retry until the request succeeds. Databricks platform guarantees to launch exactly one cluster with that idempotency token. This token should have at most 64 characters.
         * 
         * @return builder
         * 
         */
        public Builder idempotencyToken(String idempotencyToken) {
            return idempotencyToken(Output.of(idempotencyToken));
        }

        public Builder initScripts(@Nullable Output<List<ClusterInitScriptArgs>> initScripts) {
            $.initScripts = initScripts;
            return this;
        }

        public Builder initScripts(List<ClusterInitScriptArgs> initScripts) {
            return initScripts(Output.of(initScripts));
        }

        public Builder initScripts(ClusterInitScriptArgs... initScripts) {
            return initScripts(List.of(initScripts));
        }

        /**
         * @param instancePoolId To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster&#39;s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
         * 
         * @return builder
         * 
         */
        public Builder instancePoolId(@Nullable Output<String> instancePoolId) {
            $.instancePoolId = instancePoolId;
            return this;
        }

        /**
         * @param instancePoolId To reduce cluster start time, you can attach a cluster to a predefined pool of idle instances. When attached to a pool, a cluster allocates its driver and worker nodes from the pool. If the pool does not have sufficient idle resources to accommodate the cluster&#39;s request, it expands by allocating new instances from the instance provider. When an attached cluster changes its state to `TERMINATED`, the instances it used are returned to the pool and reused by a different cluster.
         * 
         * @return builder
         * 
         */
        public Builder instancePoolId(String instancePoolId) {
            return instancePoolId(Output.of(instancePoolId));
        }

        /**
         * @param isPinned boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters&#39; maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
         * 
         * @return builder
         * 
         */
        public Builder isPinned(@Nullable Output<Boolean> isPinned) {
            $.isPinned = isPinned;
            return this;
        }

        /**
         * @param isPinned boolean value specifying if the cluster is pinned (not pinned by default). You must be a Databricks administrator to use this.  The pinned clusters&#39; maximum number is [limited to 100](https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster), so `apply` may fail if you have more than that (this number may change over time, so check Databricks documentation for actual number).
         * 
         * @return builder
         * 
         */
        public Builder isPinned(Boolean isPinned) {
            return isPinned(Output.of(isPinned));
        }

        /**
         * @param isSingleNode When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
         * 
         * @return builder
         * 
         */
        public Builder isSingleNode(@Nullable Output<Boolean> isSingleNode) {
            $.isSingleNode = isSingleNode;
            return this;
        }

        /**
         * @param isSingleNode When set to true, Databricks will automatically set single node related `customTags`, `sparkConf`, and `numWorkers`.
         * 
         * @return builder
         * 
         */
        public Builder isSingleNode(Boolean isSingleNode) {
            return isSingleNode(Output.of(isSingleNode));
        }

        /**
         * @param kind The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
         * 
         * @return builder
         * 
         */
        public Builder kind(@Nullable Output<String> kind) {
            $.kind = kind;
            return this;
        }

        /**
         * @param kind The kind of compute described by this compute specification.  Possible values (see [API docs](https://docs.databricks.com/api/workspace/clusters/create#kind) for full list): `CLASSIC_PREVIEW` (if corresponding public preview is enabled).
         * 
         * @return builder
         * 
         */
        public Builder kind(String kind) {
            return kind(Output.of(kind));
        }

        public Builder libraries(@Nullable Output<List<ClusterLibraryArgs>> libraries) {
            $.libraries = libraries;
            return this;
        }

        public Builder libraries(List<ClusterLibraryArgs> libraries) {
            return libraries(Output.of(libraries));
        }

        public Builder libraries(ClusterLibraryArgs... libraries) {
            return libraries(List.of(libraries));
        }

        /**
         * @param noWait If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
         * 
         * @return builder
         * 
         */
        public Builder noWait(@Nullable Output<Boolean> noWait) {
            $.noWait = noWait;
            return this;
        }

        /**
         * @param noWait If true, the provider will not wait for the cluster to reach `RUNNING` state when creating the cluster, allowing cluster creation and library installation to continue asynchronously. Defaults to false (the provider will wait for cluster creation and library installation to succeed).
         * 
         * @return builder
         * 
         */
        public Builder noWait(Boolean noWait) {
            return noWait(Output.of(noWait));
        }

        /**
         * @param nodeTypeId Any supported databricks.getNodeType id. If `instancePoolId` is specified, this field is not needed.
         * 
         * @return builder
         * 
         */
        public Builder nodeTypeId(@Nullable Output<String> nodeTypeId) {
            $.nodeTypeId = nodeTypeId;
            return this;
        }

        /**
         * @param nodeTypeId Any supported databricks.getNodeType id. If `instancePoolId` is specified, this field is not needed.
         * 
         * @return builder
         * 
         */
        public Builder nodeTypeId(String nodeTypeId) {
            return nodeTypeId(Output.of(nodeTypeId));
        }

        /**
         * @param numWorkers Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
         * 
         * @return builder
         * 
         */
        public Builder numWorkers(@Nullable Output<Integer> numWorkers) {
            $.numWorkers = numWorkers;
            return this;
        }

        /**
         * @param numWorkers Number of worker nodes that this cluster should have. A cluster has one Spark driver and `numWorkers` executors for a total of `numWorkers` + 1 Spark nodes.
         * 
         * @return builder
         * 
         */
        public Builder numWorkers(Integer numWorkers) {
            return numWorkers(Output.of(numWorkers));
        }

        /**
         * @param policyId Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren&#39;t filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
         * 
         * @return builder
         * 
         */
        public Builder policyId(@Nullable Output<String> policyId) {
            $.policyId = policyId;
            return this;
        }

        /**
         * @param policyId Identifier of Cluster Policy to validate cluster and preset certain defaults. *The primary use for cluster policies is to allow users to create policy-scoped clusters via UI rather than sharing configuration for API-created clusters.* For example, when you specify `policyId` of [external metastore](https://docs.databricks.com/administration-guide/clusters/policies.html#external-metastore-policy) policy, you still have to fill in relevant keys for `sparkConf`.  If relevant fields aren&#39;t filled in, then it will cause the configuration drift detected on each plan/apply, and Pulumi will try to apply the detected changes.
         * 
         * @return builder
         * 
         */
        public Builder policyId(String policyId) {
            return policyId(Output.of(policyId));
        }

        /**
         * @param providerConfig Configure the provider for management through account provider. This block consists of the following fields:
         * 
         * @return builder
         * 
         */
        public Builder providerConfig(@Nullable Output<ClusterProviderConfigArgs> providerConfig) {
            $.providerConfig = providerConfig;
            return this;
        }

        /**
         * @param providerConfig Configure the provider for management through account provider. This block consists of the following fields:
         * 
         * @return builder
         * 
         */
        public Builder providerConfig(ClusterProviderConfigArgs providerConfig) {
            return providerConfig(Output.of(providerConfig));
        }

        public Builder remoteDiskThroughput(@Nullable Output<Integer> remoteDiskThroughput) {
            $.remoteDiskThroughput = remoteDiskThroughput;
            return this;
        }

        public Builder remoteDiskThroughput(Integer remoteDiskThroughput) {
            return remoteDiskThroughput(Output.of(remoteDiskThroughput));
        }

        /**
         * @param runtimeEngine The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
         * 
         * @return builder
         * 
         */
        public Builder runtimeEngine(@Nullable Output<String> runtimeEngine) {
            $.runtimeEngine = runtimeEngine;
            return this;
        }

        /**
         * @param runtimeEngine The type of runtime engine to use. If not specified, the runtime engine type is inferred based on the sparkVersion value. Allowed values include: `PHOTON`, `STANDARD`.
         * 
         * @return builder
         * 
         */
        public Builder runtimeEngine(String runtimeEngine) {
            return runtimeEngine(Output.of(runtimeEngine));
        }

        /**
         * @param singleUserName The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
         * 
         * @return builder
         * 
         */
        public Builder singleUserName(@Nullable Output<String> singleUserName) {
            $.singleUserName = singleUserName;
            return this;
        }

        /**
         * @param singleUserName The optional user name of the user (or group name if `kind` if specified) to assign to an interactive cluster. This field is required when using `dataSecurityMode` set to `SINGLE_USER` or AAD Passthrough for Azure Data Lake Storage (ADLS) with a single-user cluster (i.e., not high-concurrency clusters).
         * 
         * @return builder
         * 
         */
        public Builder singleUserName(String singleUserName) {
            return singleUserName(Output.of(singleUserName));
        }

        /**
         * @param sparkConf should have following items:
         * * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
         * * `spark.databricks.cluster.profile` set to `serverless`
         * 
         * @return builder
         * 
         */
        public Builder sparkConf(@Nullable Output<Map<String,String>> sparkConf) {
            $.sparkConf = sparkConf;
            return this;
        }

        /**
         * @param sparkConf should have following items:
         * * `spark.databricks.repl.allowedLanguages` set to a list of supported languages, for example: `python,sql`, or `python,sql,r`.  Scala is not supported!
         * * `spark.databricks.cluster.profile` set to `serverless`
         * 
         * @return builder
         * 
         */
        public Builder sparkConf(Map<String,String> sparkConf) {
            return sparkConf(Output.of(sparkConf));
        }

        /**
         * @param sparkEnvVars Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=&#39;Y&#39;) while launching the driver and workers.
         * 
         * @return builder
         * 
         */
        public Builder sparkEnvVars(@Nullable Output<Map<String,String>> sparkEnvVars) {
            $.sparkEnvVars = sparkEnvVars;
            return this;
        }

        /**
         * @param sparkEnvVars Map with environment variable key-value pairs to fine-tune Spark clusters. Key-value pairs of the form (X,Y) are exported (i.e., X=&#39;Y&#39;) while launching the driver and workers.
         * 
         * @return builder
         * 
         */
        public Builder sparkEnvVars(Map<String,String> sparkEnvVars) {
            return sparkEnvVars(Output.of(sparkEnvVars));
        }

        /**
         * @param sparkVersion [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
         * 
         * @return builder
         * 
         */
        public Builder sparkVersion(Output<String> sparkVersion) {
            $.sparkVersion = sparkVersion;
            return this;
        }

        /**
         * @param sparkVersion [Runtime version](https://docs.databricks.com/runtime/index.html) of the cluster. Any supported databricks.getSparkVersion id.  We advise using Cluster Policies to restrict the list of versions for simplicity while maintaining enough control.
         * 
         * @return builder
         * 
         */
        public Builder sparkVersion(String sparkVersion) {
            return sparkVersion(Output.of(sparkVersion));
        }

        /**
         * @param sshPublicKeys SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
         * 
         * @return builder
         * 
         */
        public Builder sshPublicKeys(@Nullable Output<List<String>> sshPublicKeys) {
            $.sshPublicKeys = sshPublicKeys;
            return this;
        }

        /**
         * @param sshPublicKeys SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
         * 
         * @return builder
         * 
         */
        public Builder sshPublicKeys(List<String> sshPublicKeys) {
            return sshPublicKeys(Output.of(sshPublicKeys));
        }

        /**
         * @param sshPublicKeys SSH public key contents that will be added to each Spark node in this cluster. The corresponding private keys can be used to login with the user name ubuntu on port 2200. You can specify up to 10 keys.
         * 
         * @return builder
         * 
         */
        public Builder sshPublicKeys(String... sshPublicKeys) {
            return sshPublicKeys(List.of(sshPublicKeys));
        }

        public Builder totalInitialRemoteDiskSize(@Nullable Output<Integer> totalInitialRemoteDiskSize) {
            $.totalInitialRemoteDiskSize = totalInitialRemoteDiskSize;
            return this;
        }

        public Builder totalInitialRemoteDiskSize(Integer totalInitialRemoteDiskSize) {
            return totalInitialRemoteDiskSize(Output.of(totalInitialRemoteDiskSize));
        }

        /**
         * @param useMlRuntime Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
         * 
         * @return builder
         * 
         */
        public Builder useMlRuntime(@Nullable Output<Boolean> useMlRuntime) {
            $.useMlRuntime = useMlRuntime;
            return this;
        }

        /**
         * @param useMlRuntime Whenever ML runtime should be selected or not.  Actual runtime is determined by `sparkVersion` (DBR release), this field `useMlRuntime`, and whether `nodeTypeId` is GPU node or not.
         * 
         * @return builder
         * 
         */
        public Builder useMlRuntime(Boolean useMlRuntime) {
            return useMlRuntime(Output.of(useMlRuntime));
        }

        /**
         * @param workerNodeTypeFlexibility a block describing the alternative driver node types if `nodeTypeId` isn&#39;t available.
         * 
         * @return builder
         * 
         */
        public Builder workerNodeTypeFlexibility(@Nullable Output<ClusterWorkerNodeTypeFlexibilityArgs> workerNodeTypeFlexibility) {
            $.workerNodeTypeFlexibility = workerNodeTypeFlexibility;
            return this;
        }

        /**
         * @param workerNodeTypeFlexibility a block describing the alternative driver node types if `nodeTypeId` isn&#39;t available.
         * 
         * @return builder
         * 
         */
        public Builder workerNodeTypeFlexibility(ClusterWorkerNodeTypeFlexibilityArgs workerNodeTypeFlexibility) {
            return workerNodeTypeFlexibility(Output.of(workerNodeTypeFlexibility));
        }

        public Builder workloadType(@Nullable Output<ClusterWorkloadTypeArgs> workloadType) {
            $.workloadType = workloadType;
            return this;
        }

        public Builder workloadType(ClusterWorkloadTypeArgs workloadType) {
            return workloadType(Output.of(workloadType));
        }

        public ClusterArgs build() {
            if ($.sparkVersion == null) {
                throw new MissingRequiredPropertyException("ClusterArgs", "sparkVersion");
            }
            return $;
        }
    }

}
