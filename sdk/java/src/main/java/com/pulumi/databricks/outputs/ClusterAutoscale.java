// *** WARNING: this file was generated by pulumi-language-java. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks.outputs;

import com.pulumi.core.annotations.CustomType;
import java.lang.Integer;
import java.util.Objects;
import java.util.Optional;
import javax.annotation.Nullable;

@CustomType
public final class ClusterAutoscale {
    /**
     * @return The maximum number of workers to which the cluster can scale up when overloaded. maxWorkers must be strictly greater than min_workers.
     * 
     * To create a [single node cluster](https://docs.databricks.com/clusters/single-node.html), set `isSingleNode = true` and `kind = &#34;CLASSIC_PREVIEW&#34;` for the cluster. Single-node clusters are suitable for small, non-distributed workloads like single-node machine learning use-cases.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .build());
     * 
     *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .longTermSupport(true)
     *             .build());
     * 
     *         var singleNode = new Cluster("singleNode", ClusterArgs.builder()
     *             .clusterName("Single Node")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .isSingleNode(true)
     *             .kind("CLASSIC_PREVIEW")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    private @Nullable Integer maxWorkers;
    /**
     * @return The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
     * 
     */
    private @Nullable Integer minWorkers;

    private ClusterAutoscale() {}
    /**
     * @return The maximum number of workers to which the cluster can scale up when overloaded. maxWorkers must be strictly greater than min_workers.
     * 
     * To create a [single node cluster](https://docs.databricks.com/clusters/single-node.html), set `isSingleNode = true` and `kind = &#34;CLASSIC_PREVIEW&#34;` for the cluster. Single-node clusters are suitable for small, non-distributed workloads like single-node machine learning use-cases.
     * 
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var smallest = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .build());
     * 
     *         final var latestLts = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .longTermSupport(true)
     *             .build());
     * 
     *         var singleNode = new Cluster("singleNode", ClusterArgs.builder()
     *             .clusterName("Single Node")
     *             .sparkVersion(latestLts.id())
     *             .nodeTypeId(smallest.id())
     *             .autoterminationMinutes(20)
     *             .isSingleNode(true)
     *             .kind("CLASSIC_PREVIEW")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * 
     */
    public Optional<Integer> maxWorkers() {
        return Optional.ofNullable(this.maxWorkers);
    }
    /**
     * @return The minimum number of workers to which the cluster can scale down when underutilized. It is also the initial number of workers the cluster will have after creation.
     * 
     */
    public Optional<Integer> minWorkers() {
        return Optional.ofNullable(this.minWorkers);
    }

    public static Builder builder() {
        return new Builder();
    }

    public static Builder builder(ClusterAutoscale defaults) {
        return new Builder(defaults);
    }
    @CustomType.Builder
    public static final class Builder {
        private @Nullable Integer maxWorkers;
        private @Nullable Integer minWorkers;
        public Builder() {}
        public Builder(ClusterAutoscale defaults) {
    	      Objects.requireNonNull(defaults);
    	      this.maxWorkers = defaults.maxWorkers;
    	      this.minWorkers = defaults.minWorkers;
        }

        @CustomType.Setter
        public Builder maxWorkers(@Nullable Integer maxWorkers) {

            this.maxWorkers = maxWorkers;
            return this;
        }
        @CustomType.Setter
        public Builder minWorkers(@Nullable Integer minWorkers) {

            this.minWorkers = minWorkers;
            return this;
        }
        public ClusterAutoscale build() {
            final var _resultValue = new ClusterAutoscale();
            _resultValue.maxWorkers = maxWorkers;
            _resultValue.minWorkers = minWorkers;
            return _resultValue;
        }
    }
}
