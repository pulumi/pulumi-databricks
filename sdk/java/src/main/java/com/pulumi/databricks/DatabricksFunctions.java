// *** WARNING: this file was generated by pulumi-java-gen. ***
// *** Do not edit by hand unless you're certain you know what you are doing! ***

package com.pulumi.databricks;

import com.pulumi.core.Output;
import com.pulumi.core.TypeShape;
import com.pulumi.databricks.Utilities;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsBucketPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyPlainArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogArgs;
import com.pulumi.databricks.inputs.GetCatalogPlainArgs;
import com.pulumi.databricks.inputs.GetCatalogsArgs;
import com.pulumi.databricks.inputs.GetCatalogsPlainArgs;
import com.pulumi.databricks.inputs.GetClusterArgs;
import com.pulumi.databricks.inputs.GetClusterPlainArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
import com.pulumi.databricks.inputs.GetClusterPolicyPlainArgs;
import com.pulumi.databricks.inputs.GetClustersArgs;
import com.pulumi.databricks.inputs.GetClustersPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigArgs;
import com.pulumi.databricks.inputs.GetCurrentConfigPlainArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
import com.pulumi.databricks.inputs.GetCurrentMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFileArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePathsPlainArgs;
import com.pulumi.databricks.inputs.GetDbfsFilePlainArgs;
import com.pulumi.databricks.inputs.GetDirectoryArgs;
import com.pulumi.databricks.inputs.GetDirectoryPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationArgs;
import com.pulumi.databricks.inputs.GetExternalLocationPlainArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
import com.pulumi.databricks.inputs.GetExternalLocationsPlainArgs;
import com.pulumi.databricks.inputs.GetGroupArgs;
import com.pulumi.databricks.inputs.GetGroupPlainArgs;
import com.pulumi.databricks.inputs.GetInstancePoolArgs;
import com.pulumi.databricks.inputs.GetInstancePoolPlainArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
import com.pulumi.databricks.inputs.GetInstanceProfilesPlainArgs;
import com.pulumi.databricks.inputs.GetJobArgs;
import com.pulumi.databricks.inputs.GetJobPlainArgs;
import com.pulumi.databricks.inputs.GetJobsArgs;
import com.pulumi.databricks.inputs.GetJobsPlainArgs;
import com.pulumi.databricks.inputs.GetMetastoreArgs;
import com.pulumi.databricks.inputs.GetMetastorePlainArgs;
import com.pulumi.databricks.inputs.GetMetastoresArgs;
import com.pulumi.databricks.inputs.GetMetastoresPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentArgs;
import com.pulumi.databricks.inputs.GetMlflowExperimentPlainArgs;
import com.pulumi.databricks.inputs.GetMlflowModelArgs;
import com.pulumi.databricks.inputs.GetMlflowModelPlainArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
import com.pulumi.databricks.inputs.GetMwsCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
import com.pulumi.databricks.inputs.GetMwsWorkspacesPlainArgs;
import com.pulumi.databricks.inputs.GetNodeTypeArgs;
import com.pulumi.databricks.inputs.GetNodeTypePlainArgs;
import com.pulumi.databricks.inputs.GetNotebookArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
import com.pulumi.databricks.inputs.GetNotebookPathsPlainArgs;
import com.pulumi.databricks.inputs.GetNotebookPlainArgs;
import com.pulumi.databricks.inputs.GetPipelinesArgs;
import com.pulumi.databricks.inputs.GetPipelinesPlainArgs;
import com.pulumi.databricks.inputs.GetSchemaArgs;
import com.pulumi.databricks.inputs.GetSchemaPlainArgs;
import com.pulumi.databricks.inputs.GetSchemasArgs;
import com.pulumi.databricks.inputs.GetSchemasPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalPlainArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsArgs;
import com.pulumi.databricks.inputs.GetServicePrincipalsPlainArgs;
import com.pulumi.databricks.inputs.GetShareArgs;
import com.pulumi.databricks.inputs.GetSharePlainArgs;
import com.pulumi.databricks.inputs.GetSharesArgs;
import com.pulumi.databricks.inputs.GetSharesPlainArgs;
import com.pulumi.databricks.inputs.GetSparkVersionArgs;
import com.pulumi.databricks.inputs.GetSparkVersionPlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousePlainArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
import com.pulumi.databricks.inputs.GetSqlWarehousesPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialPlainArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
import com.pulumi.databricks.inputs.GetStorageCredentialsPlainArgs;
import com.pulumi.databricks.inputs.GetTableArgs;
import com.pulumi.databricks.inputs.GetTablePlainArgs;
import com.pulumi.databricks.inputs.GetTablesArgs;
import com.pulumi.databricks.inputs.GetTablesPlainArgs;
import com.pulumi.databricks.inputs.GetUserArgs;
import com.pulumi.databricks.inputs.GetUserPlainArgs;
import com.pulumi.databricks.inputs.GetViewsArgs;
import com.pulumi.databricks.inputs.GetViewsPlainArgs;
import com.pulumi.databricks.inputs.GetVolumeArgs;
import com.pulumi.databricks.inputs.GetVolumePlainArgs;
import com.pulumi.databricks.inputs.GetVolumesArgs;
import com.pulumi.databricks.inputs.GetVolumesPlainArgs;
import com.pulumi.databricks.inputs.GetZonesArgs;
import com.pulumi.databricks.inputs.GetZonesPlainArgs;
import com.pulumi.databricks.outputs.GetAwsAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsBucketPolicyResult;
import com.pulumi.databricks.outputs.GetAwsCrossAccountPolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogAssumeRolePolicyResult;
import com.pulumi.databricks.outputs.GetAwsUnityCatalogPolicyResult;
import com.pulumi.databricks.outputs.GetCatalogResult;
import com.pulumi.databricks.outputs.GetCatalogsResult;
import com.pulumi.databricks.outputs.GetClusterPolicyResult;
import com.pulumi.databricks.outputs.GetClusterResult;
import com.pulumi.databricks.outputs.GetClustersResult;
import com.pulumi.databricks.outputs.GetCurrentConfigResult;
import com.pulumi.databricks.outputs.GetCurrentMetastoreResult;
import com.pulumi.databricks.outputs.GetCurrentUserResult;
import com.pulumi.databricks.outputs.GetDbfsFilePathsResult;
import com.pulumi.databricks.outputs.GetDbfsFileResult;
import com.pulumi.databricks.outputs.GetDirectoryResult;
import com.pulumi.databricks.outputs.GetExternalLocationResult;
import com.pulumi.databricks.outputs.GetExternalLocationsResult;
import com.pulumi.databricks.outputs.GetGroupResult;
import com.pulumi.databricks.outputs.GetInstancePoolResult;
import com.pulumi.databricks.outputs.GetInstanceProfilesResult;
import com.pulumi.databricks.outputs.GetJobResult;
import com.pulumi.databricks.outputs.GetJobsResult;
import com.pulumi.databricks.outputs.GetMetastoreResult;
import com.pulumi.databricks.outputs.GetMetastoresResult;
import com.pulumi.databricks.outputs.GetMlflowExperimentResult;
import com.pulumi.databricks.outputs.GetMlflowModelResult;
import com.pulumi.databricks.outputs.GetMwsCredentialsResult;
import com.pulumi.databricks.outputs.GetMwsWorkspacesResult;
import com.pulumi.databricks.outputs.GetNodeTypeResult;
import com.pulumi.databricks.outputs.GetNotebookPathsResult;
import com.pulumi.databricks.outputs.GetNotebookResult;
import com.pulumi.databricks.outputs.GetPipelinesResult;
import com.pulumi.databricks.outputs.GetSchemaResult;
import com.pulumi.databricks.outputs.GetSchemasResult;
import com.pulumi.databricks.outputs.GetServicePrincipalResult;
import com.pulumi.databricks.outputs.GetServicePrincipalsResult;
import com.pulumi.databricks.outputs.GetShareResult;
import com.pulumi.databricks.outputs.GetSharesResult;
import com.pulumi.databricks.outputs.GetSparkVersionResult;
import com.pulumi.databricks.outputs.GetSqlWarehouseResult;
import com.pulumi.databricks.outputs.GetSqlWarehousesResult;
import com.pulumi.databricks.outputs.GetStorageCredentialResult;
import com.pulumi.databricks.outputs.GetStorageCredentialsResult;
import com.pulumi.databricks.outputs.GetTableResult;
import com.pulumi.databricks.outputs.GetTablesResult;
import com.pulumi.databricks.outputs.GetUserResult;
import com.pulumi.databricks.outputs.GetViewsResult;
import com.pulumi.databricks.outputs.GetVolumeResult;
import com.pulumi.databricks.outputs.GetVolumesResult;
import com.pulumi.databricks.outputs.GetZonesResult;
import com.pulumi.deployment.Deployment;
import com.pulumi.deployment.InvokeOptions;
import com.pulumi.resources.InvokeArgs;
import java.util.concurrent.CompletableFuture;

public final class DatabricksFunctions {
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -> getAwsAssumeRolePolicyResult.json()))
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args) {
        return getAwsAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -> getAwsAssumeRolePolicyResult.json()))
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args) {
        return getAwsAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -> getAwsAssumeRolePolicyResult.json()))
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static Output<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This data source constructs necessary AWS STS assume role policy for you.
     * 
     * ## Example Usage
     * 
     * End-to-end example of provisioning Cross-account IAM role with databricks_mws_credentials:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import com.pulumi.aws.iam.RolePolicyAttachment;
     * import com.pulumi.aws.iam.RolePolicyAttachmentArgs;
     * import com.pulumi.databricks.MwsCredentials;
     * import com.pulumi.databricks.MwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var config = ctx.config();
     *         final var databricksAccountId = config.get("databricksAccountId");
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *         var crossAccountPolicy = new Policy("crossAccountPolicy", PolicyArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         final var thisGetAwsAssumeRolePolicy = DatabricksFunctions.getAwsAssumeRolePolicy(GetAwsAssumeRolePolicyArgs.builder()
     *             .externalId(databricksAccountId)
     *             .build());
     * 
     *         var crossAccount = new Role("crossAccount", RoleArgs.builder()
     *             .name(String.format("%s-crossaccount-iam-role", prefix))
     *             .assumeRolePolicy(thisGetAwsAssumeRolePolicy.applyValue(getAwsAssumeRolePolicyResult -> getAwsAssumeRolePolicyResult.json()))
     *             .description("Grants Databricks full access to VPC resources")
     *             .build());
     * 
     *         var crossAccountRolePolicyAttachment = new RolePolicyAttachment("crossAccountRolePolicyAttachment", RolePolicyAttachmentArgs.builder()
     *             .policyArn(crossAccountPolicy.arn())
     *             .role(crossAccount.name())
     *             .build());
     * 
     *         // required only in case of multi-workspace setup
     *         var thisMwsCredentials = new MwsCredentials("thisMwsCredentials", MwsCredentialsArgs.builder()
     *             .accountId(databricksAccountId)
     *             .credentialsName(String.format("%s-creds", prefix))
     *             .roleArn(crossAccount.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.getAwsCrossAccountPolicy data to construct the necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     */
    public static CompletableFuture<GetAwsAssumeRolePolicyResult> getAwsAssumeRolePolicyPlain(GetAwsAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsAssumeRolePolicy:getAwsAssumeRolePolicy", TypeShape.of(GetAwsAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new BucketV2("this", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .acl("private")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var stuff = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucketName(this_.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(this_.id())
     *             .policy(thisDatabricksAwsBucketPolicy.json())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args) {
        return getAwsBucketPolicy(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new BucketV2("this", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .acl("private")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var stuff = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucketName(this_.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(this_.id())
     *             .policy(thisDatabricksAwsBucketPolicy.json())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args) {
        return getAwsBucketPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new BucketV2("this", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .acl("private")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var stuff = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucketName(this_.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(this_.id())
     *             .policy(thisDatabricksAwsBucketPolicy.json())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static Output<GetAwsBucketPolicyResult> getAwsBucketPolicy(GetAwsBucketPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * This datasource configures a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.aws.s3.BucketV2;
     * import com.pulumi.aws.s3.BucketV2Args;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsBucketPolicyArgs;
     * import com.pulumi.aws.s3.BucketPolicy;
     * import com.pulumi.aws.s3.BucketPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var this_ = new BucketV2("this", BucketV2Args.builder()
     *             .bucket("<unique_bucket_name>")
     *             .acl("private")
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var stuff = DatabricksFunctions.getAwsBucketPolicy(GetAwsBucketPolicyArgs.builder()
     *             .bucketName(this_.bucket())
     *             .build());
     * 
     *         var thisBucketPolicy = new BucketPolicy("thisBucketPolicy", BucketPolicyArgs.builder()
     *             .bucket(this_.id())
     *             .policy(thisDatabricksAwsBucketPolicy.json())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Bucket policy with full access:
     * 
     */
    public static CompletableFuture<GetAwsBucketPolicyResult> getAwsBucketPolicyPlain(GetAwsBucketPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsBucketPolicy:getAwsBucketPolicy", TypeShape.of(GetAwsBucketPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy() {
        return getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain() {
        return getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args) {
        return getAwsCrossAccountPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args) {
        return getAwsCrossAccountPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static Output<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicy(GetAwsCrossAccountPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * This data source constructs necessary AWS cross-account policy for you, which is based on [official documentation](https://docs.databricks.com/administration-guide/account-api/iam-role.html#language-Your%C2%A0VPC,%C2%A0default).
     * 
     * ## Example Usage
     * 
     * For more detailed usage please see databricks.getAwsAssumeRolePolicy or databricks_aws_s3_mount pages.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsCrossAccountPolicyArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsCrossAccountPolicy();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning AWS Databricks workspaces with a Hub &amp; Spoke firewall for data exfiltration protection guide
     * * databricks.getAwsAssumeRolePolicy data to construct the necessary AWS STS assume role policy.
     * * databricks.getAwsBucketPolicy data to configure a simple access policy for AWS S3 buckets, so that Databricks can access data in it.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * 
     */
    public static CompletableFuture<GetAwsCrossAccountPolicyResult> getAwsCrossAccountPolicyPlain(GetAwsCrossAccountPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsCrossAccountPolicy:getAwsCrossAccountPolicy", TypeShape.of(GetAwsCrossAccountPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args) {
        return getAwsUnityCatalogAssumeRolePolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args) {
        return getAwsUnityCatalogAssumeRolePolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog assume role policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogAssumeRolePolicyResult> getAwsUnityCatalogAssumeRolePolicyPlain(GetAwsUnityCatalogAssumeRolePolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogAssumeRolePolicy:getAwsUnityCatalogAssumeRolePolicy", TypeShape.of(GetAwsUnityCatalogAssumeRolePolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args) {
        return getAwsUnityCatalogPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args) {
        return getAwsUnityCatalogPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This resource has an evolving API, which may change in future versions of the provider. Please always consult [latest documentation](https://docs.databricks.com/data-governance/unity-catalog/get-started.html#configure-a-storage-bucket-and-iam-role-in-aws) in case of any questions.
     * 
     * This data source constructs the necessary AWS Unity Catalog policy for you.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogPolicyArgs;
     * import com.pulumi.databricks.inputs.GetAwsUnityCatalogAssumeRolePolicyArgs;
     * import com.pulumi.aws.iam.Policy;
     * import com.pulumi.aws.iam.PolicyArgs;
     * import com.pulumi.aws.iam.Role;
     * import com.pulumi.aws.iam.RoleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getAwsUnityCatalogPolicy(GetAwsUnityCatalogPolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .bucketName("databricks-bucket")
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .kmsName("arn:aws:kms:us-west-2:111122223333:key/databricks-kms")
     *             .build());
     * 
     *         final var thisGetAwsUnityCatalogAssumeRolePolicy = DatabricksFunctions.getAwsUnityCatalogAssumeRolePolicy(GetAwsUnityCatalogAssumeRolePolicyArgs.builder()
     *             .awsAccountId(awsAccountId)
     *             .roleName(String.format("%s-uc-access", prefix))
     *             .externalId("12345")
     *             .build());
     * 
     *         var unityMetastore = new Policy("unityMetastore", PolicyArgs.builder()
     *             .name(String.format("%s-unity-catalog-metastore-access-iam-policy", prefix))
     *             .policy(this_.json())
     *             .build());
     * 
     *         var metastoreDataAccess = new Role("metastoreDataAccess", RoleArgs.builder()
     *             .name(String.format("%s-uc-access", prefix))
     *             .assumeRolePolicy(thisAwsIamPolicyDocument.json())
     *             .managedPolicyArns(unityMetastore.arn())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetAwsUnityCatalogPolicyResult> getAwsUnityCatalogPolicyPlain(GetAwsUnityCatalogPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getAwsUnityCatalogPolicy:getAwsUnityCatalogPolicy", TypeShape.of(GetAwsUnityCatalogPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.applyValue(getCatalogResult -> getCatalogResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args) {
        return getCatalog(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.applyValue(getCatalogResult -> getCatalogResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args) {
        return getCatalogPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.applyValue(getCatalogResult -> getCatalogResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static Output<GetCatalogResult> getCatalog(GetCatalogArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific catalog in Unity Catalog, that were created by Pulumi or manually. Use databricks.getCatalogs to retrieve IDs of multiple catalogs from Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific catalog `test`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var test = DatabricksFunctions.getCatalog(GetCatalogArgs.builder()
     *             .name("test")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .catalog(test.applyValue(getCatalogResult -> getCatalogResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges("USE_CATALOG")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getCatalogs to list all catalogs within Unity Catalog metastore.
     * 
     */
    public static CompletableFuture<GetCatalogResult> getCatalogPlain(GetCatalogPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalog:getCatalog", TypeShape.of(GetCatalogResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs() {
        return getCatalogs(GetCatalogsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain() {
        return getCatalogsPlain(GetCatalogsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args) {
        return getCatalogs(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args) {
        return getCatalogsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCatalogsResult> getCatalogs(GetCatalogsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Catalog ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all catalogs:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCatalogsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getCatalogs();
     * 
     *         ctx.export("allCatalogs", all.applyValue(getCatalogsResult -> getCatalogsResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCatalogsResult> getCatalogsPlain(GetCatalogsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCatalogs:getCatalogs", TypeShape.of(GetCatalogsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster() {
        return getCluster(GetClusterArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain() {
        return getClusterPlain(GetClusterPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args) {
        return getCluster(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args) {
        return getClusterPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClusterResult> getCluster(GetClusterArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.Cluster using its id. This could be retrieved programmatically using databricks.getClusters data source.
     * 
     * ## Example Usage
     * 
     * Retrieve attributes of each SQL warehouses in a workspace
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClusterResult> getClusterPlain(GetClusterPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCluster:getCluster", TypeShape.of(GetClusterResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy() {
        return getClusterPolicy(GetClusterPolicyArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain() {
        return getClusterPolicyPlain(GetClusterPolicyPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args) {
        return getClusterPolicy(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args) {
        return getClusterPolicyPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetClusterPolicyResult> getClusterPolicy(GetClusterPolicyArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Referring to a cluster policy by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClusterPolicyArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var personal = DatabricksFunctions.getClusterPolicy(GetClusterPolicyArgs.builder()
     *             .name("Personal Compute")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .policyId(personal.applyValue(getClusterPolicyResult -> getClusterPolicyResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetClusterPolicyResult> getClusterPolicyPlain(GetClusterPolicyPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusterPolicy:getClusterPolicy", TypeShape.of(GetClusterPolicyResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters() {
        return getClusters(GetClustersArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain() {
        return getClustersPlain(GetClustersPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args) {
        return getClusters(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args) {
        return getClustersPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static Output<GetClustersResult> getClusters(GetClustersArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Cluster ids, that were created by Pulumi or manually, with or without databricks_cluster_policy.
     * 
     * ## Example Usage
     * 
     * Retrieve cluster IDs for all clusters:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getClusters();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve cluster IDs for all clusters having &#34;Shared&#34; in the cluster name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetClustersArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getClusters(GetClustersArgs.builder()
     *             .clusterNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * 
     */
    public static CompletableFuture<GetClustersResult> getClustersPlain(GetClustersPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getClusters:getClusters", TypeShape.of(GetClustersResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig() {
        return getCurrentConfig(GetCurrentConfigArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain() {
        return getCurrentConfigPlain(GetCurrentConfigPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args) {
        return getCurrentConfig(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args) {
        return getCurrentConfigPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static Output<GetCurrentConfigResult> getCurrentConfig(GetCurrentConfigArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about the currently configured provider to make a decision, for example, add a dynamic block based on the specific cloud.
     * 
     * ## Example Usage
     * 
     * Create cloud-specific databricks_storage_credential:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Exported attributes
     * 
     * Data source exposes the following attributes:
     * 
     * * `is_account` - Whether the provider is configured at account-level
     * * `account_id` - Account Id if provider is configured at account-level
     * * `host` - Host of the Databricks workspace or account console
     * * `cloud_type` - Cloud type specified in the provider
     * * `auth_type` - Auth type used by the provider
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * * databricks.Repo to manage [Databricks Repos](https://docs.databricks.com/repos.html).
     * 
     */
    public static CompletableFuture<GetCurrentConfigResult> getCurrentConfigPlain(GetCurrentConfigPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentConfig:getCurrentConfig", TypeShape.of(GetCurrentConfigResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore() {
        return getCurrentMetastore(GetCurrentMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain() {
        return getCurrentMetastorePlain(GetCurrentMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args) {
        return getCurrentMetastore(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args) {
        return getCurrentMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetCurrentMetastoreResult> getCurrentMetastore(GetCurrentMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves information about metastore attached to a given workspace.
     * 
     * &gt; **Note** This is the workspace-level data source.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreSummary response for a metastore attached to the current workspace.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetCurrentMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getCurrentMetastore();
     * 
     *         ctx.export("someMetastore", this_.metastoreInfo());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information for a metastore with a given ID.
     * * databricks.getMetastores to get a mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetCurrentMetastoreResult> getCurrentMetastorePlain(GetCurrentMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentMetastore:getCurrentMetastore", TypeShape.of(GetCurrentMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser() {
        return getCurrentUser(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain() {
        return getCurrentUserPlain(InvokeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args) {
        return getCurrentUser(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args) {
        return getCurrentUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static Output<GetCurrentUserResult> getCurrentUser(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.User or databricks_service_principal, that is calling Databricks REST API. Might be useful in applying the same Pulumi by different users in the shared workspace for testing purposes.
     * 
     */
    public static CompletableFuture<GetCurrentUserResult> getCurrentUserPlain(InvokeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getCurrentUser:getCurrentUser", TypeShape.of(GetCurrentUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize("true")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args) {
        return getDbfsFile(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize("true")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args) {
        return getDbfsFilePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize("true")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFileResult> getDbfsFile(GetDbfsFileArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFileArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var report = DatabricksFunctions.getDbfsFile(GetDbfsFileArgs.builder()
     *             .path("dbfs:/reports/some.csv")
     *             .limitFileSize("true")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFileResult> getDbfsFilePlain(GetDbfsFilePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFile:getDbfsFile", TypeShape.of(GetDbfsFileResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args) {
        return getDbfsFilePaths(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args) {
        return getDbfsFilePathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static Output<GetDbfsFilePathsResult> getDbfsFilePaths(GetDbfsFilePathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDbfsFilePathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var partitions = DatabricksFunctions.getDbfsFilePaths(GetDbfsFilePathsArgs.builder()
     *             .path("dbfs:/user/hive/default.db/table")
     *             .recursive(false)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.DbfsFile data to get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.getDbfsFilePaths data to get list of file names from get file content from [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.DbfsFile to manage relatively small files on [Databricks File System (DBFS)](https://docs.databricks.com/data/databricks-file-system.html).
     * * databricks.Library to install a [library](https://docs.databricks.com/libraries/index.html) on databricks_cluster.
     * * databricks.Mount to [mount your cloud storage](https://docs.databricks.com/data/databricks-file-system.html#mount-object-storage-to-dbfs) on `dbfs:/mnt/name`.
     * 
     */
    public static CompletableFuture<GetDbfsFilePathsResult> getDbfsFilePathsPlain(GetDbfsFilePathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDbfsFilePaths:getDbfsFilePaths", TypeShape.of(GetDbfsFilePathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args) {
        return getDirectory(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args) {
        return getDirectoryPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetDirectoryResult> getDirectory(GetDirectoryArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to get information about a directory in a Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetDirectoryArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getDirectory(GetDirectoryArgs.builder()
     *             .path("/Production")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetDirectoryResult> getDirectoryPlain(GetDirectoryPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getDirectory:getDirectory", TypeShape.of(GetDirectoryResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args) {
        return getExternalLocation(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args) {
        return getExternalLocationPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationResult> getExternalLocation(GetExternalLocationArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.ExternalLocation that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing external location in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getExternalLocation(GetExternalLocationArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.externalLocationInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getExternalLocations to get names of all external locations
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationResult> getExternalLocationPlain(GetExternalLocationPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocation:getExternalLocation", TypeShape.of(GetExternalLocationResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations() {
        return getExternalLocations(GetExternalLocationsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain() {
        return getExternalLocationsPlain(GetExternalLocationsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args) {
        return getExternalLocations(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args) {
        return getExternalLocationsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static Output<GetExternalLocationsResult> getExternalLocations(GetExternalLocationsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.ExternalLocation objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all external locations in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetExternalLocationsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getExternalLocations();
     * 
     *         ctx.export("allExternalLocations", all.applyValue(getExternalLocationsResult -> getExternalLocationsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.ExternalLocation to get information about a single external location
     * * databricks.ExternalLocation to manage external locations within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetExternalLocationsResult> getExternalLocationsPlain(GetExternalLocationsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getExternalLocations:getExternalLocations", TypeShape.of(GetExternalLocationsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args) {
        return getGroup(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args) {
        return getGroupPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static Output<GetGroupResult> getGroup(GetGroupArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks.Group members, entitlements and instance profiles.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.User;
     * import com.pulumi.databricks.UserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         var me = new User("me", UserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.id())
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Directory to manage directories in [Databricks Workpace](https://docs.databricks.com/workspace/workspace-objects.html).
     * * databricks.GroupMember to attach users and groups as group members.
     * * databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * * databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * 
     */
    public static CompletableFuture<GetGroupResult> getGroupPlain(GetGroupPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getGroup:getGroup", TypeShape.of(GetGroupResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(poolDatabricksInstancePool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args) {
        return getInstancePool(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(poolDatabricksInstancePool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args) {
        return getInstancePoolPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(poolDatabricksInstancePool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstancePoolResult> getInstancePool(GetInstancePoolArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_instance_pool.
     * 
     * ## Example Usage
     * 
     * Referring to an instance pool by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstancePoolArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var pool = DatabricksFunctions.getInstancePool(GetInstancePoolArgs.builder()
     *             .name("All spot")
     *             .build());
     * 
     *         var myCluster = new Cluster("myCluster", ClusterArgs.builder()
     *             .instancePoolId(poolDatabricksInstancePool.id())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstancePoolResult> getInstancePoolPlain(GetInstancePoolPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstancePool:getInstancePool", TypeShape.of(GetInstancePoolResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles() {
        return getInstanceProfiles(GetInstanceProfilesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain() {
        return getInstanceProfilesPlain(GetInstanceProfilesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args) {
        return getInstanceProfiles(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args) {
        return getInstanceProfilesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetInstanceProfilesResult> getInstanceProfiles(GetInstanceProfilesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Lists all available databricks_instance_profiles.
     * 
     * ## Example Usage
     * 
     * Get all instance profiles:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetInstanceProfilesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getInstanceProfiles();
     * 
     *         ctx.export("allInstanceProfiles", all.applyValue(getInstanceProfilesResult -> getInstanceProfilesResult.instanceProfiles()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetInstanceProfilesResult> getInstanceProfilesPlain(GetInstanceProfilesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getInstanceProfiles:getInstanceProfiles", TypeShape.of(GetInstanceProfilesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob() {
        return getJob(GetJobArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain() {
        return getJobPlain(GetJobPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args) {
        return getJob(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args) {
        return getJobPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobResult> getJob(GetJobArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.Job by name or by id. Complements the feature of the databricks.getJobs data source.
     * 
     * ## Example Usage
     * 
     * Getting the existing cluster id of specific databricks.Job by name or by id:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJob(GetJobArgs.builder()
     *             .jobName("My job")
     *             .build());
     * 
     *         ctx.export("jobNumWorkers", this_.jobSettings().settings().newCluster().numWorkers());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getJobs data to get all jobs and their names from a workspace.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobResult> getJobPlain(GetJobPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJob:getJob", TypeShape.of(GetJobResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs() {
        return getJobs(GetJobsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain() {
        return getJobsPlain(GetJobsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args) {
        return getJobs(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args) {
        return getJobsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetJobsResult> getJobs(GetJobsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Job ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** Data resource will error in case of jobs with duplicate names.
     * 
     * ## Example Usage
     * 
     * Granting view databricks.Permissions to all databricks.Job within the workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import com.pulumi.databricks.Permissions;
     * import com.pulumi.databricks.PermissionsArgs;
     * import com.pulumi.databricks.inputs.PermissionsAccessControlArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         final var everyoneCanViewAllJobs = this.applyValue(getJobsResult -> {
     *             final var resources = new ArrayList<Permissions>();
     *             for (var range : KeyedValue.of(getJobsResult.ids()) {
     *                 var resource = new Permissions("everyoneCanViewAllJobs-" + range.key(), PermissionsArgs.builder()
     *                     .jobId(range.value())
     *                     .accessControls(PermissionsAccessControlArgs.builder()
     *                         .groupName("users")
     *                         .permissionLevel("CAN_VIEW")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Getting ID of specific databricks.Job by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetJobsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getJobs();
     * 
     *         ctx.export("x", String.format("ID of `x` job is %s", this_.ids().x()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetJobsResult> getJobsPlain(GetJobsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getJobs:getJobs", TypeShape.of(GetJobsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore() {
        return getMetastore(GetMetastoreArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain() {
        return getMetastorePlain(GetMetastorePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args) {
        return getMetastore(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args) {
        return getMetastorePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoreResult> getMetastore(GetMetastoreArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves information about metastore for a given id of databricks.Metastore object, that was created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * ## Example Usage
     * 
     * MetastoreInfo response for a given metastore id
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.Metastore;
     * import com.pulumi.databricks.MetastoreArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoreArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMetastore = new Metastore("thisMetastore", MetastoreArgs.builder()
     *             .name("primary")
     *             .storageRoot(String.format("s3://%s/metastore", metastore.id()))
     *             .owner(unityAdminGroup)
     *             .forceDestroy(true)
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMetastore(GetMetastoreArgs.builder()
     *             .metastoreId(thisMetastore.id())
     *             .build());
     * 
     *         ctx.export("someMetastore", this_.applyValue(this_ -> this_.metastoreInfo()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getMetastores to get mapping of name to id of all metastores.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoreResult> getMetastorePlain(GetMetastorePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastore:getMetastore", TypeShape.of(GetMetastoreResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores() {
        return getMetastores(GetMetastoresArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain() {
        return getMetastoresPlain(GetMetastoresPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args) {
        return getMetastores(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args) {
        return getMetastoresPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetMetastoresResult> getMetastores(GetMetastoresArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with account-level provider!
     * 
     * Retrieves a mapping of name to id of databricks.Metastore objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work. Data resource will error in case of metastores with duplicate names. This data source is only available for users &amp; service principals with account admin status
     * 
     * ## Example Usage
     * 
     * Mapping of name to id of all metastores:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMetastoresArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMetastores();
     * 
     *         ctx.export("allMetastores", all.applyValue(getMetastoresResult -> getMetastoresResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Metastore to get information about a single metastore.
     * * databricks.Metastore to manage Metastores within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetMetastoresResult> getMetastoresPlain(GetMetastoresPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMetastores:getMetastores", TypeShape.of(GetMetastoresResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment() {
        return getMlflowExperiment(GetMlflowExperimentArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain() {
        return getMlflowExperimentPlain(GetMlflowExperimentPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args) {
        return getMlflowExperiment(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args) {
        return getMlflowExperimentPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static Output<GetMlflowExperimentResult> getMlflowExperiment(GetMlflowExperimentArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowExperiment by id or name.
     * 
     */
    public static CompletableFuture<GetMlflowExperimentResult> getMlflowExperimentPlain(GetMlflowExperimentPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowExperiment:getMlflowExperiment", TypeShape.of(GetMlflowExperimentResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args) {
        return getMlflowModel(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args) {
        return getMlflowModelPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetMlflowModelResult> getMlflowModel(GetMlflowModelArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves the settings of databricks.MlflowModel by name.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.MlflowModel;
     * import com.pulumi.databricks.MlflowModelArgs;
     * import com.pulumi.databricks.inputs.MlflowModelTagArgs;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         var thisMlflowModel = new MlflowModel("thisMlflowModel", MlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .description("My MLflow model description")
     *             .tags(            
     *                 MlflowModelTagArgs.builder()
     *                     .key("key1")
     *                     .value("value1")
     *                     .build(),
     *                 MlflowModelTagArgs.builder()
     *                     .key("key2")
     *                     .value("value2")
     *                     .build())
     *             .build());
     * 
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model")
     *             .build());
     * 
     *         ctx.export("model", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMlflowModelArgs;
     * import com.pulumi.databricks.ModelServing;
     * import com.pulumi.databricks.ModelServingArgs;
     * import com.pulumi.databricks.inputs.ModelServingConfigArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getMlflowModel(GetMlflowModelArgs.builder()
     *             .name("My MLflow Model with multiple versions")
     *             .build());
     * 
     *         var thisModelServing = new ModelServing("thisModelServing", ModelServingArgs.builder()
     *             .name("model-serving-endpoint")
     *             .config(ModelServingConfigArgs.builder()
     *                 .servedModels(ModelServingConfigServedModelArgs.builder()
     *                     .name("model_serving_prod")
     *                     .modelName(this_.name())
     *                     .modelVersion(this_.latestVersions()[0].version())
     *                     .workloadSize("Small")
     *                     .scaleToZeroEnabled(true)
     *                     .build())
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetMlflowModelResult> getMlflowModelPlain(GetMlflowModelPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMlflowModel:getMlflowModel", TypeShape.of(GetMlflowModelResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials() {
        return getMwsCredentials(GetMwsCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain() {
        return getMwsCredentialsPlain(GetMwsCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args) {
        return getMwsCredentials(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args) {
        return getMwsCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static Output<GetMwsCredentialsResult> getMwsCredentials(GetMwsCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsCredentials in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all credentials in Databricks Account
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsCredentials();
     * 
     *         ctx.export("allMwsCredentials", all.applyValue(getMwsCredentialsResult -> getMwsCredentialsResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * Provisioning Databricks on AWS guide.
     * * databricks.MwsCustomerManagedKeys to configure KMS keys for new workspaces within AWS.
     * * databricks.MwsLogDelivery to configure delivery of [billable usage logs](https://docs.databricks.com/administration-guide/account-settings/billable-usage-delivery.html) and [audit logs](https://docs.databricks.com/administration-guide/account-settings/audit-logs.html).
     * * databricks.MwsNetworks to [configure VPC](https://docs.databricks.com/administration-guide/cloud-configurations/aws/customer-managed-vpc.html) &amp; subnets for new workspaces within AWS.
     * * databricks.MwsStorageConfigurations to configure root bucket new workspaces within AWS.
     * * databricks.MwsWorkspaces to set up [AWS and GCP workspaces](https://docs.databricks.com/getting-started/overview.html#e2-architecture-1).
     * 
     */
    public static CompletableFuture<GetMwsCredentialsResult> getMwsCredentialsPlain(GetMwsCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsCredentials:getMwsCredentials", TypeShape.of(GetMwsCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces() {
        return getMwsWorkspaces(GetMwsWorkspacesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain() {
        return getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args) {
        return getMwsWorkspaces(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args) {
        return getMwsWorkspacesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static Output<GetMwsWorkspacesResult> getMwsWorkspaces(GetMwsWorkspacesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Lists all databricks.MwsWorkspaces in Databricks Account.
     * 
     * &gt; **Note** `account_id` provider configuration property is required for this resource to work.
     * 
     * ## Example Usage
     * 
     * Listing all workspaces in
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetMwsWorkspacesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getMwsWorkspaces();
     * 
     *         ctx.export("allMwsWorkspaces", all.applyValue(getMwsWorkspacesResult -> getMwsWorkspacesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.MwsWorkspaces to manage Databricks Workspaces on AWS and GCP.
     * * databricks.MetastoreAssignment
     * 
     */
    public static CompletableFuture<GetMwsWorkspacesResult> getMwsWorkspacesPlain(GetMwsWorkspacesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getMwsWorkspaces:getMwsWorkspaces", TypeShape.of(GetMwsWorkspacesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType() {
        return getNodeType(GetNodeTypeArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain() {
        return getNodeTypePlain(GetNodeTypePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args) {
        return getNodeType(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args) {
        return getNodeTypePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetNodeTypeResult> getNodeType(GetNodeTypeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets the smallest node type for databricks.Cluster that fits search criteria, like amount of RAM or number of cores. [AWS](https://databricks.com/product/aws-pricing/instance-types) or [Azure](https://azure.microsoft.com/en-us/pricing/details/databricks/). Internally data source fetches [node types](https://docs.databricks.com/dev-tools/api/latest/clusters.html#list-node-types) available per cloud, similar to executing `databricks clusters list-node-types`, and filters it to return the smallest possible node with criteria.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. `min_gpus = 876`) or no nodes matching, data source will return cloud-default node type, even though it doesn&#39;t match search criteria specified by data source arguments: [i3.xlarge](https://aws.amazon.com/ec2/instance-types/i3/) for AWS or [Standard_D3_v2](https://docs.microsoft.com/en-us/azure/cloud-services/cloud-services-sizes-specs#dv2-series) for Azure.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetNodeTypeResult> getNodeTypePlain(GetNodeTypePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNodeType:getNodeType", TypeShape.of(GetNodeTypeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args) {
        return getNotebook(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args) {
        return getNotebookPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookResult> getNotebook(GetNotebookArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to export a notebook from Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var features = DatabricksFunctions.getNotebook(GetNotebookArgs.builder()
     *             .path("/Production/Features")
     *             .format("SOURCE")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookResult> getNotebookPlain(GetNotebookPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebook:getNotebook", TypeShape.of(GetNotebookResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args) {
        return getNotebookPaths(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args) {
        return getNotebookPathsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetNotebookPathsResult> getNotebookPaths(GetNotebookPathsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows to list notebooks in the Databricks Workspace.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNotebookPathsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var prod = DatabricksFunctions.getNotebookPaths(GetNotebookPathsArgs.builder()
     *             .path("/Production")
     *             .recursive(true)
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetNotebookPathsResult> getNotebookPathsPlain(GetNotebookPathsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getNotebookPaths:getNotebookPaths", TypeShape.of(GetNotebookPathsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines() {
        return getPipelines(GetPipelinesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain() {
        return getPipelinesPlain(GetPipelinesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args) {
        return getPipelines(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args) {
        return getPipelinesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static Output<GetPipelinesResult> getPipelines(GetPipelinesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _authentication is not configured for provider_ errors.
     * 
     * Retrieves a list of all databricks.Pipeline ([Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html)) ids deployed in a workspace, or those matching the provided search term. Maximum 100 results.
     * 
     * ## Example Usage
     * 
     * Get all Delta Live Tables pipelines:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getPipelines();
     * 
     *         ctx.export("allPipelines", all.applyValue(getPipelinesResult -> getPipelinesResult.ids()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (exact match):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("my_pipeline")
     *             .build());
     * 
     *         ctx.export("myPipeline", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Filter Delta Live Tables pipelines by name (wildcard search):
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetPipelinesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getPipelines(GetPipelinesArgs.builder()
     *             .pipelineName("%pipeline%")
     *             .build());
     * 
     *         ctx.export("wildcardPipelines", this_.ids());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Pipeline to deploy [Delta Live Tables](https://docs.databricks.com/data-engineering/delta-live-tables/index.html).
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * * databricks.Notebook to manage [Databricks Notebooks](https://docs.databricks.com/notebooks/index.html).
     * 
     */
    public static CompletableFuture<GetPipelinesResult> getPipelinesPlain(GetPipelinesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getPipelines:getPipelines", TypeShape.of(GetPipelinesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args) {
        return getSchema(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args) {
        return getSchemaPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemaResult> getSchema(GetSchemaArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Schema that was created by Pulumi or manually.
     * A schema can be identified by its two-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`) as input. This can be retrieved programmatically using databricks.getSchemas data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve details of all schemas in in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific schema by its fully qualified name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemaArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getSchema(GetSchemaArgs.builder()
     *             .name("catalog.schema")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemaResult> getSchemaPlain(GetSchemaPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchema:getSchema", TypeShape.of(GetSchemaResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox.applyValue(getSchemasResult -> getSchemasResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args) {
        return getSchemas(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox.applyValue(getSchemasResult -> getSchemasResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args) {
        return getSchemasPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox.applyValue(getSchemasResult -> getSchemasResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetSchemasResult> getSchemas(GetSchemasArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.Schema ids, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * Listing all schemas in a _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSchemasArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var sandbox = DatabricksFunctions.getSchemas(GetSchemasArgs.builder()
     *             .catalogName("sandbox")
     *             .build());
     * 
     *         ctx.export("allSandboxSchemas", sandbox.applyValue(getSchemasResult -> getSchemasResult));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetSchemasResult> getSchemasPlain(GetSchemasPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSchemas:getSchemas", TypeShape.of(GetSchemasResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal() {
        return getServicePrincipal(GetServicePrincipalArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain() {
        return getServicePrincipalPlain(GetServicePrincipalPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args) {
        return getServicePrincipal(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args) {
        return getServicePrincipalPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static Output<GetServicePrincipalResult> getServicePrincipal(GetServicePrincipalArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_service_principal.
     * 
     * ## Example Usage
     * 
     * Adding service principal `11111111-2222-3333-4444-555666777888` to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetServicePrincipalArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var spn = DatabricksFunctions.getServicePrincipal(GetServicePrincipalArgs.builder()
     *             .applicationId("11111111-2222-3333-4444-555666777888")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(spn.applyValue(getServicePrincipalResult -> getServicePrincipalResult.id()))
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks_service principal to manage service principals
     * 
     */
    public static CompletableFuture<GetServicePrincipalResult> getServicePrincipalPlain(GetServicePrincipalPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipal:getServicePrincipal", TypeShape.of(GetServicePrincipalResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals() {
        return getServicePrincipals(GetServicePrincipalsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain() {
        return getServicePrincipalsPlain(GetServicePrincipalsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args) {
        return getServicePrincipals(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args) {
        return getServicePrincipalsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static Output<GetServicePrincipalsResult> getServicePrincipals(GetServicePrincipalsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves `application_ids` of all databricks.ServicePrincipal based on their `display_name`
     * 
     */
    public static CompletableFuture<GetServicePrincipalsResult> getServicePrincipalsPlain(GetServicePrincipalsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getServicePrincipals:getServicePrincipals", TypeShape.of(GetServicePrincipalsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare() {
        return getShare(GetShareArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain() {
        return getSharePlain(GetSharePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args) {
        return getShare(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args) {
        return getSharePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetShareResult> getShare(GetShareArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about a databricks.Share that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing share in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetShareArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShare(GetShareArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetShareResult> getSharePlain(GetSharePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShare:getShare", TypeShape.of(GetShareResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares() {
        return getShares(GetSharesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain() {
        return getSharesPlain(GetSharesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args) {
        return getShares(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args) {
        return getSharesPlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static Output<GetSharesResult> getShares(GetSharesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves a list of databricks.Share name, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting all existing shares in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSharesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getShares();
     * 
     *         ctx.export("shareName", this_.shares());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Share to create Delta Sharing shares.
     * * databricks.Recipient to create Delta Sharing recipients.
     * * databricks.Grants to manage Delta Sharing permissions.
     * 
     */
    public static CompletableFuture<GetSharesResult> getSharesPlain(GetSharesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getShares:getShares", TypeShape.of(GetSharesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion() {
        return getSparkVersion(GetSparkVersionArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain() {
        return getSparkVersionPlain(GetSparkVersionPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args) {
        return getSparkVersion(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args) {
        return getSparkVersionPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static Output<GetSparkVersionResult> getSparkVersion(GetSparkVersionArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Gets [Databricks Runtime (DBR)](https://docs.databricks.com/runtime/dbr.html) version that could be used for `spark_version` parameter in databricks.Cluster and other resources that fits search criteria, like specific Spark or Scala version, ML or Genomics runtime, etc., similar to executing `databricks clusters spark-versions`, and filters it to return the latest version that matches criteria. Often used along databricks.getNodeType data source.
     * 
     * &gt; **Note** This is experimental functionality, which aims to simplify things. In case of wrong parameters given (e.g. together `ml = true` and `genomics = true`, or something like), data source will throw an error.  Similarly, if search returns multiple results, and `latest = false`, data source will throw an error.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetNodeTypeArgs;
     * import com.pulumi.databricks.inputs.GetSparkVersionArgs;
     * import com.pulumi.databricks.Cluster;
     * import com.pulumi.databricks.ClusterArgs;
     * import com.pulumi.databricks.inputs.ClusterAutoscaleArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var withGpu = DatabricksFunctions.getNodeType(GetNodeTypeArgs.builder()
     *             .localDisk(true)
     *             .minCores(16)
     *             .gbPerCore(1)
     *             .minGpus(1)
     *             .build());
     * 
     *         final var gpuMl = DatabricksFunctions.getSparkVersion(GetSparkVersionArgs.builder()
     *             .gpu(true)
     *             .ml(true)
     *             .build());
     * 
     *         var research = new Cluster("research", ClusterArgs.builder()
     *             .clusterName("Research Cluster")
     *             .sparkVersion(gpuMl.applyValue(getSparkVersionResult -> getSparkVersionResult.id()))
     *             .nodeTypeId(withGpu.applyValue(getNodeTypeResult -> getNodeTypeResult.id()))
     *             .autoterminationMinutes(20)
     *             .autoscale(ClusterAutoscaleArgs.builder()
     *                 .minWorkers(1)
     *                 .maxWorkers(50)
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.Cluster to create [Databricks Clusters](https://docs.databricks.com/clusters/index.html).
     * * databricks.ClusterPolicy to create a databricks.Cluster policy, which limits the ability to create clusters based on a set of rules.
     * * databricks.InstancePool to manage [instance pools](https://docs.databricks.com/clusters/instance-pools/index.html) to reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances.
     * * databricks.Job to manage [Databricks Jobs](https://docs.databricks.com/jobs.html) to run non-interactive code in a databricks_cluster.
     * 
     */
    public static CompletableFuture<GetSparkVersionResult> getSparkVersionPlain(GetSparkVersionPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSparkVersion:getSparkVersion", TypeShape.of(GetSparkVersionResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse() {
        return getSqlWarehouse(GetSqlWarehouseArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain() {
        return getSqlWarehousePlain(GetSqlWarehousePlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args) {
        return getSqlWarehouse(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args) {
        return getSqlWarehousePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehouseResult> getSqlWarehouse(GetSqlWarehouseArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about a databricks.getSqlWarehouse using its id. This could be retrieved programmatically using databricks.getSqlWarehouses data source.
     * 
     * ## Example Usage
     * 
     * * Retrieve attributes of each SQL warehouses in a workspace:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * * Search for a specific SQL Warehouse by name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehouseArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouse(GetSqlWarehouseArgs.builder()
     *             .name("Starter Warehouse")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehouseResult> getSqlWarehousePlain(GetSqlWarehousePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouse:getSqlWarehouse", TypeShape.of(GetSqlWarehouseResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses() {
        return getSqlWarehouses(GetSqlWarehousesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain() {
        return getSqlWarehousesPlain(GetSqlWarehousesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args) {
        return getSqlWarehouses(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args) {
        return getSqlWarehousesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static Output<GetSqlWarehousesResult> getSqlWarehouses(GetSqlWarehousesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of databricks.SqlEndpoint ids, that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Retrieve IDs for all SQL warehouses:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getSqlWarehouses();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * Retrieve IDs for all clusters having &#34;Shared&#34; in the warehouse name:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetSqlWarehousesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var allShared = DatabricksFunctions.getSqlWarehouses(GetSqlWarehousesArgs.builder()
     *             .warehouseNameContains("shared")
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are often used in the same context:
     * 
     * * End to end workspace management guide.
     * * databricks.InstanceProfile to manage AWS EC2 instance profiles that users can launch databricks.Cluster and access data, like databricks_mount.
     * * databricks.SqlDashboard to manage Databricks SQL [Dashboards](https://docs.databricks.com/sql/user/dashboards/index.html).
     * * databricks.SqlGlobalConfig to configure the security policy, databricks_instance_profile, and [data access properties](https://docs.databricks.com/sql/admin/data-access-configuration.html) for all databricks.getSqlWarehouse of workspace.
     * * databricks.SqlPermissions to manage data object access control lists in Databricks workspaces for things like tables, views, databases, and [more](https://docs.databricks.com/security/access-control/table-acls/object-privileges.html).
     * 
     */
    public static CompletableFuture<GetSqlWarehousesResult> getSqlWarehousesPlain(GetSqlWarehousesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getSqlWarehouses:getSqlWarehouses", TypeShape.of(GetSqlWarehousesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args) {
        return getStorageCredential(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args) {
        return getStorageCredentialPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialResult> getStorageCredential(GetStorageCredentialArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves details about a databricks.StorageCredential that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Getting details of an existing storage credential in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getStorageCredential(GetStorageCredentialArgs.builder()
     *             .name("this")
     *             .build());
     * 
     *         ctx.export("createdBy", this_.storageCredentialInfo().createdBy());
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.getStorageCredentials to get names of all credentials
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialResult> getStorageCredentialPlain(GetStorageCredentialPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredential:getStorageCredential", TypeShape.of(GetStorageCredentialResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials() {
        return getStorageCredentials(GetStorageCredentialsArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain() {
        return getStorageCredentialsPlain(GetStorageCredentialsPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args) {
        return getStorageCredentials(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args) {
        return getStorageCredentialsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static Output<GetStorageCredentialsResult> getStorageCredentials(GetStorageCredentialsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.StorageCredential objects, that were created by Pulumi or manually, so that special handling could be applied.
     * 
     * ## Example Usage
     * 
     * List all storage credentials in the metastore
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetStorageCredentialsArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var all = DatabricksFunctions.getStorageCredentials();
     * 
     *         ctx.export("allStorageCredentials", all.applyValue(getStorageCredentialsResult -> getStorageCredentialsResult.names()));
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.StorageCredential to get information about a single credential
     * * databricks.StorageCredential to manage Storage Credentials within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetStorageCredentialsResult> getStorageCredentialsPlain(GetStorageCredentialsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getStorageCredentials:getStorageCredentials", TypeShape.of(GetStorageCredentialsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.applyValue(getTableResult -> getTableResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args) {
        return getTable(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.applyValue(getTableResult -> getTableResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args) {
        return getTablePlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.applyValue(getTableResult -> getTableResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static Output<GetTableResult> getTable(GetTableArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves details of a specific table in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables to retrieve multiple tables in Unity Catalog
     * 
     * ## Example Usage
     * 
     * Read  on a specific table `main.certified.fct_transactions`:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTableArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var fctTransactions = DatabricksFunctions.getTable(GetTableArgs.builder()
     *             .name("main.certified.fct_transactions")
     *             .build());
     * 
     *         var things = new Grants("things", GrantsArgs.builder()
     *             .table(fctTransactions.applyValue(getTableResult -> getTableResult.name()))
     *             .grants(GrantsGrantArgs.builder()
     *                 .principal("sensitive")
     *                 .privileges(                
     *                     "SELECT",
     *                     "MODIFY")
     *                 .build())
     *             .build());
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Grant to manage grants within Unity Catalog.
     * * databricks.getTables to list all tables within a schema in Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTableResult> getTablePlain(GetTablePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTable:getTable", TypeShape.of(GetTableResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args) {
        return getTables(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args) {
        return getTablesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetTablesResult> getTables(GetTablesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of managed or external table full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getViews for retrieving a list of views.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all tables a _things_ databricks.Schema from _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetTablesArgs;
     * import com.pulumi.databricks.Grants;
     * import com.pulumi.databricks.GrantsArgs;
     * import com.pulumi.databricks.inputs.GrantsGrantArgs;
     * import com.pulumi.codegen.internal.KeyedValue;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var things = DatabricksFunctions.getTables(GetTablesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         final var thingsGrants = things.applyValue(getTablesResult -> {
     *             final var resources = new ArrayList<Grants>();
     *             for (var range : KeyedValue.of(getTablesResult.ids()) {
     *                 var resource = new Grants("thingsGrants-" + range.key(), GrantsArgs.builder()
     *                     .table(range.value())
     *                     .grants(GrantsGrantArgs.builder()
     *                         .principal("sensitive")
     *                         .privileges(                        
     *                             "SELECT",
     *                             "MODIFY")
     *                         .build())
     *                     .build());
     * 
     *                 resources.add(resource);
     *             }
     * 
     *             return resources;
     *         });
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetTablesResult> getTablesPlain(GetTablesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getTables:getTables", TypeShape.of(GetTablesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser() {
        return getUser(GetUserArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain() {
        return getUserPlain(GetUserPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args) {
        return getUser(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args) {
        return getUserPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static Output<GetUserResult> getUser(GetUserArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves information about databricks_user.
     * 
     * ## Example Usage
     * 
     * Adding user to administrative group
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetGroupArgs;
     * import com.pulumi.databricks.inputs.GetUserArgs;
     * import com.pulumi.databricks.GroupMember;
     * import com.pulumi.databricks.GroupMemberArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App }{{@code
     *     public static void main(String[] args) }{{@code
     *         Pulumi.run(App::stack);
     *     }}{@code
     * 
     *     public static void stack(Context ctx) }{{@code
     *         final var admins = DatabricksFunctions.getGroup(GetGroupArgs.builder()
     *             .displayName("admins")
     *             .build());
     * 
     *         final var me = DatabricksFunctions.getUser(GetUserArgs.builder()
     *             .userName("me}{@literal @}{@code example.com")
     *             .build());
     * 
     *         var myMemberA = new GroupMember("myMemberA", GroupMemberArgs.builder()
     *             .groupId(admins.applyValue(getGroupResult -> getGroupResult.id()))
     *             .memberId(me.applyValue(getUserResult -> getUserResult.id()))
     *             .build());
     * 
     *     }}{@code
     * }}{@code
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * - End to end workspace management guide.
     * - databricks.getCurrentUser data to retrieve information about databricks.User or databricks_service_principal, that is calling Databricks REST API.
     * - databricks.Group to manage [groups in Databricks Workspace](https://docs.databricks.com/administration-guide/users-groups/groups.html) or [Account Console](https://accounts.cloud.databricks.com/) (for AWS deployments).
     * - databricks.Group data to retrieve information about databricks.Group members, entitlements and instance profiles.
     * - databricks.GroupInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_group.
     * - databricks.GroupMember to attach users and groups as group members.
     * - databricks.Permissions to manage [access control](https://docs.databricks.com/security/access-control/index.html) in Databricks workspace.
     * - databricks.User to [manage users](https://docs.databricks.com/administration-guide/users-groups/users.html), that could be added to databricks.Group within the workspace.
     * - databricks.UserInstanceProfile to attach databricks.InstanceProfile (AWS) to databricks_user.
     * 
     */
    public static CompletableFuture<GetUserResult> getUserPlain(GetUserPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getUser:getUser", TypeShape.of(GetUserResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args) {
        return getViews(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args) {
        return getViewsPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetViewsResult> getViews(GetViewsArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * Retrieves a list of view full names in Unity Catalog, that were created by Pulumi or manually. Use databricks.getTables for retrieving a list of tables.
     * 
     * ## Example Usage
     * 
     * Granting `SELECT` and `MODIFY` to `sensitive` group on all views in a _things_ databricks.Schema from _sandbox_ databricks_catalog.
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetViewsResult> getViewsPlain(GetViewsPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getViews:getViews", TypeShape.of(GetViewsResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args) {
        return getVolume(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args) {
        return getVolumePlain(args, InvokeOptions.Empty);
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     */
    public static Output<GetVolumeResult> getVolume(GetVolumeArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * Retrieves details about databricks.Volume that was created by Pulumi or manually.
     * A volume can be identified by its three-level (fully qualified) name (in the form of: `catalog_name`.`schema_name`.`volume_name`) as input. This can be retrieved programmatically using databricks.getVolumes data source.
     * 
     */
    public static CompletableFuture<GetVolumeResult> getVolumePlain(GetVolumePlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolume:getVolume", TypeShape.of(GetVolumeResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args) {
        return getVolumes(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args) {
        return getVolumesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static Output<GetVolumesResult> getVolumes(GetVolumesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** This data source could be only used with workspace-level provider!
     * 
     * Retrieves a list of databricks.Volume ids (full names), that were created by Pulumi or manually.
     * 
     * ## Example Usage
     * 
     * Listing all volumes in a _things_ databricks.Schema of a  _sandbox_ databricks_catalog:
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetVolumesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var this = DatabricksFunctions.getVolumes(GetVolumesArgs.builder()
     *             .catalogName("sandbox")
     *             .schemaName("things")
     *             .build());
     * 
     *         ctx.export("allVolumes", this_);
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     * ## Related Resources
     * 
     * The following resources are used in the same context:
     * 
     * * databricks.Volume to manage volumes within Unity Catalog.
     * * databricks.Schema to manage schemas within Unity Catalog.
     * * databricks.Catalog to manage catalogs within Unity Catalog.
     * 
     */
    public static CompletableFuture<GetVolumesResult> getVolumesPlain(GetVolumesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getVolumes:getVolumes", TypeShape.of(GetVolumesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones() {
        return getZones(GetZonesArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain() {
        return getZonesPlain(GetZonesPlainArgs.Empty, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args) {
        return getZones(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args) {
        return getZonesPlain(args, InvokeOptions.Empty);
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static Output<GetZonesResult> getZones(GetZonesArgs args, InvokeOptions options) {
        return Deployment.getInstance().invoke("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
    /**
     * &gt; **Note** If you have a fully automated setup with workspaces created by databricks_mws_workspaces, please make sure to add depends_on attribute in order to prevent _default auth: cannot configure default credentials_ errors.
     * 
     * This data source allows you to fetch all available AWS availability zones on your workspace on AWS.
     * 
     * ## Example Usage
     * 
     * &lt;!--Start PulumiCodeChooser --&gt;
     * <pre>
     * {@code
     * package generated_program;
     * 
     * import com.pulumi.Context;
     * import com.pulumi.Pulumi;
     * import com.pulumi.core.Output;
     * import com.pulumi.databricks.DatabricksFunctions;
     * import com.pulumi.databricks.inputs.GetZonesArgs;
     * import java.util.List;
     * import java.util.ArrayList;
     * import java.util.Map;
     * import java.io.File;
     * import java.nio.file.Files;
     * import java.nio.file.Paths;
     * 
     * public class App {
     *     public static void main(String[] args) {
     *         Pulumi.run(App::stack);
     *     }
     * 
     *     public static void stack(Context ctx) {
     *         final var zones = DatabricksFunctions.getZones();
     * 
     *     }
     * }
     * }
     * </pre>
     * &lt;!--End PulumiCodeChooser --&gt;
     * 
     */
    public static CompletableFuture<GetZonesResult> getZonesPlain(GetZonesPlainArgs args, InvokeOptions options) {
        return Deployment.getInstance().invokeAsync("databricks:index/getZones:getZones", TypeShape.of(GetZonesResult.class), args, Utilities.withVersion(options));
    }
}
